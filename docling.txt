Directory structure:
└── ds4sd-docling/
    ├── README.md
    ├── CHANGELOG.md
    ├── CITATION.cff
    ├── CODE_OF_CONDUCT.md
    ├── CONTRIBUTING.md
    ├── Dockerfile
    ├── LICENSE
    ├── MAINTAINERS.md
    ├── mkdocs.yml
    ├── pyproject.toml
    ├── .pre-commit-config.yaml
    ├── docling/
    │   ├── __init__.py
    │   ├── document_converter.py
    │   ├── exceptions.py
    │   ├── py.typed
    │   ├── backend/
    │   │   ├── __init__.py
    │   │   ├── abstract_backend.py
    │   │   ├── asciidoc_backend.py
    │   │   ├── csv_backend.py
    │   │   ├── docling_parse_backend.py
    │   │   ├── docling_parse_v2_backend.py
    │   │   ├── html_backend.py
    │   │   ├── md_backend.py
    │   │   ├── msexcel_backend.py
    │   │   ├── mspowerpoint_backend.py
    │   │   ├── msword_backend.py
    │   │   ├── pdf_backend.py
    │   │   ├── pypdfium2_backend.py
    │   │   ├── json/
    │   │   │   ├── __init__.py
    │   │   │   └── docling_json_backend.py
    │   │   └── xml/
    │   │       ├── __init__.py
    │   │       ├── jats_backend.py
    │   │       └── uspto_backend.py
    │   ├── chunking/
    │   │   └── __init__.py
    │   ├── cli/
    │   │   ├── __init__.py
    │   │   ├── main.py
    │   │   ├── models.py
    │   │   └── tools.py
    │   ├── datamodel/
    │   │   ├── __init__.py
    │   │   ├── base_models.py
    │   │   ├── document.py
    │   │   ├── pipeline_options.py
    │   │   └── settings.py
    │   ├── models/
    │   │   ├── __init__.py
    │   │   ├── base_model.py
    │   │   ├── base_ocr_model.py
    │   │   ├── code_formula_model.py
    │   │   ├── document_picture_classifier.py
    │   │   ├── easyocr_model.py
    │   │   ├── hf_vlm_model.py
    │   │   ├── layout_model.py
    │   │   ├── ocr_mac_model.py
    │   │   ├── page_assemble_model.py
    │   │   ├── page_preprocessing_model.py
    │   │   ├── picture_description_api_model.py
    │   │   ├── picture_description_base_model.py
    │   │   ├── picture_description_vlm_model.py
    │   │   ├── rapid_ocr_model.py
    │   │   ├── readingorder_model.py
    │   │   ├── table_structure_model.py
    │   │   ├── tesseract_ocr_cli_model.py
    │   │   └── tesseract_ocr_model.py
    │   ├── pipeline/
    │   │   ├── __init__.py
    │   │   ├── base_pipeline.py
    │   │   ├── simple_pipeline.py
    │   │   ├── standard_pdf_pipeline.py
    │   │   └── vlm_pipeline.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── accelerator_utils.py
    │       ├── export.py
    │       ├── glm_utils.py
    │       ├── layout_postprocessor.py
    │       ├── model_downloader.py
    │       ├── ocr_utils.py
    │       ├── profiling.py
    │       ├── utils.py
    │       └── visualization.py
    ├── docs/
    │   ├── faq.md
    │   ├── index.md
    │   ├── installation.md
    │   ├── supported_formats.md
    │   ├── usage.md
    │   ├── v2.md
    │   ├── assets/
    │   │   ├── docling_arch.pptx
    │   │   └── docling_ecosystem.pptx
    │   ├── concepts/
    │   │   ├── architecture.md
    │   │   ├── chunking.md
    │   │   ├── docling_document.md
    │   │   └── index.md
    │   ├── examples/
    │   │   ├── backend_csv.ipynb
    │   │   ├── backend_xml_rag.ipynb
    │   │   ├── batch_convert.py
    │   │   ├── custom_convert.py
    │   │   ├── develop_formula_understanding.py
    │   │   ├── develop_picture_enrichment.py
    │   │   ├── export_figures.py
    │   │   ├── export_multimodal.py
    │   │   ├── export_tables.py
    │   │   ├── full_page_ocr.py
    │   │   ├── hybrid_chunking.ipynb
    │   │   ├── index.md
    │   │   ├── inspect_picture_content.py
    │   │   ├── minimal.py
    │   │   ├── minimal_vlm_pipeline.py
    │   │   ├── pictures_description.ipynb
    │   │   ├── pictures_description_api.py
    │   │   ├── rag_azuresearch.ipynb
    │   │   ├── rag_haystack.ipynb
    │   │   ├── rag_langchain.ipynb
    │   │   ├── rag_llamaindex.ipynb
    │   │   ├── rag_weaviate.ipynb
    │   │   ├── rapidocr_with_custom_models.py
    │   │   ├── retrieval_qdrant.ipynb
    │   │   ├── run_md.py
    │   │   ├── run_with_accelerator.py
    │   │   ├── run_with_formats.py
    │   │   ├── tesseract_lang_detection.py
    │   │   └── translate.py
    │   ├── integrations/
    │   │   ├── bee.md
    │   │   ├── cloudera.md
    │   │   ├── crewai.md
    │   │   ├── data_prep_kit.md
    │   │   ├── docetl.md
    │   │   ├── haystack.md
    │   │   ├── index.md
    │   │   ├── instructlab.md
    │   │   ├── kotaemon.md
    │   │   ├── langchain.md
    │   │   ├── llamaindex.md
    │   │   ├── nvidia.md
    │   │   ├── opencontracts.md
    │   │   ├── prodigy.md
    │   │   ├── rhel_ai.md
    │   │   ├── spacy.md
    │   │   ├── txtai.md
    │   │   ├── vectara.md
    │   │   └── .template.md
    │   ├── overrides/
    │   │   └── main.html
    │   ├── reference/
    │   │   ├── cli.md
    │   │   ├── docling_document.md
    │   │   ├── document_converter.md
    │   │   └── pipeline_options.md
    │   └── stylesheets/
    │       └── extra.css
    ├── tests/
    │   ├── __init__.py
    │   ├── test_backend_asciidoc.py
    │   ├── test_backend_csv.py
    │   ├── test_backend_docling_json.py
    │   ├── test_backend_docling_parse.py
    │   ├── test_backend_docling_parse_v2.py
    │   ├── test_backend_html.py
    │   ├── test_backend_jats.py
    │   ├── test_backend_markdown.py
    │   ├── test_backend_msexcel.py
    │   ├── test_backend_msword.py
    │   ├── test_backend_patent_uspto.py
    │   ├── test_backend_pdfium.py
    │   ├── test_backend_pptx.py
    │   ├── test_cli.py
    │   ├── test_code_formula.py
    │   ├── test_data_gen_flag.py
    │   ├── test_document_picture_classifier.py
    │   ├── test_e2e_conversion.py
    │   ├── test_e2e_ocr_conversion.py
    │   ├── test_input_doc.py
    │   ├── test_interfaces.py
    │   ├── test_invalid_input.py
    │   ├── test_legacy_format_transform.py
    │   ├── test_options.py
    │   ├── verify_utils.py
    │   ├── data/
    │   │   ├── asciidoc/
    │   │   │   ├── test_01.asciidoc
    │   │   │   └── test_02.asciidoc
    │   │   ├── csv/
    │   │   │   ├── csv-comma-in-cell.csv
    │   │   │   ├── csv-comma.csv
    │   │   │   ├── csv-inconsistent-header.csv
    │   │   │   ├── csv-pipe.csv
    │   │   │   ├── csv-semicolon.csv
    │   │   │   ├── csv-tab.csv
    │   │   │   ├── csv-too-few-columns.csv
    │   │   │   └── csv-too-many-columns.csv
    │   │   ├── docx/
    │   │   │   ├── lorem_ipsum.docx
    │   │   │   ├── tablecell.docx
    │   │   │   ├── test_emf_docx.docx
    │   │   │   ├── unit_test_headers.docx
    │   │   │   ├── unit_test_headers_numbered.docx
    │   │   │   ├── unit_test_lists.docx
    │   │   │   ├── word_sample.docx
    │   │   │   └── word_tables.docx
    │   │   ├── groundtruth/
    │   │   │   ├── docling_v1/
    │   │   │   │   ├── 2203.01017v2.doctags.txt
    │   │   │   │   ├── 2203.01017v2.json
    │   │   │   │   ├── 2203.01017v2.md
    │   │   │   │   ├── 2203.01017v2.pages.json
    │   │   │   │   ├── 2206.01062.doctags.txt
    │   │   │   │   ├── 2206.01062.json
    │   │   │   │   ├── 2206.01062.md
    │   │   │   │   ├── 2206.01062.pages.json
    │   │   │   │   ├── 2305.03393v1-pg9.doctags.txt
    │   │   │   │   ├── 2305.03393v1-pg9.json
    │   │   │   │   ├── 2305.03393v1-pg9.md
    │   │   │   │   ├── 2305.03393v1-pg9.pages.json
    │   │   │   │   ├── 2305.03393v1.doctags.txt
    │   │   │   │   ├── 2305.03393v1.json
    │   │   │   │   ├── 2305.03393v1.md
    │   │   │   │   ├── 2305.03393v1.pages.json
    │   │   │   │   ├── amt_handbook_sample.doctags.txt
    │   │   │   │   ├── amt_handbook_sample.json
    │   │   │   │   ├── amt_handbook_sample.md
    │   │   │   │   ├── amt_handbook_sample.pages.json
    │   │   │   │   ├── code_and_formula.doctags.txt
    │   │   │   │   ├── code_and_formula.json
    │   │   │   │   ├── code_and_formula.md
    │   │   │   │   ├── code_and_formula.pages.json
    │   │   │   │   ├── picture_classification.doctags.txt
    │   │   │   │   ├── picture_classification.json
    │   │   │   │   ├── picture_classification.md
    │   │   │   │   ├── picture_classification.pages.json
    │   │   │   │   ├── redp5110_sampled.doctags.txt
    │   │   │   │   ├── redp5110_sampled.json
    │   │   │   │   ├── redp5110_sampled.md
    │   │   │   │   ├── redp5110_sampled.pages.json
    │   │   │   │   ├── right_to_left_01.doctags.txt
    │   │   │   │   ├── right_to_left_01.json
    │   │   │   │   ├── right_to_left_01.md
    │   │   │   │   ├── right_to_left_01.pages.json
    │   │   │   │   ├── right_to_left_02.doctags.txt
    │   │   │   │   ├── right_to_left_02.json
    │   │   │   │   ├── right_to_left_02.md
    │   │   │   │   ├── right_to_left_02.pages.json
    │   │   │   │   ├── right_to_left_03.doctags.txt
    │   │   │   │   ├── right_to_left_03.json
    │   │   │   │   ├── right_to_left_03.md
    │   │   │   │   └── right_to_left_03.pages.json
    │   │   │   └── docling_v2/
    │   │   │       ├── 2203.01017v2.doctags.txt
    │   │   │       ├── 2203.01017v2.json
    │   │   │       ├── 2203.01017v2.md
    │   │   │       ├── 2203.01017v2.pages.json
    │   │   │       ├── 2206.01062.doctags.txt
    │   │   │       ├── 2206.01062.json
    │   │   │       ├── 2206.01062.md
    │   │   │       ├── 2206.01062.pages.json
    │   │   │       ├── 2305.03393v1-pg9.doctags.txt
    │   │   │       ├── 2305.03393v1-pg9.json
    │   │   │       ├── 2305.03393v1-pg9.md
    │   │   │       ├── 2305.03393v1-pg9.pages.json
    │   │   │       ├── 2305.03393v1.doctags.txt
    │   │   │       ├── 2305.03393v1.json
    │   │   │       ├── 2305.03393v1.md
    │   │   │       ├── 2305.03393v1.pages.json
    │   │   │       ├── amt_handbook_sample.doctags.txt
    │   │   │       ├── amt_handbook_sample.json
    │   │   │       ├── amt_handbook_sample.md
    │   │   │       ├── amt_handbook_sample.pages.json
    │   │   │       ├── blocks.md.md
    │   │   │       ├── bmj_sample.xml.itxt
    │   │   │       ├── bmj_sample.xml.json
    │   │   │       ├── bmj_sample.xml.md
    │   │   │       ├── code_and_formula.doctags.txt
    │   │   │       ├── code_and_formula.json
    │   │   │       ├── code_and_formula.md
    │   │   │       ├── code_and_formula.pages.json
    │   │   │       ├── csv-comma-in-cell.csv.itxt
    │   │   │       ├── csv-comma-in-cell.csv.json
    │   │   │       ├── csv-comma-in-cell.csv.md
    │   │   │       ├── csv-comma.csv.itxt
    │   │   │       ├── csv-comma.csv.json
    │   │   │       ├── csv-comma.csv.md
    │   │   │       ├── csv-inconsistent-header.csv.itxt
    │   │   │       ├── csv-inconsistent-header.csv.json
    │   │   │       ├── csv-inconsistent-header.csv.md
    │   │   │       ├── csv-pipe.csv.itxt
    │   │   │       ├── csv-pipe.csv.json
    │   │   │       ├── csv-pipe.csv.md
    │   │   │       ├── csv-semicolon.csv.itxt
    │   │   │       ├── csv-semicolon.csv.json
    │   │   │       ├── csv-semicolon.csv.md
    │   │   │       ├── csv-tab.csv.itxt
    │   │   │       ├── csv-tab.csv.json
    │   │   │       ├── csv-tab.csv.md
    │   │   │       ├── csv-too-few-columns.csv.itxt
    │   │   │       ├── csv-too-few-columns.csv.json
    │   │   │       ├── csv-too-few-columns.csv.md
    │   │   │       ├── csv-too-many-columns.csv.itxt
    │   │   │       ├── csv-too-many-columns.csv.json
    │   │   │       ├── csv-too-many-columns.csv.md
    │   │   │       ├── duck.md.md
    │   │   │       ├── elife-56337.xml.itxt
    │   │   │       ├── elife-56337.xml.md
    │   │   │       ├── ending_with_table.md.md
    │   │   │       ├── example_01.html.itxt
    │   │   │       ├── example_01.html.json
    │   │   │       ├── example_01.html.md
    │   │   │       ├── example_02.html.itxt
    │   │   │       ├── example_02.html.json
    │   │   │       ├── example_02.html.md
    │   │   │       ├── example_03.html.itxt
    │   │   │       ├── example_03.html.json
    │   │   │       ├── example_03.html.md
    │   │   │       ├── example_04.html.itxt
    │   │   │       ├── example_04.html.json
    │   │   │       ├── example_04.html.md
    │   │   │       ├── example_05.html.itxt
    │   │   │       ├── example_05.html.json
    │   │   │       ├── example_05.html.md
    │   │   │       ├── example_06.html.itxt
    │   │   │       ├── example_06.html.json
    │   │   │       ├── example_06.html.md
    │   │   │       ├── ipa20180000016.itxt
    │   │   │       ├── ipa20180000016.json
    │   │   │       ├── ipa20180000016.md
    │   │   │       ├── ipa20200022300.itxt
    │   │   │       ├── ipa20200022300.json
    │   │   │       ├── ipa20200022300.md
    │   │   │       ├── lorem_ipsum.docx.itxt
    │   │   │       ├── lorem_ipsum.docx.json
    │   │   │       ├── lorem_ipsum.docx.md
    │   │   │       ├── mixed.md.md
    │   │   │       ├── nested.md.md
    │   │   │       ├── pa20010031492.itxt
    │   │   │       ├── pa20010031492.json
    │   │   │       ├── pa20010031492.md
    │   │   │       ├── pftaps057006474.itxt
    │   │   │       ├── pftaps057006474.json
    │   │   │       ├── pftaps057006474.md
    │   │   │       ├── pg06442728.itxt
    │   │   │       ├── pg06442728.json
    │   │   │       ├── pg06442728.md
    │   │   │       ├── picture_classification.doctags.txt
    │   │   │       ├── picture_classification.json
    │   │   │       ├── picture_classification.md
    │   │   │       ├── picture_classification.pages.json
    │   │   │       ├── pnas_sample.xml.itxt
    │   │   │       ├── pnas_sample.xml.json
    │   │   │       ├── pnas_sample.xml.md
    │   │   │       ├── pntd.0008301.xml.itxt
    │   │   │       ├── pntd.0008301.xml.md
    │   │   │       ├── pone.0234687.xml.itxt
    │   │   │       ├── pone.0234687.xml.md
    │   │   │       ├── powerpoint_sample.pptx.itxt
    │   │   │       ├── powerpoint_sample.pptx.json
    │   │   │       ├── powerpoint_sample.pptx.md
    │   │   │       ├── powerpoint_with_image.pptx.itxt
    │   │   │       ├── powerpoint_with_image.pptx.json
    │   │   │       ├── powerpoint_with_image.pptx.md
    │   │   │       ├── redp5110_sampled.doctags.txt
    │   │   │       ├── redp5110_sampled.json
    │   │   │       ├── redp5110_sampled.md
    │   │   │       ├── redp5110_sampled.pages.json
    │   │   │       ├── right_to_left_01.doctags.txt
    │   │   │       ├── right_to_left_01.json
    │   │   │       ├── right_to_left_01.md
    │   │   │       ├── right_to_left_01.pages.json
    │   │   │       ├── right_to_left_02.doctags.txt
    │   │   │       ├── right_to_left_02.json
    │   │   │       ├── right_to_left_02.md
    │   │   │       ├── right_to_left_02.pages.json
    │   │   │       ├── right_to_left_03.doctags.txt
    │   │   │       ├── right_to_left_03.json
    │   │   │       ├── right_to_left_03.md
    │   │   │       ├── right_to_left_03.pages.json
    │   │   │       ├── tablecell.docx.itxt
    │   │   │       ├── tablecell.docx.json
    │   │   │       ├── tablecell.docx.md
    │   │   │       ├── test-01.xlsx.itxt
    │   │   │       ├── test-01.xlsx.json
    │   │   │       ├── test-01.xlsx.md
    │   │   │       ├── test_01.asciidoc.md
    │   │   │       ├── test_02.asciidoc.md
    │   │   │       ├── test_emf_docx.docx.itxt
    │   │   │       ├── test_emf_docx.docx.json
    │   │   │       ├── test_emf_docx.docx.md
    │   │   │       ├── unit_test_01.html.itxt
    │   │   │       ├── unit_test_01.html.json
    │   │   │       ├── unit_test_01.html.md
    │   │   │       ├── unit_test_headers.docx.itxt
    │   │   │       ├── unit_test_headers.docx.json
    │   │   │       ├── unit_test_headers.docx.md
    │   │   │       ├── unit_test_headers_numbered.docx.itxt
    │   │   │       ├── unit_test_headers_numbered.docx.json
    │   │   │       ├── unit_test_headers_numbered.docx.md
    │   │   │       ├── unit_test_lists.docx.itxt
    │   │   │       ├── unit_test_lists.docx.json
    │   │   │       ├── unit_test_lists.docx.md
    │   │   │       ├── wiki.md.md
    │   │   │       ├── wiki_duck.html.itxt
    │   │   │       ├── wiki_duck.html.json
    │   │   │       ├── wiki_duck.html.md
    │   │   │       ├── word_sample.docx.itxt
    │   │   │       ├── word_sample.docx.json
    │   │   │       ├── word_sample.docx.md
    │   │   │       ├── word_sample.json
    │   │   │       ├── word_sample.md
    │   │   │       ├── word_sample.yaml
    │   │   │       ├── word_tables.docx.html
    │   │   │       ├── word_tables.docx.itxt
    │   │   │       ├── word_tables.docx.json
    │   │   │       └── word_tables.docx.md
    │   │   ├── html/
    │   │   │   ├── example_01.html
    │   │   │   ├── example_02.html
    │   │   │   ├── example_03.html
    │   │   │   ├── example_04.html
    │   │   │   ├── example_05.html
    │   │   │   ├── example_06.html
    │   │   │   ├── unit_test_01.html
    │   │   │   └── wiki_duck.html
    │   │   ├── jats/
    │   │   │   ├── bmj_sample.xml
    │   │   │   ├── elife-56337.nxml
    │   │   │   ├── elife-56337.txt
    │   │   │   ├── elife-56337.xml
    │   │   │   ├── pnas_sample.xml
    │   │   │   ├── pntd.0008301.nxml
    │   │   │   ├── pntd.0008301.txt
    │   │   │   ├── pntd.0008301.xml
    │   │   │   ├── pone.0234687.nxml
    │   │   │   ├── pone.0234687.txt
    │   │   │   └── pone.0234687.xml
    │   │   ├── md/
    │   │   │   ├── blocks.md
    │   │   │   ├── duck.md
    │   │   │   ├── ending_with_table.md
    │   │   │   ├── mixed.md
    │   │   │   ├── nested.md
    │   │   │   └── wiki.md
    │   │   ├── pdf/
    │   │   ├── pptx/
    │   │   │   ├── powerpoint_sample.pptx
    │   │   │   └── powerpoint_with_image.pptx
    │   │   ├── uspto/
    │   │   │   ├── ipa20110039701.xml
    │   │   │   ├── ipa20180000016.xml
    │   │   │   ├── ipa20200022300.xml
    │   │   │   ├── ipg07997973.xml
    │   │   │   ├── ipg08672134.xml
    │   │   │   ├── ipgD0701016.xml
    │   │   │   ├── pa20010031492.xml
    │   │   │   ├── pftaps057006474.txt
    │   │   │   ├── pg06442728.xml
    │   │   │   └── tables_ipa20180000016.xml
    │   │   └── xlsx/
    │   │       └── test-01.xlsx
    │   └── data_scanned/
    │       └── groundtruth/
    │           ├── docling_v1/
    │           │   ├── ocr_test.doctags.txt
    │           │   ├── ocr_test.json
    │           │   ├── ocr_test.md
    │           │   └── ocr_test.pages.json
    │           └── docling_v2/
    │               ├── ocr_test.doctags.txt
    │               ├── ocr_test.json
    │               ├── ocr_test.md
    │               └── ocr_test.pages.json
    └── .github/
        ├── PULL_REQUEST_TEMPLATE.md
        ├── SECURITY.md
        ├── mergify.yml
        ├── ISSUE_TEMPLATE/
        │   ├── bug_report.md
        │   ├── config.yml
        │   ├── feature_request.md
        │   └── question.md
        ├── actions/
        │   └── setup-poetry/
        │       └── action.yml
        ├── scripts/
        │   └── release.sh
        └── workflows/
            ├── cd-docs.yml
            ├── cd.yml
            ├── checks.yml
            ├── ci-docs.yml
            ├── ci.yml
            ├── docs.yml
            └── pypi.yml

================================================
File: README.md
================================================
<p align="center">
  <a href="https://github.com/ds4sd/docling">
    <img loading="lazy" alt="Docling" src="https://github.com/DS4SD/docling/raw/main/docs/assets/docling_processing.png" width="100%"/>
  </a>
</p>

# Docling

<p align="center">
  <a href="https://trendshift.io/repositories/12132" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12132" alt="DS4SD%2Fdocling | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

[![arXiv](https://img.shields.io/badge/arXiv-2408.09869-b31b1b.svg)](https://arxiv.org/abs/2408.09869)
[![Docs](https://img.shields.io/badge/docs-live-brightgreen)](https://ds4sd.github.io/docling/)
[![PyPI version](https://img.shields.io/pypi/v/docling)](https://pypi.org/project/docling/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/docling)](https://pypi.org/project/docling/)
[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://pydantic.dev)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![License MIT](https://img.shields.io/github/license/DS4SD/docling)](https://opensource.org/licenses/MIT)
[![PyPI Downloads](https://static.pepy.tech/badge/docling/month)](https://pepy.tech/projects/docling)

Docling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.

## Features

* 🗂️ Parsing of [multiple document formats][supported_formats] incl. PDF, DOCX, XLSX, HTML, images, and more
* 📑 Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more
* 🧬 Unified, expressive [DoclingDocument][docling_document] representation format
* ↪️ Various [export formats][supported_formats] and options, including Markdown, HTML, and lossless JSON
* 🔒 Local execution capabilities for sensitive data and air-gapped environments
* 🤖 Plug-and-play [integrations][integrations] incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI
* 🔍 Extensive OCR support for scanned PDFs and images
* 💻 Simple and convenient CLI

### Coming soon

* 📝 Metadata extraction, including title, authors, references & language
* 📝 Inclusion of Visual Language Models ([SmolDocling](https://huggingface.co/blog/smolervlm#smoldocling))
* 📝 Chart understanding (Barchart, Piechart, LinePlot, etc)
* 📝 Complex chemistry understanding (Molecular structures)

## Installation

To use Docling, simply install `docling` from your package manager, e.g. pip:
```bash
pip install docling
```

Works on macOS, Linux and Windows environments. Both x86_64 and arm64 architectures.

More [detailed installation instructions](https://ds4sd.github.io/docling/installation/) are available in the docs.

## Getting started

To convert individual documents, use `convert()`, for example:

```python
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"  # document per local path or URL
converter = DocumentConverter()
result = converter.convert(source)
print(result.document.export_to_markdown())  # output: "## Docling Technical Report[...]"
```

More [advanced usage options](https://ds4sd.github.io/docling/usage/) are available in
the docs.

## Documentation

Check out Docling's [documentation](https://ds4sd.github.io/docling/), for details on
installation, usage, concepts, recipes, extensions, and more.

## Examples

Go hands-on with our [examples](https://ds4sd.github.io/docling/examples/),
demonstrating how to address different application use cases with Docling.

## Integrations

To further accelerate your AI application development, check out Docling's native
[integrations](https://ds4sd.github.io/docling/integrations/) with popular frameworks
and tools.

## Get help and support

Please feel free to connect with us using the [discussion section](https://github.com/DS4SD/docling/discussions).

## Technical report

For more details on Docling's inner workings, check out the [Docling Technical Report](https://arxiv.org/abs/2408.09869).

## Contributing

Please read [Contributing to Docling](https://github.com/DS4SD/docling/blob/main/CONTRIBUTING.md) for details.

## References

If you use Docling in your projects, please consider citing the following:

```bib
@techreport{Docling,
  author = {Deep Search Team},
  month = {8},
  title = {Docling Technical Report},
  url = {https://arxiv.org/abs/2408.09869},
  eprint = {2408.09869},
  doi = {10.48550/arXiv.2408.09869},
  version = {1.0.0},
  year = {2024}
}
```

## License

The Docling codebase is under MIT license.
For individual model usage, please refer to the model licenses found in the original packages.

## IBM ❤️ Open Source AI

Docling has been brought to you by IBM.

[supported_formats]: https://ds4sd.github.io/docling/supported_formats/
[docling_document]: https://ds4sd.github.io/docling/concepts/docling_document/
[integrations]: https://ds4sd.github.io/docling/integrations/


================================================
File: CITATION.cff
================================================
# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: Docling
message: 'If you use Docling, please consider citing as below.'
type: software
authors:
  - name: Docling Team
identifiers:
  - type: url
    value: 'https://arxiv.org/abs/2408.09869'
    description: 'arXiv:2408.09869'
repository-code: 'https://github.com/DS4SD/docling'
license: MIT


================================================
File: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement using
[deepsearch-core@zurich.ibm.com](mailto:deepsearch-core@zurich.ibm.com).

All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
[https://www.contributor-covenant.org/version/2/0/code_of_conduct.html](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html).

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

Homepage: [https://www.contributor-covenant.org](https://www.contributor-covenant.org)

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq](https://www.contributor-covenant.org/faq). Translations are available at
[https://www.contributor-covenant.org/translations](https://www.contributor-covenant.org/translations).

================================================
File: CONTRIBUTING.md
================================================
## Contributing In General
Our project welcomes external contributions. If you have an itch, please feel
free to scratch it.

To contribute code or documentation, please submit a [pull request](https://github.com/DS4SD/docling/pulls).

A good way to familiarize yourself with the codebase and contribution process is
to look for and tackle low-hanging fruit in the [issue tracker](https://github.com/DS4SD/docling/issues).
Before embarking on a more ambitious contribution, please quickly [get in touch](#communication) with us.

For general questions or support requests, please refer to the [discussion section](https://github.com/DS4SD/docling/discussions).

**Note: We appreciate your effort and want to avoid situations where a contribution
requires extensive rework (by you or by us), sits in the backlog for a long time, or
cannot be accepted at all!**

### Proposing New Features

If you would like to implement a new feature, please [raise an issue](https://github.com/DS4SD/docling/issues)
before sending a pull request so the feature can be discussed. This is to avoid
you spending valuable time working on a feature that the project developers
are not interested in accepting into the codebase.

### Fixing Bugs

If you would like to fix a bug, please [raise an issue](https://github.com/DS4SD/docling/issues) before sending a
pull request so it can be tracked.

### Merge Approval

The project maintainers use LGTM (Looks Good To Me) in comments on the code
review to indicate acceptance. A change requires LGTMs from two of the
maintainers of each component affected.

For a list of the maintainers, see the [MAINTAINERS.md](MAINTAINERS.md) page.


## Legal

Each source file must include a license header for the MIT
Software. Using the SPDX format is the simplest approach,
e.g.

```
/*
Copyright IBM Inc. All rights reserved.

SPDX-License-Identifier: MIT
*/
```

We have tried to make it as easy as possible to make contributions. This
applies to how we handle the legal aspects of contribution. We use the
same approach - the [Developer's Certificate of Origin 1.1 (DCO)](https://github.com/hyperledger/fabric/blob/master/docs/source/DCO1.1.txt) - that the Linux® Kernel [community](https://elinux.org/Developer_Certificate_Of_Origin)
uses to manage code contributions.

We simply ask that when submitting a patch for review, the developer
must include a sign-off statement in the commit message.

Here is an example Signed-off-by line, which indicates that the
submitter accepts the DCO:

```
Signed-off-by: John Doe <john.doe@example.com>
```

You can include this automatically when you commit a change to your
local git repository using the following command:

```
git commit -s
```

### New dependencies

This project strictly adheres to using dependencies that are compatible with the MIT license to ensure maximum flexibility and permissiveness in its usage and distribution. As a result, dependencies licensed under restrictive terms such as GPL, LGPL, AGPL, or similar are explicitly excluded. These licenses impose additional requirements and limitations that are incompatible with the MIT license's minimal restrictions, potentially affecting derivative works and redistribution. By maintaining this policy, the project ensures simplicity and freedom for both developers and users, avoiding conflicts with stricter copyleft provisions.


## Communication

Please feel free to connect with us using the [discussion section](https://github.com/DS4SD/docling/discussions).



## Developing

### Usage of Poetry

We use Poetry to manage dependencies.

#### Installation

To install Poetry, follow the documentation here: https://python-poetry.org/docs/master/#installing-with-the-official-installer

1. Install Poetry globally on your machine:
    ```bash
    curl -sSL https://install.python-poetry.org | python3 -
    ```
    The installation script will print the installation bin folder `POETRY_BIN` which you need in the next steps.

2. Make sure Poetry is in your `$PATH`:
    - for `zsh`:
        ```sh
        echo 'export PATH="POETRY_BIN:$PATH"' >> ~/.zshrc
        ```
    - for `bash`:
        ```sh
        echo 'export PATH="POETRY_BIN:$PATH"' >> ~/.bashrc
        ```

3. The official guidelines linked above include useful details on configuring autocomplete for most shell environments, e.g., Bash and Zsh.

#### Create a Virtual Environment and Install Dependencies

To activate the Virtual Environment, run:

```bash
poetry shell
```

This will spawn a shell with the Virtual Environment activated. If the Virtual Environment doesn't exist, Poetry will create one for you. Then, to install dependencies, run:

```bash
poetry install
```

**(Advanced) Use a Specific Python Version**

If you need to work with a specific (older) version of Python, run:

```bash
poetry env use $(which python3.8)
```

This creates a Virtual Environment with Python 3.8. For other versions, replace `$(which python3.8)` with the path to the interpreter (e.g., `/usr/bin/python3.8`) or use `$(which pythonX.Y)`.

#### Add a New Dependency

```bash
poetry add NAME
```

## Coding Style Guidelines

We use the following tools to enforce code style:

- iSort, to sort imports
- Black, to format code

We run a series of checks on the codebase on every commit using `pre-commit`. To install the hooks, run:

```bash
pre-commit install
```

To run the checks on-demand, run:

```bash
pre-commit run --all-files
```

Note: Checks like `Black` and `isort` will "fail" if they modify files. This is because `pre-commit` doesn't like to see files modified by its hooks. In these cases, `git add` the modified files and `git commit` again.

## Documentation

We use [MkDocs](https://www.mkdocs.org/) to write documentation.

To run the documentation server, run:

```bash
mkdocs serve
```

The server will be available at [http://localhost:8000](http://localhost:8000).

### Pushing Documentation to GitHub Pages

Run the following:

```bash
mkdocs gh-deploy
```

================================================
File: Dockerfile
================================================
FROM python:3.11-slim-bookworm

ENV GIT_SSH_COMMAND="ssh -o StrictHostKeyChecking=no"

RUN apt-get update \
    && apt-get install -y libgl1 libglib2.0-0 curl wget git procps \
    && apt-get clean

# This will install torch with *only* cpu support
# Remove the --extra-index-url part if you want to install all the gpu requirements
# For more details in the different torch distribution visit https://pytorch.org/.
RUN pip install --no-cache-dir docling --extra-index-url https://download.pytorch.org/whl/cpu

ENV HF_HOME=/tmp/
ENV TORCH_HOME=/tmp/

COPY docs/examples/minimal.py /root/minimal.py

RUN docling-tools models download

# On container environments, always set a thread budget to avoid undesired thread congestion.
ENV OMP_NUM_THREADS=4

# On container shell:
# > cd /root/
# > python minimal.py

# Running as `docker run -e DOCLING_ARTIFACTS_PATH=/root/.cache/docling/models` will use the
# model weights included in the container image.


================================================
File: LICENSE
================================================
MIT License

Copyright (c) 2024 International Business Machines

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
File: MAINTAINERS.md
================================================
# MAINTAINERS

- Christoph Auer - [@cau-git](https://github.com/cau-git)
- Michele Dolfi - [@dolfim-ibm](https://github.com/dolfim-ibm)
- Maxim Lysak - [@maxmnemonic](https://github.com/maxmnemonic)
- Nikos Livathinos - [@nikos-livathinos](https://github.com/nikos-livathinos)
- Ahmed Nassar - [@nassarofficial](https://github.com/nassarofficial)
- Panos Vagenas - [@vagenas](https://github.com/vagenas)
- Peter Staar - [@PeterStaar-IBM](https://github.com/PeterStaar-IBM)

Maintainers can be contacted at [deepsearch-core@zurich.ibm.com](mailto:deepsearch-core@zurich.ibm.com).


================================================
File: mkdocs.yml
================================================
site_name: Docling
site_url: https://ds4sd.github.io/docling/
repo_name: DS4SD/docling
repo_url: https://github.com/DS4SD/docling

theme:
  name: material
  custom_dir: docs/overrides
  palette:
    # Palette toggle for automatic mode
    - media: "(prefers-color-scheme)"
      scheme: default
      primary: black
      toggle:
        icon: material/brightness-auto
        name: Switch to light mode

    # Palette toggle for light mode
    - media: "(prefers-color-scheme: light)"
      scheme: default
      primary: black
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode

    # Palette toggle for dark mode
    - media: "(prefers-color-scheme: dark)"
      scheme: slate
      primary: black
      toggle:
        icon: material/brightness-4
        name: Switch to system preference

  logo: assets/logo.png
  favicon: assets/logo.png
  features:
    - content.tabs.link
    - content.code.annotate
    - content.code.copy
    - announce.dismiss
    - navigation.footer
    - navigation.tabs
    - navigation.indexes  # <= if set, each "section" can have its own page, if index.md is used
    - navigation.instant
    - navigation.instant.prefetch
    # - navigation.instant.preview
    - navigation.instant.progress
    - navigation.path
    - navigation.sections  # <=
    - navigation.top
    - navigation.tracking
    - search.suggest
    - toc.follow
nav:
  - Home:
    - "Docling": index.md
    - Installation: installation.md
    - Usage: usage.md
    - Supported formats: supported_formats.md
    - FAQ: faq.md
    - Docling v2: v2.md
  - Concepts:
    - Concepts: concepts/index.md
    - Architecture: concepts/architecture.md
    - Docling Document: concepts/docling_document.md
    - Chunking: concepts/chunking.md
  - Examples:
    - Examples: examples/index.md
    - 🔀 Conversion:
      - "Simple conversion": examples/minimal.py
      - "Custom conversion": examples/custom_convert.py
      - "Batch conversion": examples/batch_convert.py
      - "Multi-format conversion": examples/run_with_formats.py
      - "Figure export": examples/export_figures.py
      - "Figure enrichment": examples/develop_picture_enrichment.py
      - "Table export": examples/export_tables.py
      - "Multimodal export": examples/export_multimodal.py
      - "Annotate picture with local vlm": examples/pictures_description.ipynb
      - "Annotate picture with remote vlm": examples/pictures_description_api.py
      - "Force full page OCR": examples/full_page_ocr.py
      - "Automatic OCR language detection with tesseract": examples/tesseract_lang_detection.py
      - "RapidOCR with custom OCR models": examples/rapidocr_with_custom_models.py
      - "Accelerator options": examples/run_with_accelerator.py
      - "Simple translation": examples/translate.py
      - examples/backend_csv.ipynb
      - examples/backend_xml_rag.ipynb
    - ✂️ Chunking:
      - examples/hybrid_chunking.ipynb
    - 🤖 RAG with AI dev frameworks:
      - examples/rag_haystack.ipynb
      - examples/rag_langchain.ipynb
      - examples/rag_llamaindex.ipynb
    - 🗂️ More examples:
      - examples/rag_weaviate.ipynb
      - RAG with Granite [↗]: https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/Granite_Docling_RAG.ipynb
      - examples/rag_azuresearch.ipynb
      - examples/retrieval_qdrant.ipynb
  - Integrations:
    - Integrations: integrations/index.md
    - 🤖 Agentic / AI dev frameworks:
      - "Bee Agent Framework": integrations/bee.md
      - "Crew AI": integrations/crewai.md
      - "Haystack": integrations/haystack.md
      - "LangChain": integrations/langchain.md
      - "LlamaIndex": integrations/llamaindex.md
      - "txtai": integrations/txtai.md
    - ⭐️ Featured:
      - "Data Prep Kit": integrations/data_prep_kit.md
      - "InstructLab": integrations/instructlab.md
      - "NVIDIA": integrations/nvidia.md
      - "Prodigy": integrations/prodigy.md
      - "RHEL AI": integrations/rhel_ai.md
      - "spaCy": integrations/spacy.md
    - 🗂️ More integrations:
      - "Cloudera": integrations/cloudera.md
      - "DocETL": integrations/docetl.md
      - "Kotaemon": integrations/kotaemon.md
      - "OpenContracts": integrations/opencontracts.md
      - "Vectara": integrations/vectara.md
  - Reference:
    - Python API:
      - Document Converter: reference/document_converter.md
      - Pipeline options: reference/pipeline_options.md
      - Docling Document: reference/docling_document.md
    - CLI:
      - CLI reference: reference/cli.md

markdown_extensions:
  - pymdownx.superfences
  - pymdownx.tabbed:
      alternate_style: true
      slugify: !!python/object/apply:pymdownx.slugs.slugify
        kwds:
          case: lower
  - admonition
  - pymdownx.details
  - attr_list
  - mkdocs-click
plugins:
  - search
  - mkdocs-jupyter
  - mkdocstrings:
      default_handler: python
      options:
        extensions:
          - griffe_pydantic:
              schema: true
        preload_modules:
          - docling
          - docling_core

extra_css:
  - stylesheets/extra.css


================================================
File: pyproject.toml
================================================
[tool.poetry]
name = "docling"
version = "2.25.0"  # DO NOT EDIT, updated automatically
description = "SDK and CLI for parsing PDF, DOCX, HTML, and more, to a unified document representation for powering downstream workflows such as gen AI applications."
authors = ["Christoph Auer <cau@zurich.ibm.com>", "Michele Dolfi <dol@zurich.ibm.com>", "Maxim Lysak <mly@zurich.ibm.com>", "Nikos Livathinos <nli@zurich.ibm.com>", "Ahmed Nassar <ahn@zurich.ibm.com>", "Panos Vagenas <pva@zurich.ibm.com>", "Peter Staar <taa@zurich.ibm.com>"]
license = "MIT"
readme = "README.md"
repository = "https://github.com/DS4SD/docling"
homepage = "https://github.com/DS4SD/docling"
keywords= ["docling", "convert", "document", "pdf", "docx", "html", "markdown", "layout model", "segmentation", "table structure", "table former"]
 classifiers = [
     "License :: OSI Approved :: MIT License",
     "Operating System :: MacOS :: MacOS X",
     "Operating System :: POSIX :: Linux",
     "Development Status :: 5 - Production/Stable",
     "Intended Audience :: Developers",
     "Intended Audience :: Science/Research",
     "Topic :: Scientific/Engineering :: Artificial Intelligence",
     "Programming Language :: Python :: 3"
 ]
packages = [{include = "docling"}]

[tool.poetry.dependencies]
######################
# actual dependencies:
######################
python = "^3.9"
pydantic = "^2.0.0"
docling-core = {extras = ["chunking"], version = "^2.19.0"}
docling-ibm-models = "^3.4.0"
docling-parse = "^3.3.0"
filetype = "^1.2.0"
pypdfium2 = "^4.30.0"
pydantic-settings = "^2.3.0"
huggingface_hub = ">=0.23,<1"
requests = "^2.32.2"
easyocr = "^1.7"
tesserocr = { version = "^2.7.1", optional = true }
certifi = ">=2024.7.4"
rtree = "^1.3.0"
scipy = [
  { version = "^1.6.0", markers = "python_version >= '3.10'" },
  { version = ">=1.6.0,<1.14.0", markers = "python_version < '3.10'" }
]
typer = "^0.12.5"
python-docx = "^1.1.2"
python-pptx = "^1.0.2"
beautifulsoup4 = "^4.12.3"
pandas = "^2.1.4"
marko = "^2.1.2"
openpyxl = "^3.1.5"
lxml = ">=4.0.0,<6.0.0"
ocrmac = { version = "^1.0.0", markers = "sys_platform == 'darwin'", optional = true }
rapidocr-onnxruntime = { version = "^1.4.0", optional = true, markers = "python_version < '3.13'" }
onnxruntime = [
  # 1.19.2 is the last version with python3.9 support,
  # see https://github.com/microsoft/onnxruntime/releases/tag/v1.20.0
  { version = ">=1.7.0,<1.20.0", optional = true, markers = "python_version < '3.10'" },
  { version = "^1.7.0", optional = true, markers = "python_version >= '3.10'" }
]

transformers = [
  {markers = "sys_platform != 'darwin' or platform_machine != 'x86_64'", version = "^4.46.0", optional = true },
  {markers = "sys_platform == 'darwin' and platform_machine == 'x86_64'", version = "~4.42.0", optional = true }
]
accelerate = [
  {markers = "sys_platform != 'darwin' or platform_machine != 'x86_64'", version = "^1.2.1", optional = true },
]
pillow = ">=10.0.0,<12.0.0"
tqdm = "^4.65.0"

[tool.poetry.group.dev.dependencies]
black = {extras = ["jupyter"], version = "^24.4.2"}
pytest = "^7.2.2"
pre-commit = "^3.7.1"
mypy = "^1.10.1"
isort = "^5.10.1"
python-semantic-release = "^7.32.2"
flake8 = "^6.0.0"
pyproject-flake8 = "^6.0.0"
pytest-xdist = "^3.3.1"
types-requests = "^2.31.0.2"
flake8-pyproject = "^1.2.3"
pylint = "^2.17.5"
pandas-stubs = "^2.1.4.231227"
ipykernel = "^6.29.5"
ipywidgets = "^8.1.5"
nbqa = "^1.9.0"
types-openpyxl = "^3.1.5.20241114"
types-tqdm = "^4.67.0.20241221"

[tool.poetry.group.docs.dependencies]
mkdocs-material = "^9.5.40"
mkdocs-jupyter = "^0.25.0"
mkdocs-click = "^0.8.1"
mkdocstrings = {extras = ["python"], version = "^0.27.0"}
griffe-pydantic = "^1.1.0"

[tool.poetry.group.examples.dependencies]
datasets = "^2.21.0"
python-dotenv = "^1.0.1"
langchain-huggingface = "^0.0.3"
langchain-milvus = "^0.1.4"
langchain-text-splitters = "^0.2.4"

[tool.poetry.group.constraints]
optional = true

[tool.poetry.group.constraints.dependencies]
numpy = [
    { version = ">=1.24.4,<3.0.0", markers = 'python_version >= "3.10"' },
    { version = ">=1.24.4,<2.1.0", markers = 'python_version < "3.10"' },
]

[tool.poetry.group.mac_intel]
optional = true

[tool.poetry.group.mac_intel.dependencies]
torch = [
  {markers = "sys_platform != 'darwin' or platform_machine != 'x86_64'", version = "^2.2.2"},
  {markers = "sys_platform == 'darwin' and platform_machine == 'x86_64'", version = "~2.2.2"}
]
torchvision = [
  {markers = "sys_platform != 'darwin' or platform_machine != 'x86_64'", version = "^0"},
  {markers = "sys_platform == 'darwin' and platform_machine == 'x86_64'", version = "~0.17.2"}
]

[tool.poetry.extras]
tesserocr = ["tesserocr"]
ocrmac = ["ocrmac"]
vlm = ["transformers", "accelerate"]
rapidocr = ["rapidocr-onnxruntime", "onnxruntime"]

[tool.poetry.scripts]
docling = "docling.cli.main:app"
docling-tools = "docling.cli.tools:app"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.black]
line-length = 88
target-version = ["py39"]
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 88
py_version=39

[tool.mypy]
pretty = true
# strict = true
no_implicit_optional = true
plugins = "pydantic.mypy"
python_version = "3.10"

[[tool.mypy.overrides]]
module = [
    "docling_parse.*",
    "pypdfium2.*",
    "networkx.*",
    "scipy.*",
    "filetype.*",
    "tesserocr.*",
    "docling_ibm_models.*",
    "easyocr.*",
    "ocrmac.*",
    "lxml.*",
    "huggingface_hub.*",
    "transformers.*",
]
ignore_missing_imports = true

[tool.flake8]
max-line-length = 88
extend-ignore = ["E203", "E501"]

[tool.semantic_release]
# for default values check:
# https://github.com/python-semantic-release/python-semantic-release/blob/v7.32.2/semantic_release/defaults.cfg

version_source = "tag_only"
branch = "main"

# configure types which should trigger minor and patch version bumps respectively
# (note that they must be a subset of the configured allowed types):
parser_angular_allowed_types = "build,chore,ci,docs,feat,fix,perf,style,refactor,test"
parser_angular_minor_types = "feat"
parser_angular_patch_types = "fix,perf"


================================================
File: .pre-commit-config.yaml
================================================
fail_fast: true
repos:
  - repo: local
    hooks:
      - id: black
        name: Black
        entry: poetry run black docling docs/examples tests
        pass_filenames: false
        language: system
        files: '\.py$'
      - id: isort
        name: isort
        entry: poetry run isort docling docs/examples tests
        pass_filenames: false
        language: system
        files: '\.py$'
#      - id: flake8
#        name: flake8
#        entry: poetry run flake8 docling
#        pass_filenames: false
#        language: system
#        files: '\.py$'
      - id: mypy
        name: MyPy
        entry: poetry run mypy docling
        pass_filenames: false
        language: system
        files: '\.py$'
      - id: nbqa_black
        name: nbQA Black
        entry: poetry run nbqa black docs/examples
        pass_filenames: false
        language: system
        files: '\.ipynb$'
      - id: nbqa_isort
        name: nbQA isort
        entry: poetry run nbqa isort docs/examples
        pass_filenames: false
        language: system
        files: '\.ipynb$'
      - id: poetry
        name: Poetry check
        entry: poetry check --lock
        pass_filenames: false
        language: system


================================================
File: docling/document_converter.py
================================================
import logging
import math
import sys
import time
from functools import partial
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Optional, Tuple, Type, Union

from pydantic import BaseModel, ConfigDict, model_validator, validate_call

from docling.backend.abstract_backend import AbstractDocumentBackend
from docling.backend.asciidoc_backend import AsciiDocBackend
from docling.backend.csv_backend import CsvDocumentBackend
from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend
from docling.backend.html_backend import HTMLDocumentBackend
from docling.backend.json.docling_json_backend import DoclingJSONBackend
from docling.backend.md_backend import MarkdownDocumentBackend
from docling.backend.msexcel_backend import MsExcelDocumentBackend
from docling.backend.mspowerpoint_backend import MsPowerpointDocumentBackend
from docling.backend.msword_backend import MsWordDocumentBackend
from docling.backend.xml.jats_backend import JatsDocumentBackend
from docling.backend.xml.uspto_backend import PatentUsptoDocumentBackend
from docling.datamodel.base_models import (
    ConversionStatus,
    DoclingComponentType,
    DocumentStream,
    ErrorItem,
    InputFormat,
)
from docling.datamodel.document import (
    ConversionResult,
    InputDocument,
    _DocumentConversionInput,
)
from docling.datamodel.pipeline_options import PipelineOptions
from docling.datamodel.settings import (
    DEFAULT_PAGE_RANGE,
    DocumentLimits,
    PageRange,
    settings,
)
from docling.exceptions import ConversionError
from docling.pipeline.base_pipeline import BasePipeline
from docling.pipeline.simple_pipeline import SimplePipeline
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline
from docling.utils.utils import chunkify

_log = logging.getLogger(__name__)


class FormatOption(BaseModel):
    pipeline_cls: Type[BasePipeline]
    pipeline_options: Optional[PipelineOptions] = None
    backend: Type[AbstractDocumentBackend]

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def set_optional_field_default(self) -> "FormatOption":
        if self.pipeline_options is None:
            self.pipeline_options = self.pipeline_cls.get_default_options()
        return self


class CsvFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = CsvDocumentBackend


class ExcelFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = MsExcelDocumentBackend


class WordFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = MsWordDocumentBackend


class PowerpointFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = MsPowerpointDocumentBackend


class MarkdownFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = MarkdownDocumentBackend


class AsciiDocFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = AsciiDocBackend


class HTMLFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = HTMLDocumentBackend


class PatentUsptoFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[PatentUsptoDocumentBackend] = PatentUsptoDocumentBackend


class XMLJatsFormatOption(FormatOption):
    pipeline_cls: Type = SimplePipeline
    backend: Type[AbstractDocumentBackend] = JatsDocumentBackend


class ImageFormatOption(FormatOption):
    pipeline_cls: Type = StandardPdfPipeline
    backend: Type[AbstractDocumentBackend] = DoclingParseV2DocumentBackend


class PdfFormatOption(FormatOption):
    pipeline_cls: Type = StandardPdfPipeline
    backend: Type[AbstractDocumentBackend] = DoclingParseV2DocumentBackend


def _get_default_option(format: InputFormat) -> FormatOption:
    format_to_default_options = {
        InputFormat.CSV: FormatOption(
            pipeline_cls=SimplePipeline, backend=CsvDocumentBackend
        ),
        InputFormat.XLSX: FormatOption(
            pipeline_cls=SimplePipeline, backend=MsExcelDocumentBackend
        ),
        InputFormat.DOCX: FormatOption(
            pipeline_cls=SimplePipeline, backend=MsWordDocumentBackend
        ),
        InputFormat.PPTX: FormatOption(
            pipeline_cls=SimplePipeline, backend=MsPowerpointDocumentBackend
        ),
        InputFormat.MD: FormatOption(
            pipeline_cls=SimplePipeline, backend=MarkdownDocumentBackend
        ),
        InputFormat.ASCIIDOC: FormatOption(
            pipeline_cls=SimplePipeline, backend=AsciiDocBackend
        ),
        InputFormat.HTML: FormatOption(
            pipeline_cls=SimplePipeline, backend=HTMLDocumentBackend
        ),
        InputFormat.XML_USPTO: FormatOption(
            pipeline_cls=SimplePipeline, backend=PatentUsptoDocumentBackend
        ),
        InputFormat.XML_JATS: FormatOption(
            pipeline_cls=SimplePipeline, backend=JatsDocumentBackend
        ),
        InputFormat.IMAGE: FormatOption(
            pipeline_cls=StandardPdfPipeline, backend=DoclingParseV2DocumentBackend
        ),
        InputFormat.PDF: FormatOption(
            pipeline_cls=StandardPdfPipeline, backend=DoclingParseV2DocumentBackend
        ),
        InputFormat.JSON_DOCLING: FormatOption(
            pipeline_cls=SimplePipeline, backend=DoclingJSONBackend
        ),
    }
    if (options := format_to_default_options.get(format)) is not None:
        return options
    else:
        raise RuntimeError(f"No default options configured for {format}")


class DocumentConverter:
    _default_download_filename = "file"

    def __init__(
        self,
        allowed_formats: Optional[List[InputFormat]] = None,
        format_options: Optional[Dict[InputFormat, FormatOption]] = None,
    ):
        self.allowed_formats = (
            allowed_formats if allowed_formats is not None else [e for e in InputFormat]
        )
        self.format_to_options = {
            format: (
                _get_default_option(format=format)
                if (custom_option := (format_options or {}).get(format)) is None
                else custom_option
            )
            for format in self.allowed_formats
        }
        self.initialized_pipelines: Dict[Type[BasePipeline], BasePipeline] = {}

    def initialize_pipeline(self, format: InputFormat):
        """Initialize the conversion pipeline for the selected format."""
        pipeline = self._get_pipeline(doc_format=format)
        if pipeline is None:
            raise ConversionError(
                f"No pipeline could be initialized for format {format}"
            )

    @validate_call(config=ConfigDict(strict=True))
    def convert(
        self,
        source: Union[Path, str, DocumentStream],  # TODO review naming
        headers: Optional[Dict[str, str]] = None,
        raises_on_error: bool = True,
        max_num_pages: int = sys.maxsize,
        max_file_size: int = sys.maxsize,
        page_range: PageRange = DEFAULT_PAGE_RANGE,
    ) -> ConversionResult:
        all_res = self.convert_all(
            source=[source],
            raises_on_error=raises_on_error,
            max_num_pages=max_num_pages,
            max_file_size=max_file_size,
            headers=headers,
            page_range=page_range,
        )
        return next(all_res)

    @validate_call(config=ConfigDict(strict=True))
    def convert_all(
        self,
        source: Iterable[Union[Path, str, DocumentStream]],  # TODO review naming
        headers: Optional[Dict[str, str]] = None,
        raises_on_error: bool = True,  # True: raises on first conversion error; False: does not raise on conv error
        max_num_pages: int = sys.maxsize,
        max_file_size: int = sys.maxsize,
        page_range: PageRange = DEFAULT_PAGE_RANGE,
    ) -> Iterator[ConversionResult]:
        limits = DocumentLimits(
            max_num_pages=max_num_pages,
            max_file_size=max_file_size,
            page_range=page_range,
        )
        conv_input = _DocumentConversionInput(
            path_or_stream_iterator=source, limits=limits, headers=headers
        )
        conv_res_iter = self._convert(conv_input, raises_on_error=raises_on_error)

        had_result = False
        for conv_res in conv_res_iter:
            had_result = True
            if raises_on_error and conv_res.status not in {
                ConversionStatus.SUCCESS,
                ConversionStatus.PARTIAL_SUCCESS,
            }:
                raise ConversionError(
                    f"Conversion failed for: {conv_res.input.file} with status: {conv_res.status}"
                )
            else:
                yield conv_res

        if not had_result and raises_on_error:
            raise ConversionError(
                f"Conversion failed because the provided file has no recognizable format or it wasn't in the list of allowed formats."
            )

    def _convert(
        self, conv_input: _DocumentConversionInput, raises_on_error: bool
    ) -> Iterator[ConversionResult]:
        start_time = time.monotonic()

        for input_batch in chunkify(
            conv_input.docs(self.format_to_options),
            settings.perf.doc_batch_size,  # pass format_options
        ):
            _log.info(f"Going to convert document batch...")

            # parallel processing only within input_batch
            # with ThreadPoolExecutor(
            #    max_workers=settings.perf.doc_batch_concurrency
            # ) as pool:
            #   yield from pool.map(self.process_document, input_batch)
            # Note: PDF backends are not thread-safe, thread pool usage was disabled.

            for item in map(
                partial(self._process_document, raises_on_error=raises_on_error),
                input_batch,
            ):
                elapsed = time.monotonic() - start_time
                start_time = time.monotonic()
                _log.info(
                    f"Finished converting document {item.input.file.name} in {elapsed:.2f} sec."
                )
                yield item

    def _get_pipeline(self, doc_format: InputFormat) -> Optional[BasePipeline]:
        fopt = self.format_to_options.get(doc_format)

        if fopt is None:
            return None
        else:
            pipeline_class = fopt.pipeline_cls
            pipeline_options = fopt.pipeline_options

        if pipeline_options is None:
            return None
        # TODO this will ignore if different options have been defined for the same pipeline class.
        if (
            pipeline_class not in self.initialized_pipelines
            or self.initialized_pipelines[pipeline_class].pipeline_options
            != pipeline_options
        ):
            self.initialized_pipelines[pipeline_class] = pipeline_class(
                pipeline_options=pipeline_options
            )
        return self.initialized_pipelines[pipeline_class]

    def _process_document(
        self, in_doc: InputDocument, raises_on_error: bool
    ) -> ConversionResult:

        valid = (
            self.allowed_formats is not None and in_doc.format in self.allowed_formats
        )
        if valid:
            conv_res = self._execute_pipeline(in_doc, raises_on_error=raises_on_error)
        else:
            error_message = f"File format not allowed: {in_doc.file}"
            if raises_on_error:
                raise ConversionError(error_message)
            else:
                error_item = ErrorItem(
                    component_type=DoclingComponentType.USER_INPUT,
                    module_name="",
                    error_message=error_message,
                )
                conv_res = ConversionResult(
                    input=in_doc, status=ConversionStatus.SKIPPED, errors=[error_item]
                )

        return conv_res

    def _execute_pipeline(
        self, in_doc: InputDocument, raises_on_error: bool
    ) -> ConversionResult:
        if in_doc.valid:
            pipeline = self._get_pipeline(in_doc.format)
            if pipeline is not None:
                conv_res = pipeline.execute(in_doc, raises_on_error=raises_on_error)
            else:
                if raises_on_error:
                    raise ConversionError(
                        f"No pipeline could be initialized for {in_doc.file}."
                    )
                else:
                    conv_res = ConversionResult(
                        input=in_doc,
                        status=ConversionStatus.FAILURE,
                    )
        else:
            if raises_on_error:
                raise ConversionError(f"Input document {in_doc.file} is not valid.")

            else:
                # invalid doc or not of desired format
                conv_res = ConversionResult(
                    input=in_doc,
                    status=ConversionStatus.FAILURE,
                )
                # TODO add error log why it failed.

        return conv_res


================================================
File: docling/exceptions.py
================================================
class BaseError(RuntimeError):
    pass


class ConversionError(BaseError):
    pass


class OperationNotAllowed(BaseError):
    pass


================================================
File: docling/py.typed
================================================



================================================
File: docling/backend/abstract_backend.py
================================================
from abc import ABC, abstractmethod
from io import BytesIO
from pathlib import Path
from typing import TYPE_CHECKING, Set, Union

from docling_core.types.doc import DoclingDocument

if TYPE_CHECKING:
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.document import InputDocument


class AbstractDocumentBackend(ABC):
    @abstractmethod
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        self.file = in_doc.file
        self.path_or_stream = path_or_stream
        self.document_hash = in_doc.document_hash
        self.input_format = in_doc.format

    @abstractmethod
    def is_valid(self) -> bool:
        pass

    @classmethod
    @abstractmethod
    def supports_pagination(cls) -> bool:
        pass

    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()

        self.path_or_stream = None

    @classmethod
    @abstractmethod
    def supported_formats(cls) -> Set["InputFormat"]:
        pass


class PaginatedDocumentBackend(AbstractDocumentBackend):
    """DeclarativeDocumentBackend.

    A declarative document backend is a backend that can transform to DoclingDocument
    straight without a recognition pipeline.
    """

    @abstractmethod
    def page_count(self) -> int:
        pass


class DeclarativeDocumentBackend(AbstractDocumentBackend):
    """DeclarativeDocumentBackend.

    A declarative document backend is a backend that can transform to DoclingDocument
    straight without a recognition pipeline.
    """

    @abstractmethod
    def convert(self) -> DoclingDocument:
        pass


================================================
File: docling/backend/asciidoc_backend.py
================================================
import logging
import re
from io import BytesIO
from pathlib import Path
from typing import Set, Union

from docling_core.types.doc import (
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupItem,
    GroupLabel,
    ImageRef,
    Size,
    TableCell,
    TableData,
)

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class AsciiDocBackend(DeclarativeDocumentBackend):
    def __init__(self, in_doc: InputDocument, path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        self.path_or_stream = path_or_stream

        try:
            if isinstance(self.path_or_stream, BytesIO):
                text_stream = self.path_or_stream.getvalue().decode("utf-8")
                self.lines = text_stream.split("\n")
            if isinstance(self.path_or_stream, Path):
                with open(self.path_or_stream, "r", encoding="utf-8") as f:
                    self.lines = f.readlines()
            self.valid = True

        except Exception as e:
            raise RuntimeError(
                f"Could not initialize AsciiDoc backend for file with hash {self.document_hash}."
            ) from e
        return

    def is_valid(self) -> bool:
        return self.valid

    @classmethod
    def supports_pagination(cls) -> bool:
        return False

    def unload(self):
        return

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.ASCIIDOC}

    def convert(self) -> DoclingDocument:
        """
        Parses the ASCII into a structured document model.
        """

        origin = DocumentOrigin(
            filename=self.file.name or "file",
            mimetype="text/asciidoc",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file", origin=origin)

        doc = self._parse(doc)

        return doc

    def _parse(self, doc: DoclingDocument):
        """
        Main function that orchestrates the parsing by yielding components:
        title, section headers, text, lists, and tables.
        """

        content = ""

        in_list = False
        in_table = False

        text_data: list[str] = []
        table_data: list[str] = []
        caption_data: list[str] = []

        # parents: dict[int, Union[DocItem, GroupItem, None]] = {}
        parents: dict[int, Union[GroupItem, None]] = {}
        # indents: dict[int, Union[DocItem, GroupItem, None]] = {}
        indents: dict[int, Union[GroupItem, None]] = {}

        for i in range(0, 10):
            parents[i] = None
            indents[i] = None

        for line in self.lines:
            # line = line.strip()

            # Title
            if self._is_title(line):
                item = self._parse_title(line)
                level = item["level"]

                parents[level] = doc.add_text(
                    text=item["text"], label=DocItemLabel.TITLE
                )

            # Section headers
            elif self._is_section_header(line):
                item = self._parse_section_header(line)
                level = item["level"]

                parents[level] = doc.add_heading(
                    text=item["text"], level=item["level"], parent=parents[level - 1]
                )
                for k, v in parents.items():
                    if k > level:
                        parents[k] = None

            # Lists
            elif self._is_list_item(line):

                _log.debug(f"line: {line}")
                item = self._parse_list_item(line)
                _log.debug(f"parsed list-item: {item}")

                level = self._get_current_level(parents)

                if not in_list:
                    in_list = True

                    parents[level + 1] = doc.add_group(
                        parent=parents[level], name="list", label=GroupLabel.LIST
                    )
                    indents[level + 1] = item["indent"]

                elif in_list and item["indent"] > indents[level]:
                    parents[level + 1] = doc.add_group(
                        parent=parents[level], name="list", label=GroupLabel.LIST
                    )
                    indents[level + 1] = item["indent"]

                elif in_list and item["indent"] < indents[level]:

                    # print(item["indent"], " => ", indents[level])
                    while item["indent"] < indents[level]:
                        # print(item["indent"], " => ", indents[level])
                        parents[level] = None
                        indents[level] = None
                        level -= 1

                doc.add_list_item(
                    item["text"], parent=self._get_current_parent(parents)
                )

            elif in_list and not self._is_list_item(line):
                in_list = False

                level = self._get_current_level(parents)
                parents[level] = None

            # Tables
            elif line.strip() == "|===" and not in_table:  # start of table
                in_table = True

            elif self._is_table_line(line):  # within a table
                in_table = True
                table_data.append(self._parse_table_line(line))

            elif in_table and (
                (not self._is_table_line(line)) or line.strip() == "|==="
            ):  # end of table

                caption = None
                if len(caption_data) > 0:
                    caption = doc.add_text(
                        text=" ".join(caption_data), label=DocItemLabel.CAPTION
                    )

                caption_data = []

                data = self._populate_table_as_grid(table_data)
                doc.add_table(
                    data=data, parent=self._get_current_parent(parents), caption=caption
                )

                in_table = False
                table_data = []

            # Picture
            elif self._is_picture(line):

                caption = None
                if len(caption_data) > 0:
                    caption = doc.add_text(
                        text=" ".join(caption_data), label=DocItemLabel.CAPTION
                    )

                caption_data = []

                item = self._parse_picture(line)

                size = None
                if "width" in item and "height" in item:
                    size = Size(width=int(item["width"]), height=int(item["height"]))

                uri = None
                if (
                    "uri" in item
                    and not item["uri"].startswith("http")
                    and item["uri"].startswith("//")
                ):
                    uri = "file:" + item["uri"]
                elif (
                    "uri" in item
                    and not item["uri"].startswith("http")
                    and item["uri"].startswith("/")
                ):
                    uri = "file:/" + item["uri"]
                elif "uri" in item and not item["uri"].startswith("http"):
                    uri = "file://" + item["uri"]

                image = ImageRef(mimetype="image/png", size=size, dpi=70, uri=uri)
                doc.add_picture(image=image, caption=caption)

            # Caption
            elif self._is_caption(line) and len(caption_data) == 0:
                item = self._parse_caption(line)
                caption_data.append(item["text"])

            elif (
                len(line.strip()) > 0 and len(caption_data) > 0
            ):  # allow multiline captions
                item = self._parse_text(line)
                caption_data.append(item["text"])

            # Plain text
            elif len(line.strip()) == 0 and len(text_data) > 0:
                doc.add_text(
                    text=" ".join(text_data),
                    label=DocItemLabel.PARAGRAPH,
                    parent=self._get_current_parent(parents),
                )
                text_data = []

            elif len(line.strip()) > 0:  # allow multiline texts

                item = self._parse_text(line)
                text_data.append(item["text"])

        if len(text_data) > 0:
            doc.add_text(
                text=" ".join(text_data),
                label=DocItemLabel.PARAGRAPH,
                parent=self._get_current_parent(parents),
            )
            text_data = []

        if in_table and len(table_data) > 0:
            data = self._populate_table_as_grid(table_data)
            doc.add_table(data=data, parent=self._get_current_parent(parents))

            in_table = False
            table_data = []

        return doc

    def _get_current_level(self, parents):
        for k, v in parents.items():
            if v == None and k > 0:
                return k - 1

        return 0

    def _get_current_parent(self, parents):
        for k, v in parents.items():
            if v == None and k > 0:
                return parents[k - 1]

        return None

    #   =========   Title
    def _is_title(self, line):
        return re.match(r"^= ", line)

    def _parse_title(self, line):
        return {"type": "title", "text": line[2:].strip(), "level": 0}

    #   =========   Section headers
    def _is_section_header(self, line):
        return re.match(r"^==+", line)

    def _parse_section_header(self, line):
        match = re.match(r"^(=+)\s+(.*)", line)

        marker = match.group(1)  # The list marker (e.g., "*", "-", "1.")
        text = match.group(2)  # The actual text of the list item

        header_level = marker.count("=")  # number of '=' represents level
        return {
            "type": "header",
            "level": header_level - 1,
            "text": text.strip(),
        }

    #   =========   Lists
    def _is_list_item(self, line):
        return re.match(r"^(\s)*(\*|-|\d+\.|\w+\.) ", line)

    def _parse_list_item(self, line):
        """Extract the item marker (number or bullet symbol) and the text of the item."""

        match = re.match(r"^(\s*)(\*|-|\d+\.)\s+(.*)", line)
        if match:
            indent = match.group(1)
            marker = match.group(2)  # The list marker (e.g., "*", "-", "1.")
            text = match.group(3)  # The actual text of the list item

            if marker == "*" or marker == "-":
                return {
                    "type": "list_item",
                    "marker": marker,
                    "text": text.strip(),
                    "numbered": False,
                    "indent": 0 if indent == None else len(indent),
                }
            else:
                return {
                    "type": "list_item",
                    "marker": marker,
                    "text": text.strip(),
                    "numbered": True,
                    "indent": 0 if indent == None else len(indent),
                }
        else:
            # Fallback if no match
            return {
                "type": "list_item",
                "marker": "-",
                "text": line,
                "numbered": False,
                "indent": 0,
            }

    #   =========   Tables
    def _is_table_line(self, line):
        return re.match(r"^\|.*\|", line)

    def _parse_table_line(self, line):
        # Split table cells and trim extra spaces
        return [cell.strip() for cell in line.split("|") if cell.strip()]

    def _populate_table_as_grid(self, table_data):

        num_rows = len(table_data)

        # Adjust the table data into a grid format
        num_cols = max(len(row) for row in table_data)

        data = TableData(num_rows=num_rows, num_cols=num_cols, table_cells=[])
        for row_idx, row in enumerate(table_data):
            # Pad rows with empty strings to match column count
            # grid.append(row + [''] * (max_cols - len(row)))

            for col_idx, text in enumerate(row):
                row_span = 1
                col_span = 1

                cell = TableCell(
                    text=text,
                    row_span=row_span,
                    col_span=col_span,
                    start_row_offset_idx=row_idx,
                    end_row_offset_idx=row_idx + row_span,
                    start_col_offset_idx=col_idx,
                    end_col_offset_idx=col_idx + col_span,
                    col_header=False,
                    row_header=False,
                )
                data.table_cells.append(cell)

        return data

    #   =========   Pictures
    def _is_picture(self, line):
        return re.match(r"^image::", line)

    def _parse_picture(self, line):
        """
        Parse an image macro, extracting its path and attributes.
        Syntax: image::path/to/image.png[Alt Text, width=200, height=150, align=center]
        """
        mtch = re.match(r"^image::(.+)\[(.*)\]$", line)
        if mtch:
            picture_path = mtch.group(1).strip()
            attributes = mtch.group(2).split(",")
            picture_info = {"type": "picture", "uri": picture_path}

            # Extract optional attributes (alt text, width, height, alignment)
            if attributes:
                picture_info["alt"] = attributes[0].strip() if attributes[0] else ""
                for attr in attributes[1:]:
                    key, value = attr.split("=")
                    picture_info[key.strip()] = value.strip()

            return picture_info

        return {"type": "picture", "uri": line}

    #   =========   Captions
    def _is_caption(self, line):
        return re.match(r"^\.(.+)", line)

    def _parse_caption(self, line):
        mtch = re.match(r"^\.(.+)", line)
        if mtch:
            text = mtch.group(1)
            return {"type": "caption", "text": text}

        return {"type": "caption", "text": ""}

    #   =========   Plain text
    def _parse_text(self, line):
        return {"type": "text", "text": line.strip()}


================================================
File: docling/backend/csv_backend.py
================================================
import csv
import logging
import warnings
from io import BytesIO, StringIO
from pathlib import Path
from typing import Set, Union

from docling_core.types.doc import DoclingDocument, DocumentOrigin, TableCell, TableData

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class CsvDocumentBackend(DeclarativeDocumentBackend):
    content: StringIO

    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        # Load content
        try:
            if isinstance(self.path_or_stream, BytesIO):
                self.content = StringIO(self.path_or_stream.getvalue().decode("utf-8"))
            elif isinstance(self.path_or_stream, Path):
                self.content = StringIO(self.path_or_stream.read_text("utf-8"))
            self.valid = True
        except Exception as e:
            raise RuntimeError(
                f"CsvDocumentBackend could not load document with hash {self.document_hash}"
            ) from e
        return

    def is_valid(self) -> bool:
        return self.valid

    @classmethod
    def supports_pagination(cls) -> bool:
        return False

    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()
        self.path_or_stream = None

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.CSV}

    def convert(self) -> DoclingDocument:
        """
        Parses the CSV data into a structured document model.
        """

        # Detect CSV dialect
        head = self.content.readline()
        dialect = csv.Sniffer().sniff(head, ",;\t|:")
        _log.info(f'Parsing CSV with delimiter: "{dialect.delimiter}"')
        if not dialect.delimiter in {",", ";", "\t", "|", ":"}:
            raise RuntimeError(
                f"Cannot convert csv with unknown delimiter {dialect.delimiter}."
            )

        # Parce CSV
        self.content.seek(0)
        result = csv.reader(self.content, dialect=dialect, strict=True)
        self.csv_data = list(result)
        _log.info(f"Detected {len(self.csv_data)} lines")

        # Ensure uniform column length
        expected_length = len(self.csv_data[0])
        is_uniform = all(len(row) == expected_length for row in self.csv_data)
        if not is_uniform:
            warnings.warn(
                f"Inconsistent column lengths detected in CSV data. "
                f"Expected {expected_length} columns, but found rows with varying lengths. "
                f"Ensure all rows have the same number of columns."
            )

        # Parse the CSV into a structured document model
        origin = DocumentOrigin(
            filename=self.file.name or "file.csv",
            mimetype="text/csv",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file.csv", origin=origin)

        if self.is_valid():
            # Convert CSV data to table
            if self.csv_data:
                num_rows = len(self.csv_data)
                num_cols = max(len(row) for row in self.csv_data)

                table_data = TableData(
                    num_rows=num_rows,
                    num_cols=num_cols,
                    table_cells=[],
                )

                # Convert each cell to TableCell
                for row_idx, row in enumerate(self.csv_data):
                    for col_idx, cell_value in enumerate(row):
                        cell = TableCell(
                            text=str(cell_value),
                            row_span=1,  # CSV doesn't support merged cells
                            col_span=1,
                            start_row_offset_idx=row_idx,
                            end_row_offset_idx=row_idx + 1,
                            start_col_offset_idx=col_idx,
                            end_col_offset_idx=col_idx + 1,
                            col_header=row_idx == 0,  # First row as header
                            row_header=False,
                        )
                        table_data.table_cells.append(cell)

                doc.add_table(data=table_data)
        else:
            raise RuntimeError(
                f"Cannot convert doc with {self.document_hash} because the backend failed to init."
            )

        return doc


================================================
File: docling/backend/docling_parse_backend.py
================================================
import logging
import random
from io import BytesIO
from pathlib import Path
from typing import Iterable, List, Optional, Union

import pypdfium2 as pdfium
from docling_core.types.doc import BoundingBox, CoordOrigin, Size
from docling_parse.pdf_parsers import pdf_parser_v1
from PIL import Image, ImageDraw
from pypdfium2 import PdfPage

from docling.backend.pdf_backend import PdfDocumentBackend, PdfPageBackend
from docling.datamodel.base_models import Cell
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class DoclingParsePageBackend(PdfPageBackend):
    def __init__(
        self, parser: pdf_parser_v1, document_hash: str, page_no: int, page_obj: PdfPage
    ):
        self._ppage = page_obj
        parsed_page = parser.parse_pdf_from_key_on_page(document_hash, page_no)

        self.valid = "pages" in parsed_page
        if self.valid:
            self._dpage = parsed_page["pages"][0]
        else:
            _log.info(
                f"An error occurred when loading page {page_no} of document {document_hash}."
            )

    def is_valid(self) -> bool:
        return self.valid

    def get_text_in_rect(self, bbox: BoundingBox) -> str:
        if not self.valid:
            return ""
        # Find intersecting cells on the page
        text_piece = ""
        page_size = self.get_size()
        parser_width = self._dpage["width"]
        parser_height = self._dpage["height"]

        scale = (
            1  # FIX - Replace with param in get_text_in_rect across backends (optional)
        )

        for i in range(len(self._dpage["cells"])):
            rect = self._dpage["cells"][i]["box"]["device"]
            x0, y0, x1, y1 = rect
            cell_bbox = BoundingBox(
                l=x0 * scale * page_size.width / parser_width,
                b=y0 * scale * page_size.height / parser_height,
                r=x1 * scale * page_size.width / parser_width,
                t=y1 * scale * page_size.height / parser_height,
                coord_origin=CoordOrigin.BOTTOMLEFT,
            ).to_top_left_origin(page_height=page_size.height * scale)

            overlap_frac = cell_bbox.intersection_area_with(bbox) / cell_bbox.area()

            if overlap_frac > 0.5:
                if len(text_piece) > 0:
                    text_piece += " "
                text_piece += self._dpage["cells"][i]["content"]["rnormalized"]

        return text_piece

    def get_text_cells(self) -> Iterable[Cell]:
        cells: List[Cell] = []
        cell_counter = 0

        if not self.valid:
            return cells

        page_size = self.get_size()

        parser_width = self._dpage["width"]
        parser_height = self._dpage["height"]

        for i in range(len(self._dpage["cells"])):
            rect = self._dpage["cells"][i]["box"]["device"]
            x0, y0, x1, y1 = rect

            if x1 < x0:
                x0, x1 = x1, x0
            if y1 < y0:
                y0, y1 = y1, y0

            text_piece = self._dpage["cells"][i]["content"]["rnormalized"]
            cells.append(
                Cell(
                    id=cell_counter,
                    text=text_piece,
                    bbox=BoundingBox(
                        # l=x0, b=y0, r=x1, t=y1,
                        l=x0 * page_size.width / parser_width,
                        b=y0 * page_size.height / parser_height,
                        r=x1 * page_size.width / parser_width,
                        t=y1 * page_size.height / parser_height,
                        coord_origin=CoordOrigin.BOTTOMLEFT,
                    ).to_top_left_origin(page_size.height),
                )
            )
            cell_counter += 1

        def draw_clusters_and_cells():
            image = (
                self.get_page_image()
            )  # make new image to avoid drawing on the saved ones
            draw = ImageDraw.Draw(image)
            for c in cells:
                x0, y0, x1, y1 = c.bbox.as_tuple()
                cell_color = (
                    random.randint(30, 140),
                    random.randint(30, 140),
                    random.randint(30, 140),
                )
                draw.rectangle([(x0, y0), (x1, y1)], outline=cell_color)
            image.show()

        # before merge:
        # draw_clusters_and_cells()

        # cells = merge_horizontal_cells(cells)

        # after merge:
        # draw_clusters_and_cells()

        return cells

    def get_bitmap_rects(self, scale: float = 1) -> Iterable[BoundingBox]:
        AREA_THRESHOLD = 0  # 32 * 32

        for i in range(len(self._dpage["images"])):
            bitmap = self._dpage["images"][i]
            cropbox = BoundingBox.from_tuple(
                bitmap["box"], origin=CoordOrigin.BOTTOMLEFT
            ).to_top_left_origin(self.get_size().height)

            if cropbox.area() > AREA_THRESHOLD:
                cropbox = cropbox.scaled(scale=scale)

                yield cropbox

    def get_page_image(
        self, scale: float = 1, cropbox: Optional[BoundingBox] = None
    ) -> Image.Image:

        page_size = self.get_size()

        if not cropbox:
            cropbox = BoundingBox(
                l=0,
                r=page_size.width,
                t=0,
                b=page_size.height,
                coord_origin=CoordOrigin.TOPLEFT,
            )
            padbox = BoundingBox(
                l=0, r=0, t=0, b=0, coord_origin=CoordOrigin.BOTTOMLEFT
            )
        else:
            padbox = cropbox.to_bottom_left_origin(page_size.height).model_copy()
            padbox.r = page_size.width - padbox.r
            padbox.t = page_size.height - padbox.t

        image = (
            self._ppage.render(
                scale=scale * 1.5,
                rotation=0,  # no additional rotation
                crop=padbox.as_tuple(),
            )
            .to_pil()
            .resize(size=(round(cropbox.width * scale), round(cropbox.height * scale)))
        )  # We resize the image from 1.5x the given scale to make it sharper.

        return image

    def get_size(self) -> Size:
        return Size(width=self._ppage.get_width(), height=self._ppage.get_height())

    def unload(self):
        self._ppage = None
        self._dpage = None


class DoclingParseDocumentBackend(PdfDocumentBackend):
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        self._pdoc = pdfium.PdfDocument(self.path_or_stream)
        self.parser = pdf_parser_v1()

        success = False
        if isinstance(self.path_or_stream, BytesIO):
            success = self.parser.load_document_from_bytesio(
                self.document_hash, self.path_or_stream
            )
        elif isinstance(self.path_or_stream, Path):
            success = self.parser.load_document(
                self.document_hash, str(self.path_or_stream)
            )

        if not success:
            raise RuntimeError(
                f"docling-parse could not load document with hash {self.document_hash}."
            )

    def page_count(self) -> int:
        return len(self._pdoc)  # To be replaced with docling-parse API

    def load_page(self, page_no: int) -> DoclingParsePageBackend:
        return DoclingParsePageBackend(
            self.parser, self.document_hash, page_no, self._pdoc[page_no]
        )

    def is_valid(self) -> bool:
        return self.page_count() > 0

    def unload(self):
        super().unload()
        self.parser.unload_document(self.document_hash)
        self._pdoc.close()
        self._pdoc = None


================================================
File: docling/backend/docling_parse_v2_backend.py
================================================
import logging
import random
from io import BytesIO
from pathlib import Path
from typing import TYPE_CHECKING, Iterable, List, Optional, Union

import pypdfium2 as pdfium
from docling_core.types.doc import BoundingBox, CoordOrigin
from docling_parse.pdf_parsers import pdf_parser_v2
from PIL import Image, ImageDraw
from pypdfium2 import PdfPage

from docling.backend.pdf_backend import PdfDocumentBackend, PdfPageBackend
from docling.datamodel.base_models import Cell, Size

if TYPE_CHECKING:
    from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class DoclingParseV2PageBackend(PdfPageBackend):
    def __init__(
        self, parser: pdf_parser_v2, document_hash: str, page_no: int, page_obj: PdfPage
    ):
        self._ppage = page_obj
        parsed_page = parser.parse_pdf_from_key_on_page(document_hash, page_no)

        self.valid = "pages" in parsed_page and len(parsed_page["pages"]) == 1
        if self.valid:
            self._dpage = parsed_page["pages"][0]
        else:
            _log.info(
                f"An error occurred when loading page {page_no} of document {document_hash}."
            )

    def is_valid(self) -> bool:
        return self.valid

    def get_text_in_rect(self, bbox: BoundingBox) -> str:
        if not self.valid:
            return ""
        # Find intersecting cells on the page
        text_piece = ""
        page_size = self.get_size()

        parser_width = self._dpage["sanitized"]["dimension"]["width"]
        parser_height = self._dpage["sanitized"]["dimension"]["height"]

        scale = (
            1  # FIX - Replace with param in get_text_in_rect across backends (optional)
        )

        cells_data = self._dpage["sanitized"]["cells"]["data"]
        cells_header = self._dpage["sanitized"]["cells"]["header"]

        for i, cell_data in enumerate(cells_data):
            x0 = cell_data[cells_header.index("x0")]
            y0 = cell_data[cells_header.index("y0")]
            x1 = cell_data[cells_header.index("x1")]
            y1 = cell_data[cells_header.index("y1")]

            cell_bbox = BoundingBox(
                l=x0 * scale * page_size.width / parser_width,
                b=y0 * scale * page_size.height / parser_height,
                r=x1 * scale * page_size.width / parser_width,
                t=y1 * scale * page_size.height / parser_height,
                coord_origin=CoordOrigin.BOTTOMLEFT,
            ).to_top_left_origin(page_height=page_size.height * scale)

            overlap_frac = cell_bbox.intersection_area_with(bbox) / cell_bbox.area()

            if overlap_frac > 0.5:
                if len(text_piece) > 0:
                    text_piece += " "
                text_piece += cell_data[cells_header.index("text")]

        return text_piece

    def get_text_cells(self) -> Iterable[Cell]:
        cells: List[Cell] = []
        cell_counter = 0

        if not self.valid:
            return cells

        page_size = self.get_size()

        parser_width = self._dpage["sanitized"]["dimension"]["width"]
        parser_height = self._dpage["sanitized"]["dimension"]["height"]

        cells_data = self._dpage["sanitized"]["cells"]["data"]
        cells_header = self._dpage["sanitized"]["cells"]["header"]

        for i, cell_data in enumerate(cells_data):
            x0 = cell_data[cells_header.index("x0")]
            y0 = cell_data[cells_header.index("y0")]
            x1 = cell_data[cells_header.index("x1")]
            y1 = cell_data[cells_header.index("y1")]

            if x1 < x0:
                x0, x1 = x1, x0
            if y1 < y0:
                y0, y1 = y1, y0

            text_piece = cell_data[cells_header.index("text")]
            cells.append(
                Cell(
                    id=cell_counter,
                    text=text_piece,
                    bbox=BoundingBox(
                        # l=x0, b=y0, r=x1, t=y1,
                        l=x0 * page_size.width / parser_width,
                        b=y0 * page_size.height / parser_height,
                        r=x1 * page_size.width / parser_width,
                        t=y1 * page_size.height / parser_height,
                        coord_origin=CoordOrigin.BOTTOMLEFT,
                    ).to_top_left_origin(page_size.height),
                )
            )
            cell_counter += 1

        def draw_clusters_and_cells():
            image = (
                self.get_page_image()
            )  # make new image to avoid drawing on the saved ones
            draw = ImageDraw.Draw(image)
            for c in cells:
                x0, y0, x1, y1 = c.bbox.as_tuple()
                cell_color = (
                    random.randint(30, 140),
                    random.randint(30, 140),
                    random.randint(30, 140),
                )
                draw.rectangle([(x0, y0), (x1, y1)], outline=cell_color)
            image.show()

        # draw_clusters_and_cells()

        return cells

    def get_bitmap_rects(self, scale: float = 1) -> Iterable[BoundingBox]:
        AREA_THRESHOLD = 0  # 32 * 32

        images = self._dpage["sanitized"]["images"]["data"]
        images_header = self._dpage["sanitized"]["images"]["header"]

        for row in images:
            x0 = row[images_header.index("x0")]
            y0 = row[images_header.index("y0")]
            x1 = row[images_header.index("x1")]
            y1 = row[images_header.index("y1")]

            cropbox = BoundingBox.from_tuple(
                (x0, y0, x1, y1), origin=CoordOrigin.BOTTOMLEFT
            ).to_top_left_origin(self.get_size().height)

            if cropbox.area() > AREA_THRESHOLD:
                cropbox = cropbox.scaled(scale=scale)

                yield cropbox

    def get_page_image(
        self, scale: float = 1, cropbox: Optional[BoundingBox] = None
    ) -> Image.Image:

        page_size = self.get_size()

        if not cropbox:
            cropbox = BoundingBox(
                l=0,
                r=page_size.width,
                t=0,
                b=page_size.height,
                coord_origin=CoordOrigin.TOPLEFT,
            )
            padbox = BoundingBox(
                l=0, r=0, t=0, b=0, coord_origin=CoordOrigin.BOTTOMLEFT
            )
        else:
            padbox = cropbox.to_bottom_left_origin(page_size.height).model_copy()
            padbox.r = page_size.width - padbox.r
            padbox.t = page_size.height - padbox.t

        image = (
            self._ppage.render(
                scale=scale * 1.5,
                rotation=0,  # no additional rotation
                crop=padbox.as_tuple(),
            )
            .to_pil()
            .resize(size=(round(cropbox.width * scale), round(cropbox.height * scale)))
        )  # We resize the image from 1.5x the given scale to make it sharper.

        return image

    def get_size(self) -> Size:
        return Size(width=self._ppage.get_width(), height=self._ppage.get_height())

    def unload(self):
        self._ppage = None
        self._dpage = None


class DoclingParseV2DocumentBackend(PdfDocumentBackend):
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        self._pdoc = pdfium.PdfDocument(self.path_or_stream)
        self.parser = pdf_parser_v2("fatal")

        success = False
        if isinstance(self.path_or_stream, BytesIO):
            success = self.parser.load_document_from_bytesio(
                self.document_hash, self.path_or_stream
            )
        elif isinstance(self.path_or_stream, Path):
            success = self.parser.load_document(
                self.document_hash, str(self.path_or_stream)
            )

        if not success:
            raise RuntimeError(
                f"docling-parse v2 could not load document {self.document_hash}."
            )

    def page_count(self) -> int:
        # return len(self._pdoc)  # To be replaced with docling-parse API

        len_1 = len(self._pdoc)
        len_2 = self.parser.number_of_pages(self.document_hash)

        if len_1 != len_2:
            _log.error(f"Inconsistent number of pages: {len_1}!={len_2}")

        return len_2

    def load_page(self, page_no: int) -> DoclingParseV2PageBackend:
        return DoclingParseV2PageBackend(
            self.parser, self.document_hash, page_no, self._pdoc[page_no]
        )

    def is_valid(self) -> bool:
        return self.page_count() > 0

    def unload(self):
        super().unload()
        self.parser.unload_document(self.document_hash)
        self._pdoc.close()
        self._pdoc = None


================================================
File: docling/backend/html_backend.py
================================================
import logging
from io import BytesIO
from pathlib import Path
from typing import Final, Optional, Union, cast

from bs4 import BeautifulSoup, NavigableString, PageElement, Tag
from bs4.element import PreformattedString
from docling_core.types.doc import (
    DocItem,
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupItem,
    GroupLabel,
    TableCell,
    TableData,
)
from typing_extensions import override

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)

# tags that generate NodeItem elements
TAGS_FOR_NODE_ITEMS: Final = [
    "h1",
    "h2",
    "h3",
    "h4",
    "h5",
    "h6",
    "p",
    "pre",
    "ul",
    "ol",
    "li",
    "table",
    "figure",
    "img",
]


class HTMLDocumentBackend(DeclarativeDocumentBackend):
    @override
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)
        self.soup: Optional[Tag] = None
        # HTML file:
        self.path_or_stream = path_or_stream
        # Initialise the parents for the hierarchy
        self.max_levels = 10
        self.level = 0
        self.parents: dict[int, Optional[Union[DocItem, GroupItem]]] = {}
        for i in range(0, self.max_levels):
            self.parents[i] = None

        try:
            if isinstance(self.path_or_stream, BytesIO):
                text_stream = self.path_or_stream.getvalue()
                self.soup = BeautifulSoup(text_stream, "html.parser")
            if isinstance(self.path_or_stream, Path):
                with open(self.path_or_stream, "rb") as f:
                    html_content = f.read()
                    self.soup = BeautifulSoup(html_content, "html.parser")
        except Exception as e:
            raise RuntimeError(
                f"Could not initialize HTML backend for file with hash {self.document_hash}."
            ) from e

    @override
    def is_valid(self) -> bool:
        return self.soup is not None

    @classmethod
    @override
    def supports_pagination(cls) -> bool:
        return False

    @override
    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()

        self.path_or_stream = None

    @classmethod
    @override
    def supported_formats(cls) -> set[InputFormat]:
        return {InputFormat.HTML}

    @override
    def convert(self) -> DoclingDocument:
        # access self.path_or_stream to load stuff
        origin = DocumentOrigin(
            filename=self.file.name or "file",
            mimetype="text/html",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file", origin=origin)
        _log.debug("Trying to convert HTML...")

        if self.is_valid():
            assert self.soup is not None
            content = self.soup.body or self.soup
            # Replace <br> tags with newline characters
            # TODO: remove style to avoid losing text from tags like i, b, span, ...
            for br in content("br"):
                br.replace_with(NavigableString("\n"))
            self.walk(content, doc)
        else:
            raise RuntimeError(
                f"Cannot convert doc with {self.document_hash} because the backend failed to init."
            )
        return doc

    def walk(self, tag: Tag, doc: DoclingDocument) -> None:
        # Iterate over elements in the body of the document
        text: str = ""
        for element in tag.children:
            if isinstance(element, Tag):
                try:
                    self.analyze_tag(cast(Tag, element), doc)
                except Exception as exc_child:
                    _log.error(
                        f"Error processing child from tag{tag.name}: {exc_child}"
                    )
                    raise exc_child
            elif isinstance(element, NavigableString) and not isinstance(
                element, PreformattedString
            ):
                # Floating text outside paragraphs or analyzed tags
                text += element
                siblings: list[Tag] = [
                    item for item in element.next_siblings if isinstance(item, Tag)
                ]
                if element.next_sibling is None or any(
                    [item.name in TAGS_FOR_NODE_ITEMS for item in siblings]
                ):
                    text = text.strip()
                    if text and tag.name in ["div"]:
                        doc.add_text(
                            parent=self.parents[self.level],
                            label=DocItemLabel.PARAGRAPH,
                            text=text,
                        )
                    text = ""

        return

    def analyze_tag(self, tag: Tag, doc: DoclingDocument) -> None:
        if tag.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
            self.handle_header(tag, doc)
        elif tag.name in ["p"]:
            self.handle_paragraph(tag, doc)
        elif tag.name in ["pre"]:
            self.handle_code(tag, doc)
        elif tag.name in ["ul", "ol"]:
            self.handle_list(tag, doc)
        elif tag.name in ["li"]:
            self.handle_list_item(tag, doc)
        elif tag.name == "table":
            self.handle_table(tag, doc)
        elif tag.name == "figure":
            self.handle_figure(tag, doc)
        elif tag.name == "img":
            self.handle_image(doc)
        else:
            self.walk(tag, doc)

    def get_text(self, item: PageElement) -> str:
        """Get the text content of a tag."""
        parts: list[str] = self.extract_text_recursively(item)

        return "".join(parts) + " "

    # Function to recursively extract text from all child nodes
    def extract_text_recursively(self, item: PageElement) -> list[str]:
        result: list[str] = []

        if isinstance(item, NavigableString):
            return [item]

        tag = cast(Tag, item)
        if tag.name not in ["ul", "ol"]:
            for child in tag:
                # Recursively get the child's text content
                result.extend(self.extract_text_recursively(child))

        return ["".join(result) + " "]

    def handle_header(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles header tags (h1, h2, etc.)."""
        hlevel = int(element.name.replace("h", ""))
        text = element.text.strip()

        if hlevel == 1:
            for key in self.parents.keys():
                self.parents[key] = None

            self.level = 1
            self.parents[self.level] = doc.add_text(
                parent=self.parents[0], label=DocItemLabel.TITLE, text=text
            )
        else:
            if hlevel > self.level:

                # add invisible group
                for i in range(self.level + 1, hlevel):
                    self.parents[i] = doc.add_group(
                        name=f"header-{i}",
                        label=GroupLabel.SECTION,
                        parent=self.parents[i - 1],
                    )
                self.level = hlevel

            elif hlevel < self.level:

                # remove the tail
                for key in self.parents.keys():
                    if key > hlevel:
                        self.parents[key] = None
                self.level = hlevel

            self.parents[hlevel] = doc.add_heading(
                parent=self.parents[hlevel - 1],
                text=text,
                level=hlevel,
            )

    def handle_code(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles monospace code snippets (pre)."""
        if element.text is None:
            return
        text = element.text.strip()
        if text:
            doc.add_code(parent=self.parents[self.level], text=text)

    def handle_paragraph(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles paragraph tags (p)."""
        if element.text is None:
            return
        text = element.text.strip()
        label = DocItemLabel.PARAGRAPH
        if text:
            doc.add_text(parent=self.parents[self.level], label=label, text=text)

    def handle_list(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles list tags (ul, ol) and their list items."""

        if element.name == "ul":
            # create a list group
            self.parents[self.level + 1] = doc.add_group(
                parent=self.parents[self.level], name="list", label=GroupLabel.LIST
            )
        elif element.name == "ol":
            start_attr = element.get("start")
            start: int = (
                int(start_attr)
                if isinstance(start_attr, str) and start_attr.isnumeric()
                else 1
            )
            # create a list group
            self.parents[self.level + 1] = doc.add_group(
                parent=self.parents[self.level],
                name="ordered list" + (f" start {start}" if start != 1 else ""),
                label=GroupLabel.ORDERED_LIST,
            )
        self.level += 1

        self.walk(element, doc)

        self.parents[self.level + 1] = None
        self.level -= 1

    def handle_list_item(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles list item tags (li)."""
        nested_list = element.find(["ul", "ol"])

        parent = self.parents[self.level]
        if parent is None:
            _log.debug(f"list-item has no parent in DoclingDocument: {element}")
            return
        parent_label: str = parent.label
        index_in_list = len(parent.children) + 1
        if (
            parent_label == GroupLabel.ORDERED_LIST
            and isinstance(parent, GroupItem)
            and parent.name
        ):
            start_in_list: str = parent.name.split(" ")[-1]
            start: int = int(start_in_list) if start_in_list.isnumeric() else 1
            index_in_list += start - 1

        if nested_list:
            # Text in list item can be hidden within hierarchy, hence
            # we need to extract it recursively
            text: str = self.get_text(element)
            # Flatten text, remove break lines:
            text = text.replace("\n", "").replace("\r", "")
            text = " ".join(text.split()).strip()

            marker = ""
            enumerated = False
            if parent_label == GroupLabel.ORDERED_LIST:
                marker = str(index_in_list)
                enumerated = True

            if len(text) > 0:
                # create a list-item
                self.parents[self.level + 1] = doc.add_list_item(
                    text=text,
                    enumerated=enumerated,
                    marker=marker,
                    parent=parent,
                )
                self.level += 1

            self.walk(element, doc)

            self.parents[self.level + 1] = None
            self.level -= 1

        elif element.text.strip():
            text = element.text.strip()

            marker = ""
            enumerated = False
            if parent_label == GroupLabel.ORDERED_LIST:
                marker = f"{str(index_in_list)}."
                enumerated = True
            doc.add_list_item(
                text=text,
                enumerated=enumerated,
                marker=marker,
                parent=parent,
            )
        else:
            _log.debug(f"list-item has no text: {element}")

    @staticmethod
    def parse_table_data(element: Tag) -> Optional[TableData]:
        nested_tables = element.find("table")
        if nested_tables is not None:
            _log.debug("Skipping nested table.")
            return None

        # Count the number of rows (number of <tr> elements)
        num_rows = len(element("tr"))

        # Find the number of columns (taking into account colspan)
        num_cols = 0
        for row in element("tr"):
            col_count = 0
            if not isinstance(row, Tag):
                continue
            for cell in row(["td", "th"]):
                if not isinstance(row, Tag):
                    continue
                val = cast(Tag, cell).get("colspan", "1")
                colspan = int(val) if (isinstance(val, str) and val.isnumeric()) else 1
                col_count += colspan
            num_cols = max(num_cols, col_count)

        grid: list = [[None for _ in range(num_cols)] for _ in range(num_rows)]

        data = TableData(num_rows=num_rows, num_cols=num_cols, table_cells=[])

        # Iterate over the rows in the table
        for row_idx, row in enumerate(element("tr")):
            if not isinstance(row, Tag):
                continue

            # For each row, find all the column cells (both <td> and <th>)
            cells = row(["td", "th"])

            # Check if each cell in the row is a header -> means it is a column header
            col_header = True
            for html_cell in cells:
                if isinstance(html_cell, Tag) and html_cell.name == "td":
                    col_header = False

            # Extract the text content of each cell
            col_idx = 0
            for html_cell in cells:
                if not isinstance(html_cell, Tag):
                    continue

                # extract inline formulas
                for formula in html_cell("inline-formula"):
                    math_parts = formula.text.split("$$")
                    if len(math_parts) == 3:
                        math_formula = f"$${math_parts[1]}$$"
                        formula.replace_with(NavigableString(math_formula))

                # TODO: extract content correctly from table-cells with lists
                text = html_cell.text

                # label = html_cell.name
                col_val = html_cell.get("colspan", "1")
                col_span = (
                    int(col_val)
                    if isinstance(col_val, str) and col_val.isnumeric()
                    else 1
                )
                row_val = html_cell.get("rowspan", "1")
                row_span = (
                    int(row_val)
                    if isinstance(row_val, str) and row_val.isnumeric()
                    else 1
                )

                while grid[row_idx][col_idx] is not None:
                    col_idx += 1
                for r in range(row_span):
                    for c in range(col_span):
                        grid[row_idx + r][col_idx + c] = text

                table_cell = TableCell(
                    text=text,
                    row_span=row_span,
                    col_span=col_span,
                    start_row_offset_idx=row_idx,
                    end_row_offset_idx=row_idx + row_span,
                    start_col_offset_idx=col_idx,
                    end_col_offset_idx=col_idx + col_span,
                    col_header=col_header,
                    row_header=((not col_header) and html_cell.name == "th"),
                )
                data.table_cells.append(table_cell)

        return data

    def handle_table(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles table tags."""

        table_data = HTMLDocumentBackend.parse_table_data(element)

        if table_data is not None:
            doc.add_table(data=table_data, parent=self.parents[self.level])

    def get_list_text(self, list_element: Tag, level: int = 0) -> list[str]:
        """Recursively extract text from <ul> or <ol> with proper indentation."""
        result = []
        bullet_char = "*"  # Default bullet character for unordered lists

        if list_element.name == "ol":  # For ordered lists, use numbers
            for i, li in enumerate(list_element("li", recursive=False), 1):
                if not isinstance(li, Tag):
                    continue
                # Add numbering for ordered lists
                result.append(f"{'    ' * level}{i}. {li.get_text(strip=True)}")
                # Handle nested lists
                nested_list = li.find(["ul", "ol"])
                if isinstance(nested_list, Tag):
                    result.extend(self.get_list_text(nested_list, level + 1))
        elif list_element.name == "ul":  # For unordered lists, use bullet points
            for li in list_element("li", recursive=False):
                if not isinstance(li, Tag):
                    continue
                # Add bullet points for unordered lists
                result.append(
                    f"{'    ' * level}{bullet_char} {li.get_text(strip=True)}"
                )
                # Handle nested lists
                nested_list = li.find(["ul", "ol"])
                if isinstance(nested_list, Tag):
                    result.extend(self.get_list_text(nested_list, level + 1))

        return result

    def handle_figure(self, element: Tag, doc: DoclingDocument) -> None:
        """Handles image tags (img)."""

        # Extract the image URI from the <img> tag
        # image_uri = root.xpath('//figure//img/@src')[0]

        contains_captions = element.find(["figcaption"])
        if not isinstance(contains_captions, Tag):
            doc.add_picture(parent=self.parents[self.level], caption=None)
        else:
            texts = []
            for item in contains_captions:
                texts.append(item.text)

            fig_caption = doc.add_text(
                label=DocItemLabel.CAPTION, text=("".join(texts)).strip()
            )
            doc.add_picture(
                parent=self.parents[self.level],
                caption=fig_caption,
            )

    def handle_image(self, doc: DoclingDocument) -> None:
        """Handles image tags (img)."""
        doc.add_picture(parent=self.parents[self.level], caption=None)


================================================
File: docling/backend/md_backend.py
================================================
import logging
import re
import warnings
from io import BytesIO
from pathlib import Path
from typing import List, Optional, Set, Union

import marko
import marko.element
import marko.ext
import marko.ext.gfm
import marko.inline
from docling_core.types.doc import (
    DocItem,
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    NodeItem,
    TableCell,
    TableData,
    TextItem,
)
from marko import Markdown

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.backend.html_backend import HTMLDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)

_MARKER_BODY = "DOCLING_DOC_MD_HTML_EXPORT"
_START_MARKER = f"#_#_{_MARKER_BODY}_START_#_#"
_STOP_MARKER = f"#_#_{_MARKER_BODY}_STOP_#_#"


class MarkdownDocumentBackend(DeclarativeDocumentBackend):
    def _shorten_underscore_sequences(self, markdown_text: str, max_length: int = 10):
        # This regex will match any sequence of underscores
        pattern = r"_+"

        def replace_match(match):
            underscore_sequence = match.group(
                0
            )  # Get the full match (sequence of underscores)

            # Shorten the sequence if it exceeds max_length
            if len(underscore_sequence) > max_length:
                return "_" * max_length
            else:
                return underscore_sequence  # Leave it unchanged if it is shorter or equal to max_length

        # Use re.sub to replace long underscore sequences
        shortened_text = re.sub(pattern, replace_match, markdown_text)

        if len(shortened_text) != len(markdown_text):
            warnings.warn("Detected potentially incorrect Markdown, correcting...")

        return shortened_text

    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        _log.debug("MD INIT!!!")

        # Markdown file:
        self.path_or_stream = path_or_stream
        self.valid = True
        self.markdown = ""  # To store original Markdown string

        self.in_table = False
        self.md_table_buffer: list[str] = []
        self.inline_texts: list[str] = []
        self._html_blocks: int = 0

        try:
            if isinstance(self.path_or_stream, BytesIO):
                text_stream = self.path_or_stream.getvalue().decode("utf-8")
                # remove invalid sequences
                # very long sequences of underscores will lead to unnecessary long processing times.
                # In any proper Markdown files, underscores have to be escaped,
                # otherwise they represent emphasis (bold or italic)
                self.markdown = self._shorten_underscore_sequences(text_stream)
            if isinstance(self.path_or_stream, Path):
                with open(self.path_or_stream, "r", encoding="utf-8") as f:
                    md_content = f.read()
                    # remove invalid sequences
                    # very long sequences of underscores will lead to unnecessary long processing times.
                    # In any proper Markdown files, underscores have to be escaped,
                    # otherwise they represent emphasis (bold or italic)
                    self.markdown = self._shorten_underscore_sequences(md_content)
            self.valid = True

            _log.debug(self.markdown)
        except Exception as e:
            raise RuntimeError(
                f"Could not initialize MD backend for file with hash {self.document_hash}."
            ) from e
        return

    def _close_table(self, doc: DoclingDocument):
        if self.in_table:
            _log.debug("=== TABLE START ===")
            for md_table_row in self.md_table_buffer:
                _log.debug(md_table_row)
            _log.debug("=== TABLE END ===")
            tcells: List[TableCell] = []
            result_table = []
            for n, md_table_row in enumerate(self.md_table_buffer):
                data = []
                if n == 0:
                    header = [t.strip() for t in md_table_row.split("|")[1:-1]]
                    for value in header:
                        data.append(value)
                    result_table.append(data)
                if n > 1:
                    values = [t.strip() for t in md_table_row.split("|")[1:-1]]
                    for value in values:
                        data.append(value)
                    result_table.append(data)

            for trow_ind, trow in enumerate(result_table):
                for tcol_ind, cellval in enumerate(trow):
                    row_span = (
                        1  # currently supporting just simple tables (without spans)
                    )
                    col_span = (
                        1  # currently supporting just simple tables (without spans)
                    )
                    icell = TableCell(
                        text=cellval.strip(),
                        row_span=row_span,
                        col_span=col_span,
                        start_row_offset_idx=trow_ind,
                        end_row_offset_idx=trow_ind + row_span,
                        start_col_offset_idx=tcol_ind,
                        end_col_offset_idx=tcol_ind + col_span,
                        col_header=False,
                        row_header=False,
                    )
                    tcells.append(icell)

            num_rows = len(result_table)
            num_cols = len(result_table[0])
            self.in_table = False
            self.md_table_buffer = []  # clean table markdown buffer
            # Initialize Docling TableData
            table_data = TableData(
                num_rows=num_rows, num_cols=num_cols, table_cells=tcells
            )
            # Populate
            for tcell in tcells:
                table_data.table_cells.append(tcell)
            if len(tcells) > 0:
                doc.add_table(data=table_data)
        return

    def _process_inline_text(
        self, parent_item: Optional[NodeItem], doc: DoclingDocument
    ):
        txt = " ".join(self.inline_texts)
        if len(txt) > 0:
            doc.add_text(
                label=DocItemLabel.PARAGRAPH,
                parent=parent_item,
                text=txt,
            )
        self.inline_texts = []

    def _iterate_elements(
        self,
        element: marko.element.Element,
        depth: int,
        doc: DoclingDocument,
        visited: Set[marko.element.Element],
        parent_item: Optional[NodeItem] = None,
    ):

        if element in visited:
            return

        # Iterates over all elements in the AST
        # Check for different element types and process relevant details
        if isinstance(element, marko.block.Heading) and len(element.children) > 0:
            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(
                f" - Heading level {element.level}, content: {element.children[0].children}"  # type: ignore
            )
            if element.level == 1:
                doc_label = DocItemLabel.TITLE
            else:
                doc_label = DocItemLabel.SECTION_HEADER

            # Header could have arbitrary inclusion of bold, italic or emphasis,
            # hence we need to traverse the tree to get full text of a header
            strings: List[str] = []

            # Define a recursive function to traverse the tree
            def traverse(node: marko.block.BlockElement):
                # Check if the node has a "children" attribute
                if hasattr(node, "children"):
                    # If "children" is a list, continue traversal
                    if isinstance(node.children, list):
                        for child in node.children:
                            traverse(child)
                    # If "children" is text, add it to header text
                    elif isinstance(node.children, str):
                        strings.append(node.children)

            traverse(element)
            snippet_text = "".join(strings)
            if len(snippet_text) > 0:
                parent_item = doc.add_text(
                    label=doc_label, parent=parent_item, text=snippet_text
                )

        elif isinstance(element, marko.block.List):
            has_non_empty_list_items = False
            for child in element.children:
                if isinstance(child, marko.block.ListItem) and len(child.children) > 0:
                    has_non_empty_list_items = True
                    break

            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(f" - List {'ordered' if element.ordered else 'unordered'}")
            if has_non_empty_list_items:
                label = GroupLabel.ORDERED_LIST if element.ordered else GroupLabel.LIST
                parent_item = doc.add_group(
                    label=label, name=f"list", parent=parent_item
                )

        elif isinstance(element, marko.block.ListItem) and len(element.children) > 0:
            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(" - List item")

            first_child = element.children[0]
            snippet_text = str(first_child.children[0].children)  # type: ignore
            is_numbered = False
            if (
                parent_item is not None
                and isinstance(parent_item, DocItem)
                and parent_item.label == GroupLabel.ORDERED_LIST
            ):
                is_numbered = True
            doc.add_list_item(
                enumerated=is_numbered, parent=parent_item, text=snippet_text
            )
            visited.add(first_child)

        elif isinstance(element, marko.inline.Image):
            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(f" - Image with alt: {element.title}, url: {element.dest}")

            fig_caption: Optional[TextItem] = None
            if element.title is not None and element.title != "":
                fig_caption = doc.add_text(
                    label=DocItemLabel.CAPTION, text=element.title
                )

            doc.add_picture(parent=parent_item, caption=fig_caption)

        elif isinstance(element, marko.block.Paragraph) and len(element.children) > 0:
            self._process_inline_text(parent_item, doc)

        elif isinstance(element, marko.inline.RawText):
            _log.debug(f" - Paragraph (raw text): {element.children}")
            snippet_text = element.children.strip()
            # Detect start of the table:
            if "|" in snippet_text:
                # most likely part of the markdown table
                self.in_table = True
                if len(self.md_table_buffer) > 0:
                    self.md_table_buffer[len(self.md_table_buffer) - 1] += snippet_text
                else:
                    self.md_table_buffer.append(snippet_text)
            else:
                self._close_table(doc)
                # most likely just inline text
                self.inline_texts.append(str(element.children))

        elif isinstance(element, marko.inline.CodeSpan):
            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(f" - Code Span: {element.children}")
            snippet_text = str(element.children).strip()
            doc.add_code(parent=parent_item, text=snippet_text)

        elif (
            isinstance(element, (marko.block.CodeBlock, marko.block.FencedCode))
            and len(element.children) > 0
            and isinstance((first_child := element.children[0]), marko.inline.RawText)
            and len(snippet_text := (first_child.children.strip())) > 0
        ):
            self._close_table(doc)
            self._process_inline_text(parent_item, doc)
            _log.debug(f" - Code Block: {element.children}")
            doc.add_code(parent=parent_item, text=snippet_text)

        elif isinstance(element, marko.inline.LineBreak):
            if self.in_table:
                _log.debug("Line break in a table")
                self.md_table_buffer.append("")

        elif isinstance(element, marko.block.HTMLBlock):
            self._html_blocks += 1
            self._process_inline_text(parent_item, doc)
            self._close_table(doc)
            _log.debug("HTML Block: {}".format(element))
            if (
                len(element.body) > 0
            ):  # If Marko doesn't return any content for HTML block, skip it
                html_block = element.body.strip()

                # wrap in markers to enable post-processing in convert()
                text_to_add = f"{_START_MARKER}{html_block}{_STOP_MARKER}"
                doc.add_code(parent=parent_item, text=text_to_add)
        else:
            if not isinstance(element, str):
                self._close_table(doc)
                _log.debug("Some other element: {}".format(element))

        processed_block_types = (
            marko.block.Heading,
            marko.block.CodeBlock,
            marko.block.FencedCode,
            marko.inline.RawText,
        )

        # Iterate through the element's children (if any)
        if hasattr(element, "children") and not isinstance(
            element, processed_block_types
        ):
            for child in element.children:
                self._iterate_elements(
                    element=child,
                    depth=depth + 1,
                    doc=doc,
                    visited=visited,
                    parent_item=parent_item,
                )

    def is_valid(self) -> bool:
        return self.valid

    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()
        self.path_or_stream = None

    @classmethod
    def supports_pagination(cls) -> bool:
        return False

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.MD}

    def convert(self) -> DoclingDocument:
        _log.debug("converting Markdown...")

        origin = DocumentOrigin(
            filename=self.file.name or "file",
            mimetype="text/markdown",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file", origin=origin)

        if self.is_valid():
            # Parse the markdown into an abstract syntax tree (AST)
            marko_parser = Markdown()
            parsed_ast = marko_parser.parse(self.markdown)
            # Start iterating from the root of the AST
            self._iterate_elements(
                element=parsed_ast,
                depth=0,
                doc=doc,
                parent_item=None,
                visited=set(),
            )
            self._process_inline_text(None, doc)  # handle last hanging inline text
            self._close_table(doc=doc)  # handle any last hanging table

            # if HTML blocks were detected, export to HTML and delegate to HTML backend
            if self._html_blocks > 0:

                # export to HTML
                html_backend_cls = HTMLDocumentBackend
                html_str = doc.export_to_html()

                def _restore_original_html(txt, regex):
                    _txt, count = re.subn(regex, "", txt)
                    if count != self._html_blocks:
                        raise RuntimeError(
                            "An internal error has occurred during Markdown conversion."
                        )
                    return _txt

                # restore original HTML by removing previouly added markers
                for regex in [
                    rf"<pre>\s*<code>\s*{_START_MARKER}",
                    rf"{_STOP_MARKER}\s*</code>\s*</pre>",
                ]:
                    html_str = _restore_original_html(txt=html_str, regex=regex)
                self._html_blocks = 0

                # delegate to HTML backend
                stream = BytesIO(bytes(html_str, encoding="utf-8"))
                in_doc = InputDocument(
                    path_or_stream=stream,
                    format=InputFormat.HTML,
                    backend=html_backend_cls,
                    filename=self.file.name,
                )
                html_backend_obj = html_backend_cls(
                    in_doc=in_doc, path_or_stream=stream
                )
                doc = html_backend_obj.convert()
        else:
            raise RuntimeError(
                f"Cannot convert md with {self.document_hash} because the backend failed to init."
            )
        return doc


================================================
File: docling/backend/msexcel_backend.py
================================================
import logging
from io import BytesIO
from pathlib import Path
from typing import Dict, Set, Tuple, Union

from docling_core.types.doc import (
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    ImageRef,
    TableCell,
    TableData,
)

# from lxml import etree
from openpyxl import Workbook, load_workbook
from openpyxl.cell.cell import Cell
from openpyxl.drawing.image import Image
from openpyxl.worksheet.worksheet import Worksheet

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)

from typing import Any, List

from PIL import Image as PILImage
from pydantic import BaseModel


class ExcelCell(BaseModel):
    row: int
    col: int
    text: str
    row_span: int
    col_span: int


class ExcelTable(BaseModel):
    num_rows: int
    num_cols: int
    data: List[ExcelCell]


class MsExcelDocumentBackend(DeclarativeDocumentBackend):
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        # Initialise the parents for the hierarchy
        self.max_levels = 10

        self.parents: Dict[int, Any] = {}
        for i in range(-1, self.max_levels):
            self.parents[i] = None

        self.workbook = None
        try:
            if isinstance(self.path_or_stream, BytesIO):
                self.workbook = load_workbook(filename=self.path_or_stream)

            elif isinstance(self.path_or_stream, Path):
                self.workbook = load_workbook(filename=str(self.path_or_stream))

            self.valid = True
        except Exception as e:
            self.valid = False

            raise RuntimeError(
                f"MsPowerpointDocumentBackend could not load document with hash {self.document_hash}"
            ) from e

    def is_valid(self) -> bool:
        _log.info(f"valid: {self.valid}")
        return self.valid

    @classmethod
    def supports_pagination(cls) -> bool:
        return True

    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()

        self.path_or_stream = None

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.XLSX}

    def convert(self) -> DoclingDocument:
        # Parses the XLSX into a structured document model.

        origin = DocumentOrigin(
            filename=self.file.name or "file.xlsx",
            mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file.xlsx", origin=origin)

        if self.is_valid():
            doc = self._convert_workbook(doc)
        else:
            raise RuntimeError(
                f"Cannot convert doc with {self.document_hash} because the backend failed to init."
            )

        return doc

    def _convert_workbook(self, doc: DoclingDocument) -> DoclingDocument:

        if self.workbook is not None:

            # Iterate over all sheets
            for sheet_name in self.workbook.sheetnames:
                _log.info(f"Processing sheet: {sheet_name}")

                # Access the sheet by name
                sheet = self.workbook[sheet_name]

                self.parents[0] = doc.add_group(
                    parent=None,
                    label=GroupLabel.SECTION,
                    name=f"sheet: {sheet_name}",
                )

                doc = self._convert_sheet(doc, sheet)
        else:
            _log.error("Workbook is not initialized.")

        return doc

    def _convert_sheet(self, doc: DoclingDocument, sheet: Worksheet):

        doc = self._find_tables_in_sheet(doc, sheet)

        doc = self._find_images_in_sheet(doc, sheet)

        return doc

    def _find_tables_in_sheet(self, doc: DoclingDocument, sheet: Worksheet):

        tables = self._find_data_tables(sheet)

        for excel_table in tables:
            num_rows = excel_table.num_rows
            num_cols = excel_table.num_cols

            table_data = TableData(
                num_rows=num_rows,
                num_cols=num_cols,
                table_cells=[],
            )

            for excel_cell in excel_table.data:

                cell = TableCell(
                    text=excel_cell.text,
                    row_span=excel_cell.row_span,
                    col_span=excel_cell.col_span,
                    start_row_offset_idx=excel_cell.row,
                    end_row_offset_idx=excel_cell.row + excel_cell.row_span,
                    start_col_offset_idx=excel_cell.col,
                    end_col_offset_idx=excel_cell.col + excel_cell.col_span,
                    col_header=False,
                    row_header=False,
                )
                table_data.table_cells.append(cell)

            doc.add_table(data=table_data, parent=self.parents[0])

        return doc

    def _find_data_tables(self, sheet: Worksheet):
        """
        Find all compact rectangular data tables in a sheet.
        """
        # _log.info("find_data_tables")

        tables = []  # List to store found tables
        visited: set[Tuple[int, int]] = set()  # Track already visited cells

        # Iterate over all cells in the sheet
        for ri, row in enumerate(sheet.iter_rows(values_only=False)):
            for rj, cell in enumerate(row):

                # Skip empty or already visited cells
                if cell.value is None or (ri, rj) in visited:
                    continue

                # If the cell starts a new table, find its bounds
                table_bounds, visited_cells = self._find_table_bounds(
                    sheet, ri, rj, visited
                )

                visited.update(visited_cells)  # Mark these cells as visited
                tables.append(table_bounds)

        return tables

    def _find_table_bounds(
        self,
        sheet: Worksheet,
        start_row: int,
        start_col: int,
        visited: set[Tuple[int, int]],
    ):
        """
        Determine the bounds of a compact rectangular table.
        Returns:
        - A dictionary with the bounds and data.
        - A set of visited cell coordinates.
        """
        _log.info("find_table_bounds")

        max_row = self._find_table_bottom(sheet, start_row, start_col)
        max_col = self._find_table_right(sheet, start_row, start_col)

        # Collect the data within the bounds
        data = []
        visited_cells = set()
        for ri in range(start_row, max_row + 1):
            for rj in range(start_col, max_col + 1):

                cell = sheet.cell(row=ri + 1, column=rj + 1)  # 1-based indexing

                # Check if the cell belongs to a merged range
                row_span = 1
                col_span = 1

                # _log.info(sheet.merged_cells.ranges)
                for merged_range in sheet.merged_cells.ranges:

                    if (
                        merged_range.min_row <= ri + 1
                        and ri + 1 <= merged_range.max_row
                        and merged_range.min_col <= rj + 1
                        and rj + 1 <= merged_range.max_col
                    ):

                        row_span = merged_range.max_row - merged_range.min_row + 1
                        col_span = merged_range.max_col - merged_range.min_col + 1
                        break

                if (ri, rj) not in visited_cells:
                    data.append(
                        ExcelCell(
                            row=ri - start_row,
                            col=rj - start_col,
                            text=str(cell.value),
                            row_span=row_span,
                            col_span=col_span,
                        )
                    )
                    # _log.info(f"cell: {ri}, {rj} -> {ri - start_row}, {rj - start_col}, {row_span}, {col_span}: {str(cell.value)}")

                    # Mark all cells in the span as visited
                    for span_row in range(ri, ri + row_span):
                        for span_col in range(rj, rj + col_span):
                            visited_cells.add((span_row, span_col))

        return (
            ExcelTable(
                num_rows=max_row + 1 - start_row,
                num_cols=max_col + 1 - start_col,
                data=data,
            ),
            visited_cells,
        )

    def _find_table_bottom(self, sheet: Worksheet, start_row: int, start_col: int):
        """Function to find the bottom boundary of the table"""

        max_row = start_row

        while max_row < sheet.max_row - 1:
            # Get the cell value or check if it is part of a merged cell
            cell = sheet.cell(row=max_row + 2, column=start_col + 1)

            # Check if the cell is part of a merged range
            merged_range = next(
                (mr for mr in sheet.merged_cells.ranges if cell.coordinate in mr),
                None,
            )

            if cell.value is None and not merged_range:
                break  # Stop if the cell is empty and not merged

            # Expand max_row to include the merged range if applicable
            if merged_range:
                max_row = max(max_row, merged_range.max_row - 1)
            else:
                max_row += 1

        return max_row

    def _find_table_right(self, sheet: Worksheet, start_row: int, start_col: int):
        """Function to find the right boundary of the table"""

        max_col = start_col

        while max_col < sheet.max_column - 1:
            # Get the cell value or check if it is part of a merged cell
            cell = sheet.cell(row=start_row + 1, column=max_col + 2)

            # Check if the cell is part of a merged range
            merged_range = next(
                (mr for mr in sheet.merged_cells.ranges if cell.coordinate in mr),
                None,
            )

            if cell.value is None and not merged_range:
                break  # Stop if the cell is empty and not merged

            # Expand max_col to include the merged range if applicable
            if merged_range:
                max_col = max(max_col, merged_range.max_col - 1)
            else:
                max_col += 1

        return max_col

    def _find_images_in_sheet(
        self, doc: DoclingDocument, sheet: Worksheet
    ) -> DoclingDocument:

        # Iterate over byte images in the sheet
        for idx, image in enumerate(sheet._images):  # type: ignore

            try:
                pil_image = PILImage.open(image.ref)

                doc.add_picture(
                    parent=self.parents[0],
                    image=ImageRef.from_pil(image=pil_image, dpi=72),
                    caption=None,
                )
            except:
                _log.error("could not extract the image from excel sheets")

        """
        for idx, chart in enumerate(sheet._charts):  # type: ignore
            try:
                chart_path = f"chart_{idx + 1}.png"
                _log.info(
                    f"Chart found, but dynamic rendering is required for: {chart_path}"
                )

                _log.info(f"Chart {idx + 1}:")
                
                # Chart type
                # _log.info(f"Type: {type(chart).__name__}")
                print(f"Type: {type(chart).__name__}")

                # Extract series data
                for series_idx, series in enumerate(chart.series):
                    #_log.info(f"Series {series_idx + 1}:")
                    print(f"Series {series_idx + 1} type: {type(series).__name__}")
                    #print(f"x-values: {series.xVal}")
                    #print(f"y-values: {series.yVal}")

                    print(f"xval type: {type(series.xVal).__name__}")
                    
                    xvals = []
                    for _ in series.xVal.numLit.pt:
                        print(f"xval type: {type(_).__name__}")
                        if hasattr(_, 'v'):
                            xvals.append(_.v)

                    print(f"x-values: {xvals}")
                            
                    yvals = []
                    for _ in series.yVal:
                        if hasattr(_, 'v'):
                            yvals.append(_.v)
                            
                    print(f"y-values: {yvals}")                    
                    
            except Exception as exc:
                print(exc)
                continue
        """

        return doc


================================================
File: docling/backend/mspowerpoint_backend.py
================================================
import logging
from io import BytesIO
from pathlib import Path
from typing import Set, Union

from docling_core.types.doc import (
    BoundingBox,
    CoordOrigin,
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    ImageRef,
    ProvenanceItem,
    Size,
    TableCell,
    TableData,
)
from PIL import Image, UnidentifiedImageError
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE, PP_PLACEHOLDER

from docling.backend.abstract_backend import (
    DeclarativeDocumentBackend,
    PaginatedDocumentBackend,
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class MsPowerpointDocumentBackend(DeclarativeDocumentBackend, PaginatedDocumentBackend):
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)
        self.namespaces = {
            "a": "http://schemas.openxmlformats.org/drawingml/2006/main",
            "c": "http://schemas.openxmlformats.org/drawingml/2006/chart",
            "p": "http://schemas.openxmlformats.org/presentationml/2006/main",
        }
        # Powerpoint file:
        self.path_or_stream = path_or_stream

        self.pptx_obj = None
        self.valid = False
        try:
            if isinstance(self.path_or_stream, BytesIO):
                self.pptx_obj = Presentation(self.path_or_stream)
            elif isinstance(self.path_or_stream, Path):
                self.pptx_obj = Presentation(str(self.path_or_stream))

            self.valid = True
        except Exception as e:
            raise RuntimeError(
                f"MsPowerpointDocumentBackend could not load document with hash {self.document_hash}"
            ) from e

        return

    def page_count(self) -> int:
        if self.is_valid():
            assert self.pptx_obj is not None
            return len(self.pptx_obj.slides)
        else:
            return 0

    def is_valid(self) -> bool:
        return self.valid

    @classmethod
    def supports_pagination(cls) -> bool:
        return True  # True? if so, how to handle pages...

    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()

        self.path_or_stream = None

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.PPTX}

    def convert(self) -> DoclingDocument:
        # Parses the PPTX into a structured document model.
        # origin = DocumentOrigin(filename=self.path_or_stream.name, mimetype=next(iter(FormatToMimeType.get(InputFormat.PPTX))), binary_hash=self.document_hash)

        origin = DocumentOrigin(
            filename=self.file.name or "file",
            mimetype="application/vnd.ms-powerpoint",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(
            name=self.file.stem or "file", origin=origin
        )  # must add origin information
        doc = self.walk_linear(self.pptx_obj, doc)

        return doc

    def generate_prov(
        self, shape, slide_ind, text="", slide_size=Size(width=1, height=1)
    ):
        if shape.left:
            left = shape.left
            top = shape.top
            width = shape.width
            height = shape.height
        else:
            left = 0
            top = 0
            width = slide_size.width
            height = slide_size.height
        shape_bbox = [left, top, left + width, top + height]
        shape_bbox = BoundingBox.from_tuple(shape_bbox, origin=CoordOrigin.BOTTOMLEFT)
        prov = ProvenanceItem(
            page_no=slide_ind + 1, charspan=[0, len(text)], bbox=shape_bbox
        )

        return prov

    def handle_text_elements(self, shape, parent_slide, slide_ind, doc, slide_size):
        is_a_list = False
        is_list_group_created = False
        enum_list_item_value = 0
        new_list = None
        bullet_type = "None"
        list_text = ""
        list_label = GroupLabel.LIST
        doc_label = DocItemLabel.LIST_ITEM
        prov = self.generate_prov(shape, slide_ind, shape.text.strip(), slide_size)

        # Identify if shape contains lists
        for paragraph in shape.text_frame.paragraphs:
            # Check if paragraph is a bullet point using the `element` XML
            p = paragraph._element
            if (
                p.find(".//a:buChar", namespaces={"a": self.namespaces["a"]})
                is not None
            ):
                bullet_type = "Bullet"
                is_a_list = True
            elif (
                p.find(".//a:buAutoNum", namespaces={"a": self.namespaces["a"]})
                is not None
            ):
                bullet_type = "Numbered"
                is_a_list = True
            else:
                is_a_list = False

            if paragraph.level > 0:
                # Most likely a sub-list
                is_a_list = True

            if is_a_list:
                # Determine if this is an unordered list or an ordered list.
                # Set GroupLabel.ORDERED_LIST when it fits.
                if bullet_type == "Numbered":
                    list_label = GroupLabel.ORDERED_LIST

            if is_a_list:
                _log.debug("LIST DETECTED!")
            else:
                _log.debug("No List")

        # If there is a list inside of the shape, create a new docling list to assign list items to
        # if is_a_list:
        #     new_list = doc.add_group(
        #         label=list_label, name=f"list", parent=parent_slide
        #     )

        # Iterate through paragraphs to build up text
        for paragraph in shape.text_frame.paragraphs:
            # p_text = paragraph.text.strip()
            p = paragraph._element
            enum_list_item_value += 1
            inline_paragraph_text = ""
            inline_list_item_text = ""

            for e in p.iterfind(".//a:r", namespaces={"a": self.namespaces["a"]}):
                if len(e.text.strip()) > 0:
                    e_is_a_list_item = False
                    is_numbered = False
                    if (
                        p.find(".//a:buChar", namespaces={"a": self.namespaces["a"]})
                        is not None
                    ):
                        bullet_type = "Bullet"
                        e_is_a_list_item = True
                    elif (
                        p.find(".//a:buAutoNum", namespaces={"a": self.namespaces["a"]})
                        is not None
                    ):
                        bullet_type = "Numbered"
                        is_numbered = True
                        e_is_a_list_item = True
                    else:
                        e_is_a_list_item = False

                    if e_is_a_list_item:
                        if len(inline_paragraph_text) > 0:
                            # output accumulated inline text:
                            doc.add_text(
                                label=doc_label,
                                parent=parent_slide,
                                text=inline_paragraph_text,
                                prov=prov,
                            )
                        # Set marker and enumerated arguments if this is an enumeration element.
                        inline_list_item_text += e.text
                        # print(e.text)
                    else:
                        # Assign proper label to the text, depending if it's a Title or Section Header
                        # For other types of text, assign - PARAGRAPH
                        doc_label = DocItemLabel.PARAGRAPH
                        if shape.is_placeholder:
                            placeholder_type = shape.placeholder_format.type
                            if placeholder_type in [
                                PP_PLACEHOLDER.CENTER_TITLE,
                                PP_PLACEHOLDER.TITLE,
                            ]:
                                # It's a title
                                doc_label = DocItemLabel.TITLE
                            elif placeholder_type == PP_PLACEHOLDER.SUBTITLE:
                                DocItemLabel.SECTION_HEADER
                        enum_list_item_value = 0
                        inline_paragraph_text += e.text

            if len(inline_paragraph_text) > 0:
                # output accumulated inline text:
                doc.add_text(
                    label=doc_label,
                    parent=parent_slide,
                    text=inline_paragraph_text,
                    prov=prov,
                )

            if len(inline_list_item_text) > 0:
                enum_marker = ""
                if is_numbered:
                    enum_marker = str(enum_list_item_value) + "."
                if not is_list_group_created:
                    new_list = doc.add_group(
                        label=list_label, name=f"list", parent=parent_slide
                    )
                    is_list_group_created = True
                doc.add_list_item(
                    marker=enum_marker,
                    enumerated=is_numbered,
                    parent=new_list,
                    text=inline_list_item_text,
                    prov=prov,
                )
        return

    def handle_title(self, shape, parent_slide, slide_ind, doc):
        placeholder_type = shape.placeholder_format.type
        txt = shape.text.strip()
        prov = self.generate_prov(shape, slide_ind, txt)

        if len(txt.strip()) > 0:
            # title = slide.shapes.title.text if slide.shapes.title else "No title"
            if placeholder_type in [PP_PLACEHOLDER.CENTER_TITLE, PP_PLACEHOLDER.TITLE]:
                _log.info(f"Title found: {shape.text}")
                doc.add_text(
                    label=DocItemLabel.TITLE, parent=parent_slide, text=txt, prov=prov
                )
            elif placeholder_type == PP_PLACEHOLDER.SUBTITLE:
                _log.info(f"Subtitle found: {shape.text}")
                # Using DocItemLabel.FOOTNOTE, while SUBTITLE label is not avail.
                doc.add_text(
                    label=DocItemLabel.SECTION_HEADER,
                    parent=parent_slide,
                    text=txt,
                    prov=prov,
                )
        return

    def handle_pictures(self, shape, parent_slide, slide_ind, doc, slide_size):
        # Open it with PIL
        try:
            # Get the image bytes
            image = shape.image
            image_bytes = image.blob
            im_dpi, _ = image.dpi
            pil_image = Image.open(BytesIO(image_bytes))

            # shape has picture
            prov = self.generate_prov(shape, slide_ind, "", slide_size)
            doc.add_picture(
                parent=parent_slide,
                image=ImageRef.from_pil(image=pil_image, dpi=im_dpi),
                caption=None,
                prov=prov,
            )
        except (UnidentifiedImageError, OSError) as e:
            _log.warning(f"Warning: image cannot be loaded by Pillow: {e}")
        return

    def handle_tables(self, shape, parent_slide, slide_ind, doc, slide_size):
        # Handling tables, images, charts
        if shape.has_table:
            table = shape.table
            table_xml = shape._element

            prov = self.generate_prov(shape, slide_ind, "", slide_size)

            num_cols = 0
            num_rows = len(table.rows)
            tcells = []
            # Access the XML element for the shape that contains the table
            table_xml = shape._element

            for row_idx, row in enumerate(table.rows):
                if len(row.cells) > num_cols:
                    num_cols = len(row.cells)
                for col_idx, cell in enumerate(row.cells):
                    # Access the XML of the cell (this is the 'tc' element in table XML)
                    cell_xml = table_xml.xpath(
                        f".//a:tbl/a:tr[{row_idx + 1}]/a:tc[{col_idx + 1}]"
                    )

                    if not cell_xml:
                        continue  # If no cell XML is found, skip

                    cell_xml = cell_xml[0]  # Get the first matching XML node
                    row_span = cell_xml.get("rowSpan")  # Vertical span
                    col_span = cell_xml.get("gridSpan")  # Horizontal span

                    if row_span is None:
                        row_span = 1
                    else:
                        row_span = int(row_span)

                    if col_span is None:
                        col_span = 1
                    else:
                        col_span = int(col_span)

                    icell = TableCell(
                        text=cell.text.strip(),
                        row_span=row_span,
                        col_span=col_span,
                        start_row_offset_idx=row_idx,
                        end_row_offset_idx=row_idx + row_span,
                        start_col_offset_idx=col_idx,
                        end_col_offset_idx=col_idx + col_span,
                        col_header=False,
                        row_header=False,
                    )
                    if len(cell.text.strip()) > 0:
                        tcells.append(icell)
            # Initialize Docling TableData
            data = TableData(num_rows=num_rows, num_cols=num_cols, table_cells=[])
            # Populate
            for tcell in tcells:
                data.table_cells.append(tcell)
            if len(tcells) > 0:
                # If table is not fully empty...
                # Create Docling table
                doc.add_table(parent=parent_slide, data=data, prov=prov)
        return

    def walk_linear(self, pptx_obj, doc) -> DoclingDocument:
        # Units of size in PPTX by default are EMU units (English Metric Units)
        slide_width = pptx_obj.slide_width
        slide_height = pptx_obj.slide_height

        text_content = []  # type: ignore

        max_levels = 10
        parents = {}  # type: ignore
        for i in range(0, max_levels):
            parents[i] = None

        # Loop through each slide
        for slide_num, slide in enumerate(pptx_obj.slides):
            slide_ind = pptx_obj.slides.index(slide)
            parent_slide = doc.add_group(
                name=f"slide-{slide_ind}", label=GroupLabel.CHAPTER, parent=parents[0]
            )

            slide_size = Size(width=slide_width, height=slide_height)
            parent_page = doc.add_page(page_no=slide_ind + 1, size=slide_size)

            def handle_shapes(shape, parent_slide, slide_ind, doc, slide_size):
                handle_groups(shape, parent_slide, slide_ind, doc, slide_size)
                if shape.has_table:
                    # Handle Tables
                    self.handle_tables(shape, parent_slide, slide_ind, doc, slide_size)
                if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                    # Handle Pictures
                    self.handle_pictures(
                        shape, parent_slide, slide_ind, doc, slide_size
                    )
                # If shape doesn't have any text, move on to the next shape
                if not hasattr(shape, "text"):
                    return
                if shape.text is None:
                    return
                if len(shape.text.strip()) == 0:
                    return
                if not shape.has_text_frame:
                    _log.warning("Warning: shape has text but not text_frame")
                    return
                # Handle other text elements, including lists (bullet lists, numbered lists)
                self.handle_text_elements(
                    shape, parent_slide, slide_ind, doc, slide_size
                )
                return

            def handle_groups(shape, parent_slide, slide_ind, doc, slide_size):
                if shape.shape_type == MSO_SHAPE_TYPE.GROUP:
                    for groupedshape in shape.shapes:
                        handle_shapes(
                            groupedshape, parent_slide, slide_ind, doc, slide_size
                        )

            # Loop through each shape in the slide
            for shape in slide.shapes:
                handle_shapes(shape, parent_slide, slide_ind, doc, slide_size)

        return doc


================================================
File: docling/backend/msword_backend.py
================================================
import logging
import re
from io import BytesIO
from pathlib import Path
from typing import Any, Optional, Union

from docling_core.types.doc import (
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    ImageRef,
    NodeItem,
    TableCell,
    TableData,
)
from docx import Document
from docx.document import Document as DocxDocument
from docx.oxml.table import CT_Tc
from docx.oxml.xmlchemy import BaseOxmlElement
from docx.table import Table, _Cell
from docx.text.paragraph import Paragraph
from lxml import etree
from lxml.etree import XPath
from PIL import Image, UnidentifiedImageError
from typing_extensions import override

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class MsWordDocumentBackend(DeclarativeDocumentBackend):
    @override
    def __init__(
        self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]
    ) -> None:
        super().__init__(in_doc, path_or_stream)
        self.XML_KEY = (
            "{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val"
        )
        self.xml_namespaces = {
            "w": "http://schemas.microsoft.com/office/word/2003/wordml"
        }
        # self.initialise(path_or_stream)
        # Word file:
        self.path_or_stream: Union[BytesIO, Path] = path_or_stream
        self.valid: bool = False
        # Initialise the parents for the hierarchy
        self.max_levels: int = 10
        self.level_at_new_list: Optional[int] = None
        self.parents: dict[int, Optional[NodeItem]] = {}
        for i in range(-1, self.max_levels):
            self.parents[i] = None

        self.level = 0
        self.listIter = 0

        self.history: dict[str, Any] = {
            "names": [None],
            "levels": [None],
            "numids": [None],
            "indents": [None],
        }

        self.docx_obj = None
        try:
            if isinstance(self.path_or_stream, BytesIO):
                self.docx_obj = Document(self.path_or_stream)
            elif isinstance(self.path_or_stream, Path):
                self.docx_obj = Document(str(self.path_or_stream))

            self.valid = True
        except Exception as e:
            raise RuntimeError(
                f"MsPowerpointDocumentBackend could not load document with hash {self.document_hash}"
            ) from e

    @override
    def is_valid(self) -> bool:
        return self.valid

    @classmethod
    @override
    def supports_pagination(cls) -> bool:
        return False

    @override
    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()

        self.path_or_stream = None

    @classmethod
    @override
    def supported_formats(cls) -> set[InputFormat]:
        return {InputFormat.DOCX}

    @override
    def convert(self) -> DoclingDocument:
        """Parses the DOCX into a structured document model.

        Returns:
            The parsed document.
        """

        origin = DocumentOrigin(
            filename=self.file.name or "file",
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            binary_hash=self.document_hash,
        )

        doc = DoclingDocument(name=self.file.stem or "file", origin=origin)
        if self.is_valid():
            assert self.docx_obj is not None
            doc = self.walk_linear(self.docx_obj.element.body, self.docx_obj, doc)
            return doc
        else:
            raise RuntimeError(
                f"Cannot convert doc with {self.document_hash} because the backend failed to init."
            )

    def update_history(
        self,
        name: str,
        level: Optional[int],
        numid: Optional[int],
        ilevel: Optional[int],
    ):
        self.history["names"].append(name)
        self.history["levels"].append(level)

        self.history["numids"].append(numid)
        self.history["indents"].append(ilevel)

    def prev_name(self) -> Optional[str]:
        return self.history["names"][-1]

    def prev_level(self) -> Optional[int]:
        return self.history["levels"][-1]

    def prev_numid(self) -> Optional[int]:
        return self.history["numids"][-1]

    def prev_indent(self) -> Optional[int]:
        return self.history["indents"][-1]

    def get_level(self) -> int:
        """Return the first None index."""
        for k, v in self.parents.items():
            if k >= 0 and v == None:
                return k
        return 0

    def walk_linear(
        self,
        body: BaseOxmlElement,
        docx_obj: DocxDocument,
        doc: DoclingDocument,
    ) -> DoclingDocument:
        for element in body:
            tag_name = etree.QName(element).localname
            # Check for Inline Images (blip elements)
            namespaces = {
                "a": "http://schemas.openxmlformats.org/drawingml/2006/main",
                "r": "http://schemas.openxmlformats.org/officeDocument/2006/relationships",
                "w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main",
            }
            xpath_expr = XPath(".//a:blip", namespaces=namespaces)
            drawing_blip = xpath_expr(element)

            # Check for Tables
            if element.tag.endswith("tbl"):
                try:
                    self.handle_tables(element, docx_obj, doc)
                except Exception:
                    _log.debug("could not parse a table, broken docx table")

            elif drawing_blip:
                self.handle_pictures(docx_obj, drawing_blip, doc)
            # Check for the sdt containers, like table of contents
            elif tag_name in ["sdt"]:
                sdt_content = element.find(".//w:sdtContent", namespaces=namespaces)
                if sdt_content is not None:
                    # Iterate paragraphs, runs, or text inside <w:sdtContent>.
                    paragraphs = sdt_content.findall(".//w:p", namespaces=namespaces)
                    for p in paragraphs:
                        self.handle_text_elements(p, docx_obj, doc)
            # Check for Text
            elif tag_name in ["p"]:
                # "tcPr", "sectPr"
                self.handle_text_elements(element, docx_obj, doc)
            else:
                _log.debug(f"Ignoring element in DOCX with tag: {tag_name}")
        return doc

    def str_to_int(self, s: Optional[str], default: Optional[int] = 0) -> Optional[int]:
        if s is None:
            return None
        try:
            return int(s)
        except ValueError:
            return default

    def split_text_and_number(self, input_string: str) -> list[str]:
        match = re.match(r"(\D+)(\d+)$|^(\d+)(\D+)", input_string)
        if match:
            parts = list(filter(None, match.groups()))
            return parts
        else:
            return [input_string]

    def get_numId_and_ilvl(
        self, paragraph: Paragraph
    ) -> tuple[Optional[int], Optional[int]]:
        # Access the XML element of the paragraph
        numPr = paragraph._element.find(
            ".//w:numPr", namespaces=paragraph._element.nsmap
        )

        if numPr is not None:
            # Get the numId element and extract the value
            numId_elem = numPr.find("w:numId", namespaces=paragraph._element.nsmap)
            ilvl_elem = numPr.find("w:ilvl", namespaces=paragraph._element.nsmap)
            numId = numId_elem.get(self.XML_KEY) if numId_elem is not None else None
            ilvl = ilvl_elem.get(self.XML_KEY) if ilvl_elem is not None else None

            return self.str_to_int(numId, None), self.str_to_int(ilvl, None)

        return None, None  # If the paragraph is not part of a list

    def get_label_and_level(self, paragraph: Paragraph) -> tuple[str, Optional[int]]:
        if paragraph.style is None:
            return "Normal", None
        label = paragraph.style.style_id
        if label is None:
            return "Normal", None
        if ":" in label:
            parts = label.split(":")

            if len(parts) == 2:
                return parts[0], self.str_to_int(parts[1], None)

        parts = self.split_text_and_number(label)

        if "Heading" in label and len(parts) == 2:
            parts.sort()
            label_str: str = ""
            label_level: Optional[int] = 0
            if parts[0] == "Heading":
                label_str = parts[0]
                label_level = self.str_to_int(parts[1], None)
            if parts[1] == "Heading":
                label_str = parts[1]
                label_level = self.str_to_int(parts[0], None)
            return label_str, label_level
        else:
            return label, None

    def handle_text_elements(
        self,
        element: BaseOxmlElement,
        docx_obj: DocxDocument,
        doc: DoclingDocument,
    ) -> None:
        paragraph = Paragraph(element, docx_obj)

        if paragraph.text is None:
            return
        text = paragraph.text.strip()

        # Common styles for bullet and numbered lists.
        # "List Bullet", "List Number", "List Paragraph"
        # Identify wether list is a numbered list or not
        # is_numbered = "List Bullet" not in paragraph.style.name
        is_numbered = False
        p_style_id, p_level = self.get_label_and_level(paragraph)
        numid, ilevel = self.get_numId_and_ilvl(paragraph)

        if numid == 0:
            numid = None

        # Handle lists
        if (
            numid is not None
            and ilevel is not None
            and p_style_id not in ["Title", "Heading"]
        ):
            self.add_listitem(
                doc,
                numid,
                ilevel,
                text,
                is_numbered,
            )
            self.update_history(p_style_id, p_level, numid, ilevel)
            return
        elif (
            numid is None
            and self.prev_numid() is not None
            and p_style_id not in ["Title", "Heading"]
        ):  # Close list
            if self.level_at_new_list:
                for key in range(len(self.parents)):
                    if key >= self.level_at_new_list:
                        self.parents[key] = None
                self.level = self.level_at_new_list - 1
                self.level_at_new_list = None
            else:
                for key in range(len(self.parents)):
                    self.parents[key] = None
                self.level = 0

        if p_style_id in ["Title"]:
            for key in range(len(self.parents)):
                self.parents[key] = None
            self.parents[0] = doc.add_text(
                parent=None, label=DocItemLabel.TITLE, text=text
            )
        elif "Heading" in p_style_id:
            self.add_header(doc, p_level, text)

        elif p_style_id in [
            "Paragraph",
            "Normal",
            "Subtitle",
            "Author",
            "DefaultText",
            "ListParagraph",
            "ListBullet",
            "Quote",
        ]:
            level = self.get_level()
            doc.add_text(
                label=DocItemLabel.PARAGRAPH, parent=self.parents[level - 1], text=text
            )

        else:
            # Text style names can, and will have, not only default values but user values too
            # hence we treat all other labels as pure text
            level = self.get_level()
            doc.add_text(
                label=DocItemLabel.PARAGRAPH, parent=self.parents[level - 1], text=text
            )

        self.update_history(p_style_id, p_level, numid, ilevel)
        return

    def add_header(
        self, doc: DoclingDocument, curr_level: Optional[int], text: str
    ) -> None:
        level = self.get_level()
        if isinstance(curr_level, int):
            if curr_level > level:
                # add invisible group
                for i in range(level, curr_level):
                    self.parents[i] = doc.add_group(
                        parent=self.parents[i - 1],
                        label=GroupLabel.SECTION,
                        name=f"header-{i}",
                    )
            elif curr_level < level:
                # remove the tail
                for key in range(len(self.parents)):
                    if key >= curr_level:
                        self.parents[key] = None

            self.parents[curr_level] = doc.add_heading(
                parent=self.parents[curr_level - 1],
                text=text,
                level=curr_level,
            )
        else:
            self.parents[self.level] = doc.add_heading(
                parent=self.parents[self.level - 1],
                text=text,
                level=1,
            )
        return

    def add_listitem(
        self,
        doc: DoclingDocument,
        numid: int,
        ilevel: int,
        text: str,
        is_numbered: bool = False,
    ) -> None:
        enum_marker = ""

        level = self.get_level()
        prev_indent = self.prev_indent()
        if self.prev_numid() is None:  # Open new list
            self.level_at_new_list = level

            self.parents[level] = doc.add_group(
                label=GroupLabel.LIST, name="list", parent=self.parents[level - 1]
            )

            # Set marker and enumerated arguments if this is an enumeration element.
            self.listIter += 1
            if is_numbered:
                enum_marker = str(self.listIter) + "."
                is_numbered = True
            doc.add_list_item(
                marker=enum_marker,
                enumerated=is_numbered,
                parent=self.parents[level],
                text=text,
            )

        elif (
            self.prev_numid() == numid
            and self.level_at_new_list is not None
            and prev_indent is not None
            and prev_indent < ilevel
        ):  # Open indented list
            for i in range(
                self.level_at_new_list + prev_indent + 1,
                self.level_at_new_list + ilevel + 1,
            ):
                # Determine if this is an unordered list or an ordered list.
                # Set GroupLabel.ORDERED_LIST when it fits.
                self.listIter = 0
                if is_numbered:
                    self.parents[i] = doc.add_group(
                        label=GroupLabel.ORDERED_LIST,
                        name="list",
                        parent=self.parents[i - 1],
                    )
                else:
                    self.parents[i] = doc.add_group(
                        label=GroupLabel.LIST, name="list", parent=self.parents[i - 1]
                    )

            # TODO: Set marker and enumerated arguments if this is an enumeration element.
            self.listIter += 1
            if is_numbered:
                enum_marker = str(self.listIter) + "."
                is_numbered = True
            doc.add_list_item(
                marker=enum_marker,
                enumerated=is_numbered,
                parent=self.parents[self.level_at_new_list + ilevel],
                text=text,
            )

        elif (
            self.prev_numid() == numid
            and self.level_at_new_list is not None
            and prev_indent is not None
            and ilevel < prev_indent
        ):  # Close list
            for k, v in self.parents.items():
                if k > self.level_at_new_list + ilevel:
                    self.parents[k] = None

            # TODO: Set marker and enumerated arguments if this is an enumeration element.
            self.listIter += 1
            if is_numbered:
                enum_marker = str(self.listIter) + "."
                is_numbered = True
            doc.add_list_item(
                marker=enum_marker,
                enumerated=is_numbered,
                parent=self.parents[self.level_at_new_list + ilevel],
                text=text,
            )
            self.listIter = 0

        elif self.prev_numid() == numid or prev_indent == ilevel:
            # TODO: Set marker and enumerated arguments if this is an enumeration element.
            self.listIter += 1
            if is_numbered:
                enum_marker = str(self.listIter) + "."
                is_numbered = True
            doc.add_list_item(
                marker=enum_marker,
                enumerated=is_numbered,
                parent=self.parents[level - 1],
                text=text,
            )
        return

    def handle_tables(
        self,
        element: BaseOxmlElement,
        docx_obj: DocxDocument,
        doc: DoclingDocument,
    ) -> None:
        table: Table = Table(element, docx_obj)
        num_rows = len(table.rows)
        num_cols = len(table.columns)
        _log.debug(f"Table grid with {num_rows} rows and {num_cols} columns")

        if num_rows == 1 and num_cols == 1:
            cell_element = table.rows[0].cells[0]
            # In case we have a table of only 1 cell, we consider it furniture
            # And proceed processing the content of the cell as though it's in the document body
            self.walk_linear(cell_element._element, docx_obj, doc)
            return

        data = TableData(num_rows=num_rows, num_cols=num_cols)
        cell_set: set[CT_Tc] = set()
        for row_idx, row in enumerate(table.rows):
            _log.debug(f"Row index {row_idx} with {len(row.cells)} populated cells")
            col_idx = 0
            while col_idx < num_cols:
                cell: _Cell = row.cells[col_idx]
                _log.debug(
                    f" col {col_idx} grid_span {cell.grid_span} grid_cols_before {row.grid_cols_before}"
                )
                if cell is None or cell._tc in cell_set:
                    _log.debug(f"  skipped since repeated content")
                    col_idx += cell.grid_span
                    continue
                else:
                    cell_set.add(cell._tc)

                spanned_idx = row_idx
                spanned_tc: Optional[CT_Tc] = cell._tc
                while spanned_tc == cell._tc:
                    spanned_idx += 1
                    spanned_tc = (
                        table.rows[spanned_idx].cells[col_idx]._tc
                        if spanned_idx < num_rows
                        else None
                    )
                _log.debug(f"  spanned before row {spanned_idx}")

                table_cell = TableCell(
                    text=cell.text,
                    row_span=spanned_idx - row_idx,
                    col_span=cell.grid_span,
                    start_row_offset_idx=row.grid_cols_before + row_idx,
                    end_row_offset_idx=row.grid_cols_before + spanned_idx,
                    start_col_offset_idx=col_idx,
                    end_col_offset_idx=col_idx + cell.grid_span,
                    col_header=False,
                    row_header=False,
                )
                data.table_cells.append(table_cell)
                col_idx += cell.grid_span

        level = self.get_level()
        doc.add_table(data=data, parent=self.parents[level - 1])
        return

    def handle_pictures(
        self, docx_obj: DocxDocument, drawing_blip: Any, doc: DoclingDocument
    ) -> None:
        def get_docx_image(drawing_blip):
            rId = drawing_blip[0].get(
                "{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed"
            )
            if rId in docx_obj.part.rels:
                # Access the image part using the relationship ID
                image_part = docx_obj.part.rels[rId].target_part
                image_data = image_part.blob  # Get the binary image data
            return image_data

        level = self.get_level()
        # Open the BytesIO object with PIL to create an Image
        try:
            image_data = get_docx_image(drawing_blip)
            image_bytes = BytesIO(image_data)
            pil_image = Image.open(image_bytes)
            doc.add_picture(
                parent=self.parents[level - 1],
                image=ImageRef.from_pil(image=pil_image, dpi=72),
                caption=None,
            )
        except (UnidentifiedImageError, OSError) as e:
            _log.warning("Warning: image cannot be loaded by Pillow")
            doc.add_picture(
                parent=self.parents[level - 1],
                caption=None,
            )
        return


================================================
File: docling/backend/pdf_backend.py
================================================
from abc import ABC, abstractmethod
from io import BytesIO
from pathlib import Path
from typing import Iterable, Optional, Set, Union

from docling_core.types.doc import BoundingBox, Size
from PIL import Image

from docling.backend.abstract_backend import PaginatedDocumentBackend
from docling.datamodel.base_models import Cell, InputFormat
from docling.datamodel.document import InputDocument


class PdfPageBackend(ABC):
    @abstractmethod
    def get_text_in_rect(self, bbox: BoundingBox) -> str:
        pass

    @abstractmethod
    def get_text_cells(self) -> Iterable[Cell]:
        pass

    @abstractmethod
    def get_bitmap_rects(self, float: int = 1) -> Iterable[BoundingBox]:
        pass

    @abstractmethod
    def get_page_image(
        self, scale: float = 1, cropbox: Optional[BoundingBox] = None
    ) -> Image.Image:
        pass

    @abstractmethod
    def get_size(self) -> Size:
        pass

    @abstractmethod
    def is_valid(self) -> bool:
        pass

    @abstractmethod
    def unload(self):
        pass


class PdfDocumentBackend(PaginatedDocumentBackend):
    def __init__(self, in_doc: InputDocument, path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        if self.input_format is not InputFormat.PDF:
            if self.input_format is InputFormat.IMAGE:
                buf = BytesIO()
                img = Image.open(self.path_or_stream)
                img.save(buf, "PDF")
                buf.seek(0)
                self.path_or_stream = buf
            else:
                raise RuntimeError(
                    f"Incompatible file format {self.input_format} was passed to a PdfDocumentBackend."
                )

    @abstractmethod
    def load_page(self, page_no: int) -> PdfPageBackend:
        pass

    @abstractmethod
    def page_count(self) -> int:
        pass

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return {InputFormat.PDF}

    @classmethod
    def supports_pagination(cls) -> bool:
        return True


================================================
File: docling/backend/pypdfium2_backend.py
================================================
import logging
import random
from io import BytesIO
from pathlib import Path
from typing import TYPE_CHECKING, Iterable, List, Optional, Union

import pypdfium2 as pdfium
import pypdfium2.raw as pdfium_c
from docling_core.types.doc import BoundingBox, CoordOrigin, Size
from PIL import Image, ImageDraw
from pypdfium2 import PdfTextPage
from pypdfium2._helpers.misc import PdfiumError

from docling.backend.pdf_backend import PdfDocumentBackend, PdfPageBackend
from docling.datamodel.base_models import Cell

if TYPE_CHECKING:
    from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


class PyPdfiumPageBackend(PdfPageBackend):
    def __init__(
        self, pdfium_doc: pdfium.PdfDocument, document_hash: str, page_no: int
    ):
        self.valid = True  # No better way to tell from pypdfium.
        try:
            self._ppage: pdfium.PdfPage = pdfium_doc[page_no]
        except PdfiumError as e:
            _log.info(
                f"An exception occurred when loading page {page_no} of document {document_hash}.",
                exc_info=True,
            )
            self.valid = False
        self.text_page: Optional[PdfTextPage] = None

    def is_valid(self) -> bool:
        return self.valid

    def get_bitmap_rects(self, scale: float = 1) -> Iterable[BoundingBox]:
        AREA_THRESHOLD = 0  # 32 * 32
        for obj in self._ppage.get_objects(filter=[pdfium_c.FPDF_PAGEOBJ_IMAGE]):
            pos = obj.get_pos()
            cropbox = BoundingBox.from_tuple(
                pos, origin=CoordOrigin.BOTTOMLEFT
            ).to_top_left_origin(page_height=self.get_size().height)

            if cropbox.area() > AREA_THRESHOLD:
                cropbox = cropbox.scaled(scale=scale)

                yield cropbox

    def get_text_in_rect(self, bbox: BoundingBox) -> str:
        if not self.text_page:
            self.text_page = self._ppage.get_textpage()

        if bbox.coord_origin != CoordOrigin.BOTTOMLEFT:
            bbox = bbox.to_bottom_left_origin(self.get_size().height)

        text_piece = self.text_page.get_text_bounded(*bbox.as_tuple())

        return text_piece

    def get_text_cells(self) -> Iterable[Cell]:
        if not self.text_page:
            self.text_page = self._ppage.get_textpage()

        cells = []
        cell_counter = 0

        page_size = self.get_size()

        for i in range(self.text_page.count_rects()):
            rect = self.text_page.get_rect(i)
            text_piece = self.text_page.get_text_bounded(*rect)
            x0, y0, x1, y1 = rect
            cells.append(
                Cell(
                    id=cell_counter,
                    text=text_piece,
                    bbox=BoundingBox(
                        l=x0, b=y0, r=x1, t=y1, coord_origin=CoordOrigin.BOTTOMLEFT
                    ).to_top_left_origin(page_size.height),
                )
            )
            cell_counter += 1

        # PyPdfium2 produces very fragmented cells, with sub-word level boundaries, in many PDFs.
        # The cell merging code below is to clean this up.
        def merge_horizontal_cells(
            cells: List[Cell],
            horizontal_threshold_factor: float = 1.0,
            vertical_threshold_factor: float = 0.5,
        ) -> List[Cell]:
            if not cells:
                return []

            def group_rows(cells: List[Cell]) -> List[List[Cell]]:
                rows = []
                current_row = [cells[0]]
                row_top = cells[0].bbox.t
                row_bottom = cells[0].bbox.b
                row_height = cells[0].bbox.height

                for cell in cells[1:]:
                    vertical_threshold = row_height * vertical_threshold_factor
                    if (
                        abs(cell.bbox.t - row_top) <= vertical_threshold
                        and abs(cell.bbox.b - row_bottom) <= vertical_threshold
                    ):
                        current_row.append(cell)
                        row_top = min(row_top, cell.bbox.t)
                        row_bottom = max(row_bottom, cell.bbox.b)
                        row_height = row_bottom - row_top
                    else:
                        rows.append(current_row)
                        current_row = [cell]
                        row_top = cell.bbox.t
                        row_bottom = cell.bbox.b
                        row_height = cell.bbox.height

                if current_row:
                    rows.append(current_row)

                return rows

            def merge_row(row: List[Cell]) -> List[Cell]:
                merged = []
                current_group = [row[0]]

                for cell in row[1:]:
                    prev_cell = current_group[-1]
                    avg_height = (prev_cell.bbox.height + cell.bbox.height) / 2
                    if (
                        cell.bbox.l - prev_cell.bbox.r
                        <= avg_height * horizontal_threshold_factor
                    ):
                        current_group.append(cell)
                    else:
                        merged.append(merge_group(current_group))
                        current_group = [cell]

                if current_group:
                    merged.append(merge_group(current_group))

                return merged

            def merge_group(group: List[Cell]) -> Cell:
                if len(group) == 1:
                    return group[0]

                merged_text = "".join(cell.text for cell in group)
                merged_bbox = BoundingBox(
                    l=min(cell.bbox.l for cell in group),
                    t=min(cell.bbox.t for cell in group),
                    r=max(cell.bbox.r for cell in group),
                    b=max(cell.bbox.b for cell in group),
                )
                return Cell(id=group[0].id, text=merged_text, bbox=merged_bbox)

            rows = group_rows(cells)
            merged_cells = [cell for row in rows for cell in merge_row(row)]

            for i, cell in enumerate(merged_cells, 1):
                cell.id = i

            return merged_cells

        def draw_clusters_and_cells():
            image = (
                self.get_page_image()
            )  # make new image to avoid drawing on the saved ones
            draw = ImageDraw.Draw(image)
            for c in cells:
                x0, y0, x1, y1 = c.bbox.as_tuple()
                cell_color = (
                    random.randint(30, 140),
                    random.randint(30, 140),
                    random.randint(30, 140),
                )
                draw.rectangle([(x0, y0), (x1, y1)], outline=cell_color)
            image.show()

        # before merge:
        # draw_clusters_and_cells()

        cells = merge_horizontal_cells(cells)

        # after merge:
        # draw_clusters_and_cells()

        return cells

    def get_page_image(
        self, scale: float = 1, cropbox: Optional[BoundingBox] = None
    ) -> Image.Image:

        page_size = self.get_size()

        if not cropbox:
            cropbox = BoundingBox(
                l=0,
                r=page_size.width,
                t=0,
                b=page_size.height,
                coord_origin=CoordOrigin.TOPLEFT,
            )
            padbox = BoundingBox(
                l=0, r=0, t=0, b=0, coord_origin=CoordOrigin.BOTTOMLEFT
            )
        else:
            padbox = cropbox.to_bottom_left_origin(page_size.height).model_copy()
            padbox.r = page_size.width - padbox.r
            padbox.t = page_size.height - padbox.t

        image = (
            self._ppage.render(
                scale=scale * 1.5,
                rotation=0,  # no additional rotation
                crop=padbox.as_tuple(),
            )
            .to_pil()
            .resize(size=(round(cropbox.width * scale), round(cropbox.height * scale)))
        )  # We resize the image from 1.5x the given scale to make it sharper.

        return image

    def get_size(self) -> Size:
        return Size(width=self._ppage.get_width(), height=self._ppage.get_height())

    def unload(self):
        self._ppage = None
        self.text_page = None


class PyPdfiumDocumentBackend(PdfDocumentBackend):
    def __init__(self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]):
        super().__init__(in_doc, path_or_stream)

        try:
            self._pdoc = pdfium.PdfDocument(self.path_or_stream)
        except PdfiumError as e:
            raise RuntimeError(
                f"pypdfium could not load document with hash {self.document_hash}"
            ) from e

    def page_count(self) -> int:
        return len(self._pdoc)

    def load_page(self, page_no: int) -> PyPdfiumPageBackend:
        return PyPdfiumPageBackend(self._pdoc, self.document_hash, page_no)

    def is_valid(self) -> bool:
        return self.page_count() > 0

    def unload(self):
        super().unload()
        self._pdoc.close()
        self._pdoc = None


================================================
File: docling/backend/json/docling_json_backend.py
================================================
from io import BytesIO
from pathlib import Path
from typing import Union

from docling_core.types.doc import DoclingDocument
from typing_extensions import override

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument


class DoclingJSONBackend(DeclarativeDocumentBackend):
    @override
    def __init__(
        self, in_doc: InputDocument, path_or_stream: Union[BytesIO, Path]
    ) -> None:
        super().__init__(in_doc, path_or_stream)

        # given we need to store any actual conversion exception for raising it from
        # convert(), this captures the successful result or the actual error in a
        # mutually exclusive way:
        self._doc_or_err = self._get_doc_or_err()

    @override
    def is_valid(self) -> bool:
        return isinstance(self._doc_or_err, DoclingDocument)

    @classmethod
    @override
    def supports_pagination(cls) -> bool:
        return False

    @classmethod
    @override
    def supported_formats(cls) -> set[InputFormat]:
        return {InputFormat.JSON_DOCLING}

    def _get_doc_or_err(self) -> Union[DoclingDocument, Exception]:
        try:
            json_data: Union[str, bytes]
            if isinstance(self.path_or_stream, Path):
                with open(self.path_or_stream, encoding="utf-8") as f:
                    json_data = f.read()
            elif isinstance(self.path_or_stream, BytesIO):
                json_data = self.path_or_stream.getvalue()
            else:
                raise RuntimeError(f"Unexpected: {type(self.path_or_stream)=}")
            return DoclingDocument.model_validate_json(json_data=json_data)
        except Exception as e:
            return e

    @override
    def convert(self) -> DoclingDocument:
        if isinstance(self._doc_or_err, DoclingDocument):
            return self._doc_or_err
        else:
            raise self._doc_or_err


================================================
File: docling/backend/xml/jats_backend.py
================================================
import logging
import traceback
from io import BytesIO
from pathlib import Path
from typing import Final, Optional, Union

from bs4 import BeautifulSoup, Tag
from docling_core.types.doc import (
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupItem,
    GroupLabel,
    NodeItem,
    TextItem,
)
from lxml import etree
from typing_extensions import TypedDict, override

from docling.backend.abstract_backend import DeclarativeDocumentBackend
from docling.backend.html_backend import HTMLDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)

JATS_DTD_URL: Final = ["JATS-journalpublishing", "JATS-archive"]
DEFAULT_HEADER_ACKNOWLEDGMENTS: Final = "Acknowledgments"
DEFAULT_HEADER_ABSTRACT: Final = "Abstract"
DEFAULT_HEADER_REFERENCES: Final = "References"
DEFAULT_TEXT_ETAL: Final = "et al."


class Abstract(TypedDict):
    label: str
    content: str


class Author(TypedDict):
    name: str
    affiliation_names: list[str]


class Citation(TypedDict):
    author_names: str
    title: str
    source: str
    year: str
    volume: str
    page: str
    pub_id: str
    publisher_name: str
    publisher_loc: str


class Table(TypedDict):
    label: str
    caption: str
    content: str


class XMLComponents(TypedDict):
    title: str
    authors: list[Author]
    abstract: list[Abstract]


class JatsDocumentBackend(DeclarativeDocumentBackend):
    """Backend to parse articles in XML format tagged according to JATS definition.

    The Journal Article Tag Suite (JATS) is an definition standard for the
    representation of journal articles in XML format. Several publishers and journal
    archives provide content in JATS format, including PubMed Central® (PMC), bioRxiv,
    medRxiv, or Springer Nature.

    Refer to https://jats.nlm.nih.gov for more details on JATS.

    The code from this document backend has been developed by modifying parts of the
    PubMed Parser library (version 0.5.0, released on 12.08.2024):
    Achakulvisut et al., (2020).
    Pubmed Parser: A Python Parser for PubMed Open-Access XML Subset and MEDLINE XML
      Dataset XML Dataset.
    Journal of Open Source Software, 5(46), 1979,
    https://doi.org/10.21105/joss.01979
    """

    @override
    def __init__(
        self, in_doc: "InputDocument", path_or_stream: Union[BytesIO, Path]
    ) -> None:
        super().__init__(in_doc, path_or_stream)
        self.path_or_stream = path_or_stream

        # Initialize the root of the document hiearchy
        self.root: Optional[NodeItem] = None

        self.valid = False
        try:
            if isinstance(self.path_or_stream, BytesIO):
                self.path_or_stream.seek(0)
            self.tree: etree._ElementTree = etree.parse(self.path_or_stream)

            doc_info: etree.DocInfo = self.tree.docinfo
            if doc_info.system_url and any(
                [kwd in doc_info.system_url for kwd in JATS_DTD_URL]
            ):
                self.valid = True
                return
            for ent in doc_info.internalDTD.iterentities():
                if ent.system_url and any(
                    [kwd in ent.system_url for kwd in JATS_DTD_URL]
                ):
                    self.valid = True
                    return
        except Exception as exc:
            raise RuntimeError(
                f"Could not initialize JATS backend for file with hash {self.document_hash}."
            ) from exc

    @override
    def is_valid(self) -> bool:
        return self.valid

    @classmethod
    @override
    def supports_pagination(cls) -> bool:
        return False

    @override
    def unload(self):
        if isinstance(self.path_or_stream, BytesIO):
            self.path_or_stream.close()
        self.path_or_stream = None

    @classmethod
    @override
    def supported_formats(cls) -> set[InputFormat]:
        return {InputFormat.XML_JATS}

    @override
    def convert(self) -> DoclingDocument:
        try:
            # Create empty document
            origin = DocumentOrigin(
                filename=self.file.name or "file",
                mimetype="application/xml",
                binary_hash=self.document_hash,
            )
            doc = DoclingDocument(name=self.file.stem or "file", origin=origin)

            # Get metadata XML components
            xml_components: XMLComponents = self._parse_metadata()

            # Add metadata to the document
            self._add_metadata(doc, xml_components)

            # walk over the XML body
            body = self.tree.xpath("//body")
            if self.root and len(body) > 0:
                self._walk_linear(doc, self.root, body[0])

            # walk over the XML back matter
            back = self.tree.xpath("//back")
            if self.root and len(back) > 0:
                self._walk_linear(doc, self.root, back[0])
        except Exception:
            _log.error(traceback.format_exc())

        return doc

    @staticmethod
    def _get_text(node: etree._Element, sep: Optional[str] = None) -> str:
        skip_tags = ["term", "disp-formula", "inline-formula"]
        text: str = (
            node.text.replace("\n", " ")
            if (node.tag not in skip_tags and node.text)
            else ""
        )
        for child in list(node):
            if child.tag not in skip_tags:
                # TODO: apply styling according to child.tag when supported by docling-core
                text += JatsDocumentBackend._get_text(child, sep)
            if sep:
                text = text.rstrip(sep) + sep
            text += child.tail.replace("\n", " ") if child.tail else ""

        return text

    def _find_metadata(self) -> Optional[etree._Element]:
        meta_names: list[str] = ["article-meta", "book-part-meta"]
        meta: Optional[etree._Element] = None
        for name in meta_names:
            node = self.tree.xpath(f".//{name}")
            if len(node) > 0:
                meta = node[0]
                break

        return meta

    def _parse_abstract(self) -> list[Abstract]:
        # TODO: address cases with multiple sections
        abs_list: list[Abstract] = []

        for abs_node in self.tree.xpath(".//abstract"):
            abstract: Abstract = dict(label="", content="")
            texts = []
            for abs_par in abs_node.xpath("p"):
                texts.append(JatsDocumentBackend._get_text(abs_par).strip())
            abstract["content"] = " ".join(texts)

            label_node = abs_node.xpath("title|label")
            if len(label_node) > 0:
                abstract["label"] = label_node[0].text.strip()

            abs_list.append(abstract)

        return abs_list

    def _parse_authors(self) -> list[Author]:
        # Get mapping between affiliation ids and names
        authors: list[Author] = []
        meta: Optional[etree._Element] = self._find_metadata()
        if meta is None:
            return authors

        affiliation_names = []
        for affiliation_node in meta.xpath(".//aff[@id]"):
            aff = ", ".join([t for t in affiliation_node.itertext() if t.strip()])
            aff = aff.replace("\n", " ")
            label = affiliation_node.xpath("label")
            if label:
                # TODO: once superscript is supported, add label with formatting
                aff = aff.removeprefix(f"{label[0].text}, ")
            affiliation_names.append(aff)
        affiliation_ids_names = {
            id: name
            for id, name in zip(meta.xpath(".//aff[@id]/@id"), affiliation_names)
        }

        # Get author names and affiliation names
        for author_node in meta.xpath(
            './/contrib-group/contrib[@contrib-type="author"]'
        ):
            author: Author = {
                "name": "",
                "affiliation_names": [],
            }

            # Affiliation names
            affiliation_ids = [
                a.attrib["rid"] for a in author_node.xpath('xref[@ref-type="aff"]')
            ]
            for id in affiliation_ids:
                if id in affiliation_ids_names:
                    author["affiliation_names"].append(affiliation_ids_names[id])

            # Name
            author["name"] = (
                author_node.xpath("name/given-names")[0].text
                + " "
                + author_node.xpath("name/surname")[0].text
            )

            authors.append(author)

        return authors

    def _parse_title(self) -> str:
        meta_names: list[str] = [
            "article-meta",
            "collection-meta",
            "book-meta",
            "book-part-meta",
        ]
        title_names: list[str] = ["article-title", "subtitle", "title", "label"]
        titles: list[str] = [
            " ".join(
                elem.text.replace("\n", " ").strip()
                for elem in list(title_node)
                if elem.tag in title_names
            ).strip()
            for title_node in self.tree.xpath(
                "|".join([f".//{item}/title-group" for item in meta_names])
            )
        ]

        text = " - ".join(titles)

        return text

    def _parse_metadata(self) -> XMLComponents:
        """Parsing JATS document metadata."""
        xml_components: XMLComponents = {
            "title": self._parse_title(),
            "authors": self._parse_authors(),
            "abstract": self._parse_abstract(),
        }
        return xml_components

    def _add_abstract(
        self, doc: DoclingDocument, xml_components: XMLComponents
    ) -> None:

        for abstract in xml_components["abstract"]:
            text: str = abstract["content"]
            title: str = abstract["label"] or DEFAULT_HEADER_ABSTRACT
            if not text:
                continue
            parent = doc.add_heading(parent=self.root, text=title)
            doc.add_text(
                parent=parent,
                text=text,
                label=DocItemLabel.TEXT,
            )

        return

    def _add_authors(self, doc: DoclingDocument, xml_components: XMLComponents) -> None:
        # TODO: once docling supports text formatting, add affiliation reference to
        # author names through superscripts
        authors: list = [item["name"] for item in xml_components["authors"]]
        authors_str = ", ".join(authors)
        affiliations: list = [
            item
            for author in xml_components["authors"]
            for item in author["affiliation_names"]
        ]
        affiliations_str = "; ".join(list(dict.fromkeys(affiliations)))
        if authors_str:
            doc.add_text(
                parent=self.root,
                text=authors_str,
                label=DocItemLabel.PARAGRAPH,
            )
        if affiliations_str:
            doc.add_text(
                parent=self.root,
                text=affiliations_str,
                label=DocItemLabel.PARAGRAPH,
            )

        return

    def _add_citation(self, doc: DoclingDocument, parent: NodeItem, text: str) -> None:
        if isinstance(parent, GroupItem) and parent.label == GroupLabel.LIST:
            doc.add_list_item(text=text, enumerated=False, parent=parent)
        else:
            doc.add_text(text=text, label=DocItemLabel.TEXT, parent=parent)

        return

    def _parse_element_citation(self, node: etree._Element) -> str:
        citation: Citation = {
            "author_names": "",
            "title": "",
            "source": "",
            "year": "",
            "volume": "",
            "page": "",
            "pub_id": "",
            "publisher_name": "",
            "publisher_loc": "",
        }

        _log.debug("Citation parsing started")

        # Author names
        names = []
        for name_node in node.xpath(".//name"):
            name_str = (
                name_node.xpath("surname")[0].text.replace("\n", " ").strip()
                + " "
                + name_node.xpath("given-names")[0].text.replace("\n", " ").strip()
            )
            names.append(name_str)
        etal_node = node.xpath(".//etal")
        if len(etal_node) > 0:
            etal_text = etal_node[0].text or DEFAULT_TEXT_ETAL
            names.append(etal_text)
        citation["author_names"] = ", ".join(names)

        titles: list[str] = [
            "article-title",
            "chapter-title",
            "data-title",
            "issue-title",
            "part-title",
            "trans-title",
        ]
        title_node: Optional[etree._Element] = None
        for name in titles:
            name_node = node.xpath(name)
            if len(name_node) > 0:
                title_node = name_node[0]
                break
        citation["title"] = (
            JatsDocumentBackend._get_text(title_node)
            if title_node is not None
            else node.text.replace("\n", " ").strip()
        )

        # Journal, year, publisher name, publisher location, volume, elocation
        fields: list[str] = [
            "source",
            "year",
            "publisher-name",
            "publisher-loc",
            "volume",
        ]
        for item in fields:
            item_node = node.xpath(item)
            if len(item_node) > 0:
                citation[item.replace("-", "_")] = (  # type: ignore[literal-required]
                    item_node[0].text.replace("\n", " ").strip()
                )

        # Publication identifier
        if len(node.xpath("pub-id")) > 0:
            pub_id: list[str] = []
            for id_node in node.xpath("pub-id"):
                id_type = id_node.get("assigning-authority") or id_node.get(
                    "pub-id-type"
                )
                id_text = id_node.text
                if id_type and id_text:
                    pub_id.append(
                        id_type.replace("\n", " ").strip().upper()
                        + ": "
                        + id_text.replace("\n", " ").strip()
                    )
            if pub_id:
                citation["pub_id"] = ", ".join(pub_id)

        # Pages
        if len(node.xpath("elocation-id")) > 0:
            citation["page"] = (
                node.xpath("elocation-id")[0].text.replace("\n", " ").strip()
            )
        elif len(node.xpath("fpage")) > 0:
            citation["page"] = node.xpath("fpage")[0].text.replace("\n", " ").strip()
            if len(node.xpath("lpage")) > 0:
                citation["page"] += (
                    "–" + node.xpath("lpage")[0].text.replace("\n", " ").strip()
                )

        # Flatten the citation to string

        text = ""
        if citation["author_names"]:
            text += citation["author_names"].rstrip(".") + ". "
        if citation["title"]:
            text += citation["title"] + ". "
        if citation["source"]:
            text += citation["source"] + ". "
        if citation["publisher_name"]:
            if citation["publisher_loc"]:
                text += f"{citation['publisher_loc']}: "
            text += citation["publisher_name"] + ". "
        if citation["volume"]:
            text = text.rstrip(". ")
            text += f" {citation['volume']}. "
        if citation["page"]:
            text = text.rstrip(". ")
            if citation["volume"]:
                text += ":"
            text += citation["page"] + ". "
        if citation["year"]:
            text = text.rstrip(". ")
            text += f" ({citation['year']})."
        if citation["pub_id"]:
            text = text.rstrip(".") + ". "
            text += citation["pub_id"]

        _log.debug("Citation flattened")

        return text

    def _add_equation(
        self, doc: DoclingDocument, parent: NodeItem, node: etree._Element
    ) -> None:
        math_text = node.text
        math_parts = math_text.split("$$")
        if len(math_parts) == 3:
            math_formula = math_parts[1]
            doc.add_text(label=DocItemLabel.FORMULA, text=math_formula, parent=parent)

        return

    def _add_figure_captions(
        self, doc: DoclingDocument, parent: NodeItem, node: etree._Element
    ) -> None:
        label_node = node.xpath("label")
        label: Optional[str] = (
            JatsDocumentBackend._get_text(label_node[0]).strip() if label_node else ""
        )

        caption_node = node.xpath("caption")
        caption: Optional[str]
        if len(caption_node) > 0:
            caption = ""
            for caption_par in list(caption_node[0]):
                if caption_par.xpath(".//supplementary-material"):
                    continue
                caption += JatsDocumentBackend._get_text(caption_par).strip() + " "
            caption = caption.strip()
        else:
            caption = None

        # TODO: format label vs caption once styling is supported
        fig_text: str = f"{label}{' ' if label and caption else ''}{caption}"
        fig_caption: Optional[TextItem] = (
            doc.add_text(label=DocItemLabel.CAPTION, text=fig_text)
            if fig_text
            else None
        )

        doc.add_picture(parent=parent, caption=fig_caption)

        return

    # TODO: add footnotes when DocItemLabel.FOOTNOTE and styling are supported
    # def _add_footnote_group(self, doc: DoclingDocument, parent: NodeItem, node: etree._Element) -> None:
    #     new_parent = doc.add_group(label=GroupLabel.LIST, name="footnotes", parent=parent)
    #     for child in node.iterchildren(tag="fn"):
    #         text = JatsDocumentBackend._get_text(child)
    #         doc.add_list_item(text=text, parent=new_parent)

    def _add_metadata(
        self, doc: DoclingDocument, xml_components: XMLComponents
    ) -> None:
        self._add_title(doc, xml_components)
        self._add_authors(doc, xml_components)
        self._add_abstract(doc, xml_components)

        return

    def _add_table(
        self, doc: DoclingDocument, parent: NodeItem, table_xml_component: Table
    ) -> None:
        soup = BeautifulSoup(table_xml_component["content"], "html.parser")
        table_tag = soup.find("table")
        if not isinstance(table_tag, Tag):
            return

        data = HTMLDocumentBackend.parse_table_data(table_tag)

        # TODO: format label vs caption once styling is supported
        label = table_xml_component["label"]
        caption = table_xml_component["caption"]
        table_text: str = f"{label}{' ' if label and caption else ''}{caption}"
        table_caption: Optional[TextItem] = (
            doc.add_text(label=DocItemLabel.CAPTION, text=table_text)
            if table_text
            else None
        )

        if data is not None:
            doc.add_table(data=data, parent=parent, caption=table_caption)

        return

    def _add_tables(
        self, doc: DoclingDocument, parent: NodeItem, node: etree._Element
    ) -> None:
        table: Table = {"label": "", "caption": "", "content": ""}

        # Content
        if len(node.xpath("table")) > 0:
            table_content_node = node.xpath("table")[0]
        elif len(node.xpath("alternatives/table")) > 0:
            table_content_node = node.xpath("alternatives/table")[0]
        else:
            table_content_node = None
        if table_content_node is not None:
            table["content"] = etree.tostring(table_content_node).decode("utf-8")

        # Caption
        caption_node = node.xpath("caption")
        caption: Optional[str]
        if caption_node:
            caption = ""
            for caption_par in list(caption_node[0]):
                if caption_par.xpath(".//supplementary-material"):
                    continue
                caption += JatsDocumentBackend._get_text(caption_par).strip() + " "
            caption = caption.strip()
        else:
            caption = None
        if caption is not None:
            table["caption"] = caption

        # Label
        if len(node.xpath("label")) > 0:
            table["label"] = node.xpath("label")[0].text

        try:
            self._add_table(doc, parent, table)
        except Exception as e:
            _log.warning(f"Skipping unsupported table in {str(self.file)}")
            pass

        return

    def _add_title(self, doc: DoclingDocument, xml_components: XMLComponents) -> None:
        self.root = doc.add_text(
            parent=None,
            text=xml_components["title"],
            label=DocItemLabel.TITLE,
        )
        return

    def _walk_linear(
        self, doc: DoclingDocument, parent: NodeItem, node: etree._Element
    ) -> str:
        skip_tags = ["term"]
        flush_tags = ["ack", "sec", "list", "boxed-text", "disp-formula", "fig"]
        new_parent: NodeItem = parent
        node_text: str = (
            node.text.replace("\n", " ")
            if (node.tag not in skip_tags and node.text)
            else ""
        )

        for child in list(node):
            stop_walk: bool = False

            # flush text into TextItem for some tags in paragraph nodes
            if node.tag == "p" and node_text.strip() and child.tag in flush_tags:
                doc.add_text(
                    label=DocItemLabel.TEXT, text=node_text.strip(), parent=parent
                )
                node_text = ""

            # add elements and decide whether to stop walking
            if child.tag in ("sec", "ack"):
                header = child.xpath("title|label")
                text: Optional[str] = None
                if len(header) > 0:
                    text = JatsDocumentBackend._get_text(header[0])
                elif child.tag == "ack":
                    text = DEFAULT_HEADER_ACKNOWLEDGMENTS
                if text:
                    new_parent = doc.add_heading(text=text, parent=parent)
            elif child.tag == "list":
                new_parent = doc.add_group(
                    label=GroupLabel.LIST, name="list", parent=parent
                )
            elif child.tag == "list-item":
                # TODO: address any type of content (another list, formula,...)
                # TODO: address list type and item label
                text = JatsDocumentBackend._get_text(child).strip()
                new_parent = doc.add_list_item(text=text, parent=parent)
                stop_walk = True
            elif child.tag == "fig":
                self._add_figure_captions(doc, parent, child)
                stop_walk = True
            elif child.tag == "table-wrap":
                self._add_tables(doc, parent, child)
                stop_walk = True
            elif child.tag == "suplementary-material":
                stop_walk = True
            elif child.tag == "fn-group":
                # header = child.xpath(".//title") or child.xpath(".//label")
                # if header:
                #     text = JatsDocumentBackend._get_text(header[0])
                #     fn_parent = doc.add_heading(text=text, parent=new_parent)
                # self._add_footnote_group(doc, fn_parent, child)
                stop_walk = True
            elif child.tag == "ref-list" and node.tag != "ref-list":
                header = child.xpath("title|label")
                text = (
                    JatsDocumentBackend._get_text(header[0])
                    if len(header) > 0
                    else DEFAULT_HEADER_REFERENCES
                )
                new_parent = doc.add_heading(text=text, parent=parent)
                new_parent = doc.add_group(
                    parent=new_parent, label=GroupLabel.LIST, name="list"
                )
            elif child.tag == "element-citation":
                text = self._parse_element_citation(child)
                self._add_citation(doc, parent, text)
                stop_walk = True
            elif child.tag == "mixed-citation":
                text = JatsDocumentBackend._get_text(child).strip()
                self._add_citation(doc, parent, text)
                stop_walk = True
            elif child.tag == "tex-math":
                self._add_equation(doc, parent, child)
                stop_walk = True
            elif child.tag == "inline-formula":
                # TODO: address inline formulas when supported by docling-core
                stop_walk = True

            # step into child
            if not stop_walk:
                new_text = self._walk_linear(doc, new_parent, child)
                if not (node.getparent().tag == "p" and node.tag in flush_tags):
                    node_text += new_text

            # pick up the tail text
            node_text += child.tail.replace("\n", " ") if child.tail else ""

        # create paragraph
        if node.tag == "p" and node_text.strip():
            doc.add_text(label=DocItemLabel.TEXT, text=node_text.strip(), parent=parent)
            return ""
        else:
            # backpropagate the text
            return node_text


================================================
File: docling/chunking/__init__.py
================================================
#
# Copyright IBM Corp. 2024 - 2024
# SPDX-License-Identifier: MIT
#

from docling_core.transforms.chunker.base import BaseChunk, BaseChunker, BaseMeta
from docling_core.transforms.chunker.hierarchical_chunker import (
    DocChunk,
    DocMeta,
    HierarchicalChunker,
)
from docling_core.transforms.chunker.hybrid_chunker import HybridChunker


================================================
File: docling/cli/main.py
================================================
import importlib
import logging
import platform
import re
import sys
import tempfile
import time
import warnings
from pathlib import Path
from typing import Annotated, Dict, Iterable, List, Optional, Type

import typer
from docling_core.types.doc import ImageRefMode
from docling_core.utils.file import resolve_source_to_path
from pydantic import TypeAdapter

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend
from docling.backend.pdf_backend import PdfDocumentBackend
from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling.datamodel.base_models import (
    ConversionStatus,
    FormatToExtensions,
    InputFormat,
    OutputFormat,
)
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    EasyOcrOptions,
    OcrEngine,
    OcrMacOptions,
    OcrOptions,
    PdfBackend,
    PdfPipelineOptions,
    RapidOcrOptions,
    TableFormerMode,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.datamodel.settings import settings
from docling.document_converter import DocumentConverter, FormatOption, PdfFormatOption

warnings.filterwarnings(action="ignore", category=UserWarning, module="pydantic|torch")
warnings.filterwarnings(action="ignore", category=FutureWarning, module="easyocr")

_log = logging.getLogger(__name__)
from rich.console import Console

err_console = Console(stderr=True)


app = typer.Typer(
    name="Docling",
    no_args_is_help=True,
    add_completion=False,
    pretty_exceptions_enable=False,
)


def version_callback(value: bool):
    if value:
        docling_version = importlib.metadata.version("docling")
        docling_core_version = importlib.metadata.version("docling-core")
        docling_ibm_models_version = importlib.metadata.version("docling-ibm-models")
        docling_parse_version = importlib.metadata.version("docling-parse")
        platform_str = platform.platform()
        py_impl_version = sys.implementation.cache_tag
        py_lang_version = platform.python_version()
        print(f"Docling version: {docling_version}")
        print(f"Docling Core version: {docling_core_version}")
        print(f"Docling IBM Models version: {docling_ibm_models_version}")
        print(f"Docling Parse version: {docling_parse_version}")
        print(f"Python: {py_impl_version} ({py_lang_version})")
        print(f"Platform: {platform_str}")
        raise typer.Exit()


def export_documents(
    conv_results: Iterable[ConversionResult],
    output_dir: Path,
    export_json: bool,
    export_html: bool,
    export_md: bool,
    export_txt: bool,
    export_doctags: bool,
    image_export_mode: ImageRefMode,
):

    success_count = 0
    failure_count = 0

    for conv_res in conv_results:
        if conv_res.status == ConversionStatus.SUCCESS:
            success_count += 1
            doc_filename = conv_res.input.file.stem

            # Export JSON format:
            if export_json:
                fname = output_dir / f"{doc_filename}.json"
                _log.info(f"writing JSON output to {fname}")
                conv_res.document.save_as_json(
                    filename=fname, image_mode=image_export_mode
                )

            # Export HTML format:
            if export_html:
                fname = output_dir / f"{doc_filename}.html"
                _log.info(f"writing HTML output to {fname}")
                conv_res.document.save_as_html(
                    filename=fname, image_mode=image_export_mode
                )

            # Export Text format:
            if export_txt:
                fname = output_dir / f"{doc_filename}.txt"
                _log.info(f"writing TXT output to {fname}")
                conv_res.document.save_as_markdown(
                    filename=fname,
                    strict_text=True,
                    image_mode=ImageRefMode.PLACEHOLDER,
                )

            # Export Markdown format:
            if export_md:
                fname = output_dir / f"{doc_filename}.md"
                _log.info(f"writing Markdown output to {fname}")
                conv_res.document.save_as_markdown(
                    filename=fname, image_mode=image_export_mode
                )

            # Export Document Tags format:
            if export_doctags:
                fname = output_dir / f"{doc_filename}.doctags"
                _log.info(f"writing Doc Tags output to {fname}")
                conv_res.document.save_as_document_tokens(filename=fname)

        else:
            _log.warning(f"Document {conv_res.input.file} failed to convert.")
            failure_count += 1

    _log.info(
        f"Processed {success_count + failure_count} docs, of which {failure_count} failed"
    )


def _split_list(raw: Optional[str]) -> Optional[List[str]]:
    if raw is None:
        return None
    return re.split(r"[;,]", raw)


@app.command(no_args_is_help=True)
def convert(
    input_sources: Annotated[
        List[str],
        typer.Argument(
            ...,
            metavar="source",
            help="PDF files to convert. Can be local file / directory paths or URL.",
        ),
    ],
    from_formats: List[InputFormat] = typer.Option(
        None,
        "--from",
        help="Specify input formats to convert from. Defaults to all formats.",
    ),
    to_formats: List[OutputFormat] = typer.Option(
        None, "--to", help="Specify output formats. Defaults to Markdown."
    ),
    headers: str = typer.Option(
        None,
        "--headers",
        help="Specify http request headers used when fetching url input sources in the form of a JSON string",
    ),
    image_export_mode: Annotated[
        ImageRefMode,
        typer.Option(
            ...,
            help="Image export mode for the document (only in case of JSON, Markdown or HTML). With `placeholder`, only the position of the image is marked in the output. In `embedded` mode, the image is embedded as base64 encoded string. In `referenced` mode, the image is exported in PNG format and referenced from the main exported document.",
        ),
    ] = ImageRefMode.EMBEDDED,
    ocr: Annotated[
        bool,
        typer.Option(
            ..., help="If enabled, the bitmap content will be processed using OCR."
        ),
    ] = True,
    force_ocr: Annotated[
        bool,
        typer.Option(
            ...,
            help="Replace any existing text with OCR generated text over the full content.",
        ),
    ] = False,
    ocr_engine: Annotated[
        OcrEngine, typer.Option(..., help="The OCR engine to use.")
    ] = OcrEngine.EASYOCR,
    ocr_lang: Annotated[
        Optional[str],
        typer.Option(
            ...,
            help="Provide a comma-separated list of languages used by the OCR engine. Note that each OCR engine has different values for the language names.",
        ),
    ] = None,
    pdf_backend: Annotated[
        PdfBackend, typer.Option(..., help="The PDF backend to use.")
    ] = PdfBackend.DLPARSE_V2,
    table_mode: Annotated[
        TableFormerMode,
        typer.Option(..., help="The mode to use in the table structure model."),
    ] = TableFormerMode.FAST,
    enrich_code: Annotated[
        bool,
        typer.Option(..., help="Enable the code enrichment model in the pipeline."),
    ] = False,
    enrich_formula: Annotated[
        bool,
        typer.Option(..., help="Enable the formula enrichment model in the pipeline."),
    ] = False,
    enrich_picture_classes: Annotated[
        bool,
        typer.Option(
            ...,
            help="Enable the picture classification enrichment model in the pipeline.",
        ),
    ] = False,
    enrich_picture_description: Annotated[
        bool,
        typer.Option(..., help="Enable the picture description model in the pipeline."),
    ] = False,
    artifacts_path: Annotated[
        Optional[Path],
        typer.Option(..., help="If provided, the location of the model artifacts."),
    ] = None,
    enable_remote_services: Annotated[
        bool,
        typer.Option(
            ..., help="Must be enabled when using models connecting to remote services."
        ),
    ] = False,
    abort_on_error: Annotated[
        bool,
        typer.Option(
            ...,
            "--abort-on-error/--no-abort-on-error",
            help="If enabled, the bitmap content will be processed using OCR.",
        ),
    ] = False,
    output: Annotated[
        Path, typer.Option(..., help="Output directory where results are saved.")
    ] = Path("."),
    verbose: Annotated[
        int,
        typer.Option(
            "--verbose",
            "-v",
            count=True,
            help="Set the verbosity level. -v for info logging, -vv for debug logging.",
        ),
    ] = 0,
    debug_visualize_cells: Annotated[
        bool,
        typer.Option(..., help="Enable debug output which visualizes the PDF cells"),
    ] = False,
    debug_visualize_ocr: Annotated[
        bool,
        typer.Option(..., help="Enable debug output which visualizes the OCR cells"),
    ] = False,
    debug_visualize_layout: Annotated[
        bool,
        typer.Option(
            ..., help="Enable debug output which visualizes the layour clusters"
        ),
    ] = False,
    debug_visualize_tables: Annotated[
        bool,
        typer.Option(..., help="Enable debug output which visualizes the table cells"),
    ] = False,
    version: Annotated[
        Optional[bool],
        typer.Option(
            "--version",
            callback=version_callback,
            is_eager=True,
            help="Show version information.",
        ),
    ] = None,
    document_timeout: Annotated[
        Optional[float],
        typer.Option(
            ...,
            help="The timeout for processing each document, in seconds.",
        ),
    ] = None,
    num_threads: Annotated[int, typer.Option(..., help="Number of threads")] = 4,
    device: Annotated[
        AcceleratorDevice, typer.Option(..., help="Accelerator device")
    ] = AcceleratorDevice.AUTO,
):
    if verbose == 0:
        logging.basicConfig(level=logging.WARNING)
    elif verbose == 1:
        logging.basicConfig(level=logging.INFO)
    elif verbose == 2:
        logging.basicConfig(level=logging.DEBUG)

    settings.debug.visualize_cells = debug_visualize_cells
    settings.debug.visualize_layout = debug_visualize_layout
    settings.debug.visualize_tables = debug_visualize_tables
    settings.debug.visualize_ocr = debug_visualize_ocr

    if from_formats is None:
        from_formats = [e for e in InputFormat]

    parsed_headers: Optional[Dict[str, str]] = None
    if headers is not None:
        headers_t = TypeAdapter(Dict[str, str])
        parsed_headers = headers_t.validate_json(headers)

    with tempfile.TemporaryDirectory() as tempdir:
        input_doc_paths: List[Path] = []
        for src in input_sources:
            try:
                # check if we can fetch some remote url
                source = resolve_source_to_path(
                    source=src, headers=parsed_headers, workdir=Path(tempdir)
                )
                input_doc_paths.append(source)
            except FileNotFoundError:
                err_console.print(
                    f"[red]Error: The input file {src} does not exist.[/red]"
                )
                raise typer.Abort()
            except IsADirectoryError:
                # if the input matches to a file or a folder
                try:
                    local_path = TypeAdapter(Path).validate_python(src)
                    if local_path.exists() and local_path.is_dir():
                        for fmt in from_formats:
                            for ext in FormatToExtensions[fmt]:
                                input_doc_paths.extend(
                                    list(local_path.glob(f"**/*.{ext}"))
                                )
                                input_doc_paths.extend(
                                    list(local_path.glob(f"**/*.{ext.upper()}"))
                                )
                    elif local_path.exists():
                        input_doc_paths.append(local_path)
                    else:
                        err_console.print(
                            f"[red]Error: The input file {src} does not exist.[/red]"
                        )
                        raise typer.Abort()
                except Exception as err:
                    err_console.print(f"[red]Error: Cannot read the input {src}.[/red]")
                    _log.info(err)  # will print more details if verbose is activated
                    raise typer.Abort()

        if to_formats is None:
            to_formats = [OutputFormat.MARKDOWN]

        export_json = OutputFormat.JSON in to_formats
        export_html = OutputFormat.HTML in to_formats
        export_md = OutputFormat.MARKDOWN in to_formats
        export_txt = OutputFormat.TEXT in to_formats
        export_doctags = OutputFormat.DOCTAGS in to_formats

        if ocr_engine == OcrEngine.EASYOCR:
            ocr_options: OcrOptions = EasyOcrOptions(force_full_page_ocr=force_ocr)
        elif ocr_engine == OcrEngine.TESSERACT_CLI:
            ocr_options = TesseractCliOcrOptions(force_full_page_ocr=force_ocr)
        elif ocr_engine == OcrEngine.TESSERACT:
            ocr_options = TesseractOcrOptions(force_full_page_ocr=force_ocr)
        elif ocr_engine == OcrEngine.OCRMAC:
            ocr_options = OcrMacOptions(force_full_page_ocr=force_ocr)
        elif ocr_engine == OcrEngine.RAPIDOCR:
            ocr_options = RapidOcrOptions(force_full_page_ocr=force_ocr)
        else:
            raise RuntimeError(f"Unexpected OCR engine type {ocr_engine}")

        ocr_lang_list = _split_list(ocr_lang)
        if ocr_lang_list is not None:
            ocr_options.lang = ocr_lang_list

        accelerator_options = AcceleratorOptions(num_threads=num_threads, device=device)
        pipeline_options = PdfPipelineOptions(
            enable_remote_services=enable_remote_services,
            accelerator_options=accelerator_options,
            do_ocr=ocr,
            ocr_options=ocr_options,
            do_table_structure=True,
            do_code_enrichment=enrich_code,
            do_formula_enrichment=enrich_formula,
            do_picture_description=enrich_picture_description,
            do_picture_classification=enrich_picture_classes,
            document_timeout=document_timeout,
        )
        pipeline_options.table_structure_options.do_cell_matching = (
            True  # do_cell_matching
        )
        pipeline_options.table_structure_options.mode = table_mode

        if image_export_mode != ImageRefMode.PLACEHOLDER:
            pipeline_options.generate_page_images = True
            pipeline_options.generate_picture_images = (
                True  # FIXME: to be deprecated in verson 3
            )
            pipeline_options.images_scale = 2

        if artifacts_path is not None:
            pipeline_options.artifacts_path = artifacts_path

        if pdf_backend == PdfBackend.DLPARSE_V1:
            backend: Type[PdfDocumentBackend] = DoclingParseDocumentBackend
        elif pdf_backend == PdfBackend.DLPARSE_V2:
            backend = DoclingParseV2DocumentBackend
        elif pdf_backend == PdfBackend.PYPDFIUM2:
            backend = PyPdfiumDocumentBackend
        else:
            raise RuntimeError(f"Unexpected PDF backend type {pdf_backend}")

        pdf_format_option = PdfFormatOption(
            pipeline_options=pipeline_options,
            backend=backend,  # pdf_backend
        )
        format_options: Dict[InputFormat, FormatOption] = {
            InputFormat.PDF: pdf_format_option,
            InputFormat.IMAGE: pdf_format_option,
        }
        doc_converter = DocumentConverter(
            allowed_formats=from_formats,
            format_options=format_options,
        )

        start_time = time.time()

        conv_results = doc_converter.convert_all(
            input_doc_paths, headers=parsed_headers, raises_on_error=abort_on_error
        )

        output.mkdir(parents=True, exist_ok=True)
        export_documents(
            conv_results,
            output_dir=output,
            export_json=export_json,
            export_html=export_html,
            export_md=export_md,
            export_txt=export_txt,
            export_doctags=export_doctags,
            image_export_mode=image_export_mode,
        )

        end_time = time.time() - start_time

    _log.info(f"All documents were converted in {end_time:.2f} seconds.")


click_app = typer.main.get_command(app)

if __name__ == "__main__":
    app()


================================================
File: docling/cli/models.py
================================================
import logging
import warnings
from enum import Enum
from pathlib import Path
from typing import Annotated, Optional

import typer
from rich.console import Console
from rich.logging import RichHandler

from docling.datamodel.settings import settings
from docling.utils.model_downloader import download_models

warnings.filterwarnings(action="ignore", category=UserWarning, module="pydantic|torch")
warnings.filterwarnings(action="ignore", category=FutureWarning, module="easyocr")

console = Console()
err_console = Console(stderr=True)


app = typer.Typer(
    name="Docling models helper",
    no_args_is_help=True,
    add_completion=False,
    pretty_exceptions_enable=False,
)


class _AvailableModels(str, Enum):
    LAYOUT = "layout"
    TABLEFORMER = "tableformer"
    CODE_FORMULA = "code_formula"
    PICTURE_CLASSIFIER = "picture_classifier"
    SMOLVLM = "smolvlm"
    GRANITE_VISION = "granite_vision"
    EASYOCR = "easyocr"


_default_models = [
    _AvailableModels.LAYOUT,
    _AvailableModels.TABLEFORMER,
    _AvailableModels.CODE_FORMULA,
    _AvailableModels.PICTURE_CLASSIFIER,
    _AvailableModels.EASYOCR,
]


@app.command("download")
def download(
    output_dir: Annotated[
        Path,
        typer.Option(
            ...,
            "-o",
            "--output-dir",
            help="The directory where to download the models.",
        ),
    ] = (settings.cache_dir / "models"),
    force: Annotated[
        bool, typer.Option(..., help="If true, the download will be forced.")
    ] = False,
    models: Annotated[
        Optional[list[_AvailableModels]],
        typer.Argument(
            help=f"Models to download (default behavior: a predefined set of models will be downloaded).",
        ),
    ] = None,
    all: Annotated[
        bool,
        typer.Option(
            ...,
            "--all",
            help="If true, all available models will be downloaded (mutually exclusive with passing specific models).",
            show_default=True,
        ),
    ] = False,
    quiet: Annotated[
        bool,
        typer.Option(
            ...,
            "-q",
            "--quiet",
            help="No extra output is generated, the CLI prints only the directory with the cached models.",
        ),
    ] = False,
):
    if models and all:
        raise typer.BadParameter(
            "Cannot simultaneously set 'all' parameter and specify models to download."
        )
    if not quiet:
        FORMAT = "%(message)s"
        logging.basicConfig(
            level=logging.INFO,
            format="[blue]%(message)s[/blue]",
            datefmt="[%X]",
            handlers=[RichHandler(show_level=False, show_time=False, markup=True)],
        )
    to_download = models or ([m for m in _AvailableModels] if all else _default_models)
    output_dir = download_models(
        output_dir=output_dir,
        force=force,
        progress=(not quiet),
        with_layout=_AvailableModels.LAYOUT in to_download,
        with_tableformer=_AvailableModels.TABLEFORMER in to_download,
        with_code_formula=_AvailableModels.CODE_FORMULA in to_download,
        with_picture_classifier=_AvailableModels.PICTURE_CLASSIFIER in to_download,
        with_smolvlm=_AvailableModels.SMOLVLM in to_download,
        with_granite_vision=_AvailableModels.GRANITE_VISION in to_download,
        with_easyocr=_AvailableModels.EASYOCR in to_download,
    )

    if quiet:
        typer.echo(output_dir)
    else:
        typer.secho(f"\nModels downloaded into: {output_dir}.", fg="green")

        console.print(
            "\n",
            "Docling can now be configured for running offline using the local artifacts.\n\n",
            "Using the CLI:",
            f"`docling --artifacts-path={output_dir} FILE`",
            "\n",
            "Using Python: see the documentation at <https://ds4sd.github.io/docling/usage>.",
        )


click_app = typer.main.get_command(app)

if __name__ == "__main__":
    app()


================================================
File: docling/cli/tools.py
================================================
import typer

from docling.cli.models import app as models_app

app = typer.Typer(
    name="Docling helpers",
    no_args_is_help=True,
    add_completion=False,
    pretty_exceptions_enable=False,
)

app.add_typer(models_app, name="models")

click_app = typer.main.get_command(app)

if __name__ == "__main__":
    app()


================================================
File: docling/datamodel/base_models.py
================================================
from enum import Enum
from typing import TYPE_CHECKING, Dict, List, Optional, Union

from docling_core.types.doc import (
    BoundingBox,
    DocItemLabel,
    NodeItem,
    PictureDataType,
    Size,
    TableCell,
)
from docling_core.types.io import (  # DO ΝΟΤ REMOVE; explicitly exposed from this location
    DocumentStream,
)
from PIL.Image import Image
from pydantic import BaseModel, ConfigDict

if TYPE_CHECKING:
    from docling.backend.pdf_backend import PdfPageBackend


class ConversionStatus(str, Enum):
    PENDING = "pending"
    STARTED = "started"
    FAILURE = "failure"
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    SKIPPED = "skipped"


class InputFormat(str, Enum):
    """A document format supported by document backend parsers."""

    DOCX = "docx"
    PPTX = "pptx"
    HTML = "html"
    IMAGE = "image"
    PDF = "pdf"
    ASCIIDOC = "asciidoc"
    MD = "md"
    CSV = "csv"
    XLSX = "xlsx"
    XML_USPTO = "xml_uspto"
    XML_JATS = "xml_jats"
    JSON_DOCLING = "json_docling"


class OutputFormat(str, Enum):
    MARKDOWN = "md"
    JSON = "json"
    HTML = "html"
    TEXT = "text"
    DOCTAGS = "doctags"


FormatToExtensions: Dict[InputFormat, List[str]] = {
    InputFormat.DOCX: ["docx", "dotx", "docm", "dotm"],
    InputFormat.PPTX: ["pptx", "potx", "ppsx", "pptm", "potm", "ppsm"],
    InputFormat.PDF: ["pdf"],
    InputFormat.MD: ["md"],
    InputFormat.HTML: ["html", "htm", "xhtml"],
    InputFormat.XML_JATS: ["xml", "nxml"],
    InputFormat.IMAGE: ["jpg", "jpeg", "png", "tif", "tiff", "bmp"],
    InputFormat.ASCIIDOC: ["adoc", "asciidoc", "asc"],
    InputFormat.CSV: ["csv"],
    InputFormat.XLSX: ["xlsx"],
    InputFormat.XML_USPTO: ["xml", "txt"],
    InputFormat.JSON_DOCLING: ["json"],
}

FormatToMimeType: Dict[InputFormat, List[str]] = {
    InputFormat.DOCX: [
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.template",
    ],
    InputFormat.PPTX: [
        "application/vnd.openxmlformats-officedocument.presentationml.template",
        "application/vnd.openxmlformats-officedocument.presentationml.slideshow",
        "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    ],
    InputFormat.HTML: ["text/html", "application/xhtml+xml"],
    InputFormat.XML_JATS: ["application/xml"],
    InputFormat.IMAGE: [
        "image/png",
        "image/jpeg",
        "image/tiff",
        "image/gif",
        "image/bmp",
    ],
    InputFormat.PDF: ["application/pdf"],
    InputFormat.ASCIIDOC: ["text/asciidoc"],
    InputFormat.MD: ["text/markdown", "text/x-markdown"],
    InputFormat.CSV: ["text/csv"],
    InputFormat.XLSX: [
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    ],
    InputFormat.XML_USPTO: ["application/xml", "text/plain"],
    InputFormat.JSON_DOCLING: ["application/json"],
}

MimeTypeToFormat: dict[str, list[InputFormat]] = {
    mime: [fmt for fmt in FormatToMimeType if mime in FormatToMimeType[fmt]]
    for value in FormatToMimeType.values()
    for mime in value
}


class DocInputType(str, Enum):
    PATH = "path"
    STREAM = "stream"


class DoclingComponentType(str, Enum):
    DOCUMENT_BACKEND = "document_backend"
    MODEL = "model"
    DOC_ASSEMBLER = "doc_assembler"
    USER_INPUT = "user_input"


class ErrorItem(BaseModel):
    component_type: DoclingComponentType
    module_name: str
    error_message: str


class Cell(BaseModel):
    id: int
    text: str
    bbox: BoundingBox


class OcrCell(Cell):
    confidence: float


class Cluster(BaseModel):
    id: int
    label: DocItemLabel
    bbox: BoundingBox
    confidence: float = 1.0
    cells: List[Cell] = []
    children: List["Cluster"] = []  # Add child cluster support


class BasePageElement(BaseModel):
    label: DocItemLabel
    id: int
    page_no: int
    cluster: Cluster
    text: Optional[str] = None


class LayoutPrediction(BaseModel):
    clusters: List[Cluster] = []


class VlmPrediction(BaseModel):
    text: str = ""


class ContainerElement(
    BasePageElement
):  # Used for Form and Key-Value-Regions, only for typing.
    pass


class Table(BasePageElement):
    otsl_seq: List[str]
    num_rows: int = 0
    num_cols: int = 0
    table_cells: List[TableCell]


class TableStructurePrediction(BaseModel):
    table_map: Dict[int, Table] = {}


class TextElement(BasePageElement):
    text: str


class FigureElement(BasePageElement):
    annotations: List[PictureDataType] = []
    provenance: Optional[str] = None
    predicted_class: Optional[str] = None
    confidence: Optional[float] = None


class FigureClassificationPrediction(BaseModel):
    figure_count: int = 0
    figure_map: Dict[int, FigureElement] = {}


class EquationPrediction(BaseModel):
    equation_count: int = 0
    equation_map: Dict[int, TextElement] = {}


class PagePredictions(BaseModel):
    layout: Optional[LayoutPrediction] = None
    tablestructure: Optional[TableStructurePrediction] = None
    figures_classification: Optional[FigureClassificationPrediction] = None
    equations_prediction: Optional[EquationPrediction] = None
    vlm_response: Optional[VlmPrediction] = None


PageElement = Union[TextElement, Table, FigureElement, ContainerElement]


class AssembledUnit(BaseModel):
    elements: List[PageElement] = []
    body: List[PageElement] = []
    headers: List[PageElement] = []


class ItemAndImageEnrichmentElement(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    item: NodeItem
    image: Image


class Page(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    page_no: int
    # page_hash: Optional[str] = None
    size: Optional[Size] = None
    cells: List[Cell] = []
    predictions: PagePredictions = PagePredictions()
    assembled: Optional[AssembledUnit] = None

    _backend: Optional["PdfPageBackend"] = (
        None  # Internal PDF backend. By default it is cleared during assembling.
    )
    _default_image_scale: float = 1.0  # Default image scale for external usage.
    _image_cache: Dict[float, Image] = (
        {}
    )  # Cache of images in different scales. By default it is cleared during assembling.

    def get_image(
        self, scale: float = 1.0, cropbox: Optional[BoundingBox] = None
    ) -> Optional[Image]:
        if self._backend is None:
            return self._image_cache.get(scale, None)

        if not scale in self._image_cache:
            if cropbox is None:
                self._image_cache[scale] = self._backend.get_page_image(scale=scale)
            else:
                return self._backend.get_page_image(scale=scale, cropbox=cropbox)

        if cropbox is None:
            return self._image_cache[scale]
        else:
            page_im = self._image_cache[scale]
            assert self.size is not None
            return page_im.crop(
                cropbox.to_top_left_origin(page_height=self.size.height)
                .scaled(scale=scale)
                .as_tuple()
            )

    @property
    def image(self) -> Optional[Image]:
        return self.get_image(scale=self._default_image_scale)


================================================
File: docling/datamodel/document.py
================================================
import csv
import logging
import re
from enum import Enum
from io import BytesIO
from pathlib import Path, PurePath
from typing import (
    TYPE_CHECKING,
    Dict,
    Iterable,
    List,
    Literal,
    Optional,
    Set,
    Type,
    Union,
)

import filetype
from docling_core.types.doc import (
    DocItem,
    DocItemLabel,
    DoclingDocument,
    PictureItem,
    SectionHeaderItem,
    TableItem,
    TextItem,
)
from docling_core.types.doc.document import ListItem
from docling_core.types.legacy_doc.base import (
    BaseText,
    Figure,
    GlmTableCell,
    PageDimensions,
    PageReference,
    Prov,
    Ref,
)
from docling_core.types.legacy_doc.base import Table as DsSchemaTable
from docling_core.types.legacy_doc.base import TableCell
from docling_core.types.legacy_doc.document import (
    CCSDocumentDescription as DsDocumentDescription,
)
from docling_core.types.legacy_doc.document import CCSFileInfoObject as DsFileInfoObject
from docling_core.types.legacy_doc.document import ExportedCCSDocument as DsDocument
from docling_core.utils.file import resolve_source_to_stream
from docling_core.utils.legacy import docling_document_to_legacy
from pydantic import BaseModel
from typing_extensions import deprecated

from docling.backend.abstract_backend import (
    AbstractDocumentBackend,
    PaginatedDocumentBackend,
)
from docling.datamodel.base_models import (
    AssembledUnit,
    ConversionStatus,
    DocumentStream,
    ErrorItem,
    FormatToExtensions,
    FormatToMimeType,
    InputFormat,
    MimeTypeToFormat,
    Page,
)
from docling.datamodel.settings import DocumentLimits
from docling.utils.profiling import ProfilingItem
from docling.utils.utils import create_file_hash, create_hash

if TYPE_CHECKING:
    from docling.document_converter import FormatOption

_log = logging.getLogger(__name__)

layout_label_to_ds_type = {
    DocItemLabel.TITLE: "title",
    DocItemLabel.DOCUMENT_INDEX: "table",
    DocItemLabel.SECTION_HEADER: "subtitle-level-1",
    DocItemLabel.CHECKBOX_SELECTED: "checkbox-selected",
    DocItemLabel.CHECKBOX_UNSELECTED: "checkbox-unselected",
    DocItemLabel.CAPTION: "caption",
    DocItemLabel.PAGE_HEADER: "page-header",
    DocItemLabel.PAGE_FOOTER: "page-footer",
    DocItemLabel.FOOTNOTE: "footnote",
    DocItemLabel.TABLE: "table",
    DocItemLabel.FORMULA: "equation",
    DocItemLabel.LIST_ITEM: "paragraph",
    DocItemLabel.CODE: "paragraph",
    DocItemLabel.PICTURE: "figure",
    DocItemLabel.TEXT: "paragraph",
    DocItemLabel.PARAGRAPH: "paragraph",
    DocItemLabel.FORM: DocItemLabel.FORM.value,
    DocItemLabel.KEY_VALUE_REGION: DocItemLabel.KEY_VALUE_REGION.value,
}

_EMPTY_DOCLING_DOC = DoclingDocument(name="dummy")


class InputDocument(BaseModel):
    file: PurePath
    document_hash: str  # = None
    valid: bool = True
    limits: DocumentLimits = DocumentLimits()
    format: InputFormat  # = None

    filesize: Optional[int] = None
    page_count: int = 0

    _backend: AbstractDocumentBackend  # Internal PDF backend used

    def __init__(
        self,
        path_or_stream: Union[BytesIO, Path],
        format: InputFormat,
        backend: Type[AbstractDocumentBackend],
        filename: Optional[str] = None,
        limits: Optional[DocumentLimits] = None,
    ):
        super().__init__(
            file="", document_hash="", format=InputFormat.PDF
        )  # initialize with dummy values

        self.limits = limits or DocumentLimits()
        self.format = format

        try:
            if isinstance(path_or_stream, Path):
                self.file = path_or_stream
                self.filesize = path_or_stream.stat().st_size
                if self.filesize > self.limits.max_file_size:
                    self.valid = False
                else:
                    self.document_hash = create_file_hash(path_or_stream)
                    self._init_doc(backend, path_or_stream)

            elif isinstance(path_or_stream, BytesIO):
                assert (
                    filename is not None
                ), "Can't construct InputDocument from stream without providing filename arg."
                self.file = PurePath(filename)
                self.filesize = path_or_stream.getbuffer().nbytes

                if self.filesize > self.limits.max_file_size:
                    self.valid = False
                else:
                    self.document_hash = create_file_hash(path_or_stream)
                    self._init_doc(backend, path_or_stream)
            else:
                raise RuntimeError(
                    f"Unexpected type path_or_stream: {type(path_or_stream)}"
                )

            # For paginated backends, check if the maximum page count is exceeded.
            if self.valid and self._backend.is_valid():
                if self._backend.supports_pagination() and isinstance(
                    self._backend, PaginatedDocumentBackend
                ):
                    self.page_count = self._backend.page_count()
                    if not self.page_count <= self.limits.max_num_pages:
                        self.valid = False
                    elif self.page_count < self.limits.page_range[0]:
                        self.valid = False

        except (FileNotFoundError, OSError) as e:
            self.valid = False
            _log.exception(
                f"File {self.file.name} not found or cannot be opened.", exc_info=e
            )
            # raise
        except RuntimeError as e:
            self.valid = False
            _log.exception(
                f"An unexpected error occurred while opening the document {self.file.name}",
                exc_info=e,
            )
            # raise

    def _init_doc(
        self,
        backend: Type[AbstractDocumentBackend],
        path_or_stream: Union[BytesIO, Path],
    ) -> None:
        self._backend = backend(self, path_or_stream=path_or_stream)
        if not self._backend.is_valid():
            self.valid = False


class DocumentFormat(str, Enum):
    V2 = "v2"
    V1 = "v1"


class ConversionResult(BaseModel):
    input: InputDocument

    status: ConversionStatus = ConversionStatus.PENDING  # failure, success
    errors: List[ErrorItem] = []  # structure to keep errors

    pages: List[Page] = []
    assembled: AssembledUnit = AssembledUnit()
    timings: Dict[str, ProfilingItem] = {}

    document: DoclingDocument = _EMPTY_DOCLING_DOC

    @property
    @deprecated("Use document instead.")
    def legacy_document(self):
        return docling_document_to_legacy(self.document)


class _DummyBackend(AbstractDocumentBackend):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def is_valid(self) -> bool:
        return False

    @classmethod
    def supported_formats(cls) -> Set[InputFormat]:
        return set()

    @classmethod
    def supports_pagination(cls) -> bool:
        return False

    def unload(self):
        return super().unload()


class _DocumentConversionInput(BaseModel):

    path_or_stream_iterator: Iterable[Union[Path, str, DocumentStream]]
    headers: Optional[Dict[str, str]] = None
    limits: Optional[DocumentLimits] = DocumentLimits()

    def docs(
        self, format_options: Dict[InputFormat, "FormatOption"]
    ) -> Iterable[InputDocument]:
        for item in self.path_or_stream_iterator:
            obj = (
                resolve_source_to_stream(item, self.headers)
                if isinstance(item, str)
                else item
            )
            format = self._guess_format(obj)
            backend: Type[AbstractDocumentBackend]
            if format not in format_options.keys():
                _log.error(
                    f"Input document {obj.name} does not match any allowed format."
                )
                backend = _DummyBackend
            else:
                backend = format_options[format].backend

            if isinstance(obj, Path):
                yield InputDocument(
                    path_or_stream=obj,
                    format=format,  # type: ignore[arg-type]
                    filename=obj.name,
                    limits=self.limits,
                    backend=backend,
                )
            elif isinstance(obj, DocumentStream):
                yield InputDocument(
                    path_or_stream=obj.stream,
                    format=format,  # type: ignore[arg-type]
                    filename=obj.name,
                    limits=self.limits,
                    backend=backend,
                )
            else:
                raise RuntimeError(f"Unexpected obj type in iterator: {type(obj)}")

    def _guess_format(self, obj: Union[Path, DocumentStream]) -> Optional[InputFormat]:
        content = b""  # empty binary blob
        formats: list[InputFormat] = []

        if isinstance(obj, Path):
            mime = filetype.guess_mime(str(obj))
            if mime is None:
                ext = obj.suffix[1:]
                mime = _DocumentConversionInput._mime_from_extension(ext)
            if mime is None:  # must guess from
                with obj.open("rb") as f:
                    content = f.read(1024)  # Read first 1KB

        elif isinstance(obj, DocumentStream):
            content = obj.stream.read(8192)
            obj.stream.seek(0)
            mime = filetype.guess_mime(content)
            if mime is None:
                ext = (
                    obj.name.rsplit(".", 1)[-1]
                    if ("." in obj.name and not obj.name.startswith("."))
                    else ""
                )
                mime = _DocumentConversionInput._mime_from_extension(ext)

        mime = mime or _DocumentConversionInput._detect_html_xhtml(content)
        mime = mime or _DocumentConversionInput._detect_csv(content)
        mime = mime or "text/plain"
        formats = MimeTypeToFormat.get(mime, [])
        if formats:
            if len(formats) == 1 and mime not in ("text/plain"):
                return formats[0]
            else:  # ambiguity in formats
                return _DocumentConversionInput._guess_from_content(
                    content, mime, formats
                )
        else:
            return None

    @staticmethod
    def _guess_from_content(
        content: bytes, mime: str, formats: list[InputFormat]
    ) -> Optional[InputFormat]:
        """Guess the input format of a document by checking part of its content."""
        input_format: Optional[InputFormat] = None
        content_str = content.decode("utf-8")

        if mime == "application/xml":
            match_doctype = re.search(r"<!DOCTYPE [^>]+>", content_str)
            if match_doctype:
                xml_doctype = match_doctype.group()
                if InputFormat.XML_USPTO in formats and any(
                    item in xml_doctype
                    for item in (
                        "us-patent-application-v4",
                        "us-patent-grant-v4",
                        "us-grant-025",
                        "patent-application-publication",
                    )
                ):
                    input_format = InputFormat.XML_USPTO

                if InputFormat.XML_JATS in formats and (
                    "JATS-journalpublishing" in xml_doctype
                    or "JATS-archive" in xml_doctype
                ):
                    input_format = InputFormat.XML_JATS

        elif mime == "text/plain":
            if InputFormat.XML_USPTO in formats and content_str.startswith("PATN\r\n"):
                input_format = InputFormat.XML_USPTO

        return input_format

    @staticmethod
    def _mime_from_extension(ext):
        mime = None
        if ext in FormatToExtensions[InputFormat.ASCIIDOC]:
            mime = FormatToMimeType[InputFormat.ASCIIDOC][0]
        elif ext in FormatToExtensions[InputFormat.HTML]:
            mime = FormatToMimeType[InputFormat.HTML][0]
        elif ext in FormatToExtensions[InputFormat.MD]:
            mime = FormatToMimeType[InputFormat.MD][0]
        elif ext in FormatToExtensions[InputFormat.CSV]:
            mime = FormatToMimeType[InputFormat.CSV][0]
        elif ext in FormatToExtensions[InputFormat.JSON_DOCLING]:
            mime = FormatToMimeType[InputFormat.JSON_DOCLING][0]
        elif ext in FormatToExtensions[InputFormat.PDF]:
            mime = FormatToMimeType[InputFormat.PDF][0]
        return mime

    @staticmethod
    def _detect_html_xhtml(
        content: bytes,
    ) -> Optional[Literal["application/xhtml+xml", "application/xml", "text/html"]]:
        """Guess the mime type of an XHTML, HTML, or XML file from its content.

        Args:
            content: A short piece of a document from its beginning.

        Returns:
            The mime type of an XHTML, HTML, or XML file, or None if the content does
              not match any of these formats.
        """
        content_str = content.decode("ascii", errors="ignore").lower()
        # Remove XML comments
        content_str = re.sub(r"<!--(.*?)-->", "", content_str, flags=re.DOTALL)
        content_str = content_str.lstrip()

        if re.match(r"<\?xml", content_str):
            if "xhtml" in content_str[:1000]:
                return "application/xhtml+xml"
            else:
                return "application/xml"

        if re.match(r"<!doctype\s+html|<html|<head|<body", content_str):
            return "text/html"

        p = re.compile(
            r"<!doctype\s+(?P<root>[a-zA-Z_:][a-zA-Z0-9_:.-]*)\s+.*>\s*<(?P=root)\b"
        )
        if p.search(content_str):
            return "application/xml"

        return None

    @staticmethod
    def _detect_csv(
        content: bytes,
    ) -> Optional[Literal["text/csv"]]:
        """Guess the mime type of a CSV file from its content.

        Args:
            content: A short piece of a document from its beginning.

        Returns:
            The mime type of a CSV file, or None if the content does
              not match any of the format.
        """
        content_str = content.decode("ascii", errors="ignore").strip()

        # Ensure there's at least one newline (CSV is usually multi-line)
        if "\n" not in content_str:
            return None

        # Use csv.Sniffer to detect CSV characteristics
        try:
            dialect = csv.Sniffer().sniff(content_str)
            if dialect.delimiter in {",", ";", "\t", "|"}:  # Common delimiters
                return "text/csv"
        except csv.Error:
            return None

        return None


================================================
File: docling/datamodel/pipeline_options.py
================================================
import logging
import os
import re
import warnings
from enum import Enum
from pathlib import Path
from typing import Annotated, Any, Dict, List, Literal, Optional, Union

from pydantic import (
    AnyUrl,
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
    model_validator,
    validator,
)
from pydantic_settings import (
    BaseSettings,
    PydanticBaseSettingsSource,
    SettingsConfigDict,
)
from typing_extensions import deprecated

_log = logging.getLogger(__name__)


class AcceleratorDevice(str, Enum):
    """Devices to run model inference"""

    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"


class AcceleratorOptions(BaseSettings):
    model_config = SettingsConfigDict(
        env_prefix="DOCLING_", env_nested_delimiter="_", populate_by_name=True
    )

    num_threads: int = 4
    device: Union[str, AcceleratorDevice] = "auto"
    cuda_use_flash_attention2: bool = False

    @field_validator("device")
    def validate_device(cls, value):
        # "auto", "cpu", "cuda", "mps", or "cuda:N"
        if value in {d.value for d in AcceleratorDevice} or re.match(
            r"^cuda(:\d+)?$", value
        ):
            return value
        raise ValueError(
            "Invalid device option. Use 'auto', 'cpu', 'mps', 'cuda', or 'cuda:N'."
        )

    @model_validator(mode="before")
    @classmethod
    def check_alternative_envvars(cls, data: Any) -> Any:
        r"""
        Set num_threads from the "alternative" envvar OMP_NUM_THREADS.
        The alternative envvar is used only if it is valid and the regular envvar is not set.

        Notice: The standard pydantic settings mechanism with parameter "aliases" does not provide
        the same functionality. In case the alias envvar is set and the user tries to override the
        parameter in settings initialization, Pydantic treats the parameter provided in __init__()
        as an extra input instead of simply overwriting the evvar value for that parameter.
        """
        if isinstance(data, dict):
            input_num_threads = data.get("num_threads")
            # Check if to set the num_threads from the alternative envvar
            if input_num_threads is None:
                docling_num_threads = os.getenv("DOCLING_NUM_THREADS")
                omp_num_threads = os.getenv("OMP_NUM_THREADS")
                if docling_num_threads is None and omp_num_threads is not None:
                    try:
                        data["num_threads"] = int(omp_num_threads)
                    except ValueError:
                        _log.error(
                            "Ignoring misformatted envvar OMP_NUM_THREADS '%s'",
                            omp_num_threads,
                        )
        return data


class TableFormerMode(str, Enum):
    """Modes for the TableFormer model."""

    FAST = "fast"
    ACCURATE = "accurate"


class TableStructureOptions(BaseModel):
    """Options for the table structure."""

    do_cell_matching: bool = (
        True
        # True:  Matches predictions back to PDF cells. Can break table output if PDF cells
        #        are merged across table columns.
        # False: Let table structure model define the text cells, ignore PDF cells.
    )
    mode: TableFormerMode = TableFormerMode.FAST


class OcrOptions(BaseModel):
    """OCR options."""

    kind: str
    lang: List[str]
    force_full_page_ocr: bool = False  # If enabled a full page OCR is always applied
    bitmap_area_threshold: float = (
        0.05  # percentage of the area for a bitmap to processed with OCR
    )


class RapidOcrOptions(OcrOptions):
    """Options for the RapidOCR engine."""

    kind: Literal["rapidocr"] = "rapidocr"

    # English and chinese are the most commly used models and have been tested with RapidOCR.
    lang: List[str] = [
        "english",
        "chinese",
    ]  # However, language as a parameter is not supported by rapidocr yet and hence changing this options doesn't affect anything.
    # For more details on supported languages by RapidOCR visit https://rapidai.github.io/RapidOCRDocs/blog/2022/09/28/%E6%94%AF%E6%8C%81%E8%AF%86%E5%88%AB%E8%AF%AD%E8%A8%80/

    # For more details on the following options visit https://rapidai.github.io/RapidOCRDocs/install_usage/api/RapidOCR/
    text_score: float = 0.5  # same default as rapidocr

    use_det: Optional[bool] = None  # same default as rapidocr
    use_cls: Optional[bool] = None  # same default as rapidocr
    use_rec: Optional[bool] = None  # same default as rapidocr

    # class Device(Enum):
    #     CPU = "CPU"
    #     CUDA = "CUDA"
    #     DIRECTML = "DIRECTML"
    #     AUTO = "AUTO"

    # device: Device = Device.AUTO  # Default value is AUTO

    print_verbose: bool = False  # same default as rapidocr

    det_model_path: Optional[str] = None  # same default as rapidocr
    cls_model_path: Optional[str] = None  # same default as rapidocr
    rec_model_path: Optional[str] = None  # same default as rapidocr
    rec_keys_path: Optional[str] = None  # same default as rapidocr

    model_config = ConfigDict(
        extra="forbid",
    )


class EasyOcrOptions(OcrOptions):
    """Options for the EasyOCR engine."""

    kind: Literal["easyocr"] = "easyocr"
    lang: List[str] = ["fr", "de", "es", "en"]

    use_gpu: Optional[bool] = None

    confidence_threshold: float = 0.5

    model_storage_directory: Optional[str] = None
    recog_network: Optional[str] = "standard"
    download_enabled: bool = True

    model_config = ConfigDict(
        extra="forbid",
        protected_namespaces=(),
    )


class TesseractCliOcrOptions(OcrOptions):
    """Options for the TesseractCli engine."""

    kind: Literal["tesseract"] = "tesseract"
    lang: List[str] = ["fra", "deu", "spa", "eng"]
    tesseract_cmd: str = "tesseract"
    path: Optional[str] = None

    model_config = ConfigDict(
        extra="forbid",
    )


class TesseractOcrOptions(OcrOptions):
    """Options for the Tesseract engine."""

    kind: Literal["tesserocr"] = "tesserocr"
    lang: List[str] = ["fra", "deu", "spa", "eng"]
    path: Optional[str] = None

    model_config = ConfigDict(
        extra="forbid",
    )


class OcrMacOptions(OcrOptions):
    """Options for the Mac OCR engine."""

    kind: Literal["ocrmac"] = "ocrmac"
    lang: List[str] = ["fr-FR", "de-DE", "es-ES", "en-US"]
    recognition: str = "accurate"
    framework: str = "vision"

    model_config = ConfigDict(
        extra="forbid",
    )


class PictureDescriptionBaseOptions(BaseModel):
    kind: str
    batch_size: int = 8
    scale: float = 2

    bitmap_area_threshold: float = (
        0.2  # percentage of the area for a bitmap to processed with the models
    )


class PictureDescriptionApiOptions(PictureDescriptionBaseOptions):
    kind: Literal["api"] = "api"

    url: AnyUrl = AnyUrl("http://localhost:8000/v1/chat/completions")
    headers: Dict[str, str] = {}
    params: Dict[str, Any] = {}
    timeout: float = 20

    prompt: str = "Describe this image in a few sentences."
    provenance: str = ""


class PictureDescriptionVlmOptions(PictureDescriptionBaseOptions):
    kind: Literal["vlm"] = "vlm"

    repo_id: str
    prompt: str = "Describe this image in a few sentences."
    # Config from here https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig
    generation_config: Dict[str, Any] = dict(max_new_tokens=200, do_sample=False)

    @property
    def repo_cache_folder(self) -> str:
        return self.repo_id.replace("/", "--")


smolvlm_picture_description = PictureDescriptionVlmOptions(
    repo_id="HuggingFaceTB/SmolVLM-256M-Instruct"
)
# phi_picture_description = PictureDescriptionVlmOptions(repo_id="microsoft/Phi-3-vision-128k-instruct")
granite_picture_description = PictureDescriptionVlmOptions(
    repo_id="ibm-granite/granite-vision-3.1-2b-preview",
    prompt="What is shown in this image?",
)


class BaseVlmOptions(BaseModel):
    kind: str
    prompt: str


class ResponseFormat(str, Enum):
    DOCTAGS = "doctags"
    MARKDOWN = "markdown"


class HuggingFaceVlmOptions(BaseVlmOptions):
    kind: Literal["hf_model_options"] = "hf_model_options"

    repo_id: str
    load_in_8bit: bool = True
    llm_int8_threshold: float = 6.0
    quantized: bool = False

    response_format: ResponseFormat

    @property
    def repo_cache_folder(self) -> str:
        return self.repo_id.replace("/", "--")


smoldocling_vlm_conversion_options = HuggingFaceVlmOptions(
    repo_id="ds4sd/SmolDocling-256M-preview",
    prompt="Convert this page to docling.",
    response_format=ResponseFormat.DOCTAGS,
)

granite_vision_vlm_conversion_options = HuggingFaceVlmOptions(
    repo_id="ibm-granite/granite-vision-3.1-2b-preview",
    # prompt="OCR the full page to markdown.",
    prompt="OCR this image.",
    response_format=ResponseFormat.MARKDOWN,
)


# Define an enum for the backend options
class PdfBackend(str, Enum):
    """Enum of valid PDF backends."""

    PYPDFIUM2 = "pypdfium2"
    DLPARSE_V1 = "dlparse_v1"
    DLPARSE_V2 = "dlparse_v2"


# Define an enum for the ocr engines
class OcrEngine(str, Enum):
    """Enum of valid OCR engines."""

    EASYOCR = "easyocr"
    TESSERACT_CLI = "tesseract_cli"
    TESSERACT = "tesseract"
    OCRMAC = "ocrmac"
    RAPIDOCR = "rapidocr"


class PipelineOptions(BaseModel):
    """Base pipeline options."""

    create_legacy_output: bool = (
        True  # This default will be set to False on a future version of docling
    )
    document_timeout: Optional[float] = None
    accelerator_options: AcceleratorOptions = AcceleratorOptions()
    enable_remote_services: bool = False


class PaginatedPipelineOptions(PipelineOptions):
    images_scale: float = 1.0
    generate_page_images: bool = False
    generate_picture_images: bool = False


class VlmPipelineOptions(PaginatedPipelineOptions):
    artifacts_path: Optional[Union[Path, str]] = None

    generate_page_images: bool = True
    force_backend_text: bool = (
        False  # (To be used with vlms, or other generative models)
    )
    # If True, text from backend will be used instead of generated text
    vlm_options: Union[HuggingFaceVlmOptions] = smoldocling_vlm_conversion_options


class PdfPipelineOptions(PaginatedPipelineOptions):
    """Options for the PDF pipeline."""

    artifacts_path: Optional[Union[Path, str]] = None
    do_table_structure: bool = True  # True: perform table structure extraction
    do_ocr: bool = True  # True: perform OCR, replace programmatic PDF text
    do_code_enrichment: bool = False  # True: perform code OCR
    do_formula_enrichment: bool = False  # True: perform formula OCR, return Latex code
    do_picture_classification: bool = False  # True: classify pictures in documents
    do_picture_description: bool = False  # True: run describe pictures in documents
    force_backend_text: bool = (
        False  # (To be used with vlms, or other generative models)
    )
    # If True, text from backend will be used instead of generated text

    table_structure_options: TableStructureOptions = TableStructureOptions()
    ocr_options: Union[
        EasyOcrOptions,
        TesseractCliOcrOptions,
        TesseractOcrOptions,
        OcrMacOptions,
        RapidOcrOptions,
    ] = Field(EasyOcrOptions(), discriminator="kind")
    picture_description_options: Annotated[
        Union[PictureDescriptionApiOptions, PictureDescriptionVlmOptions],
        Field(discriminator="kind"),
    ] = smolvlm_picture_description

    images_scale: float = 1.0
    generate_page_images: bool = False
    generate_picture_images: bool = False
    generate_table_images: bool = Field(
        default=False,
        deprecated=(
            "Field `generate_table_images` is deprecated. "
            "To obtain table images, set `PdfPipelineOptions.generate_page_images = True` "
            "before conversion and then use the `TableItem.get_image` function."
        ),
    )


================================================
File: docling/datamodel/settings.py
================================================
import sys
from pathlib import Path
from typing import Annotated, Optional, Tuple

from pydantic import BaseModel, PlainValidator
from pydantic_settings import BaseSettings, SettingsConfigDict


def _validate_page_range(v: Tuple[int, int]) -> Tuple[int, int]:
    if v[0] < 1 or v[1] < v[0]:
        raise ValueError(
            "Invalid page range: start must be ≥ 1 and end must be ≥ start."
        )
    return v


PageRange = Annotated[Tuple[int, int], PlainValidator(_validate_page_range)]

DEFAULT_PAGE_RANGE: PageRange = (1, sys.maxsize)


class DocumentLimits(BaseModel):
    max_num_pages: int = sys.maxsize
    max_file_size: int = sys.maxsize
    page_range: PageRange = DEFAULT_PAGE_RANGE


class BatchConcurrencySettings(BaseModel):
    doc_batch_size: int = 2
    doc_batch_concurrency: int = 2
    page_batch_size: int = 4
    page_batch_concurrency: int = 2
    elements_batch_size: int = 16

    # doc_batch_size: int = 1
    # doc_batch_concurrency: int = 1
    # page_batch_size: int = 1
    # page_batch_concurrency: int = 1

    # model_concurrency: int = 2

    # To force models into single core: export OMP_NUM_THREADS=1


class DebugSettings(BaseModel):
    visualize_cells: bool = False
    visualize_ocr: bool = False
    visualize_layout: bool = False
    visualize_raw_layout: bool = False
    visualize_tables: bool = False

    profile_pipeline_timings: bool = False

    # Path used to output debug information.
    debug_output_path: str = str(Path.cwd() / "debug")


class AppSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="DOCLING_", env_nested_delimiter="_")

    perf: BatchConcurrencySettings
    debug: DebugSettings

    cache_dir: Path = Path.home() / ".cache" / "docling"
    artifacts_path: Optional[Path] = None


settings = AppSettings(perf=BatchConcurrencySettings(), debug=DebugSettings())


================================================
File: docling/models/base_model.py
================================================
from abc import ABC, abstractmethod
from typing import Any, Generic, Iterable, Optional

from docling_core.types.doc import BoundingBox, DocItem, DoclingDocument, NodeItem
from typing_extensions import TypeVar

from docling.datamodel.base_models import ItemAndImageEnrichmentElement, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.settings import settings


class BasePageModel(ABC):
    @abstractmethod
    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        pass


EnrichElementT = TypeVar("EnrichElementT", default=NodeItem)


class GenericEnrichmentModel(ABC, Generic[EnrichElementT]):

    elements_batch_size: int = settings.perf.elements_batch_size

    @abstractmethod
    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        pass

    @abstractmethod
    def prepare_element(
        self, conv_res: ConversionResult, element: NodeItem
    ) -> Optional[EnrichElementT]:
        pass

    @abstractmethod
    def __call__(
        self, doc: DoclingDocument, element_batch: Iterable[EnrichElementT]
    ) -> Iterable[NodeItem]:
        pass


class BaseEnrichmentModel(GenericEnrichmentModel[NodeItem]):

    def prepare_element(
        self, conv_res: ConversionResult, element: NodeItem
    ) -> Optional[NodeItem]:
        if self.is_processable(doc=conv_res.document, element=element):
            return element
        return None


class BaseItemAndImageEnrichmentModel(
    GenericEnrichmentModel[ItemAndImageEnrichmentElement]
):

    images_scale: float
    expansion_factor: float = 0.0

    def prepare_element(
        self, conv_res: ConversionResult, element: NodeItem
    ) -> Optional[ItemAndImageEnrichmentElement]:
        if not self.is_processable(doc=conv_res.document, element=element):
            return None

        assert isinstance(element, DocItem)
        element_prov = element.prov[0]

        bbox = element_prov.bbox
        width = bbox.r - bbox.l
        height = bbox.t - bbox.b

        # TODO: move to a utility in the BoundingBox class
        expanded_bbox = BoundingBox(
            l=bbox.l - width * self.expansion_factor,
            t=bbox.t + height * self.expansion_factor,
            r=bbox.r + width * self.expansion_factor,
            b=bbox.b - height * self.expansion_factor,
            coord_origin=bbox.coord_origin,
        )

        page_ix = element_prov.page_no - 1
        cropped_image = conv_res.pages[page_ix].get_image(
            scale=self.images_scale, cropbox=expanded_bbox
        )
        return ItemAndImageEnrichmentElement(item=element, image=cropped_image)


================================================
File: docling/models/base_ocr_model.py
================================================
import copy
import logging
from abc import abstractmethod
from pathlib import Path
from typing import Iterable, List

import numpy as np
from docling_core.types.doc import BoundingBox, CoordOrigin
from PIL import Image, ImageDraw
from rtree import index
from scipy.ndimage import binary_dilation, find_objects, label

from docling.datamodel.base_models import Cell, OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import OcrOptions
from docling.datamodel.settings import settings
from docling.models.base_model import BasePageModel

_log = logging.getLogger(__name__)


class BaseOcrModel(BasePageModel):
    def __init__(self, enabled: bool, options: OcrOptions):
        self.enabled = enabled
        self.options = options

    # Computes the optimum amount and coordinates of rectangles to OCR on a given page
    def get_ocr_rects(self, page: Page) -> List[BoundingBox]:
        BITMAP_COVERAGE_TRESHOLD = 0.75
        assert page.size is not None

        def find_ocr_rects(size, bitmap_rects):
            image = Image.new(
                "1", (round(size.width), round(size.height))
            )  # '1' mode is binary

            # Draw all bitmap rects into a binary image
            draw = ImageDraw.Draw(image)
            for rect in bitmap_rects:
                x0, y0, x1, y1 = rect.as_tuple()
                x0, y0, x1, y1 = round(x0), round(y0), round(x1), round(y1)
                draw.rectangle([(x0, y0), (x1, y1)], fill=1)

            np_image = np.array(image)

            # Dilate the image by 10 pixels to merge nearby bitmap rectangles
            structure = np.ones(
                (20, 20)
            )  # Create a 20x20 structure element (10 pixels in all directions)
            np_image = binary_dilation(np_image > 0, structure=structure)

            # Find the connected components
            labeled_image, num_features = label(
                np_image > 0
            )  # Label black (0 value) regions

            # Find enclosing bounding boxes for each connected component.
            slices = find_objects(labeled_image)
            bounding_boxes = [
                BoundingBox(
                    l=slc[1].start,
                    t=slc[0].start,
                    r=slc[1].stop - 1,
                    b=slc[0].stop - 1,
                    coord_origin=CoordOrigin.TOPLEFT,
                )
                for slc in slices
            ]

            # Compute area fraction on page covered by bitmaps
            area_frac = np.sum(np_image > 0) / (size.width * size.height)

            return (area_frac, bounding_boxes)  # fraction covered  # boxes

        if page._backend is not None:
            bitmap_rects = page._backend.get_bitmap_rects()
        else:
            bitmap_rects = []
        coverage, ocr_rects = find_ocr_rects(page.size, bitmap_rects)

        # return full-page rectangle if page is dominantly covered with bitmaps
        if self.options.force_full_page_ocr or coverage > max(
            BITMAP_COVERAGE_TRESHOLD, self.options.bitmap_area_threshold
        ):
            return [
                BoundingBox(
                    l=0,
                    t=0,
                    r=page.size.width,
                    b=page.size.height,
                    coord_origin=CoordOrigin.TOPLEFT,
                )
            ]
        # return individual rectangles if the bitmap coverage is above the threshold
        elif coverage > self.options.bitmap_area_threshold:
            return ocr_rects
        else:  # overall coverage of bitmaps is too low, drop all bitmap rectangles.
            return []

    # Filters OCR cells by dropping any OCR cell that intersects with an existing programmatic cell.
    def _filter_ocr_cells(self, ocr_cells, programmatic_cells):
        # Create R-tree index for programmatic cells
        p = index.Property()
        p.dimension = 2
        idx = index.Index(properties=p)
        for i, cell in enumerate(programmatic_cells):
            idx.insert(i, cell.bbox.as_tuple())

        def is_overlapping_with_existing_cells(ocr_cell):
            # Query the R-tree to get overlapping rectangles
            possible_matches_index = list(idx.intersection(ocr_cell.bbox.as_tuple()))

            return (
                len(possible_matches_index) > 0
            )  # this is a weak criterion but it works.

        filtered_ocr_cells = [
            rect for rect in ocr_cells if not is_overlapping_with_existing_cells(rect)
        ]
        return filtered_ocr_cells

    def post_process_cells(self, ocr_cells, programmatic_cells):
        r"""
        Post-process the ocr and programmatic cells and return the final list of of cells
        """
        if self.options.force_full_page_ocr:
            # If a full page OCR is forced, use only the OCR cells
            cells = [
                Cell(id=c_ocr.id, text=c_ocr.text, bbox=c_ocr.bbox)
                for c_ocr in ocr_cells
            ]
            return cells

        ## Remove OCR cells which overlap with programmatic cells.
        filtered_ocr_cells = self._filter_ocr_cells(ocr_cells, programmatic_cells)
        programmatic_cells.extend(filtered_ocr_cells)
        return programmatic_cells

    def draw_ocr_rects_and_cells(self, conv_res, page, ocr_rects, show: bool = False):
        image = copy.deepcopy(page.image)
        scale_x = image.width / page.size.width
        scale_y = image.height / page.size.height

        draw = ImageDraw.Draw(image, "RGBA")

        # Draw OCR rectangles as yellow filled rect
        for rect in ocr_rects:
            x0, y0, x1, y1 = rect.as_tuple()
            y0 *= scale_x
            y1 *= scale_y
            x0 *= scale_x
            x1 *= scale_x

            shade_color = (255, 255, 0, 40)  # transparent yellow
            draw.rectangle([(x0, y0), (x1, y1)], fill=shade_color, outline=None)

        # Draw OCR and programmatic cells
        for tc in page.cells:
            x0, y0, x1, y1 = tc.bbox.as_tuple()
            y0 *= scale_x
            y1 *= scale_y
            x0 *= scale_x
            x1 *= scale_x

            if y1 <= y0:
                y1, y0 = y0, y1

            color = "gray"
            if isinstance(tc, OcrCell):
                color = "magenta"
            draw.rectangle([(x0, y0), (x1, y1)], outline=color)

        if show:
            image.show()
        else:
            out_path: Path = (
                Path(settings.debug.debug_output_path)
                / f"debug_{conv_res.input.file.stem}"
            )
            out_path.mkdir(parents=True, exist_ok=True)

            out_file = out_path / f"ocr_page_{page.page_no:05}.png"
            image.save(str(out_file), format="png")

    @abstractmethod
    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        pass


================================================
File: docling/models/code_formula_model.py
================================================
import re
from pathlib import Path
from typing import Iterable, List, Literal, Optional, Tuple, Union

import numpy as np
from docling_core.types.doc import (
    CodeItem,
    DocItemLabel,
    DoclingDocument,
    NodeItem,
    TextItem,
)
from docling_core.types.doc.labels import CodeLanguageLabel
from PIL import Image
from pydantic import BaseModel

from docling.datamodel.base_models import ItemAndImageEnrichmentElement
from docling.datamodel.pipeline_options import AcceleratorOptions
from docling.models.base_model import BaseItemAndImageEnrichmentModel
from docling.utils.accelerator_utils import decide_device


class CodeFormulaModelOptions(BaseModel):
    """
    Configuration options for the CodeFormulaModel.

    Attributes
    ----------
    kind : str
        Type of the model. Fixed value "code_formula".
    do_code_enrichment : bool
        True if code enrichment is enabled, False otherwise.
    do_formula_enrichment : bool
        True if formula enrichment is enabled, False otherwise.
    """

    kind: Literal["code_formula"] = "code_formula"
    do_code_enrichment: bool = True
    do_formula_enrichment: bool = True


class CodeFormulaModel(BaseItemAndImageEnrichmentModel):
    """
    Model for processing and enriching documents with code and formula predictions.

    Attributes
    ----------
    enabled : bool
        True if the model is enabled, False otherwise.
    options : CodeFormulaModelOptions
        Configuration options for the CodeFormulaModel.
    code_formula_model : CodeFormulaPredictor
        The predictor model for code and formula processing.

    Methods
    -------
    __init__(self, enabled, artifacts_path, accelerator_options, code_formula_options)
        Initializes the CodeFormulaModel with the given configuration options.
    is_processable(self, doc, element)
        Determines if a given element in a document can be processed by the model.
    __call__(self, doc, element_batch)
        Processes the given batch of elements and enriches them with predictions.
    """

    _model_repo_folder = "ds4sd--CodeFormula"
    elements_batch_size = 5
    images_scale = 1.66  # = 120 dpi, aligned with training data resolution
    expansion_factor = 0.03

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Path],
        options: CodeFormulaModelOptions,
        accelerator_options: AcceleratorOptions,
    ):
        """
        Initializes the CodeFormulaModel with the given configuration.

        Parameters
        ----------
        enabled : bool
            True if the model is enabled, False otherwise.
        artifacts_path : Path
            Path to the directory containing the model artifacts.
        options : CodeFormulaModelOptions
            Configuration options for the model.
        accelerator_options : AcceleratorOptions
            Options specifying the device and number of threads for acceleration.
        """
        self.enabled = enabled
        self.options = options

        if self.enabled:
            device = decide_device(accelerator_options.device)

            from docling_ibm_models.code_formula_model.code_formula_predictor import (
                CodeFormulaPredictor,
            )

            if artifacts_path is None:
                artifacts_path = self.download_models()
            else:
                artifacts_path = artifacts_path / self._model_repo_folder

            self.code_formula_model = CodeFormulaPredictor(
                artifacts_path=str(artifacts_path),
                device=device,
                num_threads=accelerator_options.num_threads,
            )

    @staticmethod
    def download_models(
        local_dir: Optional[Path] = None,
        force: bool = False,
        progress: bool = False,
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id="ds4sd/CodeFormula",
            force_download=force,
            local_dir=local_dir,
            revision="v1.0.1",
        )

        return Path(download_path)

    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        """
        Determines if a given element in a document can be processed by the model.

        Parameters
        ----------
        doc : DoclingDocument
            The document being processed.
        element : NodeItem
            The element within the document to check.

        Returns
        -------
        bool
            True if the element can be processed, False otherwise.
        """
        return self.enabled and (
            (isinstance(element, CodeItem) and self.options.do_code_enrichment)
            or (
                isinstance(element, TextItem)
                and element.label == DocItemLabel.FORMULA
                and self.options.do_formula_enrichment
            )
        )

    def _extract_code_language(self, input_string: str) -> Tuple[str, Optional[str]]:
        """Extracts a programming language from the beginning of a string.

        This function checks if the input string starts with a pattern of the form
        ``<_some_language_>``. If it does, it extracts the language string and returns
        a tuple of (remainder, language). Otherwise, it returns the original string
        and `None`.

        Args:
            input_string (str): The input string, which may start with ``<_language_>``.

        Returns:
            Tuple[str, Optional[str]]:
                A tuple where:
                - The first element is either:
                    - The remainder of the string (everything after ``<_language_>``),
                    if a match is found; or
                    - The original string, if no match is found.
                - The second element is the extracted language if a match is found;
                otherwise, `None`.
        """
        pattern = r"^<_([^>]+)_>\s*(.*)"
        match = re.match(pattern, input_string, flags=re.DOTALL)
        if match:
            language = str(match.group(1))  # the captured programming language
            remainder = str(match.group(2))  # everything after the <_language_>
            return remainder, language
        else:
            return input_string, None

    def _get_code_language_enum(self, value: Optional[str]) -> CodeLanguageLabel:
        """
        Converts a string to a corresponding `CodeLanguageLabel` enum member.

        If the provided string does not match any value in `CodeLanguageLabel`,
        it defaults to `CodeLanguageLabel.UNKNOWN`.

        Args:
            value (Optional[str]): The string representation of the code language or None.

        Returns:
            CodeLanguageLabel: The corresponding enum member if the value is valid,
            otherwise `CodeLanguageLabel.UNKNOWN`.
        """
        if not isinstance(value, str):
            return CodeLanguageLabel.UNKNOWN

        try:
            return CodeLanguageLabel(value)
        except ValueError:
            return CodeLanguageLabel.UNKNOWN

    def __call__(
        self,
        doc: DoclingDocument,
        element_batch: Iterable[ItemAndImageEnrichmentElement],
    ) -> Iterable[NodeItem]:
        """
        Processes the given batch of elements and enriches them with predictions.

        Parameters
        ----------
        doc : DoclingDocument
            The document being processed.
        element_batch : Iterable[ItemAndImageEnrichmentElement]
            A batch of elements to be processed.

        Returns
        -------
        Iterable[Any]
            An iterable of enriched elements.
        """
        if not self.enabled:
            for element in element_batch:
                yield element.item
            return

        labels: List[str] = []
        images: List[Union[Image.Image, np.ndarray]] = []
        elements: List[TextItem] = []
        for el in element_batch:
            assert isinstance(el.item, TextItem)
            elements.append(el.item)
            labels.append(el.item.label)
            images.append(el.image)

        outputs = self.code_formula_model.predict(images, labels)

        for item, output in zip(elements, outputs):
            if isinstance(item, CodeItem):
                output, code_language = self._extract_code_language(output)
                item.code_language = self._get_code_language_enum(code_language)
            item.text = output

            yield item


================================================
File: docling/models/document_picture_classifier.py
================================================
from pathlib import Path
from typing import Iterable, List, Literal, Optional, Tuple, Union

import numpy as np
from docling_core.types.doc import (
    DoclingDocument,
    NodeItem,
    PictureClassificationClass,
    PictureClassificationData,
    PictureItem,
)
from PIL import Image
from pydantic import BaseModel

from docling.datamodel.pipeline_options import AcceleratorOptions
from docling.models.base_model import BaseEnrichmentModel
from docling.utils.accelerator_utils import decide_device


class DocumentPictureClassifierOptions(BaseModel):
    """
    Options for configuring the DocumentPictureClassifier.

    Attributes
    ----------
    kind : Literal["document_picture_classifier"]
        Identifier for the type of classifier.
    """

    kind: Literal["document_picture_classifier"] = "document_picture_classifier"


class DocumentPictureClassifier(BaseEnrichmentModel):
    """
    A model for classifying pictures in documents.

    This class enriches document pictures with predicted classifications
    based on a predefined set of classes.

    Attributes
    ----------
    enabled : bool
        Whether the classifier is enabled for use.
    options : DocumentPictureClassifierOptions
        Configuration options for the classifier.
    document_picture_classifier : DocumentPictureClassifierPredictor
        The underlying prediction model, loaded if the classifier is enabled.

    Methods
    -------
    __init__(enabled, artifacts_path, options, accelerator_options)
        Initializes the classifier with specified configurations.
    is_processable(doc, element)
        Checks if the given element can be processed by the classifier.
    __call__(doc, element_batch)
        Processes a batch of elements and adds classification annotations.
    """

    _model_repo_folder = "ds4sd--DocumentFigureClassifier"
    images_scale = 2

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Path],
        options: DocumentPictureClassifierOptions,
        accelerator_options: AcceleratorOptions,
    ):
        """
        Initializes the DocumentPictureClassifier.

        Parameters
        ----------
        enabled : bool
            Indicates whether the classifier is enabled.
        artifacts_path : Optional[Union[Path, str]],
            Path to the directory containing model artifacts.
        options : DocumentPictureClassifierOptions
            Configuration options for the classifier.
        accelerator_options : AcceleratorOptions
            Options for configuring the device and parallelism.
        """
        self.enabled = enabled
        self.options = options

        if self.enabled:
            device = decide_device(accelerator_options.device)
            from docling_ibm_models.document_figure_classifier_model.document_figure_classifier_predictor import (
                DocumentFigureClassifierPredictor,
            )

            if artifacts_path is None:
                artifacts_path = self.download_models()
            else:
                artifacts_path = artifacts_path / self._model_repo_folder

            self.document_picture_classifier = DocumentFigureClassifierPredictor(
                artifacts_path=str(artifacts_path),
                device=device,
                num_threads=accelerator_options.num_threads,
            )

    @staticmethod
    def download_models(
        local_dir: Optional[Path] = None, force: bool = False, progress: bool = False
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id="ds4sd/DocumentFigureClassifier",
            force_download=force,
            local_dir=local_dir,
            revision="v1.0.0",
        )

        return Path(download_path)

    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        """
        Determines if the given element can be processed by the classifier.

        Parameters
        ----------
        doc : DoclingDocument
            The document containing the element.
        element : NodeItem
            The element to be checked.

        Returns
        -------
        bool
            True if the element is a PictureItem and processing is enabled; False otherwise.
        """
        return self.enabled and isinstance(element, PictureItem)

    def __call__(
        self,
        doc: DoclingDocument,
        element_batch: Iterable[NodeItem],
    ) -> Iterable[NodeItem]:
        """
        Processes a batch of elements and enriches them with classification predictions.

        Parameters
        ----------
        doc : DoclingDocument
            The document containing the elements to be processed.
        element_batch : Iterable[NodeItem]
            A batch of pictures to classify.

        Returns
        -------
        Iterable[NodeItem]
            An iterable of NodeItem objects after processing. The field
            'data.classification' is added containing the classification for each picture.
        """
        if not self.enabled:
            for element in element_batch:
                yield element
            return

        images: List[Union[Image.Image, np.ndarray]] = []
        elements: List[PictureItem] = []
        for el in element_batch:
            assert isinstance(el, PictureItem)
            elements.append(el)
            img = el.get_image(doc)
            assert img is not None
            images.append(img)

        outputs = self.document_picture_classifier.predict(images)

        for element, output in zip(elements, outputs):
            element.annotations.append(
                PictureClassificationData(
                    provenance="DocumentPictureClassifier",
                    predicted_classes=[
                        PictureClassificationClass(
                            class_name=pred[0],
                            confidence=pred[1],
                        )
                        for pred in output
                    ],
                )
            )

            yield element


================================================
File: docling/models/easyocr_model.py
================================================
import logging
import warnings
import zipfile
from pathlib import Path
from typing import Iterable, List, Optional

import numpy
from docling_core.types.doc import BoundingBox, CoordOrigin

from docling.datamodel.base_models import Cell, OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    EasyOcrOptions,
)
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.utils.accelerator_utils import decide_device
from docling.utils.profiling import TimeRecorder
from docling.utils.utils import download_url_with_progress

_log = logging.getLogger(__name__)


class EasyOcrModel(BaseOcrModel):
    _model_repo_folder = "EasyOcr"

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Path],
        options: EasyOcrOptions,
        accelerator_options: AcceleratorOptions,
    ):
        super().__init__(enabled=enabled, options=options)
        self.options: EasyOcrOptions

        self.scale = 3  # multiplier for 72 dpi == 216 dpi.

        if self.enabled:
            try:
                import easyocr
            except ImportError:
                raise ImportError(
                    "EasyOCR is not installed. Please install it via `pip install easyocr` to use this OCR engine. "
                    "Alternatively, Docling has support for other OCR engines. See the documentation."
                )

            if self.options.use_gpu is None:
                device = decide_device(accelerator_options.device)
                # Enable easyocr GPU if running on CUDA, MPS
                use_gpu = any(
                    [
                        device.startswith(x)
                        for x in [
                            AcceleratorDevice.CUDA.value,
                            AcceleratorDevice.MPS.value,
                        ]
                    ]
                )
            else:
                warnings.warn(
                    "Deprecated field. Better to set the `accelerator_options.device` in `pipeline_options`. "
                    "When `use_gpu and accelerator_options.device == AcceleratorDevice.CUDA` the GPU is used "
                    "to run EasyOCR. Otherwise, EasyOCR runs in CPU."
                )
                use_gpu = self.options.use_gpu

            download_enabled = self.options.download_enabled
            model_storage_directory = self.options.model_storage_directory
            if artifacts_path is not None and model_storage_directory is None:
                download_enabled = False
                model_storage_directory = str(artifacts_path / self._model_repo_folder)

            self.reader = easyocr.Reader(
                lang_list=self.options.lang,
                gpu=use_gpu,
                model_storage_directory=model_storage_directory,
                recog_network=self.options.recog_network,
                download_enabled=download_enabled,
                verbose=False,
            )

    @staticmethod
    def download_models(
        detection_models: List[str] = ["craft"],
        recognition_models: List[str] = ["english_g2", "latin_g2"],
        local_dir: Optional[Path] = None,
        force: bool = False,
        progress: bool = False,
    ) -> Path:
        # Models are located in https://github.com/JaidedAI/EasyOCR/blob/master/easyocr/config.py
        from easyocr.config import detection_models as det_models_dict
        from easyocr.config import recognition_models as rec_models_dict

        if local_dir is None:
            local_dir = settings.cache_dir / "models" / EasyOcrModel._model_repo_folder

        local_dir.mkdir(parents=True, exist_ok=True)

        # Collect models to download
        download_list = []
        for model_name in detection_models:
            if model_name in det_models_dict:
                download_list.append(det_models_dict[model_name])
        for model_name in recognition_models:
            if model_name in rec_models_dict["gen2"]:
                download_list.append(rec_models_dict["gen2"][model_name])

        # Download models
        for model_details in download_list:
            buf = download_url_with_progress(model_details["url"], progress=progress)
            with zipfile.ZipFile(buf, "r") as zip_ref:
                zip_ref.extractall(local_dir)

        return local_dir

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:

            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "ocr"):
                    ocr_rects = self.get_ocr_rects(page)

                    all_ocr_cells = []
                    for ocr_rect in ocr_rects:
                        # Skip zero area boxes
                        if ocr_rect.area() == 0:
                            continue
                        high_res_image = page._backend.get_page_image(
                            scale=self.scale, cropbox=ocr_rect
                        )
                        im = numpy.array(high_res_image)
                        result = self.reader.readtext(im)

                        del high_res_image
                        del im

                        cells = [
                            OcrCell(
                                id=ix,
                                text=line[1],
                                confidence=line[2],
                                bbox=BoundingBox.from_tuple(
                                    coord=(
                                        (line[0][0][0] / self.scale) + ocr_rect.l,
                                        (line[0][0][1] / self.scale) + ocr_rect.t,
                                        (line[0][2][0] / self.scale) + ocr_rect.l,
                                        (line[0][2][1] / self.scale) + ocr_rect.t,
                                    ),
                                    origin=CoordOrigin.TOPLEFT,
                                ),
                            )
                            for ix, line in enumerate(result)
                            if line[2] >= self.options.confidence_threshold
                        ]
                        all_ocr_cells.extend(cells)

                    # Post-process the cells
                    page.cells = self.post_process_cells(all_ocr_cells, page.cells)

                # DEBUG code:
                if settings.debug.visualize_ocr:
                    self.draw_ocr_rects_and_cells(conv_res, page, ocr_rects)

                yield page


================================================
File: docling/models/hf_vlm_model.py
================================================
import logging
import time
from pathlib import Path
from typing import Iterable, List, Optional

from docling.datamodel.base_models import Page, VlmPrediction
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    HuggingFaceVlmOptions,
)
from docling.datamodel.settings import settings
from docling.models.base_model import BasePageModel
from docling.utils.accelerator_utils import decide_device
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class HuggingFaceVlmModel(BasePageModel):

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Path],
        accelerator_options: AcceleratorOptions,
        vlm_options: HuggingFaceVlmOptions,
    ):
        self.enabled = enabled

        self.vlm_options = vlm_options

        if self.enabled:
            import torch
            from transformers import (  # type: ignore
                AutoModelForVision2Seq,
                AutoProcessor,
                BitsAndBytesConfig,
            )

            device = decide_device(accelerator_options.device)
            self.device = device

            _log.debug("Available device for HuggingFace VLM: {}".format(device))

            repo_cache_folder = vlm_options.repo_id.replace("/", "--")

            # PARAMETERS:
            if artifacts_path is None:
                artifacts_path = self.download_models(self.vlm_options.repo_id)
            elif (artifacts_path / repo_cache_folder).exists():
                artifacts_path = artifacts_path / repo_cache_folder

            self.param_question = vlm_options.prompt  # "Perform Layout Analysis."
            self.param_quantization_config = BitsAndBytesConfig(
                load_in_8bit=vlm_options.load_in_8bit,  # True,
                llm_int8_threshold=vlm_options.llm_int8_threshold,  # 6.0
            )
            self.param_quantized = vlm_options.quantized  # False

            self.processor = AutoProcessor.from_pretrained(artifacts_path)
            if not self.param_quantized:
                self.vlm_model = AutoModelForVision2Seq.from_pretrained(
                    artifacts_path,
                    device_map=device,
                    torch_dtype=torch.bfloat16,
                    _attn_implementation=(
                        "flash_attention_2"
                        if self.device.startswith("cuda")
                        and accelerator_options.cuda_use_flash_attention2
                        else "eager"
                    ),
                )  # .to(self.device)

            else:
                self.vlm_model = AutoModelForVision2Seq.from_pretrained(
                    artifacts_path,
                    device_map=device,
                    torch_dtype="auto",
                    quantization_config=self.param_quantization_config,
                    _attn_implementation=(
                        "flash_attention_2"
                        if self.device.startswith("cuda")
                        and accelerator_options.cuda_use_flash_attention2
                        else "eager"
                    ),
                )  # .to(self.device)

    @staticmethod
    def download_models(
        repo_id: str,
        local_dir: Optional[Path] = None,
        force: bool = False,
        progress: bool = False,
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id=repo_id,
            force_download=force,
            local_dir=local_dir,
            # revision="v0.0.1",
        )

        return Path(download_path)

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "vlm"):
                    assert page.size is not None

                    hi_res_image = page.get_image(scale=2.0)  # 144dpi
                    # hi_res_image = page.get_image(scale=1.0)  # 72dpi

                    if hi_res_image is not None:
                        im_width, im_height = hi_res_image.size

                    # populate page_tags with predicted doc tags
                    page_tags = ""

                    if hi_res_image:
                        if hi_res_image.mode != "RGB":
                            hi_res_image = hi_res_image.convert("RGB")

                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": "This is a page from a document.",
                                },
                                {"type": "image"},
                                {"type": "text", "text": self.param_question},
                            ],
                        }
                    ]
                    prompt = self.processor.apply_chat_template(
                        messages, add_generation_prompt=False
                    )
                    inputs = self.processor(
                        text=prompt, images=[hi_res_image], return_tensors="pt"
                    )
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}

                    start_time = time.time()
                    # Call model to generate:
                    generated_ids = self.vlm_model.generate(
                        **inputs, max_new_tokens=4096, use_cache=True
                    )

                    generation_time = time.time() - start_time
                    generated_texts = self.processor.batch_decode(
                        generated_ids[:, inputs["input_ids"].shape[1] :],
                        skip_special_tokens=False,
                    )[0]

                    num_tokens = len(generated_ids[0])
                    page_tags = generated_texts

                    # inference_time = time.time() - start_time
                    # tokens_per_second = num_tokens / generation_time
                    # print("")
                    # print(f"Page Inference Time: {inference_time:.2f} seconds")
                    # print(f"Total tokens on page: {num_tokens:.2f}")
                    # print(f"Tokens/sec: {tokens_per_second:.2f}")
                    # print("")
                    page.predictions.vlm_response = VlmPrediction(text=page_tags)

                yield page


================================================
File: docling/models/layout_model.py
================================================
import copy
import logging
import warnings
from pathlib import Path
from typing import Iterable, Optional, Union

from docling_core.types.doc import DocItemLabel
from docling_ibm_models.layoutmodel.layout_predictor import LayoutPredictor
from PIL import Image

from docling.datamodel.base_models import BoundingBox, Cluster, LayoutPrediction, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import AcceleratorOptions
from docling.datamodel.settings import settings
from docling.models.base_model import BasePageModel
from docling.utils.accelerator_utils import decide_device
from docling.utils.layout_postprocessor import LayoutPostprocessor
from docling.utils.profiling import TimeRecorder
from docling.utils.visualization import draw_clusters

_log = logging.getLogger(__name__)


class LayoutModel(BasePageModel):
    _model_repo_folder = "ds4sd--docling-models"
    _model_path = "model_artifacts/layout"

    TEXT_ELEM_LABELS = [
        DocItemLabel.TEXT,
        DocItemLabel.FOOTNOTE,
        DocItemLabel.CAPTION,
        DocItemLabel.CHECKBOX_UNSELECTED,
        DocItemLabel.CHECKBOX_SELECTED,
        DocItemLabel.SECTION_HEADER,
        DocItemLabel.PAGE_HEADER,
        DocItemLabel.PAGE_FOOTER,
        DocItemLabel.CODE,
        DocItemLabel.LIST_ITEM,
        DocItemLabel.FORMULA,
    ]
    PAGE_HEADER_LABELS = [DocItemLabel.PAGE_HEADER, DocItemLabel.PAGE_FOOTER]

    TABLE_LABELS = [DocItemLabel.TABLE, DocItemLabel.DOCUMENT_INDEX]
    FIGURE_LABEL = DocItemLabel.PICTURE
    FORMULA_LABEL = DocItemLabel.FORMULA
    CONTAINER_LABELS = [DocItemLabel.FORM, DocItemLabel.KEY_VALUE_REGION]

    def __init__(
        self, artifacts_path: Optional[Path], accelerator_options: AcceleratorOptions
    ):
        device = decide_device(accelerator_options.device)

        if artifacts_path is None:
            artifacts_path = self.download_models() / self._model_path
        else:
            # will become the default in the future
            if (artifacts_path / self._model_repo_folder).exists():
                artifacts_path = (
                    artifacts_path / self._model_repo_folder / self._model_path
                )
            elif (artifacts_path / self._model_path).exists():
                warnings.warn(
                    "The usage of artifacts_path containing directly "
                    f"{self._model_path} is deprecated. Please point "
                    "the artifacts_path to the parent containing "
                    f"the {self._model_repo_folder} folder.",
                    DeprecationWarning,
                    stacklevel=3,
                )
                artifacts_path = artifacts_path / self._model_path

        self.layout_predictor = LayoutPredictor(
            artifact_path=str(artifacts_path),
            device=device,
            num_threads=accelerator_options.num_threads,
        )

    @staticmethod
    def download_models(
        local_dir: Optional[Path] = None,
        force: bool = False,
        progress: bool = False,
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id="ds4sd/docling-models",
            force_download=force,
            local_dir=local_dir,
            revision="v2.1.0",
        )

        return Path(download_path)

    def draw_clusters_and_cells_side_by_side(
        self, conv_res, page, clusters, mode_prefix: str, show: bool = False
    ):
        """
        Draws a page image side by side with clusters filtered into two categories:
        - Left: Clusters excluding FORM, KEY_VALUE_REGION, and PICTURE.
        - Right: Clusters including FORM, KEY_VALUE_REGION, and PICTURE.
        Includes label names and confidence scores for each cluster.
        """
        scale_x = page.image.width / page.size.width
        scale_y = page.image.height / page.size.height

        # Filter clusters for left and right images
        exclude_labels = {
            DocItemLabel.FORM,
            DocItemLabel.KEY_VALUE_REGION,
            DocItemLabel.PICTURE,
        }
        left_clusters = [c for c in clusters if c.label not in exclude_labels]
        right_clusters = [c for c in clusters if c.label in exclude_labels]
        # Create a deep copy of the original image for both sides
        left_image = copy.deepcopy(page.image)
        right_image = copy.deepcopy(page.image)

        # Draw clusters on both images
        draw_clusters(left_image, left_clusters, scale_x, scale_y)
        draw_clusters(right_image, right_clusters, scale_x, scale_y)
        # Combine the images side by side
        combined_width = left_image.width * 2
        combined_height = left_image.height
        combined_image = Image.new("RGB", (combined_width, combined_height))
        combined_image.paste(left_image, (0, 0))
        combined_image.paste(right_image, (left_image.width, 0))
        if show:
            combined_image.show()
        else:
            out_path: Path = (
                Path(settings.debug.debug_output_path)
                / f"debug_{conv_res.input.file.stem}"
            )
            out_path.mkdir(parents=True, exist_ok=True)
            out_file = out_path / f"{mode_prefix}_layout_page_{page.page_no:05}.png"
            combined_image.save(str(out_file), format="png")

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "layout"):
                    assert page.size is not None
                    page_image = page.get_image(scale=1.0)
                    assert page_image is not None

                    clusters = []
                    for ix, pred_item in enumerate(
                        self.layout_predictor.predict(page_image)
                    ):
                        label = DocItemLabel(
                            pred_item["label"]
                            .lower()
                            .replace(" ", "_")
                            .replace("-", "_")
                        )  # Temporary, until docling-ibm-model uses docling-core types
                        cluster = Cluster(
                            id=ix,
                            label=label,
                            confidence=pred_item["confidence"],
                            bbox=BoundingBox.model_validate(pred_item),
                            cells=[],
                        )
                        clusters.append(cluster)

                    if settings.debug.visualize_raw_layout:
                        self.draw_clusters_and_cells_side_by_side(
                            conv_res, page, clusters, mode_prefix="raw"
                        )

                    # Apply postprocessing

                    processed_clusters, processed_cells = LayoutPostprocessor(
                        page.cells, clusters, page.size
                    ).postprocess()
                    # processed_clusters, processed_cells = clusters, page.cells

                    page.cells = processed_cells
                    page.predictions.layout = LayoutPrediction(
                        clusters=processed_clusters
                    )

                if settings.debug.visualize_layout:
                    self.draw_clusters_and_cells_side_by_side(
                        conv_res, page, processed_clusters, mode_prefix="postprocessed"
                    )

                yield page


================================================
File: docling/models/ocr_mac_model.py
================================================
import logging
import tempfile
from typing import Iterable, Optional, Tuple

from docling_core.types.doc import BoundingBox, CoordOrigin

from docling.datamodel.base_models import OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import OcrMacOptions
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class OcrMacModel(BaseOcrModel):
    def __init__(self, enabled: bool, options: OcrMacOptions):
        super().__init__(enabled=enabled, options=options)
        self.options: OcrMacOptions

        self.scale = 3  # multiplier for 72 dpi == 216 dpi.

        if self.enabled:
            install_errmsg = (
                "ocrmac is not correctly installed. "
                "Please install it via `pip install ocrmac` to use this OCR engine. "
                "Alternatively, Docling has support for other OCR engines. See the documentation: "
                "https://ds4sd.github.io/docling/installation/"
            )
            try:
                from ocrmac import ocrmac
            except ImportError:
                raise ImportError(install_errmsg)

            self.reader_RIL = ocrmac.OCR

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "ocr"):

                    ocr_rects = self.get_ocr_rects(page)

                    all_ocr_cells = []
                    for ocr_rect in ocr_rects:
                        # Skip zero area boxes
                        if ocr_rect.area() == 0:
                            continue
                        high_res_image = page._backend.get_page_image(
                            scale=self.scale, cropbox=ocr_rect
                        )

                        with tempfile.NamedTemporaryFile(
                            suffix=".png", mode="w"
                        ) as image_file:
                            fname = image_file.name
                            high_res_image.save(fname)

                            boxes = self.reader_RIL(
                                fname,
                                recognition_level=self.options.recognition,
                                framework=self.options.framework,
                                language_preference=self.options.lang,
                            ).recognize()

                        im_width, im_height = high_res_image.size
                        cells = []
                        for ix, (text, confidence, box) in enumerate(boxes):
                            x = float(box[0])
                            y = float(box[1])
                            w = float(box[2])
                            h = float(box[3])

                            x1 = x * im_width
                            y2 = (1 - y) * im_height

                            x2 = x1 + w * im_width
                            y1 = y2 - h * im_height

                            left = x1 / self.scale
                            top = y1 / self.scale
                            right = x2 / self.scale
                            bottom = y2 / self.scale

                            cells.append(
                                OcrCell(
                                    id=ix,
                                    text=text,
                                    confidence=confidence,
                                    bbox=BoundingBox.from_tuple(
                                        coord=(left, top, right, bottom),
                                        origin=CoordOrigin.TOPLEFT,
                                    ),
                                )
                            )

                        # del high_res_image
                        all_ocr_cells.extend(cells)

                    # Post-process the cells
                    page.cells = self.post_process_cells(all_ocr_cells, page.cells)

                # DEBUG code:
                if settings.debug.visualize_ocr:
                    self.draw_ocr_rects_and_cells(conv_res, page, ocr_rects)

                yield page


================================================
File: docling/models/page_assemble_model.py
================================================
import logging
import re
from typing import Iterable, List

from pydantic import BaseModel

from docling.datamodel.base_models import (
    AssembledUnit,
    ContainerElement,
    FigureElement,
    Page,
    PageElement,
    Table,
    TextElement,
)
from docling.datamodel.document import ConversionResult
from docling.models.base_model import BasePageModel
from docling.models.layout_model import LayoutModel
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class PageAssembleOptions(BaseModel):
    pass


class PageAssembleModel(BasePageModel):
    def __init__(self, options: PageAssembleOptions):
        self.options = options

    def sanitize_text(self, lines):
        if len(lines) <= 1:
            return " ".join(lines)

        for ix, line in enumerate(lines[1:]):
            prev_line = lines[ix]

            if prev_line.endswith("-"):
                prev_words = re.findall(r"\b[\w]+\b", prev_line)
                line_words = re.findall(r"\b[\w]+\b", line)

                if (
                    len(prev_words)
                    and len(line_words)
                    and prev_words[-1].isalnum()
                    and line_words[0].isalnum()
                ):
                    lines[ix] = prev_line[:-1]
            else:
                lines[ix] += " "

        sanitized_text = "".join(lines)

        # Text normalization
        sanitized_text = sanitized_text.replace("⁄", "/")
        sanitized_text = sanitized_text.replace("’", "'")
        sanitized_text = sanitized_text.replace("‘", "'")
        sanitized_text = sanitized_text.replace("“", '"')
        sanitized_text = sanitized_text.replace("”", '"')
        sanitized_text = sanitized_text.replace("•", "·")

        return sanitized_text.strip()  # Strip any leading or trailing whitespace

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "page_assemble"):

                    assert page.predictions.layout is not None

                    # assembles some JSON output page by page.

                    elements: List[PageElement] = []
                    headers: List[PageElement] = []
                    body: List[PageElement] = []

                    for cluster in page.predictions.layout.clusters:
                        # _log.info("Cluster label seen:", cluster.label)
                        if cluster.label in LayoutModel.TEXT_ELEM_LABELS:

                            textlines = [
                                cell.text.replace("\x02", "-").strip()
                                for cell in cluster.cells
                                if len(cell.text.strip()) > 0
                            ]
                            text = self.sanitize_text(textlines)
                            text_el = TextElement(
                                label=cluster.label,
                                id=cluster.id,
                                text=text,
                                page_no=page.page_no,
                                cluster=cluster,
                            )
                            elements.append(text_el)

                            if cluster.label in LayoutModel.PAGE_HEADER_LABELS:
                                headers.append(text_el)
                            else:
                                body.append(text_el)
                        elif cluster.label in LayoutModel.TABLE_LABELS:
                            tbl = None
                            if page.predictions.tablestructure:
                                tbl = page.predictions.tablestructure.table_map.get(
                                    cluster.id, None
                                )
                            if (
                                not tbl
                            ):  # fallback: add table without structure, if it isn't present
                                tbl = Table(
                                    label=cluster.label,
                                    id=cluster.id,
                                    text="",
                                    otsl_seq=[],
                                    table_cells=[],
                                    cluster=cluster,
                                    page_no=page.page_no,
                                )

                            elements.append(tbl)
                            body.append(tbl)
                        elif cluster.label == LayoutModel.FIGURE_LABEL:
                            fig = None
                            if page.predictions.figures_classification:
                                fig = page.predictions.figures_classification.figure_map.get(
                                    cluster.id, None
                                )
                            if (
                                not fig
                            ):  # fallback: add figure without classification, if it isn't present
                                fig = FigureElement(
                                    label=cluster.label,
                                    id=cluster.id,
                                    text="",
                                    data=None,
                                    cluster=cluster,
                                    page_no=page.page_no,
                                )
                            elements.append(fig)
                            body.append(fig)
                        elif cluster.label in LayoutModel.CONTAINER_LABELS:
                            container_el = ContainerElement(
                                label=cluster.label,
                                id=cluster.id,
                                page_no=page.page_no,
                                cluster=cluster,
                            )
                            elements.append(container_el)
                            body.append(container_el)

                    page.assembled = AssembledUnit(
                        elements=elements, headers=headers, body=body
                    )

                yield page


================================================
File: docling/models/page_preprocessing_model.py
================================================
from pathlib import Path
from typing import Iterable, Optional

from PIL import ImageDraw
from pydantic import BaseModel

from docling.datamodel.base_models import Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.settings import settings
from docling.models.base_model import BasePageModel
from docling.utils.profiling import TimeRecorder


class PagePreprocessingOptions(BaseModel):
    images_scale: Optional[float]


class PagePreprocessingModel(BasePageModel):
    def __init__(self, options: PagePreprocessingOptions):
        self.options = options

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "page_parse"):
                    page = self._populate_page_images(page)
                    page = self._parse_page_cells(conv_res, page)
                yield page

    # Generate the page image and store it in the page object
    def _populate_page_images(self, page: Page) -> Page:
        # default scale
        page.get_image(
            scale=1.0
        )  # puts the page image on the image cache at default scale

        images_scale = self.options.images_scale
        # user requested scales
        if images_scale is not None:
            page._default_image_scale = images_scale
            page.get_image(
                scale=images_scale
            )  # this will trigger storing the image in the internal cache

        return page

    # Extract and populate the page cells and store it in the page object
    def _parse_page_cells(self, conv_res: ConversionResult, page: Page) -> Page:
        assert page._backend is not None

        page.cells = list(page._backend.get_text_cells())

        # DEBUG code:
        def draw_text_boxes(image, cells, show: bool = False):
            draw = ImageDraw.Draw(image)
            for c in cells:
                x0, y0, x1, y1 = c.bbox.as_tuple()
                draw.rectangle([(x0, y0), (x1, y1)], outline="red")
            if show:
                image.show()
            else:
                out_path: Path = (
                    Path(settings.debug.debug_output_path)
                    / f"debug_{conv_res.input.file.stem}"
                )
                out_path.mkdir(parents=True, exist_ok=True)

                out_file = out_path / f"cells_page_{page.page_no:05}.png"
                image.save(str(out_file), format="png")

        if settings.debug.visualize_cells:
            draw_text_boxes(page.get_image(scale=1.0), page.cells)

        return page


================================================
File: docling/models/picture_description_api_model.py
================================================
import base64
import io
import logging
from typing import Iterable, List, Optional

import requests
from PIL import Image
from pydantic import BaseModel, ConfigDict

from docling.datamodel.pipeline_options import PictureDescriptionApiOptions
from docling.exceptions import OperationNotAllowed
from docling.models.picture_description_base_model import PictureDescriptionBaseModel

_log = logging.getLogger(__name__)


class ChatMessage(BaseModel):
    role: str
    content: str


class ResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str


class ResponseUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ApiResponse(BaseModel):
    model_config = ConfigDict(
        protected_namespaces=(),
    )

    id: str
    model: Optional[str] = None  # returned by openai
    choices: List[ResponseChoice]
    created: int
    usage: ResponseUsage


class PictureDescriptionApiModel(PictureDescriptionBaseModel):
    # elements_batch_size = 4

    def __init__(
        self,
        enabled: bool,
        enable_remote_services: bool,
        options: PictureDescriptionApiOptions,
    ):
        super().__init__(enabled=enabled, options=options)
        self.options: PictureDescriptionApiOptions

        if self.enabled:
            if not enable_remote_services:
                raise OperationNotAllowed(
                    "Connections to remote services is only allowed when set explicitly. "
                    "pipeline_options.enable_remote_services=True."
                )

    def _annotate_images(self, images: Iterable[Image.Image]) -> Iterable[str]:
        # Note: technically we could make a batch request here,
        # but not all APIs will allow for it. For example, vllm won't allow more than 1.
        for image in images:
            img_io = io.BytesIO()
            image.save(img_io, "PNG")
            image_base64 = base64.b64encode(img_io.getvalue()).decode("utf-8")

            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": self.options.prompt,
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ]

            payload = {
                "messages": messages,
                **self.options.params,
            }

            r = requests.post(
                str(self.options.url),
                headers=self.options.headers,
                json=payload,
                timeout=self.options.timeout,
            )
            if not r.ok:
                _log.error(f"Error calling the API. Reponse was {r.text}")
            r.raise_for_status()

            api_resp = ApiResponse.model_validate_json(r.text)
            generated_text = api_resp.choices[0].message.content.strip()
            yield generated_text


================================================
File: docling/models/picture_description_base_model.py
================================================
import logging
from pathlib import Path
from typing import Any, Iterable, List, Optional, Union

from docling_core.types.doc import (
    DoclingDocument,
    NodeItem,
    PictureClassificationClass,
    PictureItem,
)
from docling_core.types.doc.document import (  # TODO: move import to docling_core.types.doc
    PictureDescriptionData,
)
from PIL import Image

from docling.datamodel.pipeline_options import PictureDescriptionBaseOptions
from docling.models.base_model import (
    BaseItemAndImageEnrichmentModel,
    ItemAndImageEnrichmentElement,
)


class PictureDescriptionBaseModel(BaseItemAndImageEnrichmentModel):
    images_scale: float = 2.0

    def __init__(
        self,
        enabled: bool,
        options: PictureDescriptionBaseOptions,
    ):
        self.enabled = enabled
        self.options = options
        self.provenance = "not-implemented"

    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        return self.enabled and isinstance(element, PictureItem)

    def _annotate_images(self, images: Iterable[Image.Image]) -> Iterable[str]:
        raise NotImplementedError

    def __call__(
        self,
        doc: DoclingDocument,
        element_batch: Iterable[ItemAndImageEnrichmentElement],
    ) -> Iterable[NodeItem]:
        if not self.enabled:
            for element in element_batch:
                yield element.item
            return

        images: List[Image.Image] = []
        elements: List[PictureItem] = []
        for el in element_batch:
            assert isinstance(el.item, PictureItem)
            elements.append(el.item)
            images.append(el.image)

        outputs = self._annotate_images(images)

        for item, output in zip(elements, outputs):
            item.annotations.append(
                PictureDescriptionData(text=output, provenance=self.provenance)
            )
            yield item


================================================
File: docling/models/picture_description_vlm_model.py
================================================
from pathlib import Path
from typing import Iterable, Optional, Union

from PIL import Image

from docling.datamodel.pipeline_options import (
    AcceleratorOptions,
    PictureDescriptionVlmOptions,
)
from docling.models.picture_description_base_model import PictureDescriptionBaseModel
from docling.utils.accelerator_utils import decide_device


class PictureDescriptionVlmModel(PictureDescriptionBaseModel):

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Union[Path, str]],
        options: PictureDescriptionVlmOptions,
        accelerator_options: AcceleratorOptions,
    ):
        super().__init__(enabled=enabled, options=options)
        self.options: PictureDescriptionVlmOptions

        if self.enabled:

            if artifacts_path is None:
                artifacts_path = self.download_models(repo_id=self.options.repo_id)
            else:
                artifacts_path = Path(artifacts_path) / self.options.repo_cache_folder

            self.device = decide_device(accelerator_options.device)

            try:
                import torch
                from transformers import AutoModelForVision2Seq, AutoProcessor
            except ImportError:
                raise ImportError(
                    "transformers >=4.46 is not installed. Please install Docling with the required extras `pip install docling[vlm]`."
                )

            # Initialize processor and model
            self.processor = AutoProcessor.from_pretrained(artifacts_path)
            self.model = AutoModelForVision2Seq.from_pretrained(
                artifacts_path,
                torch_dtype=torch.bfloat16,
                _attn_implementation=(
                    "flash_attention_2" if self.device.startswith("cuda") else "eager"
                ),
            ).to(self.device)

            self.provenance = f"{self.options.repo_id}"

    @staticmethod
    def download_models(
        repo_id: str,
        local_dir: Optional[Path] = None,
        force: bool = False,
        progress: bool = False,
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id=repo_id,
            force_download=force,
            local_dir=local_dir,
        )

        return Path(download_path)

    def _annotate_images(self, images: Iterable[Image.Image]) -> Iterable[str]:
        from transformers import GenerationConfig

        # Create input messages
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": self.options.prompt},
                ],
            },
        ]

        # TODO: do batch generation

        for image in images:
            # Prepare inputs
            prompt = self.processor.apply_chat_template(
                messages, add_generation_prompt=True
            )
            inputs = self.processor(text=prompt, images=[image], return_tensors="pt")
            inputs = inputs.to(self.device)

            # Generate outputs
            generated_ids = self.model.generate(
                **inputs,
                generation_config=GenerationConfig(**self.options.generation_config),
            )
            generated_texts = self.processor.batch_decode(
                generated_ids[:, inputs["input_ids"].shape[1] :],
                skip_special_tokens=True,
            )

            yield generated_texts[0].strip()


================================================
File: docling/models/rapid_ocr_model.py
================================================
import logging
from typing import Iterable

import numpy
from docling_core.types.doc import BoundingBox, CoordOrigin

from docling.datamodel.base_models import OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    RapidOcrOptions,
)
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.utils.accelerator_utils import decide_device
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class RapidOcrModel(BaseOcrModel):
    def __init__(
        self,
        enabled: bool,
        options: RapidOcrOptions,
        accelerator_options: AcceleratorOptions,
    ):
        super().__init__(enabled=enabled, options=options)
        self.options: RapidOcrOptions

        self.scale = 3  # multiplier for 72 dpi == 216 dpi.

        if self.enabled:
            try:
                from rapidocr_onnxruntime import RapidOCR  # type: ignore
            except ImportError:
                raise ImportError(
                    "RapidOCR is not installed. Please install it via `pip install rapidocr_onnxruntime` to use this OCR engine. "
                    "Alternatively, Docling has support for other OCR engines. See the documentation."
                )

            # Decide the accelerator devices
            device = decide_device(accelerator_options.device)
            use_cuda = str(AcceleratorDevice.CUDA.value).lower() in device
            use_dml = accelerator_options.device == AcceleratorDevice.AUTO
            intra_op_num_threads = accelerator_options.num_threads

            self.reader = RapidOCR(
                text_score=self.options.text_score,
                cls_use_cuda=use_cuda,
                rec_use_cuda=use_cuda,
                det_use_cuda=use_cuda,
                det_use_dml=use_dml,
                cls_use_dml=use_dml,
                rec_use_dml=use_dml,
                intra_op_num_threads=intra_op_num_threads,
                print_verbose=self.options.print_verbose,
                det_model_path=self.options.det_model_path,
                cls_model_path=self.options.cls_model_path,
                rec_model_path=self.options.rec_model_path,
                rec_keys_path=self.options.rec_keys_path,
            )

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:

            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "ocr"):
                    ocr_rects = self.get_ocr_rects(page)

                    all_ocr_cells = []
                    for ocr_rect in ocr_rects:
                        # Skip zero area boxes
                        if ocr_rect.area() == 0:
                            continue
                        high_res_image = page._backend.get_page_image(
                            scale=self.scale, cropbox=ocr_rect
                        )
                        im = numpy.array(high_res_image)
                        result, _ = self.reader(
                            im,
                            use_det=self.options.use_det,
                            use_cls=self.options.use_cls,
                            use_rec=self.options.use_rec,
                        )

                        del high_res_image
                        del im

                        if result is not None:
                            cells = [
                                OcrCell(
                                    id=ix,
                                    text=line[1],
                                    confidence=line[2],
                                    bbox=BoundingBox.from_tuple(
                                        coord=(
                                            (line[0][0][0] / self.scale) + ocr_rect.l,
                                            (line[0][0][1] / self.scale) + ocr_rect.t,
                                            (line[0][2][0] / self.scale) + ocr_rect.l,
                                            (line[0][2][1] / self.scale) + ocr_rect.t,
                                        ),
                                        origin=CoordOrigin.TOPLEFT,
                                    ),
                                )
                                for ix, line in enumerate(result)
                            ]
                            all_ocr_cells.extend(cells)

                    # Post-process the cells
                    page.cells = self.post_process_cells(all_ocr_cells, page.cells)

                # DEBUG code:
                if settings.debug.visualize_ocr:
                    self.draw_ocr_rects_and_cells(conv_res, page, ocr_rects)

                yield page


================================================
File: docling/models/readingorder_model.py
================================================
import copy
import random
from pathlib import Path
from typing import Dict, List

from docling_core.types.doc import (
    BoundingBox,
    CoordOrigin,
    DocItem,
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    NodeItem,
    ProvenanceItem,
    RefItem,
    TableData,
)
from docling_core.types.doc.document import ContentLayer
from docling_core.types.legacy_doc.base import Ref
from docling_core.types.legacy_doc.document import BaseText
from docling_ibm_models.reading_order.reading_order_rb import (
    PageElement as ReadingOrderPageElement,
)
from docling_ibm_models.reading_order.reading_order_rb import ReadingOrderPredictor
from PIL import ImageDraw
from pydantic import BaseModel, ConfigDict

from docling.datamodel.base_models import (
    BasePageElement,
    Cluster,
    ContainerElement,
    FigureElement,
    Table,
    TextElement,
)
from docling.datamodel.document import ConversionResult
from docling.datamodel.settings import settings
from docling.utils.profiling import ProfilingScope, TimeRecorder


class ReadingOrderOptions(BaseModel):
    model_config = ConfigDict(protected_namespaces=())

    model_names: str = ""  # e.g. "language;term;reference"


class ReadingOrderModel:
    def __init__(self, options: ReadingOrderOptions):
        self.options = options
        self.ro_model = ReadingOrderPredictor()

    def _assembled_to_readingorder_elements(
        self, conv_res: ConversionResult
    ) -> List[ReadingOrderPageElement]:

        elements: List[ReadingOrderPageElement] = []
        page_no_to_pages = {p.page_no: p for p in conv_res.pages}

        for element in conv_res.assembled.elements:

            page_height = page_no_to_pages[element.page_no].size.height  # type: ignore
            bbox = element.cluster.bbox.to_bottom_left_origin(page_height)
            text = element.text or ""

            elements.append(
                ReadingOrderPageElement(
                    cid=len(elements),
                    ref=RefItem(cref=f"#/{element.page_no}/{element.cluster.id}"),
                    text=text,
                    page_no=element.page_no,
                    page_size=page_no_to_pages[element.page_no].size,
                    label=element.label,
                    l=bbox.l,
                    r=bbox.r,
                    b=bbox.b,
                    t=bbox.t,
                    coord_origin=bbox.coord_origin,
                )
            )

        return elements

    def _add_child_elements(
        self, element: BasePageElement, doc_item: NodeItem, doc: DoclingDocument
    ):

        child: Cluster
        for child in element.cluster.children:
            c_label = child.label
            c_bbox = child.bbox.to_bottom_left_origin(
                doc.pages[element.page_no + 1].size.height
            )
            c_text = " ".join(
                [
                    cell.text.replace("\x02", "-").strip()
                    for cell in child.cells
                    if len(cell.text.strip()) > 0
                ]
            )

            c_prov = ProvenanceItem(
                page_no=element.page_no + 1, charspan=(0, len(c_text)), bbox=c_bbox
            )
            if c_label == DocItemLabel.LIST_ITEM:
                # TODO: Infer if this is a numbered or a bullet list item
                doc.add_list_item(parent=doc_item, text=c_text, prov=c_prov)
            elif c_label == DocItemLabel.SECTION_HEADER:
                doc.add_heading(parent=doc_item, text=c_text, prov=c_prov)
            else:
                doc.add_text(parent=doc_item, label=c_label, text=c_text, prov=c_prov)

    def _readingorder_elements_to_docling_doc(
        self,
        conv_res: ConversionResult,
        ro_elements: List[ReadingOrderPageElement],
        el_to_captions_mapping: Dict[int, List[int]],
        el_to_footnotes_mapping: Dict[int, List[int]],
        el_merges_mapping: Dict[int, List[int]],
    ) -> DoclingDocument:

        id_to_elem = {
            RefItem(cref=f"#/{elem.page_no}/{elem.cluster.id}").cref: elem
            for elem in conv_res.assembled.elements
        }
        cid_to_rels = {rel.cid: rel for rel in ro_elements}

        origin = DocumentOrigin(
            mimetype="application/pdf",
            filename=conv_res.input.file.name,
            binary_hash=conv_res.input.document_hash,
        )
        doc_name = Path(origin.filename).stem
        out_doc: DoclingDocument = DoclingDocument(name=doc_name, origin=origin)

        for page in conv_res.pages:
            page_no = page.page_no + 1
            size = page.size

            assert size is not None

            out_doc.add_page(page_no=page_no, size=size)

        current_list = None
        skippable_cids = {
            cid
            for mapping in (
                el_to_captions_mapping,
                el_to_footnotes_mapping,
                el_merges_mapping,
            )
            for lst in mapping.values()
            for cid in lst
        }

        page_no_to_pages = {p.page_no: p for p in conv_res.pages}

        for rel in ro_elements:
            if rel.cid in skippable_cids:
                continue
            element = id_to_elem[rel.ref.cref]

            page_height = page_no_to_pages[element.page_no].size.height  # type: ignore

            if isinstance(element, TextElement):
                if element.label == DocItemLabel.CODE:
                    cap_text = element.text
                    prov = ProvenanceItem(
                        page_no=element.page_no + 1,
                        charspan=(0, len(cap_text)),
                        bbox=element.cluster.bbox.to_bottom_left_origin(page_height),
                    )
                    code_item = out_doc.add_code(text=cap_text, prov=prov)

                    if rel.cid in el_to_captions_mapping.keys():
                        for caption_cid in el_to_captions_mapping[rel.cid]:
                            caption_elem = id_to_elem[cid_to_rels[caption_cid].ref.cref]
                            new_cap_item = self._add_caption_or_footnote(
                                caption_elem, out_doc, code_item, page_height
                            )

                            code_item.captions.append(new_cap_item.get_ref())

                    if rel.cid in el_to_footnotes_mapping.keys():
                        for footnote_cid in el_to_footnotes_mapping[rel.cid]:
                            footnote_elem = id_to_elem[
                                cid_to_rels[footnote_cid].ref.cref
                            ]
                            new_footnote_item = self._add_caption_or_footnote(
                                footnote_elem, out_doc, code_item, page_height
                            )

                            code_item.footnotes.append(new_footnote_item.get_ref())
                else:

                    new_item, current_list = self._handle_text_element(
                        element, out_doc, current_list, page_height
                    )

                    if rel.cid in el_merges_mapping.keys():
                        for merged_cid in el_merges_mapping[rel.cid]:
                            merged_elem = id_to_elem[cid_to_rels[merged_cid].ref.cref]

                            self._merge_elements(
                                element, merged_elem, new_item, page_height
                            )

            elif isinstance(element, Table):

                tbl_data = TableData(
                    num_rows=element.num_rows,
                    num_cols=element.num_cols,
                    table_cells=element.table_cells,
                )

                prov = ProvenanceItem(
                    page_no=element.page_no + 1,
                    charspan=(0, 0),
                    bbox=element.cluster.bbox.to_bottom_left_origin(page_height),
                )

                tbl = out_doc.add_table(
                    data=tbl_data, prov=prov, label=element.cluster.label
                )

                if rel.cid in el_to_captions_mapping.keys():
                    for caption_cid in el_to_captions_mapping[rel.cid]:
                        caption_elem = id_to_elem[cid_to_rels[caption_cid].ref.cref]
                        new_cap_item = self._add_caption_or_footnote(
                            caption_elem, out_doc, tbl, page_height
                        )

                        tbl.captions.append(new_cap_item.get_ref())

                if rel.cid in el_to_footnotes_mapping.keys():
                    for footnote_cid in el_to_footnotes_mapping[rel.cid]:
                        footnote_elem = id_to_elem[cid_to_rels[footnote_cid].ref.cref]
                        new_footnote_item = self._add_caption_or_footnote(
                            footnote_elem, out_doc, tbl, page_height
                        )

                        tbl.footnotes.append(new_footnote_item.get_ref())

                # TODO: Consider adding children of Table.

            elif isinstance(element, FigureElement):
                cap_text = ""
                prov = ProvenanceItem(
                    page_no=element.page_no + 1,
                    charspan=(0, len(cap_text)),
                    bbox=element.cluster.bbox.to_bottom_left_origin(page_height),
                )
                pic = out_doc.add_picture(prov=prov)

                if rel.cid in el_to_captions_mapping.keys():
                    for caption_cid in el_to_captions_mapping[rel.cid]:
                        caption_elem = id_to_elem[cid_to_rels[caption_cid].ref.cref]
                        new_cap_item = self._add_caption_or_footnote(
                            caption_elem, out_doc, pic, page_height
                        )

                        pic.captions.append(new_cap_item.get_ref())

                if rel.cid in el_to_footnotes_mapping.keys():
                    for footnote_cid in el_to_footnotes_mapping[rel.cid]:
                        footnote_elem = id_to_elem[cid_to_rels[footnote_cid].ref.cref]
                        new_footnote_item = self._add_caption_or_footnote(
                            footnote_elem, out_doc, pic, page_height
                        )

                        pic.footnotes.append(new_footnote_item.get_ref())

                self._add_child_elements(element, pic, out_doc)

            elif isinstance(element, ContainerElement):  # Form, KV region
                label = element.label
                group_label = GroupLabel.UNSPECIFIED
                if label == DocItemLabel.FORM:
                    group_label = GroupLabel.FORM_AREA
                elif label == DocItemLabel.KEY_VALUE_REGION:
                    group_label = GroupLabel.KEY_VALUE_AREA

                container_el = out_doc.add_group(label=group_label)

                self._add_child_elements(element, container_el, out_doc)

        return out_doc

    def _add_caption_or_footnote(self, elem, out_doc, parent, page_height):
        assert isinstance(elem, TextElement)
        text = elem.text
        prov = ProvenanceItem(
            page_no=elem.page_no + 1,
            charspan=(0, len(text)),
            bbox=elem.cluster.bbox.to_bottom_left_origin(page_height),
        )
        new_item = out_doc.add_text(
            label=elem.label, text=text, prov=prov, parent=parent
        )
        return new_item

    def _handle_text_element(self, element, out_doc, current_list, page_height):
        cap_text = element.text

        prov = ProvenanceItem(
            page_no=element.page_no + 1,
            charspan=(0, len(cap_text)),
            bbox=element.cluster.bbox.to_bottom_left_origin(page_height),
        )
        label = element.label
        if label == DocItemLabel.LIST_ITEM:
            if current_list is None:
                current_list = out_doc.add_group(label=GroupLabel.LIST, name="list")

            # TODO: Infer if this is a numbered or a bullet list item
            new_item = out_doc.add_list_item(
                text=cap_text, enumerated=False, prov=prov, parent=current_list
            )
        elif label == DocItemLabel.SECTION_HEADER:
            current_list = None

            new_item = out_doc.add_heading(text=cap_text, prov=prov)
        elif label == DocItemLabel.FORMULA:
            current_list = None

            new_item = out_doc.add_text(
                label=DocItemLabel.FORMULA, text="", orig=cap_text, prov=prov
            )
        else:
            current_list = None

            content_layer = ContentLayer.BODY
            if element.label in [DocItemLabel.PAGE_HEADER, DocItemLabel.PAGE_FOOTER]:
                content_layer = ContentLayer.FURNITURE

            new_item = out_doc.add_text(
                label=element.label,
                text=cap_text,
                prov=prov,
                content_layer=content_layer,
            )
        return new_item, current_list

    def _merge_elements(self, element, merged_elem, new_item, page_height):
        assert isinstance(
            merged_elem, type(element)
        ), "Merged element must be of same type as element."
        assert (
            merged_elem.label == new_item.label
        ), "Labels of merged elements must match."
        prov = ProvenanceItem(
            page_no=element.page_no + 1,
            charspan=(
                len(new_item.text) + 1,
                len(new_item.text) + 1 + len(merged_elem.text),
            ),
            bbox=element.cluster.bbox.to_bottom_left_origin(page_height),
        )
        new_item.text += f" {merged_elem.text}"
        new_item.orig += f" {merged_elem.text}"  # TODO: This is incomplete, we don't have the `orig` field of the merged element.
        new_item.prov.append(prov)

    def __call__(self, conv_res: ConversionResult) -> DoclingDocument:
        with TimeRecorder(conv_res, "glm", scope=ProfilingScope.DOCUMENT):
            page_elements = self._assembled_to_readingorder_elements(conv_res)

            # Apply reading order
            sorted_elements = self.ro_model.predict_reading_order(
                page_elements=page_elements
            )
            el_to_captions_mapping = self.ro_model.predict_to_captions(
                sorted_elements=sorted_elements
            )
            el_to_footnotes_mapping = self.ro_model.predict_to_footnotes(
                sorted_elements=sorted_elements
            )
            el_merges_mapping = self.ro_model.predict_merges(
                sorted_elements=sorted_elements
            )

            docling_doc: DoclingDocument = self._readingorder_elements_to_docling_doc(
                conv_res,
                sorted_elements,
                el_to_captions_mapping,
                el_to_footnotes_mapping,
                el_merges_mapping,
            )

        return docling_doc


================================================
File: docling/models/table_structure_model.py
================================================
import copy
import warnings
from pathlib import Path
from typing import Iterable, Optional, Union

import numpy
from docling_core.types.doc import BoundingBox, DocItemLabel, TableCell
from docling_ibm_models.tableformer.data_management.tf_predictor import TFPredictor
from PIL import ImageDraw

from docling.datamodel.base_models import Page, Table, TableStructurePrediction
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    TableFormerMode,
    TableStructureOptions,
)
from docling.datamodel.settings import settings
from docling.models.base_model import BasePageModel
from docling.utils.accelerator_utils import decide_device
from docling.utils.profiling import TimeRecorder


class TableStructureModel(BasePageModel):
    _model_repo_folder = "ds4sd--docling-models"
    _model_path = "model_artifacts/tableformer"

    def __init__(
        self,
        enabled: bool,
        artifacts_path: Optional[Path],
        options: TableStructureOptions,
        accelerator_options: AcceleratorOptions,
    ):
        self.options = options
        self.do_cell_matching = self.options.do_cell_matching
        self.mode = self.options.mode

        self.enabled = enabled
        if self.enabled:

            if artifacts_path is None:
                artifacts_path = self.download_models() / self._model_path
            else:
                # will become the default in the future
                if (artifacts_path / self._model_repo_folder).exists():
                    artifacts_path = (
                        artifacts_path / self._model_repo_folder / self._model_path
                    )
                elif (artifacts_path / self._model_path).exists():
                    warnings.warn(
                        "The usage of artifacts_path containing directly "
                        f"{self._model_path} is deprecated. Please point "
                        "the artifacts_path to the parent containing "
                        f"the {self._model_repo_folder} folder.",
                        DeprecationWarning,
                        stacklevel=3,
                    )
                    artifacts_path = artifacts_path / self._model_path

            if self.mode == TableFormerMode.ACCURATE:
                artifacts_path = artifacts_path / "accurate"
            else:
                artifacts_path = artifacts_path / "fast"

            # Third Party
            import docling_ibm_models.tableformer.common as c

            device = decide_device(accelerator_options.device)

            # Disable MPS here, until we know why it makes things slower.
            if device == AcceleratorDevice.MPS.value:
                device = AcceleratorDevice.CPU.value

            self.tm_config = c.read_config(f"{artifacts_path}/tm_config.json")
            self.tm_config["model"]["save_dir"] = artifacts_path
            self.tm_model_type = self.tm_config["model"]["type"]

            self.tf_predictor = TFPredictor(
                self.tm_config, device, accelerator_options.num_threads
            )
            self.scale = 2.0  # Scale up table input images to 144 dpi

    @staticmethod
    def download_models(
        local_dir: Optional[Path] = None, force: bool = False, progress: bool = False
    ) -> Path:
        from huggingface_hub import snapshot_download
        from huggingface_hub.utils import disable_progress_bars

        if not progress:
            disable_progress_bars()
        download_path = snapshot_download(
            repo_id="ds4sd/docling-models",
            force_download=force,
            local_dir=local_dir,
            revision="v2.1.0",
        )

        return Path(download_path)

    def draw_table_and_cells(
        self,
        conv_res: ConversionResult,
        page: Page,
        tbl_list: Iterable[Table],
        show: bool = False,
    ):
        assert page._backend is not None
        assert page.size is not None

        image = (
            page._backend.get_page_image()
        )  # make new image to avoid drawing on the saved ones

        scale_x = image.width / page.size.width
        scale_y = image.height / page.size.height

        draw = ImageDraw.Draw(image)

        for table_element in tbl_list:
            x0, y0, x1, y1 = table_element.cluster.bbox.as_tuple()
            y0 *= scale_x
            y1 *= scale_y
            x0 *= scale_x
            x1 *= scale_x

            draw.rectangle([(x0, y0), (x1, y1)], outline="red")

            for cell in table_element.cluster.cells:
                x0, y0, x1, y1 = cell.bbox.as_tuple()
                x0 *= scale_x
                x1 *= scale_x
                y0 *= scale_x
                y1 *= scale_y

                draw.rectangle([(x0, y0), (x1, y1)], outline="green")

            for tc in table_element.table_cells:
                if tc.bbox is not None:
                    x0, y0, x1, y1 = tc.bbox.as_tuple()
                    x0 *= scale_x
                    x1 *= scale_x
                    y0 *= scale_x
                    y1 *= scale_y

                    if tc.column_header:
                        width = 3
                    else:
                        width = 1
                    draw.rectangle([(x0, y0), (x1, y1)], outline="blue", width=width)
                    draw.text(
                        (x0 + 3, y0 + 3),
                        text=f"{tc.start_row_offset_idx}, {tc.start_col_offset_idx}",
                        fill="black",
                    )
        if show:
            image.show()
        else:
            out_path: Path = (
                Path(settings.debug.debug_output_path)
                / f"debug_{conv_res.input.file.stem}"
            )
            out_path.mkdir(parents=True, exist_ok=True)

            out_file = out_path / f"table_struct_page_{page.page_no:05}.png"
            image.save(str(out_file), format="png")

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "table_structure"):

                    assert page.predictions.layout is not None
                    assert page.size is not None

                    page.predictions.tablestructure = (
                        TableStructurePrediction()
                    )  # dummy

                    in_tables = [
                        (
                            cluster,
                            [
                                round(cluster.bbox.l) * self.scale,
                                round(cluster.bbox.t) * self.scale,
                                round(cluster.bbox.r) * self.scale,
                                round(cluster.bbox.b) * self.scale,
                            ],
                        )
                        for cluster in page.predictions.layout.clusters
                        if cluster.label
                        in [DocItemLabel.TABLE, DocItemLabel.DOCUMENT_INDEX]
                    ]
                    if not len(in_tables):
                        yield page
                        continue

                    page_input = {
                        "width": page.size.width * self.scale,
                        "height": page.size.height * self.scale,
                        "image": numpy.asarray(page.get_image(scale=self.scale)),
                    }

                    table_clusters, table_bboxes = zip(*in_tables)

                    if len(table_bboxes):
                        for table_cluster, tbl_box in in_tables:

                            tokens = []
                            for c in table_cluster.cells:
                                # Only allow non empty stings (spaces) into the cells of a table
                                if len(c.text.strip()) > 0:
                                    new_cell = copy.deepcopy(c)
                                    new_cell.bbox = new_cell.bbox.scaled(
                                        scale=self.scale
                                    )

                                    tokens.append(new_cell.model_dump())
                            page_input["tokens"] = tokens

                            tf_output = self.tf_predictor.multi_table_predict(
                                page_input, [tbl_box], do_matching=self.do_cell_matching
                            )
                            table_out = tf_output[0]
                            table_cells = []
                            for element in table_out["tf_responses"]:

                                if not self.do_cell_matching:
                                    the_bbox = BoundingBox.model_validate(
                                        element["bbox"]
                                    ).scaled(1 / self.scale)
                                    text_piece = page._backend.get_text_in_rect(
                                        the_bbox
                                    )
                                    element["bbox"]["token"] = text_piece

                                tc = TableCell.model_validate(element)
                                if self.do_cell_matching and tc.bbox is not None:
                                    tc.bbox = tc.bbox.scaled(1 / self.scale)
                                table_cells.append(tc)

                            assert "predict_details" in table_out

                            # Retrieving cols/rows, after post processing:
                            num_rows = table_out["predict_details"].get("num_rows", 0)
                            num_cols = table_out["predict_details"].get("num_cols", 0)
                            otsl_seq = (
                                table_out["predict_details"]
                                .get("prediction", {})
                                .get("rs_seq", [])
                            )

                            tbl = Table(
                                otsl_seq=otsl_seq,
                                table_cells=table_cells,
                                num_rows=num_rows,
                                num_cols=num_cols,
                                id=table_cluster.id,
                                page_no=page.page_no,
                                cluster=table_cluster,
                                label=table_cluster.label,
                            )

                            page.predictions.tablestructure.table_map[
                                table_cluster.id
                            ] = tbl

                    # For debugging purposes:
                    if settings.debug.visualize_tables:
                        self.draw_table_and_cells(
                            conv_res,
                            page,
                            page.predictions.tablestructure.table_map.values(),
                        )

                yield page


================================================
File: docling/models/tesseract_ocr_cli_model.py
================================================
import csv
import io
import logging
import os
import tempfile
from subprocess import DEVNULL, PIPE, Popen
from typing import Iterable, List, Optional, Tuple

import pandas as pd
from docling_core.types.doc import BoundingBox, CoordOrigin

from docling.datamodel.base_models import Cell, OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import TesseractCliOcrOptions
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.utils.ocr_utils import map_tesseract_script
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class TesseractOcrCliModel(BaseOcrModel):
    def __init__(self, enabled: bool, options: TesseractCliOcrOptions):
        super().__init__(enabled=enabled, options=options)
        self.options: TesseractCliOcrOptions

        self.scale = 3  # multiplier for 72 dpi == 216 dpi.

        self._name: Optional[str] = None
        self._version: Optional[str] = None
        self._tesseract_languages: Optional[List[str]] = None
        self._script_prefix: Optional[str] = None

        if self.enabled:
            try:
                self._get_name_and_version()
                self._set_languages_and_prefix()

            except Exception as exc:
                raise RuntimeError(
                    f"Tesseract is not available, aborting: {exc} "
                    "Install tesseract on your system and the tesseract binary is discoverable. "
                    "The actual command for Tesseract can be specified in `pipeline_options.ocr_options.tesseract_cmd='tesseract'`. "
                    "Alternatively, Docling has support for other OCR engines. See the documentation."
                )

    def _get_name_and_version(self) -> Tuple[str, str]:

        if self._name != None and self._version != None:
            return self._name, self._version  # type: ignore

        cmd = [self.options.tesseract_cmd, "--version"]

        proc = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = proc.communicate()

        proc.wait()

        # HACK: Windows versions of Tesseract output the version to stdout, Linux versions
        # to stderr, so check both.
        version_line = (
            (stdout.decode("utf8").strip() or stderr.decode("utf8").strip())
            .split("\n")[0]
            .strip()
        )

        # If everything else fails...
        if not version_line:
            version_line = "tesseract XXX"

        name, version = version_line.split(" ")

        self._name = name
        self._version = version

        return name, version

    def _run_tesseract(self, ifilename: str):
        r"""
        Run tesseract CLI
        """
        cmd = [self.options.tesseract_cmd]

        if "auto" in self.options.lang:
            lang = self._detect_language(ifilename)
            if lang is not None:
                cmd.append("-l")
                cmd.append(lang)
        elif self.options.lang is not None and len(self.options.lang) > 0:
            cmd.append("-l")
            cmd.append("+".join(self.options.lang))

        if self.options.path is not None:
            cmd.append("--tessdata-dir")
            cmd.append(self.options.path)

        cmd += [ifilename, "stdout", "tsv"]
        _log.info("command: {}".format(" ".join(cmd)))

        proc = Popen(cmd, stdout=PIPE, stderr=DEVNULL)
        output, _ = proc.communicate()

        # _log.info(output)

        # Decode the byte string to a regular string
        decoded_data = output.decode("utf-8")
        # _log.info(decoded_data)

        # Read the TSV file generated by Tesseract
        df = pd.read_csv(io.StringIO(decoded_data), quoting=csv.QUOTE_NONE, sep="\t")

        # Display the dataframe (optional)
        # _log.info("df: ", df.head())

        # Filter rows that contain actual text (ignore header or empty rows)
        df_filtered = df[
            df["text"].notnull() & (df["text"].apply(str).str.strip() != "")
        ]

        return df_filtered

    def _detect_language(self, ifilename: str):
        r"""
        Run tesseract in PSM 0 mode to detect the language
        """
        assert self._tesseract_languages is not None

        cmd = [self.options.tesseract_cmd]
        cmd.extend(["--psm", "0", "-l", "osd", ifilename, "stdout"])
        _log.info("command: {}".format(" ".join(cmd)))
        proc = Popen(cmd, stdout=PIPE, stderr=DEVNULL)
        output, _ = proc.communicate()
        decoded_data = output.decode("utf-8")
        df = pd.read_csv(
            io.StringIO(decoded_data), sep=":", header=None, names=["key", "value"]
        )
        scripts = df.loc[df["key"] == "Script"].value.tolist()
        if len(scripts) == 0:
            _log.warning("Tesseract cannot detect the script of the page")
            return None

        script = map_tesseract_script(scripts[0].strip())
        lang = f"{self._script_prefix}{script}"

        # Check if the detected language has been installed
        if lang not in self._tesseract_languages:
            msg = f"Tesseract detected the script '{script}' and language '{lang}'."
            msg += " However this language is not installed in your system and will be ignored."
            _log.warning(msg)
            return None

        _log.debug(
            f"Using tesseract model for the detected script '{script}' and language '{lang}'"
        )
        return lang

    def _set_languages_and_prefix(self):
        r"""
        Read and set the languages installed in tesseract and decide the script prefix
        """
        # Get all languages
        cmd = [self.options.tesseract_cmd]
        cmd.append("--list-langs")
        _log.info("command: {}".format(" ".join(cmd)))
        proc = Popen(cmd, stdout=PIPE, stderr=DEVNULL)
        output, _ = proc.communicate()
        decoded_data = output.decode("utf-8")
        df = pd.read_csv(io.StringIO(decoded_data), header=None)
        self._tesseract_languages = df[0].tolist()[1:]

        # Decide the script prefix
        if any([l.startswith("script/") for l in self._tesseract_languages]):
            script_prefix = "script/"
        else:
            script_prefix = ""

        self._script_prefix = script_prefix

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:

        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "ocr"):
                    ocr_rects = self.get_ocr_rects(page)

                    all_ocr_cells = []
                    for ocr_rect in ocr_rects:
                        # Skip zero area boxes
                        if ocr_rect.area() == 0:
                            continue
                        high_res_image = page._backend.get_page_image(
                            scale=self.scale, cropbox=ocr_rect
                        )
                        try:
                            with tempfile.NamedTemporaryFile(
                                suffix=".png", mode="w+b", delete=False
                            ) as image_file:
                                fname = image_file.name
                                high_res_image.save(image_file)

                            df = self._run_tesseract(fname)
                        finally:
                            if os.path.exists(fname):
                                os.remove(fname)

                        # _log.info(df)

                        # Print relevant columns (bounding box and text)
                        for ix, row in df.iterrows():
                            text = row["text"]
                            conf = row["conf"]

                            l = float(row["left"])
                            b = float(row["top"])
                            w = float(row["width"])
                            h = float(row["height"])

                            t = b + h
                            r = l + w

                            cell = OcrCell(
                                id=ix,
                                text=text,
                                confidence=conf / 100.0,
                                bbox=BoundingBox.from_tuple(
                                    coord=(
                                        (l / self.scale) + ocr_rect.l,
                                        (b / self.scale) + ocr_rect.t,
                                        (r / self.scale) + ocr_rect.l,
                                        (t / self.scale) + ocr_rect.t,
                                    ),
                                    origin=CoordOrigin.TOPLEFT,
                                ),
                            )
                            all_ocr_cells.append(cell)

                    # Post-process the cells
                    page.cells = self.post_process_cells(all_ocr_cells, page.cells)

                # DEBUG code:
                if settings.debug.visualize_ocr:
                    self.draw_ocr_rects_and_cells(conv_res, page, ocr_rects)

                yield page


================================================
File: docling/models/tesseract_ocr_model.py
================================================
import logging
from typing import Iterable

from docling_core.types.doc import BoundingBox, CoordOrigin

from docling.datamodel.base_models import Cell, OcrCell, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import TesseractOcrOptions
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.utils.ocr_utils import map_tesseract_script
from docling.utils.profiling import TimeRecorder

_log = logging.getLogger(__name__)


class TesseractOcrModel(BaseOcrModel):
    def __init__(self, enabled: bool, options: TesseractOcrOptions):
        super().__init__(enabled=enabled, options=options)
        self.options: TesseractOcrOptions

        self.scale = 3  # multiplier for 72 dpi == 216 dpi.
        self.reader = None
        self.osd_reader = None
        self.script_readers: dict[str, tesserocr.PyTessBaseAPI] = {}

        if self.enabled:
            install_errmsg = (
                "tesserocr is not correctly installed. "
                "Please install it via `pip install tesserocr` to use this OCR engine. "
                "Note that tesserocr might have to be manually compiled for working with "
                "your Tesseract installation. The Docling documentation provides examples for it. "
                "Alternatively, Docling has support for other OCR engines. See the documentation: "
                "https://ds4sd.github.io/docling/installation/"
            )
            missing_langs_errmsg = (
                "tesserocr is not correctly configured. No language models have been detected. "
                "Please ensure that the TESSDATA_PREFIX envvar points to tesseract languages dir. "
                "You can find more information how to setup other OCR engines in Docling "
                "documentation: "
                "https://ds4sd.github.io/docling/installation/"
            )

            try:
                import tesserocr
            except ImportError:
                raise ImportError(install_errmsg)
            try:
                tesseract_version = tesserocr.tesseract_version()
            except:
                raise ImportError(install_errmsg)

            _, self._tesserocr_languages = tesserocr.get_languages()
            if not self._tesserocr_languages:
                raise ImportError(missing_langs_errmsg)

            # Initialize the tesseractAPI
            _log.debug("Initializing TesserOCR: %s", tesseract_version)
            lang = "+".join(self.options.lang)

            if any([l.startswith("script/") for l in self._tesserocr_languages]):
                self.script_prefix = "script/"
            else:
                self.script_prefix = ""

            tesserocr_kwargs = {
                "psm": tesserocr.PSM.AUTO,
                "init": True,
                "oem": tesserocr.OEM.DEFAULT,
            }

            if self.options.path is not None:
                tesserocr_kwargs["path"] = self.options.path

            if lang == "auto":
                self.reader = tesserocr.PyTessBaseAPI(**tesserocr_kwargs)
                self.osd_reader = tesserocr.PyTessBaseAPI(
                    **{"lang": "osd", "psm": tesserocr.PSM.OSD_ONLY} | tesserocr_kwargs
                )
            else:
                self.reader = tesserocr.PyTessBaseAPI(
                    **{"lang": lang} | tesserocr_kwargs,
                )
            self.reader_RIL = tesserocr.RIL

    def __del__(self):
        if self.reader is not None:
            # Finalize the tesseractAPI
            self.reader.End()
        for script in self.script_readers:
            self.script_readers[script].End()

    def __call__(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        if not self.enabled:
            yield from page_batch
            return

        for page in page_batch:
            assert page._backend is not None
            if not page._backend.is_valid():
                yield page
            else:
                with TimeRecorder(conv_res, "ocr"):
                    assert self.reader is not None
                    assert self._tesserocr_languages is not None

                    ocr_rects = self.get_ocr_rects(page)

                    all_ocr_cells = []
                    for ocr_rect in ocr_rects:
                        # Skip zero area boxes
                        if ocr_rect.area() == 0:
                            continue
                        high_res_image = page._backend.get_page_image(
                            scale=self.scale, cropbox=ocr_rect
                        )

                        local_reader = self.reader
                        if "auto" in self.options.lang:
                            assert self.osd_reader is not None

                            self.osd_reader.SetImage(high_res_image)
                            osd = self.osd_reader.DetectOrientationScript()

                            # No text, probably
                            if osd is None:
                                continue

                            script = osd["script_name"]
                            script = map_tesseract_script(script)
                            lang = f"{self.script_prefix}{script}"

                            # Check if the detected languge is present in the system
                            if lang not in self._tesserocr_languages:
                                msg = f"Tesseract detected the script '{script}' and language '{lang}'."
                                msg += " However this language is not installed in your system and will be ignored."
                                _log.warning(msg)
                            else:
                                if script not in self.script_readers:
                                    import tesserocr

                                    self.script_readers[script] = (
                                        tesserocr.PyTessBaseAPI(
                                            path=self.reader.GetDatapath(),
                                            lang=lang,
                                            psm=tesserocr.PSM.AUTO,
                                            init=True,
                                            oem=tesserocr.OEM.DEFAULT,
                                        )
                                    )
                                local_reader = self.script_readers[script]

                        local_reader.SetImage(high_res_image)
                        boxes = local_reader.GetComponentImages(
                            self.reader_RIL.TEXTLINE, True
                        )

                        cells = []
                        for ix, (im, box, _, _) in enumerate(boxes):
                            # Set the area of interest. Tesseract uses Bottom-Left for the origin
                            local_reader.SetRectangle(
                                box["x"], box["y"], box["w"], box["h"]
                            )

                            # Extract text within the bounding box
                            text = local_reader.GetUTF8Text().strip()
                            confidence = local_reader.MeanTextConf()
                            left = box["x"] / self.scale
                            bottom = box["y"] / self.scale
                            right = (box["x"] + box["w"]) / self.scale
                            top = (box["y"] + box["h"]) / self.scale

                            cells.append(
                                OcrCell(
                                    id=ix,
                                    text=text,
                                    confidence=confidence,
                                    bbox=BoundingBox.from_tuple(
                                        coord=(left, top, right, bottom),
                                        origin=CoordOrigin.TOPLEFT,
                                    ),
                                )
                            )

                        # del high_res_image
                        all_ocr_cells.extend(cells)

                    # Post-process the cells
                    page.cells = self.post_process_cells(all_ocr_cells, page.cells)

                # DEBUG code:
                if settings.debug.visualize_ocr:
                    self.draw_ocr_rects_and_cells(conv_res, page, ocr_rects)

                yield page


================================================
File: docling/pipeline/base_pipeline.py
================================================
import functools
import logging
import time
import traceback
from abc import ABC, abstractmethod
from typing import Any, Callable, Iterable, List

from docling_core.types.doc import DoclingDocument, NodeItem

from docling.backend.abstract_backend import AbstractDocumentBackend
from docling.backend.pdf_backend import PdfDocumentBackend
from docling.datamodel.base_models import (
    ConversionStatus,
    DoclingComponentType,
    ErrorItem,
    Page,
)
from docling.datamodel.document import ConversionResult, InputDocument
from docling.datamodel.pipeline_options import PipelineOptions
from docling.datamodel.settings import settings
from docling.models.base_model import GenericEnrichmentModel
from docling.utils.profiling import ProfilingScope, TimeRecorder
from docling.utils.utils import chunkify

_log = logging.getLogger(__name__)


class BasePipeline(ABC):
    def __init__(self, pipeline_options: PipelineOptions):
        self.pipeline_options = pipeline_options
        self.keep_images = False
        self.build_pipe: List[Callable] = []
        self.enrichment_pipe: List[GenericEnrichmentModel[Any]] = []

    def execute(self, in_doc: InputDocument, raises_on_error: bool) -> ConversionResult:
        conv_res = ConversionResult(input=in_doc)

        _log.info(f"Processing document {in_doc.file.name}")
        try:
            with TimeRecorder(
                conv_res, "pipeline_total", scope=ProfilingScope.DOCUMENT
            ):
                # These steps are building and assembling the structure of the
                # output DoclingDocument.
                conv_res = self._build_document(conv_res)
                conv_res = self._assemble_document(conv_res)
                # From this stage, all operations should rely only on conv_res.output
                conv_res = self._enrich_document(conv_res)
                conv_res.status = self._determine_status(conv_res)
        except Exception as e:
            conv_res.status = ConversionStatus.FAILURE
            if raises_on_error:
                raise e
        finally:
            self._unload(conv_res)

        return conv_res

    @abstractmethod
    def _build_document(self, conv_res: ConversionResult) -> ConversionResult:
        pass

    def _assemble_document(self, conv_res: ConversionResult) -> ConversionResult:
        return conv_res

    def _enrich_document(self, conv_res: ConversionResult) -> ConversionResult:

        def _prepare_elements(
            conv_res: ConversionResult, model: GenericEnrichmentModel[Any]
        ) -> Iterable[NodeItem]:
            for doc_element, _level in conv_res.document.iterate_items():
                prepared_element = model.prepare_element(
                    conv_res=conv_res, element=doc_element
                )
                if prepared_element is not None:
                    yield prepared_element

        with TimeRecorder(conv_res, "doc_enrich", scope=ProfilingScope.DOCUMENT):
            for model in self.enrichment_pipe:
                for element_batch in chunkify(
                    _prepare_elements(conv_res, model),
                    model.elements_batch_size,
                ):
                    for element in model(
                        doc=conv_res.document, element_batch=element_batch
                    ):  # Must exhaust!
                        pass

        return conv_res

    @abstractmethod
    def _determine_status(self, conv_res: ConversionResult) -> ConversionStatus:
        pass

    def _unload(self, conv_res: ConversionResult):
        pass

    @classmethod
    @abstractmethod
    def get_default_options(cls) -> PipelineOptions:
        pass

    @classmethod
    @abstractmethod
    def is_backend_supported(cls, backend: AbstractDocumentBackend):
        pass

    # def _apply_on_elements(self, element_batch: Iterable[NodeItem]) -> Iterable[Any]:
    #    for model in self.build_pipe:
    #        element_batch = model(element_batch)
    #
    #    yield from element_batch


class PaginatedPipeline(BasePipeline):  # TODO this is a bad name.

    def __init__(self, pipeline_options: PipelineOptions):
        super().__init__(pipeline_options)
        self.keep_backend = False

    def _apply_on_pages(
        self, conv_res: ConversionResult, page_batch: Iterable[Page]
    ) -> Iterable[Page]:
        for model in self.build_pipe:
            page_batch = model(conv_res, page_batch)

        yield from page_batch

    def _build_document(self, conv_res: ConversionResult) -> ConversionResult:

        if not isinstance(conv_res.input._backend, PdfDocumentBackend):
            raise RuntimeError(
                f"The selected backend {type(conv_res.input._backend).__name__} for {conv_res.input.file} is not a PDF backend. "
                f"Can not convert this with a PDF pipeline. "
                f"Please check your format configuration on DocumentConverter."
            )
            # conv_res.status = ConversionStatus.FAILURE
            # return conv_res

        total_elapsed_time = 0.0
        with TimeRecorder(conv_res, "doc_build", scope=ProfilingScope.DOCUMENT):

            for i in range(0, conv_res.input.page_count):
                start_page, end_page = conv_res.input.limits.page_range
                if (start_page - 1) <= i <= (end_page - 1):
                    conv_res.pages.append(Page(page_no=i))

            try:
                # Iterate batches of pages (page_batch_size) in the doc
                for page_batch in chunkify(
                    conv_res.pages, settings.perf.page_batch_size
                ):
                    start_batch_time = time.monotonic()

                    # 1. Initialise the page resources
                    init_pages = map(
                        functools.partial(self.initialize_page, conv_res), page_batch
                    )

                    # 2. Run pipeline stages
                    pipeline_pages = self._apply_on_pages(conv_res, init_pages)

                    for p in pipeline_pages:  # Must exhaust!

                        # Cleanup cached images
                        if not self.keep_images:
                            p._image_cache = {}

                        # Cleanup page backends
                        if not self.keep_backend and p._backend is not None:
                            p._backend.unload()

                    end_batch_time = time.monotonic()
                    total_elapsed_time += end_batch_time - start_batch_time
                    if (
                        self.pipeline_options.document_timeout is not None
                        and total_elapsed_time > self.pipeline_options.document_timeout
                    ):
                        _log.warning(
                            f"Document processing time ({total_elapsed_time:.3f} seconds) exceeded the specified timeout of {self.pipeline_options.document_timeout:.3f} seconds"
                        )
                        conv_res.status = ConversionStatus.PARTIAL_SUCCESS
                        break

                    _log.debug(
                        f"Finished converting page batch time={end_batch_time:.3f}"
                    )

            except Exception as e:
                conv_res.status = ConversionStatus.FAILURE
                trace = "\n".join(
                    traceback.format_exception(type(e), e, e.__traceback__)
                )
                _log.warning(
                    f"Encountered an error during conversion of document {conv_res.input.document_hash}:\n"
                    f"{trace}"
                )
                raise e

        return conv_res

    def _unload(self, conv_res: ConversionResult) -> ConversionResult:
        for page in conv_res.pages:
            if page._backend is not None:
                page._backend.unload()

        if conv_res.input._backend:
            conv_res.input._backend.unload()

        return conv_res

    def _determine_status(self, conv_res: ConversionResult) -> ConversionStatus:
        status = ConversionStatus.SUCCESS
        for page in conv_res.pages:
            if page._backend is None or not page._backend.is_valid():
                conv_res.errors.append(
                    ErrorItem(
                        component_type=DoclingComponentType.DOCUMENT_BACKEND,
                        module_name=type(page._backend).__name__,
                        error_message=f"Page {page.page_no} failed to parse.",
                    )
                )
                status = ConversionStatus.PARTIAL_SUCCESS

        return status

    # Initialise and load resources for a page
    @abstractmethod
    def initialize_page(self, conv_res: ConversionResult, page: Page) -> Page:
        pass


================================================
File: docling/pipeline/simple_pipeline.py
================================================
import logging

from docling.backend.abstract_backend import (
    AbstractDocumentBackend,
    DeclarativeDocumentBackend,
)
from docling.datamodel.base_models import ConversionStatus
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import PipelineOptions
from docling.pipeline.base_pipeline import BasePipeline
from docling.utils.profiling import ProfilingScope, TimeRecorder

_log = logging.getLogger(__name__)


class SimplePipeline(BasePipeline):
    """SimpleModelPipeline.

    This class is used at the moment for formats / backends
    which produce straight DoclingDocument output.
    """

    def __init__(self, pipeline_options: PipelineOptions):
        super().__init__(pipeline_options)

    def _build_document(self, conv_res: ConversionResult) -> ConversionResult:

        if not isinstance(conv_res.input._backend, DeclarativeDocumentBackend):
            raise RuntimeError(
                f"The selected backend {type(conv_res.input._backend).__name__} for {conv_res.input.file} is not a declarative backend. "
                f"Can not convert this with simple pipeline. "
                f"Please check your format configuration on DocumentConverter."
            )
            # conv_res.status = ConversionStatus.FAILURE
            # return conv_res

        # Instead of running a page-level pipeline to build up the document structure,
        # the backend is expected to be of type DeclarativeDocumentBackend, which can output
        # a DoclingDocument straight.
        with TimeRecorder(conv_res, "doc_build", scope=ProfilingScope.DOCUMENT):
            conv_res.document = conv_res.input._backend.convert()
        return conv_res

    def _determine_status(self, conv_res: ConversionResult) -> ConversionStatus:
        # This is called only if the previous steps didn't raise.
        # Since we don't have anything else to evaluate, we can
        # safely return SUCCESS.
        return ConversionStatus.SUCCESS

    @classmethod
    def get_default_options(cls) -> PipelineOptions:
        return PipelineOptions()

    @classmethod
    def is_backend_supported(cls, backend: AbstractDocumentBackend):
        return isinstance(backend, DeclarativeDocumentBackend)


================================================
File: docling/pipeline/standard_pdf_pipeline.py
================================================
import logging
import sys
import warnings
from pathlib import Path
from typing import Optional

from docling_core.types.doc import DocItem, ImageRef, PictureItem, TableItem

from docling.backend.abstract_backend import AbstractDocumentBackend
from docling.backend.pdf_backend import PdfDocumentBackend
from docling.datamodel.base_models import AssembledUnit, Page
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    EasyOcrOptions,
    OcrMacOptions,
    PdfPipelineOptions,
    PictureDescriptionApiOptions,
    PictureDescriptionVlmOptions,
    RapidOcrOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.datamodel.settings import settings
from docling.models.base_ocr_model import BaseOcrModel
from docling.models.code_formula_model import CodeFormulaModel, CodeFormulaModelOptions
from docling.models.document_picture_classifier import (
    DocumentPictureClassifier,
    DocumentPictureClassifierOptions,
)
from docling.models.easyocr_model import EasyOcrModel
from docling.models.layout_model import LayoutModel
from docling.models.ocr_mac_model import OcrMacModel
from docling.models.page_assemble_model import PageAssembleModel, PageAssembleOptions
from docling.models.page_preprocessing_model import (
    PagePreprocessingModel,
    PagePreprocessingOptions,
)
from docling.models.picture_description_api_model import PictureDescriptionApiModel
from docling.models.picture_description_base_model import PictureDescriptionBaseModel
from docling.models.picture_description_vlm_model import PictureDescriptionVlmModel
from docling.models.rapid_ocr_model import RapidOcrModel
from docling.models.readingorder_model import ReadingOrderModel, ReadingOrderOptions
from docling.models.table_structure_model import TableStructureModel
from docling.models.tesseract_ocr_cli_model import TesseractOcrCliModel
from docling.models.tesseract_ocr_model import TesseractOcrModel
from docling.pipeline.base_pipeline import PaginatedPipeline
from docling.utils.model_downloader import download_models
from docling.utils.profiling import ProfilingScope, TimeRecorder

_log = logging.getLogger(__name__)


class StandardPdfPipeline(PaginatedPipeline):
    _layout_model_path = LayoutModel._model_path
    _table_model_path = TableStructureModel._model_path

    def __init__(self, pipeline_options: PdfPipelineOptions):
        super().__init__(pipeline_options)
        self.pipeline_options: PdfPipelineOptions

        artifacts_path: Optional[Path] = None
        if pipeline_options.artifacts_path is not None:
            artifacts_path = Path(pipeline_options.artifacts_path).expanduser()
        elif settings.artifacts_path is not None:
            artifacts_path = Path(settings.artifacts_path).expanduser()

        if artifacts_path is not None and not artifacts_path.is_dir():
            raise RuntimeError(
                f"The value of {artifacts_path=} is not valid. "
                "When defined, it must point to a folder containing all models required by the pipeline."
            )

        self.keep_images = (
            self.pipeline_options.generate_page_images
            or self.pipeline_options.generate_picture_images
            or self.pipeline_options.generate_table_images
        )

        self.glm_model = ReadingOrderModel(options=ReadingOrderOptions())

        if (ocr_model := self.get_ocr_model(artifacts_path=artifacts_path)) is None:
            raise RuntimeError(
                f"The specified OCR kind is not supported: {pipeline_options.ocr_options.kind}."
            )

        self.build_pipe = [
            # Pre-processing
            PagePreprocessingModel(
                options=PagePreprocessingOptions(
                    images_scale=pipeline_options.images_scale
                )
            ),
            # OCR
            ocr_model,
            # Layout model
            LayoutModel(
                artifacts_path=artifacts_path,
                accelerator_options=pipeline_options.accelerator_options,
            ),
            # Table structure model
            TableStructureModel(
                enabled=pipeline_options.do_table_structure,
                artifacts_path=artifacts_path,
                options=pipeline_options.table_structure_options,
                accelerator_options=pipeline_options.accelerator_options,
            ),
            # Page assemble
            PageAssembleModel(options=PageAssembleOptions()),
        ]

        # Picture description model
        if (
            picture_description_model := self.get_picture_description_model(
                artifacts_path=artifacts_path
            )
        ) is None:
            raise RuntimeError(
                f"The specified picture description kind is not supported: {pipeline_options.picture_description_options.kind}."
            )

        self.enrichment_pipe = [
            # Code Formula Enrichment Model
            CodeFormulaModel(
                enabled=pipeline_options.do_code_enrichment
                or pipeline_options.do_formula_enrichment,
                artifacts_path=artifacts_path,
                options=CodeFormulaModelOptions(
                    do_code_enrichment=pipeline_options.do_code_enrichment,
                    do_formula_enrichment=pipeline_options.do_formula_enrichment,
                ),
                accelerator_options=pipeline_options.accelerator_options,
            ),
            # Document Picture Classifier
            DocumentPictureClassifier(
                enabled=pipeline_options.do_picture_classification,
                artifacts_path=artifacts_path,
                options=DocumentPictureClassifierOptions(),
                accelerator_options=pipeline_options.accelerator_options,
            ),
            # Document Picture description
            picture_description_model,
        ]

        if (
            self.pipeline_options.do_formula_enrichment
            or self.pipeline_options.do_code_enrichment
            or self.pipeline_options.do_picture_description
        ):
            self.keep_backend = True

    @staticmethod
    def download_models_hf(
        local_dir: Optional[Path] = None, force: bool = False
    ) -> Path:
        warnings.warn(
            "The usage of StandardPdfPipeline.download_models_hf() is deprecated "
            "use instead the utility `docling-tools models download`, or "
            "the upstream method docling.utils.models_downloader.download_all()",
            DeprecationWarning,
            stacklevel=3,
        )

        output_dir = download_models(output_dir=local_dir, force=force, progress=False)
        return output_dir

    def get_ocr_model(
        self, artifacts_path: Optional[Path] = None
    ) -> Optional[BaseOcrModel]:
        if isinstance(self.pipeline_options.ocr_options, EasyOcrOptions):
            return EasyOcrModel(
                enabled=self.pipeline_options.do_ocr,
                artifacts_path=artifacts_path,
                options=self.pipeline_options.ocr_options,
                accelerator_options=self.pipeline_options.accelerator_options,
            )
        elif isinstance(self.pipeline_options.ocr_options, TesseractCliOcrOptions):
            return TesseractOcrCliModel(
                enabled=self.pipeline_options.do_ocr,
                options=self.pipeline_options.ocr_options,
            )
        elif isinstance(self.pipeline_options.ocr_options, TesseractOcrOptions):
            return TesseractOcrModel(
                enabled=self.pipeline_options.do_ocr,
                options=self.pipeline_options.ocr_options,
            )
        elif isinstance(self.pipeline_options.ocr_options, RapidOcrOptions):
            return RapidOcrModel(
                enabled=self.pipeline_options.do_ocr,
                options=self.pipeline_options.ocr_options,
                accelerator_options=self.pipeline_options.accelerator_options,
            )
        elif isinstance(self.pipeline_options.ocr_options, OcrMacOptions):
            if "darwin" != sys.platform:
                raise RuntimeError(
                    f"The specified OCR type is only supported on Mac: {self.pipeline_options.ocr_options.kind}."
                )
            return OcrMacModel(
                enabled=self.pipeline_options.do_ocr,
                options=self.pipeline_options.ocr_options,
            )
        return None

    def get_picture_description_model(
        self, artifacts_path: Optional[Path] = None
    ) -> Optional[PictureDescriptionBaseModel]:
        if isinstance(
            self.pipeline_options.picture_description_options,
            PictureDescriptionApiOptions,
        ):
            return PictureDescriptionApiModel(
                enabled=self.pipeline_options.do_picture_description,
                enable_remote_services=self.pipeline_options.enable_remote_services,
                options=self.pipeline_options.picture_description_options,
            )
        elif isinstance(
            self.pipeline_options.picture_description_options,
            PictureDescriptionVlmOptions,
        ):
            return PictureDescriptionVlmModel(
                enabled=self.pipeline_options.do_picture_description,
                artifacts_path=artifacts_path,
                options=self.pipeline_options.picture_description_options,
                accelerator_options=self.pipeline_options.accelerator_options,
            )
        return None

    def initialize_page(self, conv_res: ConversionResult, page: Page) -> Page:
        with TimeRecorder(conv_res, "page_init"):
            page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore
            if page._backend is not None and page._backend.is_valid():
                page.size = page._backend.get_size()

        return page

    def _assemble_document(self, conv_res: ConversionResult) -> ConversionResult:
        all_elements = []
        all_headers = []
        all_body = []

        with TimeRecorder(conv_res, "doc_assemble", scope=ProfilingScope.DOCUMENT):
            for p in conv_res.pages:
                if p.assembled is not None:
                    for el in p.assembled.body:
                        all_body.append(el)
                    for el in p.assembled.headers:
                        all_headers.append(el)
                    for el in p.assembled.elements:
                        all_elements.append(el)

            conv_res.assembled = AssembledUnit(
                elements=all_elements, headers=all_headers, body=all_body
            )

            conv_res.document = self.glm_model(conv_res)

            # Generate page images in the output
            if self.pipeline_options.generate_page_images:
                for page in conv_res.pages:
                    assert page.image is not None
                    page_no = page.page_no + 1
                    conv_res.document.pages[page_no].image = ImageRef.from_pil(
                        page.image, dpi=int(72 * self.pipeline_options.images_scale)
                    )

            # Generate images of the requested element types
            if (
                self.pipeline_options.generate_picture_images
                or self.pipeline_options.generate_table_images
            ):
                scale = self.pipeline_options.images_scale
                for element, _level in conv_res.document.iterate_items():
                    if not isinstance(element, DocItem) or len(element.prov) == 0:
                        continue
                    if (
                        isinstance(element, PictureItem)
                        and self.pipeline_options.generate_picture_images
                    ) or (
                        isinstance(element, TableItem)
                        and self.pipeline_options.generate_table_images
                    ):
                        page_ix = element.prov[0].page_no - 1
                        page = conv_res.pages[page_ix]
                        assert page.size is not None
                        assert page.image is not None

                        crop_bbox = (
                            element.prov[0]
                            .bbox.scaled(scale=scale)
                            .to_top_left_origin(page_height=page.size.height * scale)
                        )

                        cropped_im = page.image.crop(crop_bbox.as_tuple())
                        element.image = ImageRef.from_pil(
                            cropped_im, dpi=int(72 * scale)
                        )

        return conv_res

    @classmethod
    def get_default_options(cls) -> PdfPipelineOptions:
        return PdfPipelineOptions()

    @classmethod
    def is_backend_supported(cls, backend: AbstractDocumentBackend):
        return isinstance(backend, PdfDocumentBackend)


================================================
File: docling/pipeline/vlm_pipeline.py
================================================
import itertools
import logging
import re
import warnings
from io import BytesIO

# from io import BytesIO
from pathlib import Path
from typing import Optional

from docling_core.types import DoclingDocument
from docling_core.types.doc import (
    BoundingBox,
    DocItem,
    DocItemLabel,
    DoclingDocument,
    GroupLabel,
    ImageRef,
    ImageRefMode,
    PictureItem,
    ProvenanceItem,
    Size,
    TableCell,
    TableData,
    TableItem,
)
from docling_core.types.doc.tokens import DocumentToken, TableToken

from docling.backend.abstract_backend import AbstractDocumentBackend
from docling.backend.md_backend import MarkdownDocumentBackend
from docling.backend.pdf_backend import PdfDocumentBackend
from docling.datamodel.base_models import InputFormat, Page
from docling.datamodel.document import ConversionResult, InputDocument
from docling.datamodel.pipeline_options import (
    PdfPipelineOptions,
    ResponseFormat,
    VlmPipelineOptions,
)
from docling.datamodel.settings import settings
from docling.models.hf_vlm_model import HuggingFaceVlmModel
from docling.pipeline.base_pipeline import PaginatedPipeline
from docling.utils.profiling import ProfilingScope, TimeRecorder

_log = logging.getLogger(__name__)


class VlmPipeline(PaginatedPipeline):

    def __init__(self, pipeline_options: VlmPipelineOptions):
        super().__init__(pipeline_options)
        self.keep_backend = True

        warnings.warn(
            "The VlmPipeline is currently experimental and may change in upcoming versions without notice.",
            category=UserWarning,
            stacklevel=2,
        )

        self.pipeline_options: VlmPipelineOptions

        artifacts_path: Optional[Path] = None
        if pipeline_options.artifacts_path is not None:
            artifacts_path = Path(pipeline_options.artifacts_path).expanduser()
        elif settings.artifacts_path is not None:
            artifacts_path = Path(settings.artifacts_path).expanduser()

        if artifacts_path is not None and not artifacts_path.is_dir():
            raise RuntimeError(
                f"The value of {artifacts_path=} is not valid. "
                "When defined, it must point to a folder containing all models required by the pipeline."
            )

        # force_backend_text = False - use text that is coming from VLM response
        # force_backend_text = True - get text from backend using bounding boxes predicted by SmolDocling doctags
        self.force_backend_text = (
            pipeline_options.force_backend_text
            and pipeline_options.vlm_options.response_format == ResponseFormat.DOCTAGS
        )

        self.keep_images = self.pipeline_options.generate_page_images

        self.build_pipe = [
            HuggingFaceVlmModel(
                enabled=True,  # must be always enabled for this pipeline to make sense.
                artifacts_path=artifacts_path,
                accelerator_options=pipeline_options.accelerator_options,
                vlm_options=self.pipeline_options.vlm_options,
            ),
        ]

        self.enrichment_pipe = [
            # Other models working on `NodeItem` elements in the DoclingDocument
        ]

    def initialize_page(self, conv_res: ConversionResult, page: Page) -> Page:
        with TimeRecorder(conv_res, "page_init"):
            page._backend = conv_res.input._backend.load_page(page.page_no)  # type: ignore
            if page._backend is not None and page._backend.is_valid():
                page.size = page._backend.get_size()

        return page

    def _assemble_document(self, conv_res: ConversionResult) -> ConversionResult:
        with TimeRecorder(conv_res, "doc_assemble", scope=ProfilingScope.DOCUMENT):

            if (
                self.pipeline_options.vlm_options.response_format
                == ResponseFormat.DOCTAGS
            ):
                conv_res.document = self._turn_tags_into_doc(conv_res.pages)
            elif (
                self.pipeline_options.vlm_options.response_format
                == ResponseFormat.MARKDOWN
            ):
                conv_res.document = self._turn_md_into_doc(conv_res)

            else:
                raise RuntimeError(
                    f"Unsupported VLM response format {self.pipeline_options.vlm_options.response_format}"
                )

            # Generate images of the requested element types
            if self.pipeline_options.generate_picture_images:
                scale = self.pipeline_options.images_scale
                for element, _level in conv_res.document.iterate_items():
                    if not isinstance(element, DocItem) or len(element.prov) == 0:
                        continue
                    if (
                        isinstance(element, PictureItem)
                        and self.pipeline_options.generate_picture_images
                    ):
                        page_ix = element.prov[0].page_no - 1
                        page = conv_res.pages[page_ix]
                        assert page.size is not None
                        assert page.image is not None

                        crop_bbox = (
                            element.prov[0]
                            .bbox.scaled(scale=scale)
                            .to_top_left_origin(page_height=page.size.height * scale)
                        )

                        cropped_im = page.image.crop(crop_bbox.as_tuple())
                        element.image = ImageRef.from_pil(
                            cropped_im, dpi=int(72 * scale)
                        )

        return conv_res

    def _turn_md_into_doc(self, conv_res):
        predicted_text = ""
        for pg_idx, page in enumerate(conv_res.pages):
            if page.predictions.vlm_response:
                predicted_text += page.predictions.vlm_response.text + "\n\n"
        response_bytes = BytesIO(predicted_text.encode("utf8"))
        out_doc = InputDocument(
            path_or_stream=response_bytes,
            filename=conv_res.input.file.name,
            format=InputFormat.MD,
            backend=MarkdownDocumentBackend,
        )
        backend = MarkdownDocumentBackend(
            in_doc=out_doc,
            path_or_stream=response_bytes,
        )
        return backend.convert()

    def _turn_tags_into_doc(self, pages: list[Page]) -> DoclingDocument:
        ###############################################
        # Tag definitions and color mappings
        ###############################################

        # Maps the recognized tag to a Docling label.
        # Code items will be given DocItemLabel.CODE
        tag_to_doclabel = {
            "title": DocItemLabel.TITLE,
            "document_index": DocItemLabel.DOCUMENT_INDEX,
            "otsl": DocItemLabel.TABLE,
            "section_header_level_1": DocItemLabel.SECTION_HEADER,
            "checkbox_selected": DocItemLabel.CHECKBOX_SELECTED,
            "checkbox_unselected": DocItemLabel.CHECKBOX_UNSELECTED,
            "text": DocItemLabel.TEXT,
            "page_header": DocItemLabel.PAGE_HEADER,
            "page_footer": DocItemLabel.PAGE_FOOTER,
            "formula": DocItemLabel.FORMULA,
            "caption": DocItemLabel.CAPTION,
            "picture": DocItemLabel.PICTURE,
            "list_item": DocItemLabel.LIST_ITEM,
            "footnote": DocItemLabel.FOOTNOTE,
            "code": DocItemLabel.CODE,
        }

        # Maps each tag to an associated bounding box color.
        tag_to_color = {
            "title": "blue",
            "document_index": "darkblue",
            "otsl": "green",
            "section_header_level_1": "purple",
            "checkbox_selected": "black",
            "checkbox_unselected": "gray",
            "text": "red",
            "page_header": "orange",
            "page_footer": "cyan",
            "formula": "pink",
            "caption": "magenta",
            "picture": "yellow",
            "list_item": "brown",
            "footnote": "darkred",
            "code": "lightblue",
        }

        def extract_bounding_box(text_chunk: str) -> Optional[BoundingBox]:
            """Extracts <loc_...> bounding box coords from the chunk, normalized by / 500."""
            coords = re.findall(r"<loc_(\d+)>", text_chunk)
            if len(coords) == 4:
                l, t, r, b = map(float, coords)
                return BoundingBox(l=l / 500, t=t / 500, r=r / 500, b=b / 500)
            return None

        def extract_inner_text(text_chunk: str) -> str:
            """Strips all <...> tags inside the chunk to get the raw text content."""
            return re.sub(r"<.*?>", "", text_chunk, flags=re.DOTALL).strip()

        def extract_text_from_backend(page: Page, bbox: BoundingBox | None) -> str:
            # Convert bounding box normalized to 0-100 into page coordinates for cropping
            text = ""
            if bbox:
                if page.size:
                    bbox.l = bbox.l * page.size.width
                    bbox.t = bbox.t * page.size.height
                    bbox.r = bbox.r * page.size.width
                    bbox.b = bbox.b * page.size.height
                    if page._backend:
                        text = page._backend.get_text_in_rect(bbox)
            return text

        def otsl_parse_texts(texts, tokens):
            split_word = TableToken.OTSL_NL.value
            split_row_tokens = [
                list(y)
                for x, y in itertools.groupby(tokens, lambda z: z == split_word)
                if not x
            ]
            table_cells = []
            r_idx = 0
            c_idx = 0

            def count_right(tokens, c_idx, r_idx, which_tokens):
                span = 0
                c_idx_iter = c_idx
                while tokens[r_idx][c_idx_iter] in which_tokens:
                    c_idx_iter += 1
                    span += 1
                    if c_idx_iter >= len(tokens[r_idx]):
                        return span
                return span

            def count_down(tokens, c_idx, r_idx, which_tokens):
                span = 0
                r_idx_iter = r_idx
                while tokens[r_idx_iter][c_idx] in which_tokens:
                    r_idx_iter += 1
                    span += 1
                    if r_idx_iter >= len(tokens):
                        return span
                return span

            for i, text in enumerate(texts):
                cell_text = ""
                if text in [
                    TableToken.OTSL_FCEL.value,
                    TableToken.OTSL_ECEL.value,
                    TableToken.OTSL_CHED.value,
                    TableToken.OTSL_RHED.value,
                    TableToken.OTSL_SROW.value,
                ]:
                    row_span = 1
                    col_span = 1
                    right_offset = 1
                    if text != TableToken.OTSL_ECEL.value:
                        cell_text = texts[i + 1]
                        right_offset = 2

                    # Check next element(s) for lcel / ucel / xcel, set properly row_span, col_span
                    next_right_cell = ""
                    if i + right_offset < len(texts):
                        next_right_cell = texts[i + right_offset]

                    next_bottom_cell = ""
                    if r_idx + 1 < len(split_row_tokens):
                        if c_idx < len(split_row_tokens[r_idx + 1]):
                            next_bottom_cell = split_row_tokens[r_idx + 1][c_idx]

                    if next_right_cell in [
                        TableToken.OTSL_LCEL.value,
                        TableToken.OTSL_XCEL.value,
                    ]:
                        # we have horisontal spanning cell or 2d spanning cell
                        col_span += count_right(
                            split_row_tokens,
                            c_idx + 1,
                            r_idx,
                            [TableToken.OTSL_LCEL.value, TableToken.OTSL_XCEL.value],
                        )
                    if next_bottom_cell in [
                        TableToken.OTSL_UCEL.value,
                        TableToken.OTSL_XCEL.value,
                    ]:
                        # we have a vertical spanning cell or 2d spanning cell
                        row_span += count_down(
                            split_row_tokens,
                            c_idx,
                            r_idx + 1,
                            [TableToken.OTSL_UCEL.value, TableToken.OTSL_XCEL.value],
                        )

                    table_cells.append(
                        TableCell(
                            text=cell_text.strip(),
                            row_span=row_span,
                            col_span=col_span,
                            start_row_offset_idx=r_idx,
                            end_row_offset_idx=r_idx + row_span,
                            start_col_offset_idx=c_idx,
                            end_col_offset_idx=c_idx + col_span,
                        )
                    )
                if text in [
                    TableToken.OTSL_FCEL.value,
                    TableToken.OTSL_ECEL.value,
                    TableToken.OTSL_CHED.value,
                    TableToken.OTSL_RHED.value,
                    TableToken.OTSL_SROW.value,
                    TableToken.OTSL_LCEL.value,
                    TableToken.OTSL_UCEL.value,
                    TableToken.OTSL_XCEL.value,
                ]:
                    c_idx += 1
                if text == TableToken.OTSL_NL.value:
                    r_idx += 1
                    c_idx = 0
            return table_cells, split_row_tokens

        def otsl_extract_tokens_and_text(s: str):
            # Pattern to match anything enclosed by < > (including the angle brackets themselves)
            pattern = r"(<[^>]+>)"
            # Find all tokens (e.g. "<otsl>", "<loc_140>", etc.)
            tokens = re.findall(pattern, s)
            # Remove any tokens that start with "<loc_"
            tokens = [
                token
                for token in tokens
                if not (
                    token.startswith(rf"<{DocumentToken.LOC.value}")
                    or token
                    in [
                        rf"<{DocumentToken.OTSL.value}>",
                        rf"</{DocumentToken.OTSL.value}>",
                    ]
                )
            ]
            # Split the string by those tokens to get the in-between text
            text_parts = re.split(pattern, s)
            text_parts = [
                token
                for token in text_parts
                if not (
                    token.startswith(rf"<{DocumentToken.LOC.value}")
                    or token
                    in [
                        rf"<{DocumentToken.OTSL.value}>",
                        rf"</{DocumentToken.OTSL.value}>",
                    ]
                )
            ]
            # Remove any empty or purely whitespace strings from text_parts
            text_parts = [part for part in text_parts if part.strip()]

            return tokens, text_parts

        def parse_table_content(otsl_content: str) -> TableData:
            tokens, mixed_texts = otsl_extract_tokens_and_text(otsl_content)
            table_cells, split_row_tokens = otsl_parse_texts(mixed_texts, tokens)

            return TableData(
                num_rows=len(split_row_tokens),
                num_cols=(
                    max(len(row) for row in split_row_tokens) if split_row_tokens else 0
                ),
                table_cells=table_cells,
            )

        doc = DoclingDocument(name="Document")
        for pg_idx, page in enumerate(pages):
            xml_content = ""
            predicted_text = ""
            if page.predictions.vlm_response:
                predicted_text = page.predictions.vlm_response.text
            image = page.image

            page_no = pg_idx + 1
            bounding_boxes = []

            if page.size:
                pg_width = page.size.width
                pg_height = page.size.height
                size = Size(width=pg_width, height=pg_height)
                parent_page = doc.add_page(page_no=page_no, size=size)

            """
            1. Finds all <tag>...</tag> blocks in the entire string (multi-line friendly) in the order they appear.
            2. For each chunk, extracts bounding box (if any) and inner text.
            3. Adds the item to a DoclingDocument structure with the right label.
            4. Tracks bounding boxes + color in a separate list for later visualization.
            """

            # Regex for all recognized tags
            tag_pattern = (
                rf"<(?P<tag>{DocItemLabel.TITLE}|{DocItemLabel.DOCUMENT_INDEX}|"
                rf"{DocItemLabel.CHECKBOX_UNSELECTED}|{DocItemLabel.CHECKBOX_SELECTED}|"
                rf"{DocItemLabel.TEXT}|{DocItemLabel.PAGE_HEADER}|"
                rf"{DocItemLabel.PAGE_FOOTER}|{DocItemLabel.FORMULA}|"
                rf"{DocItemLabel.CAPTION}|{DocItemLabel.PICTURE}|"
                rf"{DocItemLabel.LIST_ITEM}|{DocItemLabel.FOOTNOTE}|{DocItemLabel.CODE}|"
                rf"{DocItemLabel.SECTION_HEADER}_level_1|{DocumentToken.OTSL.value})>.*?</(?P=tag)>"
            )

            # DocumentToken.OTSL
            pattern = re.compile(tag_pattern, re.DOTALL)

            # Go through each match in order
            for match in pattern.finditer(predicted_text):
                full_chunk = match.group(0)
                tag_name = match.group("tag")

                bbox = extract_bounding_box(full_chunk)
                doc_label = tag_to_doclabel.get(tag_name, DocItemLabel.PARAGRAPH)
                color = tag_to_color.get(tag_name, "white")

                # Store bounding box + color
                if bbox:
                    bounding_boxes.append((bbox, color))

                if tag_name == DocumentToken.OTSL.value:
                    table_data = parse_table_content(full_chunk)
                    bbox = extract_bounding_box(full_chunk)

                    if bbox:
                        prov = ProvenanceItem(
                            bbox=bbox.resize_by_scale(pg_width, pg_height),
                            charspan=(0, 0),
                            page_no=page_no,
                        )
                        doc.add_table(data=table_data, prov=prov)
                    else:
                        doc.add_table(data=table_data)

                elif tag_name == DocItemLabel.PICTURE:
                    text_caption_content = extract_inner_text(full_chunk)
                    if image:
                        if bbox:
                            im_width, im_height = image.size

                            crop_box = (
                                int(bbox.l * im_width),
                                int(bbox.t * im_height),
                                int(bbox.r * im_width),
                                int(bbox.b * im_height),
                            )
                            cropped_image = image.crop(crop_box)
                            pic = doc.add_picture(
                                parent=None,
                                image=ImageRef.from_pil(image=cropped_image, dpi=72),
                                prov=(
                                    ProvenanceItem(
                                        bbox=bbox.resize_by_scale(pg_width, pg_height),
                                        charspan=(0, 0),
                                        page_no=page_no,
                                    )
                                ),
                            )
                            # If there is a caption to an image, add it as well
                            if len(text_caption_content) > 0:
                                caption_item = doc.add_text(
                                    label=DocItemLabel.CAPTION,
                                    text=text_caption_content,
                                    parent=None,
                                )
                                pic.captions.append(caption_item.get_ref())
                    else:
                        if bbox:
                            # In case we don't have access to an binary of an image
                            doc.add_picture(
                                parent=None,
                                prov=ProvenanceItem(
                                    bbox=bbox, charspan=(0, 0), page_no=page_no
                                ),
                            )
                            # If there is a caption to an image, add it as well
                            if len(text_caption_content) > 0:
                                caption_item = doc.add_text(
                                    label=DocItemLabel.CAPTION,
                                    text=text_caption_content,
                                    parent=None,
                                )
                                pic.captions.append(caption_item.get_ref())
                else:
                    # For everything else, treat as text
                    if self.force_backend_text:
                        text_content = extract_text_from_backend(page, bbox)
                    else:
                        text_content = extract_inner_text(full_chunk)
                    doc.add_text(
                        label=doc_label,
                        text=text_content,
                        prov=(
                            ProvenanceItem(
                                bbox=bbox.resize_by_scale(pg_width, pg_height),
                                charspan=(0, len(text_content)),
                                page_no=page_no,
                            )
                            if bbox
                            else None
                        ),
                    )
        return doc

    @classmethod
    def get_default_options(cls) -> VlmPipelineOptions:
        return VlmPipelineOptions()

    @classmethod
    def is_backend_supported(cls, backend: AbstractDocumentBackend):
        return isinstance(backend, PdfDocumentBackend)


================================================
File: docling/utils/accelerator_utils.py
================================================
import logging

import torch

from docling.datamodel.pipeline_options import AcceleratorDevice

_log = logging.getLogger(__name__)


def decide_device(accelerator_device: str) -> str:
    r"""
    Resolve the device based on the acceleration options and the available devices in the system.

    Rules:
    1. AUTO: Check for the best available device on the system.
    2. User-defined: Check if the device actually exists, otherwise fall-back to CPU
    """
    device = "cpu"

    has_cuda = torch.backends.cuda.is_built() and torch.cuda.is_available()
    has_mps = torch.backends.mps.is_built() and torch.backends.mps.is_available()

    if accelerator_device == AcceleratorDevice.AUTO.value:  # Handle 'auto'
        if has_cuda:
            device = "cuda:0"
        elif has_mps:
            device = "mps"

    elif accelerator_device.startswith("cuda"):
        if has_cuda:
            # if cuda device index specified extract device id
            parts = accelerator_device.split(":")
            if len(parts) == 2 and parts[1].isdigit():
                # select cuda device's id
                cuda_index = int(parts[1])
                if cuda_index < torch.cuda.device_count():
                    device = f"cuda:{cuda_index}"
                else:
                    _log.warning(
                        "CUDA device 'cuda:%d' is not available. Fall back to 'CPU'.",
                        cuda_index,
                    )
            elif len(parts) == 1:  # just "cuda"
                device = "cuda:0"
            else:
                _log.warning(
                    "Invalid CUDA device format '%s'. Fall back to 'CPU'",
                    accelerator_device,
                )
        else:
            _log.warning("CUDA is not available in the system. Fall back to 'CPU'")

    elif accelerator_device == AcceleratorDevice.MPS.value:
        if has_mps:
            device = "mps"
        else:
            _log.warning("MPS is not available in the system. Fall back to 'CPU'")

    elif accelerator_device == AcceleratorDevice.CPU.value:
        device = "cpu"

    else:
        _log.warning(
            "Unknown device option '%s'. Fall back to 'CPU'", accelerator_device
        )

    _log.info("Accelerator device: '%s'", device)
    return device


================================================
File: docling/utils/export.py
================================================
import logging
from typing import Any, Dict, Iterable, List, Tuple, Union

from docling_core.types.doc import BoundingBox, CoordOrigin
from docling_core.types.legacy_doc.base import BaseCell, BaseText, Ref, Table

from docling.datamodel.base_models import OcrCell
from docling.datamodel.document import ConversionResult, Page

_log = logging.getLogger(__name__)


def generate_multimodal_pages(
    doc_result: ConversionResult,
) -> Iterable[Tuple[str, str, List[Dict[str, Any]], List[Dict[str, Any]], Page]]:

    label_to_doclaynet = {
        "title": "title",
        "table-of-contents": "document_index",
        "subtitle-level-1": "section_header",
        "checkbox-selected": "checkbox_selected",
        "checkbox-unselected": "checkbox_unselected",
        "caption": "caption",
        "page-header": "page_header",
        "page-footer": "page_footer",
        "footnote": "footnote",
        "table": "table",
        "formula": "formula",
        "list-item": "list_item",
        "code": "code",
        "figure": "picture",
        "picture": "picture",
        "reference": "text",
        "paragraph": "text",
        "text": "text",
    }

    content_text = ""
    page_no = 0
    start_ix = 0
    end_ix = 0
    doc_items: List[Tuple[int, Union[BaseCell, BaseText]]] = []

    doc = doc_result.legacy_document

    def _process_page_segments(doc_items: list[Tuple[int, BaseCell]], page: Page):
        segments = []

        for ix, item in doc_items:
            item_type = item.obj_type
            label = label_to_doclaynet.get(item_type, None)

            if label is None or item.prov is None or page.size is None:
                continue

            bbox = BoundingBox.from_tuple(
                tuple(item.prov[0].bbox), origin=CoordOrigin.BOTTOMLEFT
            )
            new_bbox = bbox.to_top_left_origin(page_height=page.size.height).normalized(
                page_size=page.size
            )

            new_segment = {
                "index_in_doc": ix,
                "label": label,
                "text": item.text if item.text is not None else "",
                "bbox": new_bbox.as_tuple(),
                "data": [],
            }

            if isinstance(item, Table):
                table_html = item.export_to_html()
                new_segment["data"].append(
                    {
                        "html_seq": table_html,
                        "otsl_seq": "",
                    }
                )

            segments.append(new_segment)

        return segments

    def _process_page_cells(page: Page):
        cells: List[dict] = []
        if page.size is None:
            return cells
        for cell in page.cells:
            new_bbox = cell.bbox.to_top_left_origin(
                page_height=page.size.height
            ).normalized(page_size=page.size)
            is_ocr = isinstance(cell, OcrCell)
            ocr_confidence = cell.confidence if isinstance(cell, OcrCell) else 1.0
            cells.append(
                {
                    "text": cell.text,
                    "bbox": new_bbox.as_tuple(),
                    "ocr": is_ocr,
                    "ocr_confidence": ocr_confidence,
                }
            )
        return cells

    def _process_page():
        page_ix = page_no - 1
        page = doc_result.pages[page_ix]

        page_cells = _process_page_cells(page=page)
        page_segments = _process_page_segments(doc_items=doc_items, page=page)
        content_md = doc.export_to_markdown(
            main_text_start=start_ix, main_text_stop=end_ix
        )
        # No page-tagging since we only do 1 page at the time
        content_dt = doc.export_to_document_tokens(
            main_text_start=start_ix, main_text_stop=end_ix, add_page_index=False
        )

        return content_text, content_md, content_dt, page_cells, page_segments, page

    if doc.main_text is None:
        return
    for ix, orig_item in enumerate(doc.main_text):

        item = doc._resolve_ref(orig_item) if isinstance(orig_item, Ref) else orig_item
        if item is None or item.prov is None or len(item.prov) == 0:
            _log.debug(f"Skipping item {orig_item}")
            continue

        item_page = item.prov[0].page

        # Page is complete
        if page_no > 0 and item_page > page_no:
            yield _process_page()

            start_ix = ix
            doc_items = []
            content_text = ""

        page_no = item_page
        end_ix = ix
        doc_items.append((ix, item))
        if item.text is not None and item.text != "":
            content_text += item.text + " "

    if len(doc_items) > 0:
        yield _process_page()


================================================
File: docling/utils/glm_utils.py
================================================
import re
from pathlib import Path
from typing import List

import pandas as pd
from docling_core.types.doc import (
    BoundingBox,
    CoordOrigin,
    DocItemLabel,
    DoclingDocument,
    DocumentOrigin,
    GroupLabel,
    ProvenanceItem,
    Size,
    TableCell,
    TableData,
)
from docling_core.types.doc.document import ContentLayer


def resolve_item(paths, obj):
    """Find item in document from a reference path"""

    if len(paths) == 0:
        return obj

    if paths[0] == "#":
        return resolve_item(paths[1:], obj)

    try:
        key = int(paths[0])
    except:
        key = paths[0]

    if len(paths) == 1:
        if isinstance(key, str) and key in obj:
            return obj[key]
        elif isinstance(key, int) and key < len(obj):
            return obj[key]
        else:
            return None

    elif len(paths) > 1:
        if isinstance(key, str) and key in obj:
            return resolve_item(paths[1:], obj[key])
        elif isinstance(key, int) and key < len(obj):
            return resolve_item(paths[1:], obj[key])
        else:
            return None

    else:
        return None


def _flatten_table_grid(grid: List[List[dict]]) -> List[dict]:
    unique_objects = []
    seen_spans = set()

    for sublist in grid:
        for obj in sublist:
            # Convert the spans list to a tuple of tuples for hashing
            spans_tuple = tuple(tuple(span) for span in obj["spans"])
            if spans_tuple not in seen_spans:
                seen_spans.add(spans_tuple)
                unique_objects.append(obj)

    return unique_objects


def to_docling_document(doc_glm, update_name_label=False) -> DoclingDocument:
    origin = DocumentOrigin(
        mimetype="application/pdf",
        filename=doc_glm["file-info"]["filename"],
        binary_hash=doc_glm["file-info"]["document-hash"],
    )
    doc_name = Path(origin.filename).stem

    doc: DoclingDocument = DoclingDocument(name=doc_name, origin=origin)

    for page_dim in doc_glm["page-dimensions"]:
        page_no = int(page_dim["page"])
        size = Size(width=page_dim["width"], height=page_dim["height"])

        doc.add_page(page_no=page_no, size=size)

    if "properties" in doc_glm:
        props = pd.DataFrame(
            doc_glm["properties"]["data"], columns=doc_glm["properties"]["headers"]
        )
    else:
        props = pd.DataFrame()

    current_list = None

    for ix, pelem in enumerate(doc_glm["page-elements"]):
        ptype = pelem["type"]
        span_i = pelem["span"][0]
        span_j = pelem["span"][1]

        if "iref" not in pelem:
            # print(json.dumps(pelem, indent=2))
            continue

        iref = pelem["iref"]

        if re.match("#/figures/(\\d+)/captions/(.+)", iref):
            # print(f"skip {iref}")
            continue

        if re.match("#/tables/(\\d+)/captions/(.+)", iref):
            # print(f"skip {iref}")
            continue

        path = iref.split("/")
        obj = resolve_item(path, doc_glm)

        if obj is None:
            current_list = None
            print(f"warning: undefined {path}")
            continue

        if ptype == "figure":
            current_list = None
            text = ""
            caption_refs = []
            for caption in obj["captions"]:
                text += caption["text"]

                for nprov in caption["prov"]:
                    npaths = nprov["$ref"].split("/")
                    nelem = resolve_item(npaths, doc_glm)

                    if nelem is None:
                        # print(f"warning: undefined caption {npaths}")
                        continue

                    span_i = nelem["span"][0]
                    span_j = nelem["span"][1]

                    cap_text = caption["text"][span_i:span_j]

                    # doc_glm["page-elements"].remove(nelem)

                    prov = ProvenanceItem(
                        page_no=nelem["page"],
                        charspan=tuple(nelem["span"]),
                        bbox=BoundingBox.from_tuple(
                            nelem["bbox"], origin=CoordOrigin.BOTTOMLEFT
                        ),
                    )

                    caption_obj = doc.add_text(
                        label=DocItemLabel.CAPTION, text=cap_text, prov=prov
                    )
                    caption_refs.append(caption_obj.get_ref())

            prov = ProvenanceItem(
                page_no=pelem["page"],
                charspan=(0, len(text)),
                bbox=BoundingBox.from_tuple(
                    pelem["bbox"], origin=CoordOrigin.BOTTOMLEFT
                ),
            )

            pic = doc.add_picture(prov=prov)
            pic.captions.extend(caption_refs)
            _add_child_elements(pic, doc, obj, pelem)

        elif ptype == "table":
            current_list = None
            text = ""
            caption_refs = []
            item_label = DocItemLabel(pelem["name"])

            for caption in obj["captions"]:
                text += caption["text"]

                for nprov in caption["prov"]:
                    npaths = nprov["$ref"].split("/")
                    nelem = resolve_item(npaths, doc_glm)

                    if nelem is None:
                        # print(f"warning: undefined caption {npaths}")
                        continue

                    span_i = nelem["span"][0]
                    span_j = nelem["span"][1]

                    cap_text = caption["text"][span_i:span_j]

                    # doc_glm["page-elements"].remove(nelem)

                    prov = ProvenanceItem(
                        page_no=nelem["page"],
                        charspan=tuple(nelem["span"]),
                        bbox=BoundingBox.from_tuple(
                            nelem["bbox"], origin=CoordOrigin.BOTTOMLEFT
                        ),
                    )

                    caption_obj = doc.add_text(
                        label=DocItemLabel.CAPTION, text=cap_text, prov=prov
                    )
                    caption_refs.append(caption_obj.get_ref())

            table_cells_glm = _flatten_table_grid(obj["data"])

            table_cells = []
            for tbl_cell_glm in table_cells_glm:
                if tbl_cell_glm["bbox"] is not None:
                    bbox = BoundingBox.from_tuple(
                        tbl_cell_glm["bbox"], origin=CoordOrigin.BOTTOMLEFT
                    )
                else:
                    bbox = None

                is_col_header = False
                is_row_header = False
                is_row_section = False

                if tbl_cell_glm["type"] == "col_header":
                    is_col_header = True
                elif tbl_cell_glm["type"] == "row_header":
                    is_row_header = True
                elif tbl_cell_glm["type"] == "row_section":
                    is_row_section = True

                table_cells.append(
                    TableCell(
                        row_span=tbl_cell_glm["row-span"][1]
                        - tbl_cell_glm["row-span"][0],
                        col_span=tbl_cell_glm["col-span"][1]
                        - tbl_cell_glm["col-span"][0],
                        start_row_offset_idx=tbl_cell_glm["row-span"][0],
                        end_row_offset_idx=tbl_cell_glm["row-span"][1],
                        start_col_offset_idx=tbl_cell_glm["col-span"][0],
                        end_col_offset_idx=tbl_cell_glm["col-span"][1],
                        text=tbl_cell_glm["text"],
                        bbox=bbox,
                        column_header=is_col_header,
                        row_header=is_row_header,
                        row_section=is_row_section,
                    )
                )

            tbl_data = TableData(
                num_rows=obj.get("#-rows", 0),
                num_cols=obj.get("#-cols", 0),
                table_cells=table_cells,
            )

            prov = ProvenanceItem(
                page_no=pelem["page"],
                charspan=(0, 0),
                bbox=BoundingBox.from_tuple(
                    pelem["bbox"], origin=CoordOrigin.BOTTOMLEFT
                ),
            )

            tbl = doc.add_table(data=tbl_data, prov=prov, label=item_label)
            tbl.captions.extend(caption_refs)

        elif ptype in [DocItemLabel.FORM.value, DocItemLabel.KEY_VALUE_REGION.value]:
            label = DocItemLabel(ptype)
            group_label = GroupLabel.UNSPECIFIED
            if label == DocItemLabel.FORM:
                group_label = GroupLabel.FORM_AREA
            elif label == DocItemLabel.KEY_VALUE_REGION:
                group_label = GroupLabel.KEY_VALUE_AREA

            container_el = doc.add_group(label=group_label)

            _add_child_elements(container_el, doc, obj, pelem)
        elif "text" in obj:
            text = obj["text"][span_i:span_j]

            type_label = pelem["type"]
            name_label = pelem["name"]
            if update_name_label and len(props) > 0 and type_label == "paragraph":
                prop = props[
                    (props["type"] == "semantic") & (props["subj_path"] == iref)
                ]
                if len(prop) == 1 and prop.iloc[0]["confidence"] > 0.85:
                    name_label = prop.iloc[0]["label"]

            prov = ProvenanceItem(
                page_no=pelem["page"],
                charspan=(0, len(text)),
                bbox=BoundingBox.from_tuple(
                    pelem["bbox"], origin=CoordOrigin.BOTTOMLEFT
                ),
            )
            label = DocItemLabel(name_label)

            if label == DocItemLabel.LIST_ITEM:
                if current_list is None:
                    current_list = doc.add_group(label=GroupLabel.LIST, name="list")

                # TODO: Infer if this is a numbered or a bullet list item
                doc.add_list_item(
                    text=text, enumerated=False, prov=prov, parent=current_list
                )
            elif label == DocItemLabel.SECTION_HEADER:
                current_list = None

                doc.add_heading(text=text, prov=prov)
            elif label == DocItemLabel.CODE:
                current_list = None

                doc.add_code(text=text, prov=prov)
            elif label == DocItemLabel.FORMULA:
                current_list = None

                doc.add_text(label=DocItemLabel.FORMULA, text="", orig=text, prov=prov)
            elif label in [DocItemLabel.PAGE_HEADER, DocItemLabel.PAGE_FOOTER]:
                current_list = None

                doc.add_text(
                    label=DocItemLabel(name_label),
                    text=text,
                    prov=prov,
                    content_layer=ContentLayer.FURNITURE,
                )
            else:
                current_list = None

                doc.add_text(label=DocItemLabel(name_label), text=text, prov=prov)

    return doc


def _add_child_elements(container_el, doc, obj, pelem):
    payload = obj.get("payload")
    if payload is not None:
        children = payload.get("children", [])

        for child in children:
            c_label = DocItemLabel(child["label"])
            c_bbox = BoundingBox.model_validate(child["bbox"]).to_bottom_left_origin(
                doc.pages[pelem["page"]].size.height
            )
            c_text = " ".join(
                [
                    cell["text"].replace("\x02", "-").strip()
                    for cell in child["cells"]
                    if len(cell["text"].strip()) > 0
                ]
            )

            c_prov = ProvenanceItem(
                page_no=pelem["page"], charspan=(0, len(c_text)), bbox=c_bbox
            )
            if c_label == DocItemLabel.LIST_ITEM:
                # TODO: Infer if this is a numbered or a bullet list item
                doc.add_list_item(parent=container_el, text=c_text, prov=c_prov)
            elif c_label == DocItemLabel.SECTION_HEADER:
                doc.add_heading(parent=container_el, text=c_text, prov=c_prov)
            else:
                doc.add_text(
                    parent=container_el, label=c_label, text=c_text, prov=c_prov
                )


================================================
File: docling/utils/layout_postprocessor.py
================================================
import bisect
import logging
import sys
from collections import defaultdict
from typing import Dict, List, Set, Tuple

from docling_core.types.doc import DocItemLabel, Size
from rtree import index

from docling.datamodel.base_models import BoundingBox, Cell, Cluster, OcrCell

_log = logging.getLogger(__name__)


class UnionFind:
    """Efficient Union-Find data structure for grouping elements."""

    def __init__(self, elements):
        self.parent = {elem: elem for elem in elements}
        self.rank = {elem: 0 for elem in elements}

    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])  # Path compression
        return self.parent[x]

    def union(self, x, y):
        root_x, root_y = self.find(x), self.find(y)
        if root_x == root_y:
            return

        if self.rank[root_x] > self.rank[root_y]:
            self.parent[root_y] = root_x
        elif self.rank[root_x] < self.rank[root_y]:
            self.parent[root_x] = root_y
        else:
            self.parent[root_y] = root_x
            self.rank[root_x] += 1

    def get_groups(self) -> Dict[int, List[int]]:
        """Returns groups as {root: [elements]}."""
        groups = defaultdict(list)
        for elem in self.parent:
            groups[self.find(elem)].append(elem)
        return groups


class SpatialClusterIndex:
    """Efficient spatial indexing for clusters using R-tree and interval trees."""

    def __init__(self, clusters: List[Cluster]):
        p = index.Property()
        p.dimension = 2
        self.spatial_index = index.Index(properties=p)
        self.x_intervals = IntervalTree()
        self.y_intervals = IntervalTree()
        self.clusters_by_id: Dict[int, Cluster] = {}

        for cluster in clusters:
            self.add_cluster(cluster)

    def add_cluster(self, cluster: Cluster):
        bbox = cluster.bbox
        self.spatial_index.insert(cluster.id, bbox.as_tuple())
        self.x_intervals.insert(bbox.l, bbox.r, cluster.id)
        self.y_intervals.insert(bbox.t, bbox.b, cluster.id)
        self.clusters_by_id[cluster.id] = cluster

    def remove_cluster(self, cluster: Cluster):
        self.spatial_index.delete(cluster.id, cluster.bbox.as_tuple())
        del self.clusters_by_id[cluster.id]

    def find_candidates(self, bbox: BoundingBox) -> Set[int]:
        """Find potential overlapping cluster IDs using all indexes."""
        spatial = set(self.spatial_index.intersection(bbox.as_tuple()))
        x_candidates = self.x_intervals.find_containing(
            bbox.l
        ) | self.x_intervals.find_containing(bbox.r)
        y_candidates = self.y_intervals.find_containing(
            bbox.t
        ) | self.y_intervals.find_containing(bbox.b)
        return spatial.union(x_candidates).union(y_candidates)

    def check_overlap(
        self,
        bbox1: BoundingBox,
        bbox2: BoundingBox,
        overlap_threshold: float,
        containment_threshold: float,
    ) -> bool:
        """Check if two bboxes overlap sufficiently."""
        area1, area2 = bbox1.area(), bbox2.area()
        if area1 <= 0 or area2 <= 0:
            return False

        overlap_area = bbox1.intersection_area_with(bbox2)
        if overlap_area <= 0:
            return False

        iou = overlap_area / (area1 + area2 - overlap_area)
        containment1 = overlap_area / area1
        containment2 = overlap_area / area2

        return (
            iou > overlap_threshold
            or containment1 > containment_threshold
            or containment2 > containment_threshold
        )


class Interval:
    """Helper class for sortable intervals."""

    def __init__(self, min_val: float, max_val: float, id: int):
        self.min_val = min_val
        self.max_val = max_val
        self.id = id

    def __lt__(self, other):
        if isinstance(other, Interval):
            return self.min_val < other.min_val
        return self.min_val < other


class IntervalTree:
    """Memory-efficient interval tree for 1D overlap queries."""

    def __init__(self):
        self.intervals: List[Interval] = []  # Sorted by min_val

    def insert(self, min_val: float, max_val: float, id: int):
        interval = Interval(min_val, max_val, id)
        bisect.insort(self.intervals, interval)

    def find_containing(self, point: float) -> Set[int]:
        """Find all intervals containing the point."""
        pos = bisect.bisect_left(self.intervals, point)
        result = set()

        # Check intervals starting before point
        for interval in reversed(self.intervals[:pos]):
            if interval.min_val <= point <= interval.max_val:
                result.add(interval.id)
            else:
                break

        # Check intervals starting at/after point
        for interval in self.intervals[pos:]:
            if point <= interval.max_val:
                if interval.min_val <= point:
                    result.add(interval.id)
            else:
                break

        return result


class LayoutPostprocessor:
    """Postprocesses layout predictions by cleaning up clusters and mapping cells."""

    # Cluster type-specific parameters for overlap resolution
    OVERLAP_PARAMS = {
        "regular": {"area_threshold": 1.3, "conf_threshold": 0.05},
        "picture": {"area_threshold": 2.0, "conf_threshold": 0.3},
        "wrapper": {"area_threshold": 2.0, "conf_threshold": 0.2},
    }

    WRAPPER_TYPES = {
        DocItemLabel.FORM,
        DocItemLabel.KEY_VALUE_REGION,
        DocItemLabel.TABLE,
        DocItemLabel.DOCUMENT_INDEX,
    }
    SPECIAL_TYPES = WRAPPER_TYPES.union({DocItemLabel.PICTURE})

    CONFIDENCE_THRESHOLDS = {
        DocItemLabel.CAPTION: 0.5,
        DocItemLabel.FOOTNOTE: 0.5,
        DocItemLabel.FORMULA: 0.5,
        DocItemLabel.LIST_ITEM: 0.5,
        DocItemLabel.PAGE_FOOTER: 0.5,
        DocItemLabel.PAGE_HEADER: 0.5,
        DocItemLabel.PICTURE: 0.5,
        DocItemLabel.SECTION_HEADER: 0.45,
        DocItemLabel.TABLE: 0.5,
        DocItemLabel.TEXT: 0.5,  # 0.45,
        DocItemLabel.TITLE: 0.45,
        DocItemLabel.CODE: 0.45,
        DocItemLabel.CHECKBOX_SELECTED: 0.45,
        DocItemLabel.CHECKBOX_UNSELECTED: 0.45,
        DocItemLabel.FORM: 0.45,
        DocItemLabel.KEY_VALUE_REGION: 0.45,
        DocItemLabel.DOCUMENT_INDEX: 0.45,
    }

    LABEL_REMAPPING = {
        # DocItemLabel.DOCUMENT_INDEX: DocItemLabel.TABLE,
        DocItemLabel.TITLE: DocItemLabel.SECTION_HEADER,
    }

    def __init__(self, cells: List[Cell], clusters: List[Cluster], page_size: Size):
        """Initialize processor with cells and clusters."""
        """Initialize processor with cells and spatial indices."""
        self.cells = cells
        self.page_size = page_size
        self.regular_clusters = [
            c for c in clusters if c.label not in self.SPECIAL_TYPES
        ]
        self.special_clusters = [c for c in clusters if c.label in self.SPECIAL_TYPES]

        # Build spatial indices once
        self.regular_index = SpatialClusterIndex(self.regular_clusters)
        self.picture_index = SpatialClusterIndex(
            [c for c in self.special_clusters if c.label == DocItemLabel.PICTURE]
        )
        self.wrapper_index = SpatialClusterIndex(
            [c for c in self.special_clusters if c.label in self.WRAPPER_TYPES]
        )

    def postprocess(self) -> Tuple[List[Cluster], List[Cell]]:
        """Main processing pipeline."""
        self.regular_clusters = self._process_regular_clusters()
        self.special_clusters = self._process_special_clusters()

        # Remove regular clusters that are included in wrappers
        contained_ids = {
            child.id
            for wrapper in self.special_clusters
            if wrapper.label in self.SPECIAL_TYPES
            for child in wrapper.children
        }
        self.regular_clusters = [
            c for c in self.regular_clusters if c.id not in contained_ids
        ]

        # Combine and sort final clusters
        final_clusters = self._sort_clusters(
            self.regular_clusters + self.special_clusters, mode="id"
        )
        for cluster in final_clusters:
            cluster.cells = self._sort_cells(cluster.cells)
            # Also sort cells in children if any
            for child in cluster.children:
                child.cells = self._sort_cells(child.cells)

        return final_clusters, self.cells

    def _process_regular_clusters(self) -> List[Cluster]:
        """Process regular clusters with iterative refinement."""
        clusters = [
            c
            for c in self.regular_clusters
            if c.confidence >= self.CONFIDENCE_THRESHOLDS[c.label]
        ]

        # Apply label remapping
        for cluster in clusters:
            if cluster.label in self.LABEL_REMAPPING:
                cluster.label = self.LABEL_REMAPPING[cluster.label]

        # Initial cell assignment
        clusters = self._assign_cells_to_clusters(clusters)

        # Remove clusters with no cells
        clusters = [cluster for cluster in clusters if cluster.cells]

        # Handle orphaned cells
        unassigned = self._find_unassigned_cells(clusters)
        if unassigned:
            next_id = max((c.id for c in clusters), default=0) + 1
            orphan_clusters = []
            for i, cell in enumerate(unassigned):
                conf = 1.0
                if isinstance(cell, OcrCell):
                    conf = cell.confidence

                orphan_clusters.append(
                    Cluster(
                        id=next_id + i,
                        label=DocItemLabel.TEXT,
                        bbox=cell.bbox,
                        confidence=conf,
                        cells=[cell],
                    )
                )
            clusters.extend(orphan_clusters)

        # Iterative refinement
        prev_count = len(clusters) + 1
        for _ in range(3):  # Maximum 3 iterations
            if prev_count == len(clusters):
                break
            prev_count = len(clusters)
            clusters = self._adjust_cluster_bboxes(clusters)
            clusters = self._remove_overlapping_clusters(clusters, "regular")

        return clusters

    def _process_special_clusters(self) -> List[Cluster]:
        special_clusters = [
            c
            for c in self.special_clusters
            if c.confidence >= self.CONFIDENCE_THRESHOLDS[c.label]
        ]

        special_clusters = self._handle_cross_type_overlaps(special_clusters)

        # Calculate page area from known page size
        page_area = self.page_size.width * self.page_size.height
        if page_area > 0:
            # Filter out full-page pictures
            special_clusters = [
                cluster
                for cluster in special_clusters
                if not (
                    cluster.label == DocItemLabel.PICTURE
                    and cluster.bbox.area() / page_area > 0.90
                )
            ]

        for special in special_clusters:
            contained = []
            for cluster in self.regular_clusters:
                overlap = cluster.bbox.intersection_area_with(special.bbox)
                if overlap > 0:
                    containment = overlap / cluster.bbox.area()
                    if containment > 0.8:
                        contained.append(cluster)

            if contained:
                # Sort contained clusters by minimum cell ID:
                contained = self._sort_clusters(contained, mode="id")
                special.children = contained

                # Adjust bbox only for Form and Key-Value-Region, not Table or Picture
                if special.label in [DocItemLabel.FORM, DocItemLabel.KEY_VALUE_REGION]:
                    special.bbox = BoundingBox(
                        l=min(c.bbox.l for c in contained),
                        t=min(c.bbox.t for c in contained),
                        r=max(c.bbox.r for c in contained),
                        b=max(c.bbox.b for c in contained),
                    )

                # Collect all cells from children
                all_cells = []
                for child in contained:
                    all_cells.extend(child.cells)
                special.cells = self._deduplicate_cells(all_cells)
                special.cells = self._sort_cells(special.cells)

        picture_clusters = [
            c for c in special_clusters if c.label == DocItemLabel.PICTURE
        ]
        picture_clusters = self._remove_overlapping_clusters(
            picture_clusters, "picture"
        )

        wrapper_clusters = [
            c for c in special_clusters if c.label in self.WRAPPER_TYPES
        ]
        wrapper_clusters = self._remove_overlapping_clusters(
            wrapper_clusters, "wrapper"
        )

        return picture_clusters + wrapper_clusters

    def _handle_cross_type_overlaps(self, special_clusters) -> List[Cluster]:
        """Handle overlaps between regular and wrapper clusters before child assignment.

        In particular, KEY_VALUE_REGION proposals that are almost identical to a TABLE
        should be removed.
        """
        wrappers_to_remove = set()

        for wrapper in special_clusters:
            if wrapper.label not in self.WRAPPER_TYPES:
                continue  # only treat KEY_VALUE_REGION for now.

            for regular in self.regular_clusters:
                if regular.label == DocItemLabel.TABLE:
                    # Calculate overlap
                    overlap = regular.bbox.intersection_area_with(wrapper.bbox)
                    wrapper_area = wrapper.bbox.area()
                    overlap_ratio = overlap / wrapper_area

                    conf_diff = wrapper.confidence - regular.confidence

                    # If wrapper is mostly overlapping with a TABLE, remove the wrapper
                    if (
                        overlap_ratio > 0.9 and conf_diff < 0.1
                    ):  # self.OVERLAP_PARAMS["wrapper"]["conf_threshold"]):  # 80% overlap threshold
                        wrappers_to_remove.add(wrapper.id)
                        break

        # Filter out the identified wrappers
        special_clusters = [
            cluster
            for cluster in special_clusters
            if cluster.id not in wrappers_to_remove
        ]

        return special_clusters

    def _should_prefer_cluster(
        self, candidate: Cluster, other: Cluster, params: dict
    ) -> bool:
        """Determine if candidate cluster should be preferred over other cluster based on rules.
        Returns True if candidate should be preferred, False if not."""

        # Rule 1: LIST_ITEM vs TEXT
        if (
            candidate.label == DocItemLabel.LIST_ITEM
            and other.label == DocItemLabel.TEXT
        ):
            # Check if areas are similar (within 20% of each other)
            area_ratio = candidate.bbox.area() / other.bbox.area()
            area_similarity = abs(1 - area_ratio) < 0.2
            if area_similarity:
                return True

        # Rule 2: CODE vs others
        if candidate.label == DocItemLabel.CODE:
            # Calculate how much of the other cluster is contained within the CODE cluster
            overlap = other.bbox.intersection_area_with(candidate.bbox)
            containment = overlap / other.bbox.area()
            if containment > 0.8:  # other is 80% contained within CODE
                return True

        # If no label-based rules matched, fall back to area/confidence thresholds
        area_ratio = candidate.bbox.area() / other.bbox.area()
        conf_diff = other.confidence - candidate.confidence

        if (
            area_ratio <= params["area_threshold"]
            and conf_diff > params["conf_threshold"]
        ):
            return False

        return True  # Default to keeping candidate if no rules triggered rejection

    def _select_best_cluster_from_group(
        self,
        group_clusters: List[Cluster],
        params: dict,
    ) -> Cluster:
        """Select best cluster from a group of overlapping clusters based on all rules."""
        current_best = None

        for candidate in group_clusters:
            should_select = True

            for other in group_clusters:
                if other == candidate:
                    continue

                if not self._should_prefer_cluster(candidate, other, params):
                    should_select = False
                    break

            if should_select:
                if current_best is None:
                    current_best = candidate
                else:
                    # If both clusters pass rules, prefer the larger one unless confidence differs significantly
                    if (
                        candidate.bbox.area() > current_best.bbox.area()
                        and current_best.confidence - candidate.confidence
                        <= params["conf_threshold"]
                    ):
                        current_best = candidate

        return current_best if current_best else group_clusters[0]

    def _remove_overlapping_clusters(
        self,
        clusters: List[Cluster],
        cluster_type: str,
        overlap_threshold: float = 0.8,
        containment_threshold: float = 0.8,
    ) -> List[Cluster]:
        if not clusters:
            return []

        spatial_index = (
            self.regular_index
            if cluster_type == "regular"
            else self.picture_index if cluster_type == "picture" else self.wrapper_index
        )

        # Map of currently valid clusters
        valid_clusters = {c.id: c for c in clusters}
        uf = UnionFind(valid_clusters.keys())
        params = self.OVERLAP_PARAMS[cluster_type]

        for cluster in clusters:
            candidates = spatial_index.find_candidates(cluster.bbox)
            candidates &= valid_clusters.keys()  # Only keep existing candidates
            candidates.discard(cluster.id)

            for other_id in candidates:
                if spatial_index.check_overlap(
                    cluster.bbox,
                    valid_clusters[other_id].bbox,
                    overlap_threshold,
                    containment_threshold,
                ):
                    uf.union(cluster.id, other_id)

        result = []
        for group in uf.get_groups().values():
            if len(group) == 1:
                result.append(valid_clusters[group[0]])
                continue

            group_clusters = [valid_clusters[cid] for cid in group]
            best = self._select_best_cluster_from_group(group_clusters, params)

            # Simple cell merging - no special cases
            for cluster in group_clusters:
                if cluster != best:
                    best.cells.extend(cluster.cells)

            best.cells = self._deduplicate_cells(best.cells)
            best.cells = self._sort_cells(best.cells)
            result.append(best)

        return result

    def _select_best_cluster(
        self,
        clusters: List[Cluster],
        area_threshold: float,
        conf_threshold: float,
    ) -> Cluster:
        """Iteratively select best cluster based on area and confidence thresholds."""
        current_best = None
        for candidate in clusters:
            should_select = True
            for other in clusters:
                if other == candidate:
                    continue

                area_ratio = candidate.bbox.area() / other.bbox.area()
                conf_diff = other.confidence - candidate.confidence

                if area_ratio <= area_threshold and conf_diff > conf_threshold:
                    should_select = False
                    break

            if should_select:
                if current_best is None or (
                    candidate.bbox.area() > current_best.bbox.area()
                    and current_best.confidence - candidate.confidence <= conf_threshold
                ):
                    current_best = candidate

        return current_best if current_best else clusters[0]

    def _deduplicate_cells(self, cells: List[Cell]) -> List[Cell]:
        """Ensure each cell appears only once, maintaining order of first appearance."""
        seen_ids = set()
        unique_cells = []
        for cell in cells:
            if cell.id not in seen_ids:
                seen_ids.add(cell.id)
                unique_cells.append(cell)
        return unique_cells

    def _assign_cells_to_clusters(
        self, clusters: List[Cluster], min_overlap: float = 0.2
    ) -> List[Cluster]:
        """Assign cells to best overlapping cluster."""
        for cluster in clusters:
            cluster.cells = []

        for cell in self.cells:
            if not cell.text.strip():
                continue

            best_overlap = min_overlap
            best_cluster = None

            for cluster in clusters:
                if cell.bbox.area() <= 0:
                    continue

                overlap = cell.bbox.intersection_area_with(cluster.bbox)
                overlap_ratio = overlap / cell.bbox.area()

                if overlap_ratio > best_overlap:
                    best_overlap = overlap_ratio
                    best_cluster = cluster

            if best_cluster is not None:
                best_cluster.cells.append(cell)

        # Deduplicate cells in each cluster after assignment
        for cluster in clusters:
            cluster.cells = self._deduplicate_cells(cluster.cells)

        return clusters

    def _find_unassigned_cells(self, clusters: List[Cluster]) -> List[Cell]:
        """Find cells not assigned to any cluster."""
        assigned = {cell.id for cluster in clusters for cell in cluster.cells}
        return [
            cell for cell in self.cells if cell.id not in assigned and cell.text.strip()
        ]

    def _adjust_cluster_bboxes(self, clusters: List[Cluster]) -> List[Cluster]:
        """Adjust cluster bounding boxes to contain their cells."""
        for cluster in clusters:
            if not cluster.cells:
                continue

            cells_bbox = BoundingBox(
                l=min(cell.bbox.l for cell in cluster.cells),
                t=min(cell.bbox.t for cell in cluster.cells),
                r=max(cell.bbox.r for cell in cluster.cells),
                b=max(cell.bbox.b for cell in cluster.cells),
            )

            if cluster.label == DocItemLabel.TABLE:
                # For tables, take union of current bbox and cells bbox
                cluster.bbox = BoundingBox(
                    l=min(cluster.bbox.l, cells_bbox.l),
                    t=min(cluster.bbox.t, cells_bbox.t),
                    r=max(cluster.bbox.r, cells_bbox.r),
                    b=max(cluster.bbox.b, cells_bbox.b),
                )
            else:
                cluster.bbox = cells_bbox

        return clusters

    def _sort_cells(self, cells: List[Cell]) -> List[Cell]:
        """Sort cells in native reading order."""
        return sorted(cells, key=lambda c: (c.id))

    def _sort_clusters(
        self, clusters: List[Cluster], mode: str = "id"
    ) -> List[Cluster]:
        """Sort clusters in reading order (top-to-bottom, left-to-right)."""
        if mode == "id":  # sort in the order the cells are printed in the PDF.
            return sorted(
                clusters,
                key=lambda cluster: (
                    (
                        min(cell.id for cell in cluster.cells)
                        if cluster.cells
                        else sys.maxsize
                    ),
                    cluster.bbox.t,
                    cluster.bbox.l,
                ),
            )
        elif mode == "tblr":  # Sort top-to-bottom, then left-to-right ("row first")
            return sorted(
                clusters, key=lambda cluster: (cluster.bbox.t, cluster.bbox.l)
            )
        elif mode == "lrtb":  # Sort left-to-right, then top-to-bottom ("column first")
            return sorted(
                clusters, key=lambda cluster: (cluster.bbox.l, cluster.bbox.t)
            )
        else:
            return clusters


================================================
File: docling/utils/model_downloader.py
================================================
import logging
from pathlib import Path
from typing import Optional

from docling.datamodel.pipeline_options import (
    granite_picture_description,
    smolvlm_picture_description,
)
from docling.datamodel.settings import settings
from docling.models.code_formula_model import CodeFormulaModel
from docling.models.document_picture_classifier import DocumentPictureClassifier
from docling.models.easyocr_model import EasyOcrModel
from docling.models.layout_model import LayoutModel
from docling.models.picture_description_vlm_model import PictureDescriptionVlmModel
from docling.models.table_structure_model import TableStructureModel

_log = logging.getLogger(__name__)


def download_models(
    output_dir: Optional[Path] = None,
    *,
    force: bool = False,
    progress: bool = False,
    with_layout: bool = True,
    with_tableformer: bool = True,
    with_code_formula: bool = True,
    with_picture_classifier: bool = True,
    with_smolvlm: bool = False,
    with_granite_vision: bool = False,
    with_easyocr: bool = True,
):
    if output_dir is None:
        output_dir = settings.cache_dir / "models"

    # Make sure the folder exists
    output_dir.mkdir(exist_ok=True, parents=True)

    if with_layout:
        _log.info(f"Downloading layout model...")
        LayoutModel.download_models(
            local_dir=output_dir / LayoutModel._model_repo_folder,
            force=force,
            progress=progress,
        )

    if with_tableformer:
        _log.info(f"Downloading tableformer model...")
        TableStructureModel.download_models(
            local_dir=output_dir / TableStructureModel._model_repo_folder,
            force=force,
            progress=progress,
        )

    if with_picture_classifier:
        _log.info(f"Downloading picture classifier model...")
        DocumentPictureClassifier.download_models(
            local_dir=output_dir / DocumentPictureClassifier._model_repo_folder,
            force=force,
            progress=progress,
        )

    if with_code_formula:
        _log.info(f"Downloading code formula model...")
        CodeFormulaModel.download_models(
            local_dir=output_dir / CodeFormulaModel._model_repo_folder,
            force=force,
            progress=progress,
        )

    if with_smolvlm:
        _log.info(f"Downloading SmolVlm model...")
        PictureDescriptionVlmModel.download_models(
            repo_id=smolvlm_picture_description.repo_id,
            local_dir=output_dir / smolvlm_picture_description.repo_cache_folder,
            force=force,
            progress=progress,
        )

    if with_granite_vision:
        _log.info(f"Downloading Granite Vision model...")
        PictureDescriptionVlmModel.download_models(
            repo_id=granite_picture_description.repo_id,
            local_dir=output_dir / granite_picture_description.repo_cache_folder,
            force=force,
            progress=progress,
        )

    if with_easyocr:
        _log.info(f"Downloading easyocr models...")
        EasyOcrModel.download_models(
            local_dir=output_dir / EasyOcrModel._model_repo_folder,
            force=force,
            progress=progress,
        )

    return output_dir


================================================
File: docling/utils/ocr_utils.py
================================================
def map_tesseract_script(script: str) -> str:
    r""" """
    if script == "Katakana" or script == "Hiragana":
        script = "Japanese"
    elif script == "Han":
        script = "HanS"
    elif script == "Korean":
        script = "Hangul"
    return script


================================================
File: docling/utils/profiling.py
================================================
import time
from datetime import datetime
from enum import Enum
from typing import TYPE_CHECKING, List

import numpy as np
from pydantic import BaseModel

from docling.datamodel.settings import settings

if TYPE_CHECKING:
    from docling.datamodel.document import ConversionResult


class ProfilingScope(str, Enum):
    PAGE = "page"
    DOCUMENT = "document"


class ProfilingItem(BaseModel):
    scope: ProfilingScope
    count: int = 0
    times: List[float] = []
    start_timestamps: List[datetime] = []

    def avg(self) -> float:
        return np.average(self.times)  # type: ignore

    def std(self) -> float:
        return np.std(self.times)  # type: ignore

    def mean(self) -> float:
        return np.mean(self.times)  # type: ignore

    def percentile(self, perc: float) -> float:
        return np.percentile(self.times, perc)  # type: ignore


class TimeRecorder:
    def __init__(
        self,
        conv_res: "ConversionResult",
        key: str,
        scope: ProfilingScope = ProfilingScope.PAGE,
    ):
        if settings.debug.profile_pipeline_timings:
            if key not in conv_res.timings.keys():
                conv_res.timings[key] = ProfilingItem(scope=scope)
            self.conv_res = conv_res
            self.key = key

    def __enter__(self):
        if settings.debug.profile_pipeline_timings:
            self.start = time.monotonic()
            self.conv_res.timings[self.key].start_timestamps.append(datetime.utcnow())
        return self

    def __exit__(self, *args):
        if settings.debug.profile_pipeline_timings:
            elapsed = time.monotonic() - self.start
            self.conv_res.timings[self.key].times.append(elapsed)
            self.conv_res.timings[self.key].count += 1


================================================
File: docling/utils/utils.py
================================================
import hashlib
from io import BytesIO
from itertools import islice
from pathlib import Path
from typing import List, Union

import requests
from tqdm import tqdm


def chunkify(iterator, chunk_size):
    """Yield successive chunks of chunk_size from the iterable."""
    if isinstance(iterator, List):
        iterator = iter(iterator)
    for first in iterator:  # Take the first element from the iterator
        yield [first] + list(islice(iterator, chunk_size - 1))


def create_file_hash(path_or_stream: Union[BytesIO, Path]) -> str:
    """Create a stable page_hash of the path_or_stream of a file"""

    block_size = 65536
    hasher = hashlib.sha256()

    def _hash_buf(binary_stream):
        buf = binary_stream.read(block_size)  # read and page_hash in chunks
        while len(buf) > 0:
            hasher.update(buf)
            buf = binary_stream.read(block_size)

    if isinstance(path_or_stream, Path):
        with path_or_stream.open("rb") as afile:
            _hash_buf(afile)
    elif isinstance(path_or_stream, BytesIO):
        _hash_buf(path_or_stream)

    return hasher.hexdigest()


def create_hash(string: str):
    hasher = hashlib.sha256()
    hasher.update(string.encode("utf-8"))

    return hasher.hexdigest()


def download_url_with_progress(url: str, progress: bool = False) -> BytesIO:
    buf = BytesIO()
    with requests.get(url, stream=True, allow_redirects=True) as response:
        total_size = int(response.headers.get("content-length", 0))
        progress_bar = tqdm(
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
            disable=(not progress),
        )

        for chunk in response.iter_content(10 * 1024):
            buf.write(chunk)
            progress_bar.update(len(chunk))
        progress_bar.close()

    buf.seek(0)
    return buf


================================================
File: docling/utils/visualization.py
================================================
from docling_core.types.doc import DocItemLabel
from PIL import Image, ImageDraw, ImageFont
from PIL.ImageFont import FreeTypeFont

from docling.datamodel.base_models import Cluster


def draw_clusters(
    image: Image.Image, clusters: list[Cluster], scale_x: float, scale_y: float
) -> None:
    """
    Draw clusters on an image
    """
    draw = ImageDraw.Draw(image, "RGBA")
    # Create a smaller font for the labels
    font: ImageFont.ImageFont | FreeTypeFont
    try:
        font = ImageFont.truetype("arial.ttf", 12)
    except OSError:
        # Fallback to default font if arial is not available
        font = ImageFont.load_default()
    for c_tl in clusters:
        all_clusters = [c_tl, *c_tl.children]
        for c in all_clusters:
            # Draw cells first (underneath)
            cell_color = (0, 0, 0, 40)  # Transparent black for cells
            for tc in c.cells:
                cx0, cy0, cx1, cy1 = tc.bbox.as_tuple()
                cx0 *= scale_x
                cx1 *= scale_x
                cy0 *= scale_x
                cy1 *= scale_y

                draw.rectangle(
                    [(cx0, cy0), (cx1, cy1)],
                    outline=None,
                    fill=cell_color,
                )
            # Draw cluster rectangle
            x0, y0, x1, y1 = c.bbox.as_tuple()
            x0 *= scale_x
            x1 *= scale_x
            y0 *= scale_x
            y1 *= scale_y

            if y1 <= y0:
                y1, y0 = y0, y1
            if x1 <= x0:
                x1, x0 = x0, x1

            cluster_fill_color = (*list(DocItemLabel.get_color(c.label)), 70)
            cluster_outline_color = (
                *list(DocItemLabel.get_color(c.label)),
                255,
            )
            draw.rectangle(
                [(x0, y0), (x1, y1)],
                outline=cluster_outline_color,
                fill=cluster_fill_color,
            )
            # Add label name and confidence
            label_text = f"{c.label.name} ({c.confidence:.2f})"
            # Create semi-transparent background for text
            text_bbox = draw.textbbox((x0, y0), label_text, font=font)
            text_bg_padding = 2
            draw.rectangle(
                [
                    (
                        text_bbox[0] - text_bg_padding,
                        text_bbox[1] - text_bg_padding,
                    ),
                    (
                        text_bbox[2] + text_bg_padding,
                        text_bbox[3] + text_bg_padding,
                    ),
                ],
                fill=(255, 255, 255, 180),  # Semi-transparent white
            )
            # Draw text
            draw.text(
                (x0, y0),
                label_text,
                fill=(0, 0, 0, 255),  # Solid black
                font=font,
            )


================================================
File: docs/faq.md
================================================
# FAQ

This is a collection of FAQ collected from the user questions on <https://github.com/DS4SD/docling/discussions>.


??? question "Is Python 3.13 supported?"

    ### Is Python 3.13 supported?

    Python 3.13 is supported from Docling 2.18.0.


??? question "Install conflicts with numpy (python 3.13)"

    ### Install conflicts with numpy (python 3.13)

    When using `docling-ibm-models>=2.0.7` and `deepsearch-glm>=0.26.2` these issues should not show up anymore.
    Docling supports numpy versions `>=1.24.4,<3.0.0` which should match all usages.

    **For older versions**

    This has been observed installing docling and langchain via poetry.

    ```
    ...
    Thus, docling (>=2.7.0,<3.0.0) requires numpy (>=1.26.4,<2.0.0).
    So, because ... depends on both numpy (>=2.0.2,<3.0.0) and docling (^2.7.0), version solving failed.
    ```

    Numpy is only adding Python 3.13 support starting in some 2.x.y version. In order to prepare for 3.13, Docling depends on a 2.x.y for 3.13, otherwise depending an 1.x.y version. If you are allowing 3.13 in your pyproject.toml, Poetry will try to find some way to reconcile Docling's numpy version for 3.13 (some 2.x.y) with LangChain's version for that (some 1.x.y) — leading to the error above.

    Check if Python 3.13 is among the Python versions allowed by your pyproject.toml and if so, remove it and try again.
    E.g., if you have python = "^3.10", use python = ">=3.10,<3.13" instead.

    If you want to retain compatibility with python 3.9-3.13, you can also use a selector in pyproject.toml similar to the following

    ```toml
    numpy = [
        { version = "^2.1.0", markers = 'python_version >= "3.13"' },
        { version = "^1.24.4", markers = 'python_version < "3.13"' },
    ]
    ```

    Source: Issue [#283](https://github.com/DS4SD/docling/issues/283#issuecomment-2465035868)


??? question "Are text styles (bold, underline, etc) supported?"

    ### Are text styles (bold, underline, etc) supported?

    Currently text styles are not supported in the `DoclingDocument` format.
    If you are interest in contributing this feature, please open a discussion topic to brainstorm on the design.

    _Note: this is not a simple topic_


??? question "How do I run completely offline?"

    ### How do I run completely offline?

    Docling is not using any remote service, hence it can run in completely isolated air-gapped environments.

    The only requirement is pointing the Docling runtime to the location where the model artifacts have been stored.

    For example

    ```py

    pipeline_options = PdfPipelineOptions(artifacts_path="your location")
    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )
    ```

    Source: Issue [#326](https://github.com/DS4SD/docling/issues/326)


??? question " Which model weights are needed to run Docling?"
    ### Which model weights are needed to run Docling?

    Model weights are needed for the AI models used in the PDF pipeline. Other document types (docx, pptx, etc) do not have any such requirement.

    For processing PDF documents, Docling requires the model weights from <https://huggingface.co/ds4sd/docling-models>.

    When OCR is enabled, some engines also require model artifacts. For example EasyOCR, for which Docling has [special pipeline options](https://github.com/DS4SD/docling/blob/main/docling/datamodel/pipeline_options.py#L68) to control the runtime behavior.


??? question "SSL error downloading model weights"

    ### SSL error downloading model weights

    ```
    URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>
    ```

    Similar SSL download errors have been observed by some users. This happens when model weights are fetched from Hugging Face.
    The error could happen when the python environment doesn't have an up-to-date list of trusted certificates.

    Possible solutions were

    - Update to the latest version of [certifi](https://pypi.org/project/certifi/), i.e. `pip install --upgrade certifi`
    - Use [pip-system-certs](https://pypi.org/project/pip-system-certs/) to use the latest trusted certificates on your system.
    - Set environment variables `SSL_CERT_FILE` and `REQUESTS_CA_BUNDLE` to the value of `python -m certifi`:
        ```
        CERT_PATH=$(python -m certifi)
        export SSL_CERT_FILE=${CERT_PATH}
        export REQUESTS_CA_BUNDLE=${CERT_PATH}
        ```


??? question "Which OCR languages are supported?"

    ### Which OCR languages are supported?

    Docling supports multiple OCR engine, each one has its own list of supported languages.
    Here is a collection of links to the original OCR engine's documentation listing the OCR languages.

    - [EasyOCR](https://www.jaided.ai/easyocr/)
    - [Tesseract](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)
    - [RapidOCR](https://rapidai.github.io/RapidOCRDocs/blog/2022/09/28/%E6%94%AF%E6%8C%81%E8%AF%86%E5%88%AB%E8%AF%AD%E8%A8%80/)
    - [Mac OCR](https://github.com/straussmaximilian/ocrmac/tree/main?tab=readme-ov-file#example-select-language-preference)

    Setting the OCR language in Docling is done via the OCR pipeline options:

    ```py
    from docling.datamodel.pipeline_options import PdfPipelineOptions

    pipeline_options = PdfPipelineOptions()
    pipeline_options.ocr_options.lang = ["fr", "de", "es", "en"]  # example of languages for EasyOCR
    ```


??? question "Some images are missing from MS Word and Powerpoint"

    ### Some images are missing from MS Word and Powerpoint

    The image processing library used by Docling is able to handle embedded WMF images only on Windows platform.
    If you are on other operaring systems, these images will be ignored.


??? question "`HybridChunker` triggers warning: 'Token indices sequence length is longer than the specified maximum sequence length for this model'"

    ### `HybridChunker` triggers warning: 'Token indices sequence length is longer than the specified maximum sequence length for this model'

    **TLDR**:
    In the context of the `HybridChunker`, this is a known & ancitipated "false alarm".

    **Details**:

    Using the [`HybridChunker`](./concepts/chunking.md#hybrid-chunker) often triggers a warning like this:
    > Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors

    This is a warning that is emitted by transformers, saying that actually *running this sequence through the model* will result in indexing errors, i.e. the problematic case is only if one indeed passes the particular sequence through the (embedding) model.

    In our case though, this occurs as a "false alarm", since what happens is the following:

    - the chunker invokes the tokenizer on a potentially long sequence (e.g. 530 tokens as mentioned in the warning) in order to count its tokens, i.e. to assess if it is short enough. At this point transformers already emits the warning above!
    - whenever the sequence at hand is oversized, the chunker proceeds to split it (but the transformers warning has already been shown nonetheless)

    What is important is the actual token length of the produced chunks.
    The snippet below can be used for getting the actual maximum chunk size (for users wanting to confirm that this does not exceed the model limit):

    ```python
    max_len = 0
    for i, chunk in enumerate(chunks):
        ser_txt = chunker.serialize(chunk=chunk)
        ser_tokens = len(tokenizer.tokenize(ser_txt, max_len_length=None))
        if ser_tokens > max_len:
            max_len = ser_tokens
        print(f"{i}\t{ser_tokens}\t{repr(ser_txt[:100])}...")
    print(f"{max_len=}")
    ```

    Source: Issue [docling-core#119](https://github.com/DS4SD/docling-core/issues/119)


================================================
File: docs/index.md
================================================
<p align="center">
  <img loading="lazy" alt="Docling" src="assets/docling_processing.png" width="100%" />
  <a href="https://trendshift.io/repositories/12132" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12132" alt="DS4SD%2Fdocling | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

[![arXiv](https://img.shields.io/badge/arXiv-2408.09869-b31b1b.svg)](https://arxiv.org/abs/2408.09869)
[![PyPI version](https://img.shields.io/pypi/v/docling)](https://pypi.org/project/docling/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/docling)](https://pypi.org/project/docling/)
[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://pydantic.dev)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![License MIT](https://img.shields.io/github/license/DS4SD/docling)](https://opensource.org/licenses/MIT)
[![PyPI Downloads](https://static.pepy.tech/badge/docling/month)](https://pepy.tech/projects/docling)

Docling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.

## Features

* 🗂️ Parsing of [multiple document formats][supported_formats] incl. PDF, DOCX, XLSX, HTML, images, and more
* 📑 Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more
* 🧬 Unified, expressive [DoclingDocument][docling_document] representation format
* ↪️ Various [export formats][supported_formats] and options, including Markdown, HTML, and lossless JSON
* 🔒 Local execution capabilities for sensitive data and air-gapped environments
* 🤖 Plug-and-play [integrations][integrations] incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI
* 🔍 Extensive OCR support for scanned PDFs and images
* 💻 Simple and convenient CLI

### Coming soon

* 📝 Metadata extraction, including title, authors, references & language
* 📝 Inclusion of Visual Language Models ([SmolDocling](https://huggingface.co/blog/smolervlm#smoldocling))
* 📝 Chart understanding (Barchart, Piechart, LinePlot, etc)
* 📝 Complex chemistry understanding (Molecular structures)

## Get started

<div class="grid">
  <a href="concepts/" class="card"><b>Concepts</b><br />Learn Docling fundamendals</a>
  <a href="examples/" class="card"><b>Examples</b><br />Try out recipes for various use cases, including conversion, RAG, and more</a>
  <a href="integrations/" class="card"><b>Integrations</b><br />Check out integrations with popular frameworks and tools</a>
  <a href="reference/document_converter/" class="card"><b>Reference</b><br />See more API details</a>
</div>

## IBM ❤️ Open Source AI

Docling has been brought to you by IBM.

[supported_formats]: ./supported_formats.md
[docling_document]: ./concepts/docling_document.md
[integrations]: ./integrations/index.md


================================================
File: docs/installation.md
================================================
To use Docling, simply install `docling` from your Python package manager, e.g. pip:
```bash
pip install docling
```

Works on macOS, Linux, and Windows, with support for both x86_64 and arm64 architectures.

??? "Alternative PyTorch distributions"

    The Docling models depend on the [PyTorch](https://pytorch.org/) library.
    Depending on your architecture, you might want to use a different distribution of `torch`.
    For example, you might want support for different accelerator or for a cpu-only version.
    All the different ways for installing `torch` are listed on their website <https://pytorch.org/>.

    One common situation is the installation on Linux systems with cpu-only support.
    In this case, we suggest the installation of Docling with the following options

    ```bash
    # Example for installing on the Linux cpu-only version
    pip install docling --extra-index-url https://download.pytorch.org/whl/cpu
    ```

??? "Alternative OCR engines"

    Docling supports multiple OCR engines for processing scanned documents. The current version provides
    the following engines.

    | Engine | Installation | Usage |
    | ------ | ------------ | ----- |
    | [EasyOCR](https://github.com/JaidedAI/EasyOCR) | Default in Docling or via `pip install easyocr`. | `EasyOcrOptions` |
    | Tesseract | System dependency. See description for Tesseract and Tesserocr below.  | `TesseractOcrOptions` |
    | Tesseract CLI | System dependency. See description below. | `TesseractCliOcrOptions` |
    | OcrMac | System dependency. See description below. | `OcrMacOptions` |
    | [RapidOCR](https://github.com/RapidAI/RapidOCR) | Extra feature not included in Default Docling installation can be installed via `pip install rapidocr_onnxruntime` | `RapidOcrOptions` |

    The Docling `DocumentConverter` allows to choose the OCR engine with the `ocr_options` settings. For example

    ```python
    from docling.datamodel.base_models import ConversionStatus, PipelineOptions
    from docling.datamodel.pipeline_options import PipelineOptions, EasyOcrOptions, TesseractOcrOptions
    from docling.document_converter import DocumentConverter

    pipeline_options = PipelineOptions()
    pipeline_options.do_ocr = True
    pipeline_options.ocr_options = TesseractOcrOptions()  # Use Tesseract

    doc_converter = DocumentConverter(
        pipeline_options=pipeline_options,
    )
    ```

    <h3>Tesseract installation</h3>

    [Tesseract](https://github.com/tesseract-ocr/tesseract) is a popular OCR engine which is available
    on most operating systems. For using this engine with Docling, Tesseract must be installed on your
    system, using the packaging tool of your choice. Below we provide example commands.
    After installing Tesseract you are expected to provide the path to its language files using the
    `TESSDATA_PREFIX` environment variable (note that it must terminate with a slash `/`).

    === "macOS (via [Homebrew](https://brew.sh/))"

        ```console
        brew install tesseract leptonica pkg-config
        TESSDATA_PREFIX=/opt/homebrew/share/tessdata/
        echo "Set TESSDATA_PREFIX=${TESSDATA_PREFIX}"
        ```

    === "Debian-based"

        ```console
        apt-get install tesseract-ocr tesseract-ocr-eng libtesseract-dev libleptonica-dev pkg-config
        TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)
        echo "Set TESSDATA_PREFIX=${TESSDATA_PREFIX}"
        ```

    === "RHEL"

        ```console
        dnf install tesseract tesseract-devel tesseract-langpack-eng leptonica-devel
        TESSDATA_PREFIX=/usr/share/tesseract/tessdata/
        echo "Set TESSDATA_PREFIX=${TESSDATA_PREFIX}"
        ```

    <h3>Linking to Tesseract</h3>
    The most efficient usage of the Tesseract library is via linking. Docling is using
    the [Tesserocr](https://github.com/sirfz/tesserocr) package for this.

    If you get into installation issues of Tesserocr, we suggest using the following
    installation options:

    ```console
    pip uninstall tesserocr
    pip install --no-binary :all: tesserocr
    ```

    <h3>ocrmac installation</h3>

    [ocrmac](https://github.com/straussmaximilian/ocrmac) is using
    Apple's vision(or livetext) framework as OCR backend.
    For using this engine with Docling, ocrmac must be installed on your system.
    This only works on macOS systems with newer macOS versions (10.15+).

    ```console
    pip install ocrmac
    ```

## Development setup

To develop Docling features, bugfixes etc., install as follows from your local clone's root dir:

```bash
poetry install --all-extras
```


================================================
File: docs/supported_formats.md
================================================
Docling can parse various documents formats into a unified representation (Docling
Document), which it can export to different formats too — check out
[Architecture](./concepts/architecture.md) for more details.

Below you can find a listing of all supported input and output formats.

## Supported input formats

| Format | Description |
|--------|-------------|
| PDF | |
| DOCX, XLSX, PPTX | Default formats in MS Office 2007+, based on Office Open XML |
| Markdown | |
| AsciiDoc | |
| HTML, XHTML | |
| CSV | |
| PNG, JPEG, TIFF, BMP | Image formats |

Schema-specific support:

| Format | Description |
|--------|-------------|
| USPTO XML | XML format followed by [USPTO](https://www.uspto.gov/patents) patents |
| JATS XML | XML format followed by [JATS](https://jats.nlm.nih.gov/) articles |
| Docling JSON | JSON-serialized [Docling Document](./concepts/docling_document.md) |

## Supported output formats

| Format | Description |
|--------|-------------|
| HTML | Both image embedding and referencing are supported |
| Markdown | |
| JSON | Lossless serialization of Docling Document |
| Text | Plain text, i.e. without Markdown markers |
| Doctags | |


================================================
File: docs/usage.md
================================================
## Conversion

### Convert a single document

To convert individual PDF documents, use `convert()`, for example:

```python
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"  # PDF path or URL
converter = DocumentConverter()
result = converter.convert(source)
print(result.document.export_to_markdown())  # output: "### Docling Technical Report[...]"
```

### CLI

You can also use Docling directly from your command line to convert individual files —be it local or by URL— or whole directories.

A simple example would look like this:
```console
docling https://arxiv.org/pdf/2206.01062
```

To see all available options (export formats etc.) run `docling --help`. More details in the [CLI reference page](./reference/cli.md).

### Advanced options

#### Model prefetching and offline usage

By default, models are downloaded automatically upon first usage. If you would prefer
to explicitly prefetch them for offline use (e.g. in air-gapped environments) you can do
that as follows:

**Step 1: Prefetch the models**

Use the `docling-tools models download` utility:

```sh
$ docling-tools models download
Downloading layout model...
Downloading tableformer model...
Downloading picture classifier model...
Downloading code formula model...
Downloading easyocr models...
Models downloaded into $HOME/.cache/docling/models.
```

Alternatively, models can be programmatically downloaded using `docling.utils.model_downloader.download_models()`.

**Step 2: Use the prefetched models**

```python
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

artifacts_path = "/local/path/to/models"

pipeline_options = PdfPipelineOptions(artifacts_path=artifacts_path)
doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

Or using the CLI:

```sh
docling --artifacts-path="/local/path/to/models" FILE
```

#### Using remote services

The main purpose of Docling is to run local models which are not sharing any user data with remote services.
Anyhow, there are valid use cases for processing part of the pipeline using remote services, for example invoking OCR engines from cloud vendors or the usage of hosted LLMs.

In Docling we decided to allow such models, but we require the user to explicitly opt-in in communicating with external services.

```py
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

pipeline_options = PdfPipelineOptions(enable_remote_services=True)
doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

When the value `enable_remote_services=True` is not set, the system will raise an exception `OperationNotAllowed()`.

_Note: This option is only related to the system sending user data to remote services. Control of pulling data (e.g. model weights) follows the logic described in [Model prefetching and offline usage](#model-prefetching-and-offline-usage)._

##### List of remote model services

The options in this list require the explicit `enable_remote_services=True` when processing the documents.

- `PictureDescriptionApiOptions`: Using vision models via API calls.


#### Adjust pipeline features

The example file [custom_convert.py](./examples/custom_convert.py) contains multiple ways
one can adjust the conversion pipeline and features.

##### Control PDF table extraction options

You can control if table structure recognition should map the recognized structure back to PDF cells (default) or use text cells from the structure prediction itself.
This can improve output quality if you find that multiple columns in extracted tables are erroneously merged into one.


```python
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions

pipeline_options = PdfPipelineOptions(do_table_structure=True)
pipeline_options.table_structure_options.do_cell_matching = False  # uses text cells predicted from table structure model

doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

Since docling 1.16.0: You can control which TableFormer mode you want to use. Choose between `TableFormerMode.FAST` (default) and `TableFormerMode.ACCURATE` (better, but slower) to receive better quality with difficult table structures.

```python
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode

pipeline_options = PdfPipelineOptions(do_table_structure=True)
pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model

doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```


#### Impose limits on the document size

You can limit the file size and number of pages which should be allowed to process per document:

```python
from pathlib import Path
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"
converter = DocumentConverter()
result = converter.convert(source, max_num_pages=100, max_file_size=20971520)
```

#### Convert from binary PDF streams

You can convert PDFs from a binary stream instead of from the filesystem as follows:

```python
from io import BytesIO
from docling.datamodel.base_models import DocumentStream
from docling.document_converter import DocumentConverter

buf = BytesIO(your_binary_stream)
source = DocumentStream(name="my_doc.pdf", stream=buf)
converter = DocumentConverter()
result = converter.convert(source)
```

#### Limit resource usage

You can limit the CPU threads used by Docling by setting the environment variable `OMP_NUM_THREADS` accordingly. The default setting is using 4 CPU threads.


#### Use specific backend converters

!!! note

    This section discusses directly invoking a [backend](./concepts/architecture.md),
    i.e. using a low-level API. This should only be done when necessary. For most cases,
    using a `DocumentConverter` (high-level API) as discussed in the sections above
    should suffice — and is the recommended way.

By default, Docling will try to identify the document format to apply the appropriate conversion backend (see the list of [supported formats](./supported_formats.md)).
You can restrict the `DocumentConverter` to a set of allowed document formats, as shown in the [Multi-format conversion](./examples/run_with_formats.py) example.
Alternatively, you can also use the specific backend that matches your document content. For instance, you can use `HTMLDocumentBackend` for HTML pages:

```python
import urllib.request
from io import BytesIO
from docling.backend.html_backend import HTMLDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

url = "https://en.wikipedia.org/wiki/Duck"
text = urllib.request.urlopen(url).read()
in_doc = InputDocument(
    path_or_stream=BytesIO(text),
    format=InputFormat.HTML,
    backend=HTMLDocumentBackend,
    filename="duck.html",
)
backend = HTMLDocumentBackend(in_doc=in_doc, path_or_stream=BytesIO(text))
dl_doc = backend.convert()
print(dl_doc.export_to_markdown())
```

## Chunking

You can chunk a Docling document using a [chunker](concepts/chunking.md), such as a
`HybridChunker`, as shown below (for more details check out
[this example](examples/hybrid_chunking.ipynb)):

```python
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker

conv_res = DocumentConverter().convert("https://arxiv.org/pdf/2206.01062")
doc = conv_res.document

chunker = HybridChunker(tokenizer="BAAI/bge-small-en-v1.5")  # set tokenizer as needed
chunk_iter = chunker.chunk(doc)
```

An example chunk would look like this:

```python
print(list(chunk_iter)[11])
# {
#   "text": "In this paper, we present the DocLayNet dataset. [...]",
#   "meta": {
#     "doc_items": [{
#       "self_ref": "#/texts/28",
#       "label": "text",
#       "prov": [{
#         "page_no": 2,
#         "bbox": {"l": 53.29, "t": 287.14, "r": 295.56, "b": 212.37, ...},
#       }], ...,
#     }, ...],
#     "headings": ["1 INTRODUCTION"],
#   }
# }
```


================================================
File: docs/v2.md
================================================
## What's new

Docling v2 introduces several new features:

- Understands and converts PDF, MS Word, MS Powerpoint, HTML and several image formats
- Produces a new, universal document representation which can encapsulate document hierarchy
- Comes with a fresh new API and CLI

## Changes in Docling v2

### CLI

We updated the command line syntax of Docling v2 to support many formats. Examples are seen below.
```shell
# Convert a single file to Markdown (default)
docling myfile.pdf

# Convert a single file to Markdown and JSON, without OCR
docling myfile.pdf --to json --to md --no-ocr

# Convert PDF files in input directory to Markdown (default)
docling ./input/dir --from pdf

# Convert PDF and Word files in input directory to Markdown and JSON
docling ./input/dir --from pdf --from docx --to md --to json --output ./scratch

# Convert all supported files in input directory to Markdown, but abort on first error
docling ./input/dir --output ./scratch --abort-on-error

```

**Notable changes from Docling v1:**

- The standalone switches for different export formats are removed, and replaced with `--from` and `--to` arguments, to define input and output formats respectively.
- The new `--abort-on-error` will abort any batch conversion as soon an error is encountered
- The `--backend` option for PDFs was removed

### Setting up a `DocumentConverter`

To accomodate many input formats, we changed the way you need to set up your `DocumentConverter` object.
You can now define a list of allowed formats on the `DocumentConverter` initialization, and specify custom options
per-format if desired. By default, all supported formats are allowed. If you don't provide `format_options`, defaults
will be used for all `allowed_formats`.

Format options can include the pipeline class to use, the options to provide to the pipeline, and the document backend.
They are provided as format-specific types, such as `PdfFormatOption` or `WordFormatOption`, as seen below.

```python
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat
from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
    WordFormatOption,
)
from docling.pipeline.simple_pipeline import SimplePipeline
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend

## Default initialization still works as before:
# doc_converter = DocumentConverter()


# previous `PipelineOptions` is now `PdfPipelineOptions`
pipeline_options = PdfPipelineOptions()
pipeline_options.do_ocr = False
pipeline_options.do_table_structure = True
#...

## Custom options are now defined per format.
doc_converter = (
    DocumentConverter(  # all of the below is optional, has internal defaults.
        allowed_formats=[
            InputFormat.PDF,
            InputFormat.IMAGE,
            InputFormat.DOCX,
            InputFormat.HTML,
            InputFormat.PPTX,
        ],  # whitelist formats, non-matching files are ignored.
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options, # pipeline options go here.
                backend=PyPdfiumDocumentBackend # optional: pick an alternative backend
            ),
            InputFormat.DOCX: WordFormatOption(
                pipeline_cls=SimplePipeline # default for office formats and HTML
            ),
        },
    )
)
```

**Note**: If you work only with defaults, all remains the same as in Docling v1.

More options are shown in the following example units:

- [run_with_formats.py](examples/run_with_formats.py)
- [custom_convert.py](examples/custom_convert.py)

### Converting documents

We have simplified the way you can feed input to the `DocumentConverter` and renamed the conversion methods for
better semantics. You can now call the conversion directly with a single file, or a list of input files,
or `DocumentStream` objects, without constructing a `DocumentConversionInput` object first.

* `DocumentConverter.convert` now converts a single file input (previously `DocumentConverter.convert_single`).
* `DocumentConverter.convert_all` now converts many files at once (previously `DocumentConverter.convert`).


```python
...
from docling.datamodel.document import ConversionResult
## Convert a single file (from URL or local path)
conv_result: ConversionResult = doc_converter.convert("https://arxiv.org/pdf/2408.09869") # previously `convert_single`

## Convert several files at once:

input_files = [
    "tests/data/html/wiki_duck.html",
    "tests/data/docx/word_sample.docx",
    "tests/data/docx/lorem_ipsum.docx",
    "tests/data/pptx/powerpoint_sample.pptx",
    "tests/data/2305.03393v1-pg9-img.png",
    "tests/data/pdf/2206.01062.pdf",
]

# Directly pass list of files or streams to `convert_all`
conv_results_iter = doc_converter.convert_all(input_files) # previously `convert`

```
Through the `raises_on_error` argument, you can also control if the conversion should raise exceptions when first
encountering a problem, or resiliently convert all files first and reflect errors in each file's conversion status.
By default, any error is immediately raised and the conversion aborts (previously, exceptions were swallowed).

```python
...
conv_results_iter = doc_converter.convert_all(input_files, raises_on_error=False) # previously `convert`

```

### Access document structures

We have simplified how you can access and export the converted document data, too. Our universal document representation
is now available in conversion results as a `DoclingDocument` object.
`DoclingDocument` provides a neat set of APIs to construct, iterate and export content in the document, as shown below.

```python
conv_result: ConversionResult = doc_converter.convert("https://arxiv.org/pdf/2408.09869") # previously `convert_single`

## Inspect the converted document:
conv_result.document.print_element_tree()

## Iterate the elements in reading order, including hierachy level:
for item, level in conv_result.document.iterate_items():
    if isinstance(item, TextItem):
        print(item.text)
    elif isinstance(item, TableItem):
        table_df: pd.DataFrame = item.export_to_dataframe()
        print(table_df.to_markdown())
    elif ...:
        #...
```

**Note**: While it is deprecated, you can _still_ work with the Docling v1 document representation, it is available as:
```shell
conv_result.legacy_document # provides the representation in previous ExportedCCSDocument type
```

### Export into JSON, Markdown, Doctags
**Note**: All `render_...` methods in `ConversionResult` have been removed in Docling v2,
and are now available on `DoclingDocument` as:

- `DoclingDocument.export_to_dict`
- `DoclingDocument.export_to_markdown`
- `DoclingDocument.export_to_document_tokens`

```python
conv_result: ConversionResult = doc_converter.convert("https://arxiv.org/pdf/2408.09869") # previously `convert_single`

## Export to desired format:
print(json.dumps(conv_res.document.export_to_dict()))
print(conv_res.document.export_to_markdown())
print(conv_res.document.export_to_document_tokens())
```

**Note**: While it is deprecated, you can _still_ export Docling v1 JSON format. This is available through the same
methods as on the `DoclingDocument` type:
```shell
## Export legacy document representation to desired format, for v1 compatibility:
print(json.dumps(conv_res.legacy_document.export_to_dict()))
print(conv_res.legacy_document.export_to_markdown())
print(conv_res.legacy_document.export_to_document_tokens())
```

### Reload a `DoclingDocument` stored as JSON

You can save and reload a `DoclingDocument` to disk in JSON format using the following codes:

```python
# Save to disk:
doc: DoclingDocument = conv_res.document # produced from conversion result...

with Path("./doc.json").open("w") as fp:
    fp.write(json.dumps(doc.export_to_dict())) # use `export_to_dict` to ensure consistency

# Load from disk:
with Path("./doc.json").open("r") as fp:
    doc_dict = json.loads(fp.read())
    doc = DoclingDocument.model_validate(doc_dict) # use standard pydantic API to populate doc

```

### Chunking

Docling v2 defines new base classes for chunking:

- `BaseMeta` for chunk metadata
- `BaseChunk` containing the chunk text and metadata, and
- `BaseChunker` for chunkers, producing chunks out of a `DoclingDocument`.

Additionally, it provides an updated `HierarchicalChunker` implementation, which
leverages the new `DoclingDocument` and provides a new, richer chunk output format, including:

- the respective doc items for grounding
- any applicable headings for context
- any applicable captions for context

For an example, check out [Chunking usage](usage.md#chunking).


================================================
File: docs/concepts/architecture.md
================================================
![docling_architecture](../assets/docling_arch.png)

In a nutshell, Docling's architecture is outlined in the diagram above.

For each document format, the *document converter* knows which format-specific *backend* to employ for parsing the document and which *pipeline* to use for orchestrating the execution, along with any relevant *options*.

!!! tip

    While the document converter holds a default mapping, this configuration is parametrizable, so e.g. for the PDF format, different backends and different pipeline options can be used — see [Usage](../usage.md#adjust-pipeline-features).

The *conversion result* contains the [*Docling document*](./docling_document.md), Docling's fundamental document representation.

Some typical scenarios for using a Docling document include directly calling its *export methods*, such as for markdown, dictionary etc., or having it chunked by a [*chunker*](./chunking.md).

For more details on Docling's architecture, check out the [Docling Technical Report](https://arxiv.org/abs/2408.09869).

!!! note

    The components illustrated with dashed outline indicate base classes that can be subclassed for specialized implementations.


================================================
File: docs/concepts/chunking.md
================================================
## Introduction

!!! note "Chunking approaches"

    Starting from a `DoclingDocument`, there are in principle two possible chunking
    approaches:

    1. exporting the `DoclingDocument` to Markdown (or similar format) and then
      performing user-defined chunking as a post-processing step, or
    2. using native Docling chunkers, i.e. operating directly on the `DoclingDocument`

    This page is about the latter, i.e. using native Docling chunkers.
    For an example of using approach (1) check out e.g.
    [this recipe](../examples/rag_langchain.ipynb) looking at the Markdown export mode.

A *chunker* is a Docling abstraction that, given a
[`DoclingDocument`](./docling_document.md), returns a stream of chunks, each of which
captures some part of the document as a string accompanied by respective metadata.

To enable both flexibility for downstream applications and out-of-the-box utility,
Docling defines a chunker class hierarchy, providing a base type, `BaseChunker`, as well
as specific subclasses.

Docling integration with gen AI frameworks like LlamaIndex is done using the
`BaseChunker` interface, so users can easily plug in any built-in, self-defined, or
third-party `BaseChunker` implementation.

## Base Chunker

The `BaseChunker` base class API defines that any chunker should provide the following:

- `def chunk(self, dl_doc: DoclingDocument, **kwargs) -> Iterator[BaseChunk]`:
  Returning the chunks for the provided document.
- `def serialize(self, chunk: BaseChunk) -> str`:
  Returning the potentially metadata-enriched serialization of the chunk, typically
  used to feed an embedding model (or generation model).

## Hybrid Chunker

!!! note "To access `HybridChunker`"

    - If you are using the `docling` package, you can import as follows:
        ```python
        from docling.chunking import HybridChunker
        ```
    - If you are only using the `docling-core` package, you must ensure to install
        the `chunking` extra, e.g.
        ```shell
        pip install 'docling-core[chunking]'
        ```
        and then you
        can import as follows:
        ```python
        from docling_core.transforms.chunker.hybrid_chunker import HybridChunker
        ```

The `HybridChunker` implementation uses a hybrid approach, applying tokenization-aware
refinements on top of document-based [hierarchical](#hierarchical-chunker) chunking.

More precisely:

- it starts from the result of the hierarchical chunker and, based on the user-provided
  tokenizer (typically to be aligned to the embedding model tokenizer), it:
- does one pass where it splits chunks only when needed (i.e. oversized w.r.t.
tokens), &
- another pass where it merges chunks only when possible (i.e. undersized successive
chunks with same headings & captions) — users can opt out of this step via param
`merge_peers` (by default `True`)

👉 Example: see  [here](../examples/hybrid_chunking.ipynb).

## Hierarchical Chunker

The `HierarchicalChunker` implementation uses the document structure information from
the [`DoclingDocument`](./docling_document.md) to create one chunk for each individual
detected document element, by default only merging together list items (can be opted out
via param `merge_list_items`). It also takes care of attaching all relevant document
metadata, including headers and captions.


================================================
File: docs/concepts/docling_document.md
================================================
With Docling v2, we introduce a unified document representation format called `DoclingDocument`. It is defined as a
pydantic datatype, which can express several features common to documents, such as:

* Text, Tables, Pictures, and more
* Document hierarchy with sections and groups
* Disambiguation between main body and headers, footers (furniture)
* Layout information (i.e. bounding boxes) for all items, if available
* Provenance information

The definition of the Pydantic types is implemented in the module `docling_core.types.doc`, more details in [source code definitions](https://github.com/DS4SD/docling-core/tree/main/docling_core/types/doc).

It also brings a set of document construction APIs to build up a `DoclingDocument` from scratch.

## Example document structures

To illustrate the features of the `DoclingDocument` format, in the subsections below we consider the
`DoclingDocument` converted from `tests/data/word_sample.docx` and we present some side-by-side comparisons,
where the left side shows snippets from the converted document
serialized as YAML and the right one shows the corresponding parts of the original MS Word.

### Basic structure

A `DoclingDocument` exposes top-level fields for the document content, organized in two categories.
The first category is the _content items_, which are stored in these fields:

- `texts`: All items that have a text representation (paragraph, section heading, equation, ...). Base class is `TextItem`.
- `tables`: All tables, type `TableItem`. Can carry structure annotations.
- `pictures`: All pictures, type `PictureItem`. Can carry structure annotations.
- `key_value_items`: All key-value items.

All of the above fields are lists and store items inheriting from the `DocItem` type. They can express different
data structures depending on their type, and reference parents and children through JSON pointers.

The second category is _content structure_, which is encapsualted in:

- `body`: The root node of a tree-structure for the main document body
- `furniture`: The root node of a tree-structure for all items that don't belong into the body (headers, footers, ...)
- `groups`: A set of items that don't represent content, but act as containers for other content items (e.g. a list, a chapter)

All of the above fields are only storing `NodeItem` instances, which reference children and parents
through JSON pointers.

The reading order of the document is encapsulated through the `body` tree and the order of _children_ in each item
in the tree.

Below example shows how all items in the first page are nested below the `title` item (`#/texts/1`).

![doc_hierarchy_1](../assets/docling_doc_hierarchy_1.png)

### Grouping

Below example shows how all items under the heading "Let's swim" (`#/texts/5`) are nested as chilrden. The children of
"Let's swim" are both text items and groups, which contain the list elements. The group items are stored in the
top-level `groups` field.

![doc_hierarchy_2](../assets/docling_doc_hierarchy_2.png)

<!--
### Tables

TBD

### Pictures

TBD

### Provenance

TBD
 -->


================================================
File: docs/concepts/index.md
================================================
Use the navigation on the left to browse through some core Docling concepts.


================================================
File: docs/examples/backend_csv.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Conversion of CSV files

This example shows how to convert CSV files to a structured Docling Document.

* Multiple delimiters are supported: `,` `;` `|` `[tab]`
* Additional CSV dialect settings are detected automatically (e.g. quotes, line separator, escape character)
"""

"""
## Example Code
"""

from pathlib import Path

from docling.document_converter import DocumentConverter

# Convert CSV to Docling document
converter = DocumentConverter()
result = converter.convert(Path("../../tests/data/csv/csv-comma.csv"))
output = result.document.export_to_markdown()

"""
This code generates the following output:
"""

"""
|   Index | Customer Id     | First Name   | Last Name   | Company                         | City              | Country                    | Phone 1                | Phone 2               | Email                       | Subscription Date   | Website                     |
|---------|-----------------|--------------|-------------|---------------------------------|-------------------|----------------------------|------------------------|-----------------------|-----------------------------|---------------------|-----------------------------|
|       1 | DD37Cf93aecA6Dc | Sheryl       | Baxter      | Rasmussen Group                 | East Leonard      | Chile                      | 229.077.5154           | 397.884.0519x718      | zunigavanessa@smith.info    | 2020-08-24          | http://www.stephenson.com/  |
|       2 | 1Ef7b82A4CAAD10 | Preston      | Lozano, Dr  | Vega-Gentry                     | East Jimmychester | Djibouti                   | 5153435776             | 686-620-1820x944      | vmata@colon.com             | 2021-04-23          | http://www.hobbs.com/       |
|       3 | 6F94879bDAfE5a6 | Roy          | Berry       | Murillo-Perry                   | Isabelborough     | Antigua and Barbuda        | +1-539-402-0259        | (496)978-3969x58947   | beckycarr@hogan.com         | 2020-03-25          | http://www.lawrence.com/    |
|       4 | 5Cef8BFA16c5e3c | Linda        | Olsen       | Dominguez, Mcmillan and Donovan | Bensonview        | Dominican Republic         | 001-808-617-6467x12895 | +1-813-324-8756       | stanleyblackwell@benson.org | 2020-06-02          | http://www.good-lyons.com/  |
|       5 | 053d585Ab6b3159 | Joanna       | Bender      | Martin, Lang and Andrade        | West Priscilla    | Slovakia (Slovak Republic) | 001-234-203-0635x76146 | 001-199-446-3860x3486 | colinalvarado@miles.net     | 2021-04-17          | https://goodwin-ingram.com/ |
"""


================================================
File: docs/examples/backend_xml_rag.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/backend_xml_rag.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
# Conversion of custom XML
"""

"""
| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | Hugging Face / Sentence Transformers | 💻 Local |
| Vector store | Milvus | 💻 Local |
| Gen AI | Hugging Face Inference API | 🌐 Remote | 
"""

"""
## Overview
"""

"""
This is an example of using [Docling](https://ds4sd.github.io/docling/) for converting structured data (XML) into a unified document
representation format, `DoclingDocument`, and leverage its riched structured content for RAG applications.

Data used in this example consist of patents from the [United States Patent and Trademark Office (USPTO)](https://www.uspto.gov/) and medical
articles from [PubMed Central® (PMC)](https://pmc.ncbi.nlm.nih.gov/).

In this notebook, we accomplish the following:
- [Simple conversion](#simple-conversion) of supported XML files in a nutshell
- An [end-to-end application](#end-to-end-application) using public collections of XML files supported by Docling
  - [Setup](#setup) the API access for generative AI
  - [Fetch the data](#fetch-the-data) from USPTO and PubMed Central® sites, using Docling custom backends
  - [Parse, chunk, and index](#parse-chunk-and-index) the documents in a vector database
  - [Perform RAG](#question-answering-with-rag) using [LlamaIndex Docling extension](../../integrations/llamaindex/)

For more details on document chunking with Docling, refer to the [Chunking](../../concepts/chunking/) documentation. For RAG with Docling and LlamaIndex, also check the example [RAG with LlamaIndex](../rag_llamaindex/).
"""

"""
## Simple conversion

The XML file format defines and stores data in a format that is both human-readable and machine-readable.
Because of this flexibility, Docling requires custom backend processors to interpret XML definitions and convert them into `DoclingDocument` objects.

Some public data collections in XML format are already supported by Docling (USTPO patents and PMC articles). In these cases, the document conversion is straightforward and the same as with any other supported format, such as PDF or HTML. The execution example in [Simple Conversion](../minimal/) is the recommended usage of Docling for a single file:
"""

from docling.document_converter import DocumentConverter

# a sample PMC article:
source = "../../tests/data/jats/elife-56337.nxml"
converter = DocumentConverter()
result = converter.convert(source)
print(result.status)
# Output:
#   ConversionStatus.SUCCESS


"""
Once the document is converted, it can be exported to any format supported by Docling. For instance, to markdown (showing here the first lines only):
"""

md_doc = result.document.export_to_markdown()

delim = "\n"
print(delim.join(md_doc.split(delim)[:8]))
# Output:
#   # KRAB-zinc finger protein gene expansion in response to active retrotransposons in the murine lineage

#   

#   Gernot Wolf, Alberto de Iaco, Ming-An Sun, Melania Bruno, Matthew Tinkham, Don Hoang, Apratim Mitra, Sherry Ralls, Didier Trono, Todd S Macfarlan

#   

#   The Eunice Kennedy Shriver National Institute of Child Health and Human Development, The National Institutes of Health, Bethesda, United States; School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland

#   

#   ## Abstract

#   


"""
If the XML file is not supported, a `ConversionError` message will be raised.
"""

from io import BytesIO

from docling.datamodel.base_models import DocumentStream
from docling.exceptions import ConversionError

xml_content = (
    b'<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE docling_test SYSTEM '
    b'"test.dtd"><docling>Random content</docling>'
)
stream = DocumentStream(name="docling_test.xml", stream=BytesIO(xml_content))
try:
    result = converter.convert(stream)
except ConversionError as ce:
    print(ce)
# Output:
#   Input document docling_test.xml does not match any allowed format.

#   File format not allowed: docling_test.xml


"""
You can always refer to the [Usage](../../usage/#supported-formats) documentation page for a list of supported formats.
"""

"""
## End-to-end application

This section describes a step-by-step application for processing XML files from supported public collections and use them for question-answering.
"""

"""
### Setup
"""

"""
Requirements can be installed as shown below. The `--no-warn-conflicts` argument is meant for Colab's pre-populated Python environment, feel free to remove for stricter usage.
"""

%pip install -q --progress-bar off --no-warn-conflicts llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-vector-stores-milvus llama-index-readers-file python-dotenv
# Output:
#   Note: you may need to restart the kernel to use updated packages.


"""
This notebook uses HuggingFace's Inference API. For an increased LLM quota, a token can be provided via the environment variable `HF_TOKEN`.

If you're running this notebook in Google Colab, make sure you [add](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) your API key as a secret.
"""

import os
from warnings import filterwarnings

from dotenv import load_dotenv


def _get_env_from_colab_or_os(key):
    try:
        from google.colab import userdata

        try:
            return userdata.get(key)
        except userdata.SecretNotFoundError:
            pass
    except ImportError:
        pass
    return os.getenv(key)


load_dotenv()

filterwarnings(action="ignore", category=UserWarning, module="pydantic")

"""
We can now define the main parameters:
"""

from pathlib import Path
from tempfile import mkdtemp

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

EMBED_MODEL_ID = "BAAI/bge-small-en-v1.5"
EMBED_MODEL = HuggingFaceEmbedding(model_name=EMBED_MODEL_ID)
TEMP_DIR = Path(mkdtemp())
MILVUS_URI = str(TEMP_DIR / "docling.db")
GEN_MODEL = HuggingFaceInferenceAPI(
    token=_get_env_from_colab_or_os("HF_TOKEN"),
    model_name="mistralai/Mixtral-8x7B-Instruct-v0.1",
)
embed_dim = len(EMBED_MODEL.get_text_embedding("hi"))
# https://github.com/huggingface/transformers/issues/5486:
os.environ["TOKENIZERS_PARALLELISM"] = "false"

"""
### Fetch the data
"""

"""
In this notebook we will use XML data from collections supported by Docling:
- Medical articles from the [PubMed Central® (PMC)](https://pmc.ncbi.nlm.nih.gov/). They are available in an [FTP server](https://ftp.ncbi.nlm.nih.gov/pub/pmc/) as `.tar.gz` files. Each file contains the full article data in XML format, among other supplementary files like images or spreadsheets.
- Patents from the [United States Patent and Trademark Office](https://www.uspto.gov/). They are available in the [Bulk Data Storage System (BDSS)](https://bulkdata.uspto.gov/) as zip files. Each zip file may contain several patents in XML format.

The raw files will be downloaded form the source and saved in a temporary directory.
"""

"""
#### PMC articles

The [OA file](https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv) is a manifest file of all the PMC articles, including the URL path to download the source files. In this notebook we will use as example the article [Pathogens spread by high-altitude windborne mosquitoes](https://pmc.ncbi.nlm.nih.gov/articles/PMC11703268/), which is available in the archive file [PMC11703268.tar.gz](https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/e3/6b/PMC11703268.tar.gz).
"""

import tarfile
from io import BytesIO

import requests

# PMC article PMC11703268
url: str = "https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/e3/6b/PMC11703268.tar.gz"

print(f"Downloading {url}...")
buf = BytesIO(requests.get(url).content)
print("Extracting and storing the XML file containing the article text...")
with tarfile.open(fileobj=buf, mode="r:gz") as tar_file:
    for tarinfo in tar_file:
        if tarinfo.isreg():
            file_path = Path(tarinfo.name)
            if file_path.suffix == ".nxml":
                with open(TEMP_DIR / file_path.name, "wb") as file_obj:
                    file_obj.write(tar_file.extractfile(tarinfo).read())
                print(f"Stored XML file {file_path.name}")
# Output:
#   Downloading https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/e3/6b/PMC11703268.tar.gz...

#   Extracting and storing the XML file containing the article text...

#   Stored XML file nihpp-2024.12.26.630351v1.nxml


"""
#### USPTO patents

Since each USPTO file is a concatenation of several patents, we need to split its content into valid XML pieces. The following code downloads a sample zip file, split its content in sections, and dumps each section as an XML file. For simplicity, this pipeline is shown here in a sequential manner, but it could be parallelized.
"""

import zipfile

# Patent grants from December 17-23, 2024
url: str = (
    "https://bulkdata.uspto.gov/data/patent/grant/redbook/fulltext/2024/ipg241217.zip"
)
XML_SPLITTER: str = '<?xml version="1.0"'
doc_num: int = 0

print(f"Downloading {url}...")
buf = BytesIO(requests.get(url).content)
print(f"Parsing zip file, splitting into XML sections, and exporting to files...")
with zipfile.ZipFile(buf) as zf:
    res = zf.testzip()
    if res:
        print("Error validating zip file")
    else:
        with zf.open(zf.namelist()[0]) as xf:
            is_patent = False
            patent_buffer = BytesIO()
            for xf_line in xf:
                decoded_line = xf_line.decode(errors="ignore").rstrip()
                xml_index = decoded_line.find(XML_SPLITTER)
                if xml_index != -1:
                    if (
                        xml_index > 0
                    ):  # cases like </sequence-cwu><?xml version="1.0"...
                        patent_buffer.write(xf_line[:xml_index])
                        patent_buffer.write(b"\r\n")
                        xf_line = xf_line[xml_index:]
                    if patent_buffer.getbuffer().nbytes > 0 and is_patent:
                        doc_num += 1
                        patent_id = f"ipg241217-{doc_num}"
                        with open(TEMP_DIR / f"{patent_id}.xml", "wb") as file_obj:
                            file_obj.write(patent_buffer.getbuffer())
                    is_patent = False
                    patent_buffer = BytesIO()
                elif decoded_line.startswith("<!DOCTYPE"):
                    is_patent = True
                patent_buffer.write(xf_line)
# Output:
#   Downloading https://bulkdata.uspto.gov/data/patent/grant/redbook/fulltext/2024/ipg241217.zip...

#   Parsing zip file, splitting into XML sections, and exporting to files...


print(f"Fetched and exported {doc_num} documents.")
# Output:
#   Fetched and exported 4014 documents.


"""
### Using the backend converter (optional)

- The custom backend converters `PubMedDocumentBackend` and `PatentUsptoDocumentBackend` aim at handling the parsing of PMC articles and USPTO patents, respectively.
- As any other backends, you can leverage the function `is_valid()` to check if the input document is supported by the this backend.
- Note that some XML sections in the original USPTO zip file may not represent patents, like sequence listings, and therefore they will show as invalid by the backend.
"""

from tqdm.notebook import tqdm

from docling.backend.xml.jats_backend import JatsDocumentBackend
from docling.backend.xml.uspto_backend import PatentUsptoDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

# check PMC
in_doc = InputDocument(
    path_or_stream=TEMP_DIR / "nihpp-2024.12.26.630351v1.nxml",
    format=InputFormat.XML_JATS,
    backend=JatsDocumentBackend,
)
backend = JatsDocumentBackend(
    in_doc=in_doc, path_or_stream=TEMP_DIR / "nihpp-2024.12.26.630351v1.nxml"
)
print(f"Document {in_doc.file.name} is a valid PMC article? {backend.is_valid()}")

# check USPTO
in_doc = InputDocument(
    path_or_stream=TEMP_DIR / "ipg241217-1.xml",
    format=InputFormat.XML_USPTO,
    backend=PatentUsptoDocumentBackend,
)
backend = PatentUsptoDocumentBackend(
    in_doc=in_doc, path_or_stream=TEMP_DIR / "ipg241217-1.xml"
)
print(f"Document {in_doc.file.name} is a valid patent? {backend.is_valid()}")

patent_valid = 0
pbar = tqdm(TEMP_DIR.glob("*.xml"), total=doc_num)
for in_path in pbar:
    in_doc = InputDocument(
        path_or_stream=in_path,
        format=InputFormat.XML_USPTO,
        backend=PatentUsptoDocumentBackend,
    )
    backend = PatentUsptoDocumentBackend(in_doc=in_doc, path_or_stream=in_path)
    patent_valid += int(backend.is_valid())

print(f"Found {patent_valid} patents out of {doc_num} XML files.")
# Output:
#   Document nihpp-2024.12.26.630351v1.nxml is a valid PMC article? True

#   Document ipg241217-1.xml is a valid patent? True

#     0%|          | 0/4014 [00:00<?, ?it/s]
#   Found 3928 patents out of 4014 XML files.


"""
Calling the function `convert()` will convert the input document into a `DoclingDocument`
"""

doc = backend.convert()

claims_sec = [item for item in doc.texts if item.text == "CLAIMS"][0]
print(f'Patent "{doc.texts[0].text}" has {len(claims_sec.children)} claims')
# Output:
#   Patent "Semiconductor package" has 19 claims


"""
✏️ **Tip**: in general, there is no need to use the backend converters to parse USPTO or JATS (PubMed) XML files. The generic `DocumentConverter` object tries to guess the input document format and applies the corresponding backend parser. The conversion shown in [Simple Conversion](#simple-conversion) is the recommended usage for the supported XML files.
"""

"""
### Parse, chunk, and index
"""

"""
The `DoclingDocument` format of the converted patents has a rich hierarchical structure, inherited from the original XML document and preserved by the Docling custom backend.
In this notebook, we will leverage:
- The `SimpleDirectoryReader` pattern to iterate over the exported XML files created in section [Fetch the data](#fetch-the-data).
- The LlamaIndex extensions, `DoclingReader` and `DoclingNodeParser`, to ingest the patent chunks into a Milvus vectore store.
- The `HierarchicalChunker` implementation, which applies a document-based hierarchical chunking, to leverage the patent structures like sections and paragraphs within sections.

Refer to other possible implementations and usage patterns in the [Chunking](../../concepts/chunking/) documentation and the [RAG with LlamaIndex](../rag_llamaindex/) notebook.
"""

"""
##### Set the Docling reader and the directory reader

Note that `DoclingReader` uses Docling's `DocumentConverter` by default and therefore it will recognize the format of the XML files and leverage the `PatentUsptoDocumentBackend` automatically.

For demonstration purposes, we limit the scope of the analysis to the first 100 patents.
"""

from llama_index.core import SimpleDirectoryReader
from llama_index.readers.docling import DoclingReader

reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)
dir_reader = SimpleDirectoryReader(
    input_dir=TEMP_DIR,
    exclude=["docling.db", "*.nxml"],
    file_extractor={".xml": reader},
    filename_as_id=True,
    num_files_limit=100,
)

"""
##### Set the node parser

Note that the `HierarchicalChunker` is the default chunking implementation of the `DoclingNodeParser`.
"""

from llama_index.node_parser.docling import DoclingNodeParser

node_parser = DoclingNodeParser()

"""
##### Set a local Milvus database and run the ingestion
"""

from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.vector_stores.milvus import MilvusVectorStore

vector_store = MilvusVectorStore(
    uri=MILVUS_URI,
    dim=embed_dim,
    overwrite=True,
)

index = VectorStoreIndex.from_documents(
    documents=dir_reader.load_data(show_progress=True),
    transformations=[node_parser],
    storage_context=StorageContext.from_defaults(vector_store=vector_store),
    embed_model=EMBED_MODEL,
    show_progress=True,
)

"""
Finally, add the PMC article to the vector store directly from the reader.
"""

index.from_documents(
    documents=reader.load_data(TEMP_DIR / "nihpp-2024.12.26.630351v1.nxml"),
    transformations=[node_parser],
    storage_context=StorageContext.from_defaults(vector_store=vector_store),
    embed_model=EMBED_MODEL,
)
# Output:
#   <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x373a7f7d0>

"""
### Question-answering with RAG
"""

"""
The retriever can be used to identify highly relevant documents:
"""

retriever = index.as_retriever(similarity_top_k=3)
results = retriever.retrieve("What patents are related to fitness devices?")

for item in results:
    print(item)
# Output:
#   Node ID: 5afd36c0-a739-4a88-a51c-6d0f75358db5

#   Text: The portable fitness monitoring device 102 may be a device such

#   as, for example, a mobile phone, a personal digital assistant, a music

#   file player (e.g. and MP3 player), an intelligent article for wearing

#   (e.g. a fitness monitoring garment, wrist band, or watch), a dongle

#   (e.g. a small hardware device that protects software) that includes a

#   fitn...

#   Score:  0.772

#   

#   Node ID: f294b5fd-9089-43cb-8c4e-d1095a634ff1

#   Text: US Patent Application US 20120071306 entitled “Portable

#   Multipurpose Whole Body Exercise Device” discloses a portable

#   multipurpose whole body exercise device which can be used for general

#   fitness, Pilates-type, core strengthening, therapeutic, and

#   rehabilitative exercises as well as stretching and physical therapy

#   and which includes storable acc...

#   Score:  0.749

#   

#   Node ID: 8251c7ef-1165-42e1-8c91-c99c8a711bf7

#   Text: Program products, methods, and systems for providing fitness

#   monitoring services of the present invention can include any software

#   application executed by one or more computing devices. A computing

#   device can be any type of computing device having one or more

#   processors. For example, a computing device can be a workstation,

#   mobile device (e.g., ...

#   Score:  0.744

#   


"""
With the query engine, we can run the question-answering with the RAG pattern on the set of indexed documents.

First, we can prompt the LLM directly:
"""

from llama_index.core.base.llms.types import ChatMessage, MessageRole
from rich.console import Console
from rich.panel import Panel

console = Console()
query = "Do mosquitoes in high altitude expand viruses over large distances?"

usr_msg = ChatMessage(role=MessageRole.USER, content=query)
response = GEN_MODEL.chat(messages=[usr_msg])

console.print(Panel(query, title="Prompt", border_style="bold red"))
console.print(
    Panel(
        response.message.content.strip(),
        title="Generated Content",
        border_style="bold green",
    )
)
# Output:
#   [1;31m╭─[0m[1;31m───────────────────────────────────────────────────[0m[1;31m Prompt [0m[1;31m────────────────────────────────────────────────────[0m[1;31m─╮[0m

#   [1;31m│[0m Do mosquitoes in high altitude expand viruses over large distances?                                             [1;31m│[0m

#   [1;31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m

#   [1;32m╭─[0m[1;32m──────────────────────────────────────────────[0m[1;32m Generated Content [0m[1;32m──────────────────────────────────────────────[0m[1;32m─╮[0m

#   [1;32m│[0m Mosquitoes can be found at high altitudes, but their ability to transmit viruses over long distances is not     [1;32m│[0m

#   [1;32m│[0m primarily dependent on altitude. Mosquitoes are vectors for various diseases, such as malaria, dengue fever,    [1;32m│[0m

#   [1;32m│[0m and Zika virus, and their transmission range is more closely related to their movement, the presence of a host, [1;32m│[0m

#   [1;32m│[0m and environmental conditions that support their survival and reproduction.                                      [1;32m│[0m

#   [1;32m│[0m                                                                                                                 [1;32m│[0m

#   [1;32m│[0m At high altitudes, the environment can be less suitable for mosquitoes due to factors such as colder            [1;32m│[0m

#   [1;32m│[0m temperatures, lower humidity, and stronger winds, which can limit their population size and distribution.       [1;32m│[0m

#   [1;32m│[0m However, some species of mosquitoes have adapted to high-altitude environments and can still transmit diseases  [1;32m│[0m

#   [1;32m│[0m in these areas.                                                                                                 [1;32m│[0m

#   [1;32m│[0m                                                                                                                 [1;32m│[0m

#   [1;32m│[0m It is possible for mosquitoes to be transported by wind or human activities to higher altitudes, but this is    [1;32m│[0m

#   [1;32m│[0m not a significant factor in their ability to transmit viruses over long distances. Instead, long-distance       [1;32m│[0m

#   [1;32m│[0m transmission of viruses is more often associated with human travel and transportation, which can rapidly spread [1;32m│[0m

#   [1;32m│[0m infected mosquitoes or humans to new areas, leading to the spread of disease.                                   [1;32m│[0m

#   [1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m


"""
Now, we can compare the response when the model is prompted with the indexed PMC article as supporting context:
"""

from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters

filters = MetadataFilters(
    filters=[
        ExactMatchFilter(key="filename", value="nihpp-2024.12.26.630351v1.nxml"),
    ]
)

query_engine = index.as_query_engine(llm=GEN_MODEL, filter=filters, similarity_top_k=3)
result = query_engine.query(query)

console.print(
    Panel(
        result.response.strip(),
        title="Generated Content with RAG",
        border_style="bold green",
    )
)
# Output:
#   [1;32m╭─[0m[1;32m─────────────────────────────────────────[0m[1;32m Generated Content with RAG [0m[1;32m──────────────────────────────────────────[0m[1;32m─╮[0m

#   [1;32m│[0m Yes, mosquitoes in high altitude can expand viruses over large distances. A study intercepted 1,017 female      [1;32m│[0m

#   [1;32m│[0m mosquitoes at altitudes of 120-290 m above ground over Mali and Ghana and screened them for infection with      [1;32m│[0m

#   [1;32m│[0m arboviruses, plasmodia, and filariae. The study found that 3.5% of the mosquitoes were infected with            [1;32m│[0m

#   [1;32m│[0m flaviviruses, and 1.1% were infectious. Additionally, the study identified 19 mosquito-borne pathogens,         [1;32m│[0m

#   [1;32m│[0m including three arboviruses that affect humans (dengue, West Nile, and M’Poko viruses). The study provides      [1;32m│[0m

#   [1;32m│[0m compelling evidence that mosquito-borne pathogens are often spread by windborne mosquitoes at altitude.         [1;32m│[0m

#   [1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m



================================================
File: docs/examples/batch_convert.py
================================================
import json
import logging
import time
from pathlib import Path
from typing import Iterable

import yaml
from docling_core.types.doc import ImageRefMode

from docling.datamodel.base_models import ConversionStatus, InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.settings import settings
from docling.document_converter import DocumentConverter, PdfFormatOption

_log = logging.getLogger(__name__)

USE_V2 = True
USE_LEGACY = False


def export_documents(
    conv_results: Iterable[ConversionResult],
    output_dir: Path,
):
    output_dir.mkdir(parents=True, exist_ok=True)

    success_count = 0
    failure_count = 0
    partial_success_count = 0

    for conv_res in conv_results:
        if conv_res.status == ConversionStatus.SUCCESS:
            success_count += 1
            doc_filename = conv_res.input.file.stem

            if USE_V2:
                conv_res.document.save_as_json(
                    output_dir / f"{doc_filename}.json",
                    image_mode=ImageRefMode.PLACEHOLDER,
                )
                conv_res.document.save_as_html(
                    output_dir / f"{doc_filename}.html",
                    image_mode=ImageRefMode.EMBEDDED,
                )
                conv_res.document.save_as_document_tokens(
                    output_dir / f"{doc_filename}.doctags.txt"
                )
                conv_res.document.save_as_markdown(
                    output_dir / f"{doc_filename}.md",
                    image_mode=ImageRefMode.PLACEHOLDER,
                )
                conv_res.document.save_as_markdown(
                    output_dir / f"{doc_filename}.txt",
                    image_mode=ImageRefMode.PLACEHOLDER,
                    strict_text=True,
                )

                # Export Docling document format to YAML:
                with (output_dir / f"{doc_filename}.yaml").open("w") as fp:
                    fp.write(yaml.safe_dump(conv_res.document.export_to_dict()))

            if USE_LEGACY:
                # Export Deep Search document JSON format:
                with (output_dir / f"{doc_filename}.legacy.json").open(
                    "w", encoding="utf-8"
                ) as fp:
                    fp.write(json.dumps(conv_res.legacy_document.export_to_dict()))

                # Export Text format:
                with (output_dir / f"{doc_filename}.legacy.txt").open(
                    "w", encoding="utf-8"
                ) as fp:
                    fp.write(
                        conv_res.legacy_document.export_to_markdown(strict_text=True)
                    )

                # Export Markdown format:
                with (output_dir / f"{doc_filename}.legacy.md").open(
                    "w", encoding="utf-8"
                ) as fp:
                    fp.write(conv_res.legacy_document.export_to_markdown())

                # Export Document Tags format:
                with (output_dir / f"{doc_filename}.legacy.doctags.txt").open(
                    "w", encoding="utf-8"
                ) as fp:
                    fp.write(conv_res.legacy_document.export_to_document_tokens())

        elif conv_res.status == ConversionStatus.PARTIAL_SUCCESS:
            _log.info(
                f"Document {conv_res.input.file} was partially converted with the following errors:"
            )
            for item in conv_res.errors:
                _log.info(f"\t{item.error_message}")
            partial_success_count += 1
        else:
            _log.info(f"Document {conv_res.input.file} failed to convert.")
            failure_count += 1

    _log.info(
        f"Processed {success_count + partial_success_count + failure_count} docs, "
        f"of which {failure_count} failed "
        f"and {partial_success_count} were partially converted."
    )
    return success_count, partial_success_count, failure_count


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_paths = [
        Path("./tests/data/pdf/2206.01062.pdf"),
        Path("./tests/data/pdf/2203.01017v2.pdf"),
        Path("./tests/data/pdf/2305.03393v1.pdf"),
        Path("./tests/data/pdf/redp5110_sampled.pdf"),
    ]

    # buf = BytesIO(Path("./test/data/2206.01062.pdf").open("rb").read())
    # docs = [DocumentStream(name="my_doc.pdf", stream=buf)]
    # input = DocumentConversionInput.from_streams(docs)

    # # Turn on inline debug visualizations:
    # settings.debug.visualize_layout = True
    # settings.debug.visualize_ocr = True
    # settings.debug.visualize_tables = True
    # settings.debug.visualize_cells = True

    pipeline_options = PdfPipelineOptions()
    pipeline_options.generate_page_images = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    start_time = time.time()

    conv_results = doc_converter.convert_all(
        input_doc_paths,
        raises_on_error=True,  # to let conversion run through all and examine results at the end
    )
    success_count, partial_success_count, failure_count = export_documents(
        conv_results, output_dir=Path("scratch")
    )

    end_time = time.time() - start_time

    _log.info(f"Document conversion complete in {end_time:.2f} seconds.")

    if failure_count > 0:
        raise RuntimeError(
            f"The example failed converting {failure_count} on {len(input_doc_paths)}."
        )


if __name__ == "__main__":
    main()


================================================
File: docs/examples/custom_convert.py
================================================
import json
import logging
import time
from pathlib import Path

from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    PdfPipelineOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.models.ocr_mac_model import OcrMacOptions
from docling.models.tesseract_ocr_cli_model import TesseractCliOcrOptions
from docling.models.tesseract_ocr_model import TesseractOcrOptions

_log = logging.getLogger(__name__)


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")

    ###########################################################################

    # The following sections contain a combination of PipelineOptions
    # and PDF Backends for various configurations.
    # Uncomment one section at the time to see the differences in the output.

    # PyPdfium without EasyOCR
    # --------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = False
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = False

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(
    #             pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend
    #         )
    #     }
    # )

    # PyPdfium with EasyOCR
    # -----------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(
    #             pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend
    #         )
    #     }
    # )

    # Docling Parse without EasyOCR
    # -------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = False
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with EasyOCR
    # ----------------------
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = True
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True
    pipeline_options.ocr_options.lang = ["es"]
    pipeline_options.accelerator_options = AcceleratorOptions(
        num_threads=4, device=AcceleratorDevice.AUTO
    )

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    # Docling Parse with EasyOCR (CPU only)
    # ----------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.ocr_options.use_gpu = False  # <-- set this.
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with Tesseract
    # ----------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = TesseractOcrOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with Tesseract CLI
    # ----------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = TesseractCliOcrOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with ocrmac(Mac only)
    # ----------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = OcrMacOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    ###########################################################################

    start_time = time.time()
    conv_result = doc_converter.convert(input_doc_path)
    end_time = time.time() - start_time

    _log.info(f"Document converted in {end_time:.2f} seconds.")

    ## Export results
    output_dir = Path("scratch")
    output_dir.mkdir(parents=True, exist_ok=True)
    doc_filename = conv_result.input.file.stem

    # Export Deep Search document JSON format:
    with (output_dir / f"{doc_filename}.json").open("w", encoding="utf-8") as fp:
        fp.write(json.dumps(conv_result.document.export_to_dict()))

    # Export Text format:
    with (output_dir / f"{doc_filename}.txt").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_text())

    # Export Markdown format:
    with (output_dir / f"{doc_filename}.md").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_markdown())

    # Export Document Tags format:
    with (output_dir / f"{doc_filename}.doctags").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_document_tokens())


if __name__ == "__main__":
    main()


================================================
File: docs/examples/develop_formula_understanding.py
================================================
import logging
from pathlib import Path
from typing import Iterable

from docling_core.types.doc import DocItemLabel, DoclingDocument, NodeItem, TextItem

from docling.datamodel.base_models import InputFormat, ItemAndImageEnrichmentElement
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.models.base_model import BaseItemAndImageEnrichmentModel
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline


class ExampleFormulaUnderstandingPipelineOptions(PdfPipelineOptions):
    do_formula_understanding: bool = True


# A new enrichment model using both the document element and its image as input
class ExampleFormulaUnderstandingEnrichmentModel(BaseItemAndImageEnrichmentModel):
    images_scale = 2.6

    def __init__(self, enabled: bool):
        self.enabled = enabled

    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        return (
            self.enabled
            and isinstance(element, TextItem)
            and element.label == DocItemLabel.FORMULA
        )

    def __call__(
        self,
        doc: DoclingDocument,
        element_batch: Iterable[ItemAndImageEnrichmentElement],
    ) -> Iterable[NodeItem]:
        if not self.enabled:
            return

        for enrich_element in element_batch:
            enrich_element.image.show()

            yield enrich_element.item


# How the pipeline can be extended.
class ExampleFormulaUnderstandingPipeline(StandardPdfPipeline):

    def __init__(self, pipeline_options: ExampleFormulaUnderstandingPipelineOptions):
        super().__init__(pipeline_options)
        self.pipeline_options: ExampleFormulaUnderstandingPipelineOptions

        self.enrichment_pipe = [
            ExampleFormulaUnderstandingEnrichmentModel(
                enabled=self.pipeline_options.do_formula_understanding
            )
        ]

        if self.pipeline_options.do_formula_understanding:
            self.keep_backend = True

    @classmethod
    def get_default_options(cls) -> ExampleFormulaUnderstandingPipelineOptions:
        return ExampleFormulaUnderstandingPipelineOptions()


# Example main. In the final version, we simply have to set do_formula_understanding to true.
def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2203.01017v2.pdf")

    pipeline_options = ExampleFormulaUnderstandingPipelineOptions()
    pipeline_options.do_formula_understanding = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_cls=ExampleFormulaUnderstandingPipeline,
                pipeline_options=pipeline_options,
            )
        }
    )
    result = doc_converter.convert(input_doc_path)


if __name__ == "__main__":
    main()


================================================
File: docs/examples/develop_picture_enrichment.py
================================================
import logging
from pathlib import Path
from typing import Any, Iterable

from docling_core.types.doc import (
    DoclingDocument,
    NodeItem,
    PictureClassificationClass,
    PictureClassificationData,
    PictureItem,
)

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.models.base_model import BaseEnrichmentModel
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline


class ExamplePictureClassifierPipelineOptions(PdfPipelineOptions):
    do_picture_classifer: bool = True


class ExamplePictureClassifierEnrichmentModel(BaseEnrichmentModel):
    def __init__(self, enabled: bool):
        self.enabled = enabled

    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:
        return self.enabled and isinstance(element, PictureItem)

    def __call__(
        self, doc: DoclingDocument, element_batch: Iterable[NodeItem]
    ) -> Iterable[Any]:
        if not self.enabled:
            return

        for element in element_batch:
            assert isinstance(element, PictureItem)

            # uncomment this to interactively visualize the image
            # element.get_image(doc).show()

            element.annotations.append(
                PictureClassificationData(
                    provenance="example_classifier-0.0.1",
                    predicted_classes=[
                        PictureClassificationClass(class_name="dummy", confidence=0.42)
                    ],
                )
            )

            yield element


class ExamplePictureClassifierPipeline(StandardPdfPipeline):
    def __init__(self, pipeline_options: ExamplePictureClassifierPipelineOptions):
        super().__init__(pipeline_options)
        self.pipeline_options: ExamplePictureClassifierPipeline

        self.enrichment_pipe = [
            ExamplePictureClassifierEnrichmentModel(
                enabled=pipeline_options.do_picture_classifer
            )
        ]

    @classmethod
    def get_default_options(cls) -> ExamplePictureClassifierPipelineOptions:
        return ExamplePictureClassifierPipelineOptions()


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")

    pipeline_options = ExamplePictureClassifierPipelineOptions()
    pipeline_options.images_scale = 2.0
    pipeline_options.generate_picture_images = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_cls=ExamplePictureClassifierPipeline,
                pipeline_options=pipeline_options,
            )
        }
    )
    result = doc_converter.convert(input_doc_path)

    for element, _level in result.document.iterate_items():
        if isinstance(element, PictureItem):
            print(
                f"The model populated the `data` portion of picture {element.self_ref}:\n{element.annotations}"
            )


if __name__ == "__main__":
    main()


================================================
File: docs/examples/export_figures.py
================================================
import logging
import time
from pathlib import Path

from docling_core.types.doc import ImageRefMode, PictureItem, TableItem

from docling.datamodel.base_models import FigureElement, InputFormat, Table
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

_log = logging.getLogger(__name__)

IMAGE_RESOLUTION_SCALE = 2.0


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    output_dir = Path("scratch")

    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter
    # will destroy them for cleaning up memory.
    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.
    # scale=1 correspond of a standard 72 DPI image
    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched
    # with the image field
    pipeline_options = PdfPipelineOptions()
    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
    pipeline_options.generate_page_images = True
    pipeline_options.generate_picture_images = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    start_time = time.time()

    conv_res = doc_converter.convert(input_doc_path)

    output_dir.mkdir(parents=True, exist_ok=True)
    doc_filename = conv_res.input.file.stem

    # Save page images
    for page_no, page in conv_res.document.pages.items():
        page_no = page.page_no
        page_image_filename = output_dir / f"{doc_filename}-{page_no}.png"
        with page_image_filename.open("wb") as fp:
            page.image.pil_image.save(fp, format="PNG")

    # Save images of figures and tables
    table_counter = 0
    picture_counter = 0
    for element, _level in conv_res.document.iterate_items():
        if isinstance(element, TableItem):
            table_counter += 1
            element_image_filename = (
                output_dir / f"{doc_filename}-table-{table_counter}.png"
            )
            with element_image_filename.open("wb") as fp:
                element.get_image(conv_res.document).save(fp, "PNG")

        if isinstance(element, PictureItem):
            picture_counter += 1
            element_image_filename = (
                output_dir / f"{doc_filename}-picture-{picture_counter}.png"
            )
            with element_image_filename.open("wb") as fp:
                element.get_image(conv_res.document).save(fp, "PNG")

    # Save markdown with embedded pictures
    md_filename = output_dir / f"{doc_filename}-with-images.md"
    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)

    # Save markdown with externally referenced pictures
    md_filename = output_dir / f"{doc_filename}-with-image-refs.md"
    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)

    # Save HTML with externally referenced pictures
    html_filename = output_dir / f"{doc_filename}-with-image-refs.html"
    conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)

    end_time = time.time() - start_time

    _log.info(f"Document converted and figures exported in {end_time:.2f} seconds.")


if __name__ == "__main__":
    main()


================================================
File: docs/examples/export_multimodal.py
================================================
import datetime
import logging
import time
from pathlib import Path

import pandas as pd

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.utils.export import generate_multimodal_pages
from docling.utils.utils import create_hash

_log = logging.getLogger(__name__)

IMAGE_RESOLUTION_SCALE = 2.0


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    output_dir = Path("scratch")

    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter
    # will destroy them for cleaning up memory.
    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.
    # scale=1 correspond of a standard 72 DPI image
    pipeline_options = PdfPipelineOptions()
    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
    pipeline_options.generate_page_images = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    start_time = time.time()

    conv_res = doc_converter.convert(input_doc_path)

    output_dir.mkdir(parents=True, exist_ok=True)

    rows = []
    for (
        content_text,
        content_md,
        content_dt,
        page_cells,
        page_segments,
        page,
    ) in generate_multimodal_pages(conv_res):

        dpi = page._default_image_scale * 72

        rows.append(
            {
                "document": conv_res.input.file.name,
                "hash": conv_res.input.document_hash,
                "page_hash": create_hash(
                    conv_res.input.document_hash + ":" + str(page.page_no - 1)
                ),
                "image": {
                    "width": page.image.width,
                    "height": page.image.height,
                    "bytes": page.image.tobytes(),
                },
                "cells": page_cells,
                "contents": content_text,
                "contents_md": content_md,
                "contents_dt": content_dt,
                "segments": page_segments,
                "extra": {
                    "page_num": page.page_no + 1,
                    "width_in_points": page.size.width,
                    "height_in_points": page.size.height,
                    "dpi": dpi,
                },
            }
        )

    # Generate one parquet from all documents
    df = pd.json_normalize(rows)
    now = datetime.datetime.now()
    output_filename = output_dir / f"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet"
    df.to_parquet(output_filename)

    end_time = time.time() - start_time

    _log.info(
        f"Document converted and multimodal pages generated in {end_time:.2f} seconds."
    )

    # This block demonstrates how the file can be opened with the HF datasets library
    # from datasets import Dataset
    # from PIL import Image
    # multimodal_df = pd.read_parquet(output_filename)

    # # Convert pandas DataFrame to Hugging Face Dataset and load bytes into image
    # dataset = Dataset.from_pandas(multimodal_df)
    # def transforms(examples):
    #     examples["image"] = Image.frombytes('RGB', (examples["image.width"], examples["image.height"]), examples["image.bytes"], 'raw')
    #     return examples
    # dataset = dataset.map(transforms)


if __name__ == "__main__":
    main()


================================================
File: docs/examples/export_tables.py
================================================
import logging
import time
from pathlib import Path

import pandas as pd

from docling.document_converter import DocumentConverter

_log = logging.getLogger(__name__)


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    output_dir = Path("scratch")

    doc_converter = DocumentConverter()

    start_time = time.time()

    conv_res = doc_converter.convert(input_doc_path)

    output_dir.mkdir(parents=True, exist_ok=True)

    doc_filename = conv_res.input.file.stem

    # Export tables
    for table_ix, table in enumerate(conv_res.document.tables):
        table_df: pd.DataFrame = table.export_to_dataframe()
        print(f"## Table {table_ix}")
        print(table_df.to_markdown())

        # Save the table as csv
        element_csv_filename = output_dir / f"{doc_filename}-table-{table_ix+1}.csv"
        _log.info(f"Saving CSV table to {element_csv_filename}")
        table_df.to_csv(element_csv_filename)

        # Save the table as html
        element_html_filename = output_dir / f"{doc_filename}-table-{table_ix+1}.html"
        _log.info(f"Saving HTML table to {element_html_filename}")
        with element_html_filename.open("w") as fp:
            fp.write(table.export_to_html())

    end_time = time.time() - start_time

    _log.info(f"Document converted and tables exported in {end_time:.2f} seconds.")


if __name__ == "__main__":
    main()


================================================
File: docs/examples/full_page_ocr.py
================================================
from pathlib import Path

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    EasyOcrOptions,
    OcrMacOptions,
    PdfPipelineOptions,
    RapidOcrOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption


def main():
    input_doc = Path("./tests/data/pdf/2206.01062.pdf")

    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = True
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True

    # Any of the OCR options can be used:EasyOcrOptions, TesseractOcrOptions, TesseractCliOcrOptions, OcrMacOptions(Mac only), RapidOcrOptions
    # ocr_options = EasyOcrOptions(force_full_page_ocr=True)
    # ocr_options = TesseractOcrOptions(force_full_page_ocr=True)
    # ocr_options = OcrMacOptions(force_full_page_ocr=True)
    # ocr_options = RapidOcrOptions(force_full_page_ocr=True)
    ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True)
    pipeline_options.ocr_options = ocr_options

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            )
        }
    )

    doc = converter.convert(input_doc).document
    md = doc.export_to_markdown()
    print(md)


if __name__ == "__main__":
    main()


================================================
File: docs/examples/hybrid_chunking.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Hybrid chunking
"""

"""
## Overview
"""

"""
Hybrid chunking applies tokenization-aware refinements on top of document-based hierarchical chunking.

For more details, see [here](../../concepts/chunking#hybrid-chunker).
"""

"""
## Setup
"""

%pip install -qU docling transformers
# Output:
#   Note: you may need to restart the kernel to use updated packages.


"""
## Conversion
"""

from docling.document_converter import DocumentConverter

DOC_SOURCE = "../../tests/data/md/wiki.md"

doc = DocumentConverter().convert(source=DOC_SOURCE).document

"""
## Chunking

### Basic usage

For a basic usage scenario, we can just instantiate a `HybridChunker`, which will use
the default parameters.
"""

from docling.chunking import HybridChunker

chunker = HybridChunker()
chunk_iter = chunker.chunk(dl_doc=doc)

"""
Note that the text you would typically want to embed is the context-enriched one as
returned by the `serialize()` method:
"""

for i, chunk in enumerate(chunk_iter):
    print(f"=== {i} ===")
    print(f"chunk.text:\n{repr(f'{chunk.text[:300]}…')}")

    enriched_text = chunker.serialize(chunk=chunk)
    print(f"chunker.serialize(chunk):\n{repr(f'{enriched_text[:300]}…')}")

    print()
# Output:
#   === 0 ===

#   chunk.text:

#   'International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Aver…'

#   chunker.serialize(chunk):

#   'IBM\nInternational Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial …'

#   

#   === 1 ===

#   chunk.text:

#   'IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889);[19] and Willa…'

#   chunker.serialize(chunk):

#   'IBM\n1910s–1950s\nIBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889…'

#   

#   === 2 ===

#   chunk.text:

#   'Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson,…'

#   chunker.serialize(chunk):

#   'IBM\n1910s–1950s\nCollectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John …'

#   

#   === 3 ===

#   chunk.text:

#   'In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.…'

#   chunker.serialize(chunk):

#   'IBM\n1960s–1980s\nIn 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.…'

#   


"""
### Advanced usage

For more control on the chunking, we can parametrize through the `HybridChunker`
arguments illustrated below.

Notice how `tokenizer` and `embed_model` further below are single-sourced from
`EMBED_MODEL_ID`.
This is important for making sure the chunker and the embedding model are using the same
tokenizer.
"""

from transformers import AutoTokenizer

from docling.chunking import HybridChunker

EMBED_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
MAX_TOKENS = 64  # set to a small number for illustrative purposes

tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)

chunker = HybridChunker(
    tokenizer=tokenizer,  # instance or model name, defaults to "sentence-transformers/all-MiniLM-L6-v2"
    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer`
    merge_peers=True,  # optional, defaults to True
)
chunk_iter = chunker.chunk(dl_doc=doc)
chunks = list(chunk_iter)

"""
Points to notice looking at the output chunks below:
- Where possible, we fit the limit of 64 tokens for the metadata-enriched serialization form (see chunk 2)
- Where neeeded, we stop before the limit, e.g. see cases of 63 as it would otherwise run into a comma (see chunk 6)
- Where possible, we merge undersized peer chunks (see chunk 0)
- "Tail" chunks trailing right after merges may still be undersized (see chunk 8)
"""

for i, chunk in enumerate(chunks):
    print(f"=== {i} ===")
    txt_tokens = len(tokenizer.tokenize(chunk.text, max_length=None))
    print(f"chunk.text ({txt_tokens} tokens):\n{repr(chunk.text)}")

    ser_txt = chunker.serialize(chunk=chunk)
    ser_tokens = len(tokenizer.tokenize(ser_txt, max_length=None))
    print(f"chunker.serialize(chunk) ({ser_tokens} tokens):\n{repr(ser_txt)}")

    print()
# Output:
#   === 0 ===

#   chunk.text (55 tokens):

#   'International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.'

#   chunker.serialize(chunk) (56 tokens):

#   'IBM\nInternational Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.'

#   

#   === 1 ===

#   chunk.text (45 tokens):

#   'IBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.'

#   chunker.serialize(chunk) (46 tokens):

#   'IBM\nIBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.'

#   

#   === 2 ===

#   chunk.text (63 tokens):

#   'IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed "International Business Machines" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the'

#   chunker.serialize(chunk) (64 tokens):

#   'IBM\nIBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed "International Business Machines" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the'

#   

#   === 3 ===

#   chunk.text (44 tokens):

#   "IBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]"

#   chunker.serialize(chunk) (45 tokens):

#   "IBM\nIBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]"

#   

#   === 4 ===

#   chunk.text (63 tokens):

#   'IBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s,'

#   chunker.serialize(chunk) (64 tokens):

#   'IBM\nIBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s,'

#   

#   === 5 ===

#   chunk.text (61 tokens):

#   'IBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.'

#   chunker.serialize(chunk) (62 tokens):

#   'IBM\nIBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.'

#   

#   === 6 ===

#   chunk.text (62 tokens):

#   "As one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming"

#   chunker.serialize(chunk) (63 tokens):

#   "IBM\nAs one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming"

#   

#   === 7 ===

#   chunk.text (63 tokens):

#   'language, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing'

#   chunker.serialize(chunk) (64 tokens):

#   'IBM\nlanguage, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing'

#   

#   === 8 ===

#   chunk.text (5 tokens):

#   'Awards.[16]'

#   chunker.serialize(chunk) (6 tokens):

#   'IBM\nAwards.[16]'

#   

#   === 9 ===

#   chunk.text (56 tokens):

#   'IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine'

#   chunker.serialize(chunk) (60 tokens):

#   'IBM\n1910s–1950s\nIBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine'

#   

#   === 10 ===

#   chunk.text (60 tokens):

#   "(1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the"

#   chunker.serialize(chunk) (64 tokens):

#   "IBM\n1910s–1950s\n(1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the"

#   

#   === 11 ===

#   chunk.text (59 tokens):

#   'Computing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington,'

#   chunker.serialize(chunk) (63 tokens):

#   'IBM\n1910s–1950s\nComputing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington,'

#   

#   === 12 ===

#   chunk.text (13 tokens):

#   'D.C.; and Toronto, Canada.[22]'

#   chunker.serialize(chunk) (17 tokens):

#   'IBM\n1910s–1950s\nD.C.; and Toronto, Canada.[22]'

#   

#   === 13 ===

#   chunk.text (60 tokens):

#   'Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called'

#   chunker.serialize(chunk) (64 tokens):

#   'IBM\n1910s–1950s\nCollectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called'

#   

#   === 14 ===

#   chunk.text (59 tokens):

#   "on Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business"

#   chunker.serialize(chunk) (63 tokens):

#   "IBM\n1910s–1950s\non Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business"

#   

#   === 15 ===

#   chunk.text (23 tokens):

#   "practices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]:\n105"

#   chunker.serialize(chunk) (27 tokens):

#   "IBM\n1910s–1950s\npractices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]:\n105"

#   

#   === 16 ===

#   chunk.text (59 tokens):

#   'He implemented sales conventions, "generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker".[25][26] His favorite slogan,'

#   chunker.serialize(chunk) (63 tokens):

#   'IBM\n1910s–1950s\nHe implemented sales conventions, "generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker".[25][26] His favorite slogan,'

#   

#   === 17 ===

#   chunk.text (60 tokens):

#   '"THINK", became a mantra for each company\'s employees.[25] During Watson\'s first four years, revenues reached $9 million ($158 million today) and the company\'s operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the'

#   chunker.serialize(chunk) (64 tokens):

#   'IBM\n1910s–1950s\n"THINK", became a mantra for each company\'s employees.[25] During Watson\'s first four years, revenues reached $9 million ($158 million today) and the company\'s operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the'

#   

#   === 18 ===

#   chunk.text (57 tokens):

#   'clumsy hyphenated name "Computing-Tabulating-Recording Company" and chose to replace it with the more expansive title "International Business Machines" which had previously been used as the name of CTR\'s Canadian Division;[27] the name was changed on February 14,'

#   chunker.serialize(chunk) (61 tokens):

#   'IBM\n1910s–1950s\nclumsy hyphenated name "Computing-Tabulating-Recording Company" and chose to replace it with the more expansive title "International Business Machines" which had previously been used as the name of CTR\'s Canadian Division;[27] the name was changed on February 14,'

#   

#   === 19 ===

#   chunk.text (21 tokens):

#   '1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.'

#   chunker.serialize(chunk) (25 tokens):

#   'IBM\n1910s–1950s\n1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.'

#   

#   === 20 ===

#   chunk.text (22 tokens):

#   'In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.'

#   chunker.serialize(chunk) (26 tokens):

#   'IBM\n1960s–1980s\nIn 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.'

#   



================================================
File: docs/examples/index.md
================================================
Use the navigation on the left to browse through examples covering a range of possible workflows and use cases.


================================================
File: docs/examples/inspect_picture_content.py
================================================
from docling_core.types.doc import TextItem

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

source = "tests/data/pdf/amt_handbook_sample.pdf"

pipeline_options = PdfPipelineOptions()
pipeline_options.images_scale = 2
pipeline_options.generate_page_images = True

doc_converter = DocumentConverter(
    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
)

result = doc_converter.convert(source)

doc = result.document

for picture in doc.pictures:
    # picture.get_image(doc).show() # display the picture
    print(picture.caption_text(doc), " contains these elements:")

    for item, level in doc.iterate_items(root=picture, traverse_pictures=True):
        if isinstance(item, TextItem):
            print(item.text)

    print("\n")


================================================
File: docs/examples/minimal.py
================================================
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"  # document per local path or URL
converter = DocumentConverter()
result = converter.convert(source)
print(result.document.export_to_markdown())
# output: ## Docling Technical Report [...]"


================================================
File: docs/examples/minimal_vlm_pipeline.py
================================================
import json
import time
from pathlib import Path

import yaml

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    VlmPipelineOptions,
    granite_vision_vlm_conversion_options,
    smoldocling_vlm_conversion_options,
)
from docling.datamodel.settings import settings
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.vlm_pipeline import VlmPipeline

sources = [
    "tests/data/2305.03393v1-pg9-img.png",
]

## Use experimental VlmPipeline
pipeline_options = VlmPipelineOptions()
# If force_backend_text = True, text from backend will be used instead of generated text
pipeline_options.force_backend_text = False

## On GPU systems, enable flash_attention_2 with CUDA:
# pipeline_options.accelerator_options.device = AcceleratorDevice.CUDA
# pipeline_options.accelerator_options.cuda_use_flash_attention2 = True

## Pick a VLM model. We choose SmolDocling-256M by default
pipeline_options.vlm_options = smoldocling_vlm_conversion_options

## Alternative VLM models:
# pipeline_options.vlm_options = granite_vision_vlm_conversion_options

from docling_core.types.doc import DocItemLabel, ImageRefMode
from docling_core.types.doc.document import DEFAULT_EXPORT_LABELS

## Set up pipeline for PDF or image inputs
converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(
            pipeline_cls=VlmPipeline,
            pipeline_options=pipeline_options,
        ),
        InputFormat.IMAGE: PdfFormatOption(
            pipeline_cls=VlmPipeline,
            pipeline_options=pipeline_options,
        ),
    }
)

out_path = Path("scratch")
out_path.mkdir(parents=True, exist_ok=True)

for source in sources:
    start_time = time.time()
    print("================================================")
    print("Processing... {}".format(source))
    print("================================================")
    print("")

    res = converter.convert(source)

    print("------------------------------------------------")
    print("MD:")
    print("------------------------------------------------")
    print("")
    print(res.document.export_to_markdown())

    for page in res.pages:
        print("")
        print("Predicted page in DOCTAGS:")
        print(page.predictions.vlm_response.text)

    res.document.save_as_html(
        filename=Path("{}/{}.html".format(out_path, res.input.file.stem)),
        image_mode=ImageRefMode.REFERENCED,
        labels=[*DEFAULT_EXPORT_LABELS, DocItemLabel.FOOTNOTE],
    )

    with (out_path / f"{res.input.file.stem}.json").open("w") as fp:
        fp.write(json.dumps(res.document.export_to_dict()))

    pg_num = res.document.num_pages()

    print("")
    inference_time = time.time() - start_time
    print(
        f"Total document prediction time: {inference_time:.2f} seconds, pages: {pg_num}"
    )

print("================================================")
print("done!")
print("================================================")


================================================
File: docs/examples/pictures_description_api.py
================================================
import logging
import os
from pathlib import Path

import requests
from docling_core.types.doc import PictureItem
from dotenv import load_dotenv

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    PdfPipelineOptions,
    PictureDescriptionApiOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption


def vllm_local_options(model: str):
    options = PictureDescriptionApiOptions(
        url="http://localhost:8000/v1/chat/completions",
        params=dict(
            model=model,
            seed=42,
            max_completion_tokens=200,
        ),
        prompt="Describe the image in three sentences. Be consise and accurate.",
        timeout=90,
    )
    return options


def watsonx_vlm_options():
    load_dotenv()
    api_key = os.environ.get("WX_API_KEY")
    project_id = os.environ.get("WX_PROJECT_ID")

    def _get_iam_access_token(api_key: str) -> str:
        res = requests.post(
            url="https://iam.cloud.ibm.com/identity/token",
            headers={
                "Content-Type": "application/x-www-form-urlencoded",
            },
            data=f"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={api_key}",
        )
        res.raise_for_status()
        api_out = res.json()
        print(f"{api_out=}")
        return api_out["access_token"]

    options = PictureDescriptionApiOptions(
        url="https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29",
        params=dict(
            model_id="meta-llama/llama-3-2-11b-vision-instruct",
            project_id=project_id,
            parameters=dict(
                max_new_tokens=400,
            ),
        ),
        headers={
            "Authorization": "Bearer " + _get_iam_access_token(api_key=api_key),
        },
        prompt="Describe the image in three sentences. Be consise and accurate.",
        timeout=60,
    )
    return options


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")

    pipeline_options = PdfPipelineOptions(
        enable_remote_services=True  # <-- this is required!
    )
    pipeline_options.do_picture_description = True

    # The PictureDescriptionApiOptions() allows to interface with APIs supporting
    # the multi-modal chat interface. Here follow a few example on how to configure those.
    #
    # One possibility is self-hosting model, e.g. via VLLM.
    # $ vllm serve MODEL_NAME
    # Then PictureDescriptionApiOptions can point to the localhost endpoint.
    #
    # Example for the Granite Vision model: (uncomment the following lines)
    # pipeline_options.picture_description_options = vllm_local_options(
    #     model="ibm-granite/granite-vision-3.1-2b-preview"
    # )
    #
    # Example for the SmolVLM model: (uncomment the following lines)
    pipeline_options.picture_description_options = vllm_local_options(
        model="HuggingFaceTB/SmolVLM-256M-Instruct"
    )
    #
    # Another possibility is using online services, e.g. watsonx.ai.
    # Using requires setting the env variables WX_API_KEY and WX_PROJECT_ID.
    # Uncomment the following line for this option:
    # pipeline_options.picture_description_options = watsonx_vlm_options()

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            )
        }
    )
    result = doc_converter.convert(input_doc_path)

    for element, _level in result.document.iterate_items():
        if isinstance(element, PictureItem):
            print(
                f"Picture {element.self_ref}\n"
                f"Caption: {element.caption_text(doc=result.document)}\n"
                f"Annotations: {element.annotations}"
            )


if __name__ == "__main__":
    main()


================================================
File: docs/examples/rag_haystack.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/rag_haystack.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
# RAG with Haystack
"""

"""
| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | Hugging Face / Sentence Transformers | 💻 Local |
| Vector store | Milvus | 💻 Local |
| Gen AI | Hugging Face Inference API | 🌐 Remote | 
"""

"""
## Overview
"""

"""
This example leverages the
[Haystack Docling extension](../../integrations/haystack/), along with
Milvus-based document store and retriever instances, as well as sentence-transformers
embeddings.

The presented `DoclingConverter` component enables you to:
- use various document types in your LLM applications with ease and speed, and
- leverage Docling's rich format for advanced, document-native grounding.

`DoclingConverter` supports two different export modes:
- `ExportType.MARKDOWN`: if you want to capture each input document as a separate
  Haystack document, or
- `ExportType.DOC_CHUNKS` (default): if you want to have each input document chunked and
  to then capture each individual chunk as a separate Haystack document downstream.

The example allows to explore both modes via parameter `EXPORT_TYPE`; depending on the
value set, the ingestion and RAG pipelines are then set up accordingly.
"""

"""
## Setup
"""

"""
- 👉 For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.
- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.
- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):
"""

%pip install -q --progress-bar off --no-warn-conflicts docling-haystack haystack-ai docling pymilvus milvus-haystack sentence-transformers python-dotenv
# Output:
#   Note: you may need to restart the kernel to use updated packages.


import os
from pathlib import Path
from tempfile import mkdtemp

from docling_haystack.converter import ExportType
from dotenv import load_dotenv


def _get_env_from_colab_or_os(key):
    try:
        from google.colab import userdata

        try:
            return userdata.get(key)
        except userdata.SecretNotFoundError:
            pass
    except ImportError:
        pass
    return os.getenv(key)


load_dotenv()
HF_TOKEN = _get_env_from_colab_or_os("HF_TOKEN")
PATHS = ["https://arxiv.org/pdf/2408.09869"]  # Docling Technical Report
EMBED_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
GENERATION_MODEL_ID = "mistralai/Mixtral-8x7B-Instruct-v0.1"
EXPORT_TYPE = ExportType.DOC_CHUNKS
QUESTION = "Which are the main AI models in Docling?"
TOP_K = 3
MILVUS_URI = str(Path(mkdtemp()) / "docling.db")

"""
## Indexing pipeline
"""

from docling_haystack.converter import DoclingConverter
from haystack import Pipeline
from haystack.components.embedders import (
    SentenceTransformersDocumentEmbedder,
    SentenceTransformersTextEmbedder,
)
from haystack.components.preprocessors import DocumentSplitter
from haystack.components.writers import DocumentWriter
from milvus_haystack import MilvusDocumentStore, MilvusEmbeddingRetriever

from docling.chunking import HybridChunker

document_store = MilvusDocumentStore(
    connection_args={"uri": MILVUS_URI},
    drop_old=True,
    text_field="txt",  # set for preventing conflict with same-name metadata field
)

idx_pipe = Pipeline()
idx_pipe.add_component(
    "converter",
    DoclingConverter(
        export_type=EXPORT_TYPE,
        chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),
    ),
)
idx_pipe.add_component(
    "embedder",
    SentenceTransformersDocumentEmbedder(model=EMBED_MODEL_ID),
)
idx_pipe.add_component("writer", DocumentWriter(document_store=document_store))
if EXPORT_TYPE == ExportType.DOC_CHUNKS:
    idx_pipe.connect("converter", "embedder")
elif EXPORT_TYPE == ExportType.MARKDOWN:
    idx_pipe.add_component(
        "splitter",
        DocumentSplitter(split_by="sentence", split_length=1),
    )
    idx_pipe.connect("converter.documents", "splitter.documents")
    idx_pipe.connect("splitter.documents", "embedder.documents")
else:
    raise ValueError(f"Unexpected export type: {EXPORT_TYPE}")
idx_pipe.connect("embedder", "writer")
idx_pipe.run({"converter": {"paths": PATHS}})
# Output:
#   Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors

#   Batches:   0%|          | 0/2 [00:00<?, ?it/s]
#   {'writer': {'documents_written': 54}}

"""
## RAG pipeline
"""

from haystack.components.builders import AnswerBuilder
from haystack.components.builders.prompt_builder import PromptBuilder
from haystack.components.generators import HuggingFaceAPIGenerator
from haystack.utils import Secret

prompt_template = """
    Given these documents, answer the question.
    Documents:
    {% for doc in documents %}
        {{ doc.content }}
    {% endfor %}
    Question: {{query}}
    Answer:
    """

rag_pipe = Pipeline()
rag_pipe.add_component(
    "embedder",
    SentenceTransformersTextEmbedder(model=EMBED_MODEL_ID),
)
rag_pipe.add_component(
    "retriever",
    MilvusEmbeddingRetriever(document_store=document_store, top_k=TOP_K),
)
rag_pipe.add_component("prompt_builder", PromptBuilder(template=prompt_template))
rag_pipe.add_component(
    "llm",
    HuggingFaceAPIGenerator(
        api_type="serverless_inference_api",
        api_params={"model": GENERATION_MODEL_ID},
        token=Secret.from_token(HF_TOKEN) if HF_TOKEN else None,
    ),
)
rag_pipe.add_component("answer_builder", AnswerBuilder())
rag_pipe.connect("embedder.embedding", "retriever")
rag_pipe.connect("retriever", "prompt_builder.documents")
rag_pipe.connect("prompt_builder", "llm")
rag_pipe.connect("llm.replies", "answer_builder.replies")
rag_pipe.connect("llm.meta", "answer_builder.meta")
rag_pipe.connect("retriever", "answer_builder.documents")
rag_res = rag_pipe.run(
    {
        "embedder": {"text": QUESTION},
        "prompt_builder": {"query": QUESTION},
        "answer_builder": {"query": QUESTION},
    }
)
# Output:
#   Batches:   0%|          | 0/1 [00:00<?, ?it/s]
#   /Users/pva/work/github.com/DS4SD/docling/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.

#     warnings.warn(


"""
Below we print out the RAG results. If you have used `ExportType.DOC_CHUNKS`, notice how
the sources contain document-level grounding (e.g. page number or bounding box
information):
"""

from docling.chunking import DocChunk

print(f"Question:\n{QUESTION}\n")
print(f"Answer:\n{rag_res['answer_builder']['answers'][0].data.strip()}\n")
print("Sources:")
sources = rag_res["answer_builder"]["answers"][0].documents
for source in sources:
    if EXPORT_TYPE == ExportType.DOC_CHUNKS:
        doc_chunk = DocChunk.model_validate(source.meta["dl_meta"])
        print(f"- text: {repr(doc_chunk.text)}")
        if doc_chunk.meta.origin:
            print(f"  file: {doc_chunk.meta.origin.filename}")
        if doc_chunk.meta.headings:
            print(f"  section: {' / '.join(doc_chunk.meta.headings)}")
        bbox = doc_chunk.meta.doc_items[0].prov[0].bbox
        print(
            f"  page: {doc_chunk.meta.doc_items[0].prov[0].page_no}, "
            f"bounding box: [{int(bbox.l)}, {int(bbox.t)}, {int(bbox.r)}, {int(bbox.b)}]"
        )
    elif EXPORT_TYPE == ExportType.MARKDOWN:
        print(repr(source.content))
    else:
        raise ValueError(f"Unexpected export type: {EXPORT_TYPE}")
# Output:
#   Question:

#   Which are the main AI models in Docling?

#   

#   Answer:

#   The main AI models in Docling are a layout analysis model and TableFormer. The layout analysis model is an accurate object-detector for page elements, while TableFormer is a state-of-the-art table structure recognition model. These models are provided with pre-trained weights and a separate package for the inference code as docling-ibm-models. They are also used in the open-access deepsearch-experience, a cloud-native service for knowledge exploration tasks. Additionally, Docling plans to extend its model library with a figure-classifier model, an equation-recognition model, a code-recognition model, and more in the future.

#   

#   Sources:

#   - text: 'As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.'

#     file: 2408.09869v5.pdf

#     section: 3.2 AI models

#     page: 3, bounding box: [107, 406, 504, 330]

#   - text: 'Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.'

#     file: 2408.09869v5.pdf

#     section: 3 Processing pipeline

#     page: 2, bounding box: [107, 273, 504, 176]

#   - text: 'Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.'

#     section: 6 Future work and contributions

#     page: 5, bounding box: [106, 323, 504, 258]



================================================
File: docs/examples/rag_langchain.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/rag_langchain.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
# RAG with LangChain
"""

"""
| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | Hugging Face / Sentence Transformers | 💻 Local |
| Vector store | Milvus | 💻 Local |
| Gen AI | Hugging Face Inference API | 🌐 Remote | 
"""

"""
This example leverages the
[LangChain Docling integration](../../integrations/langchain/), along with a Milvus
vector store, as well as sentence-transformers embeddings.

The presented `DoclingLoader` component enables you to:
- use various document types in your LLM applications with ease and speed, and
- leverage Docling's rich format for advanced, document-native grounding.

`DoclingLoader` supports two different export modes:
- `ExportType.MARKDOWN`: if you want to capture each input document as a separate
  LangChain document, or
- `ExportType.DOC_CHUNKS` (default): if you want to have each input document chunked and
  to then capture each individual chunk as a separate LangChain document downstream.

The example allows exploring both modes via parameter `EXPORT_TYPE`; depending on the
value set, the example pipeline is then set up accordingly.
"""

"""
## Setup
"""

"""
- 👉 For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.
- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.
- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):
"""

%pip install -q --progress-bar off --no-warn-conflicts langchain-docling langchain-core langchain-huggingface langchain_milvus langchain python-dotenv
# Output:
#   Note: you may need to restart the kernel to use updated packages.


import os
from pathlib import Path
from tempfile import mkdtemp

from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_docling.loader import ExportType


def _get_env_from_colab_or_os(key):
    try:
        from google.colab import userdata

        try:
            return userdata.get(key)
        except userdata.SecretNotFoundError:
            pass
    except ImportError:
        pass
    return os.getenv(key)


load_dotenv()

# https://github.com/huggingface/transformers/issues/5486:
os.environ["TOKENIZERS_PARALLELISM"] = "false"

HF_TOKEN = _get_env_from_colab_or_os("HF_TOKEN")
FILE_PATH = ["https://arxiv.org/pdf/2408.09869"]  # Docling Technical Report
EMBED_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
GEN_MODEL_ID = "mistralai/Mixtral-8x7B-Instruct-v0.1"
EXPORT_TYPE = ExportType.DOC_CHUNKS
QUESTION = "Which are the main AI models in Docling?"
PROMPT = PromptTemplate.from_template(
    "Context information is below.\n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {input}\nAnswer:\n",
)
TOP_K = 3
MILVUS_URI = str(Path(mkdtemp()) / "docling.db")

"""
## Document loading

Now we can instantiate our loader and load documents.
"""

from langchain_docling import DoclingLoader

from docling.chunking import HybridChunker

loader = DoclingLoader(
    file_path=FILE_PATH,
    export_type=EXPORT_TYPE,
    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),
)

docs = loader.load()
# Output:
#   Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors


"""
> Note: a message saying `"Token indices sequence length is longer than the specified
maximum sequence length..."` can be ignored in this case — details
[here](https://github.com/DS4SD/docling-core/issues/119#issuecomment-2577418826).
"""

"""
Determining the splits:
"""

if EXPORT_TYPE == ExportType.DOC_CHUNKS:
    splits = docs
elif EXPORT_TYPE == ExportType.MARKDOWN:
    from langchain_text_splitters import MarkdownHeaderTextSplitter

    splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[
            ("#", "Header_1"),
            ("##", "Header_2"),
            ("###", "Header_3"),
        ],
    )
    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]
else:
    raise ValueError(f"Unexpected export type: {EXPORT_TYPE}")

"""
Inspecting some sample splits:
"""

for d in splits[:3]:
    print(f"- {d.page_content=}")
print("...")
# Output:
#   - d.page_content='arXiv:2408.09869v5  [cs.CL]  9 Dec 2024'

#   - d.page_content='Docling Technical Report\nVersion 1.0\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\nAI4K Group, IBM Research R¨uschlikon, Switzerland'

#   - d.page_content='Abstract\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.'

#   ...


"""
## Ingestion
"""

import json
from pathlib import Path
from tempfile import mkdtemp

from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_milvus import Milvus

embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)


milvus_uri = str(Path(mkdtemp()) / "docling.db")  # or set as needed
vectorstore = Milvus.from_documents(
    documents=splits,
    embedding=embedding,
    collection_name="docling_demo",
    connection_args={"uri": milvus_uri},
    index_params={"index_type": "FLAT"},
    drop_old=True,
)

"""
## RAG
"""

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_huggingface import HuggingFaceEndpoint

retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K})
llm = HuggingFaceEndpoint(
    repo_id=GEN_MODEL_ID,
    huggingfacehub_api_token=HF_TOKEN,
)


def clip_text(text, threshold=100):
    return f"{text[:threshold]}..." if len(text) > threshold else text
# Output:
#   Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.


question_answer_chain = create_stuff_documents_chain(llm, PROMPT)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
resp_dict = rag_chain.invoke({"input": QUESTION})

clipped_answer = clip_text(resp_dict["answer"], threshold=200)
print(f"Question:\n{resp_dict['input']}\n\nAnswer:\n{clipped_answer}")
for i, doc in enumerate(resp_dict["context"]):
    print()
    print(f"Source {i+1}:")
    print(f"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}")
    for key in doc.metadata:
        if key != "pk":
            val = doc.metadata.get(key)
            clipped_val = clip_text(val) if isinstance(val, str) else val
            print(f"  {key}: {clipped_val}")
# Output:
#   Question:

#   Which are the main AI models in Docling?

#   

#   Answer:

#   Docling initially releases two AI models, a layout analysis model and TableFormer. The layout analysis model is an accurate object-detector for page elements, and TableFormer is a state-of-the-art tab...

#   

#   Source 1:

#     text: "3.2 AI models\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure re..."

#     dl_meta: {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/50', 'parent': {'$ref': '#/body'}, 'children': [], 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 405.1419982910156, 'r': 504.00299072265625, 'b': 330.7799987792969, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 608]}]}], 'headings': ['3.2 AI models'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}

#     source: https://arxiv.org/pdf/2408.09869

#   

#   Source 2:

#     text: "3 Processing pipeline\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support ..."

#     dl_meta: {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/26', 'parent': {'$ref': '#/body'}, 'children': [], 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 273.01800537109375, 'r': 504.00299072265625, 'b': 176.83799743652344, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 796]}]}], 'headings': ['3 Processing pipeline'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}

#     source: https://arxiv.org/pdf/2408.09869

#   

#   Source 3:

#     text: "6 Future work and contributions\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of ..."

#     dl_meta: {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/76', 'parent': {'$ref': '#/body'}, 'children': [], 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 322.468994140625, 'r': 504.00299072265625, 'b': 259.0169982910156, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 543]}]}, {'self_ref': '#/texts/77', 'parent': {'$ref': '#/body'}, 'children': [], 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 251.6540069580078, 'r': 504.00299072265625, 'b': 198.99200439453125, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 402]}]}], 'headings': ['6 Future work and contributions'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}

#     source: https://arxiv.org/pdf/2408.09869



================================================
File: docs/examples/rag_llamaindex.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/rag_llamaindex.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
# RAG with LlamaIndex
"""

"""
| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | Hugging Face / Sentence Transformers | 💻 Local |
| Vector store | Milvus | 💻 Local |
| Gen AI | Hugging Face Inference API | 🌐 Remote | 
"""

"""
## Overview
"""

"""
This example leverages the official [LlamaIndex Docling extension](../../integrations/llamaindex/).

Presented extensions `DoclingReader` and `DoclingNodeParser` enable you to:
- use various document types in your LLM applications with ease and speed, and
- leverage Docling's rich format for advanced, document-native grounding.
"""

"""
## Setup
"""

"""
- 👉 For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.
- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.
- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):
"""

%pip install -q --progress-bar off --no-warn-conflicts llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-vector-stores-milvus llama-index-readers-file python-dotenv
# Output:
#   Note: you may need to restart the kernel to use updated packages.


import os
from pathlib import Path
from tempfile import mkdtemp
from warnings import filterwarnings

from dotenv import load_dotenv


def _get_env_from_colab_or_os(key):
    try:
        from google.colab import userdata

        try:
            return userdata.get(key)
        except userdata.SecretNotFoundError:
            pass
    except ImportError:
        pass
    return os.getenv(key)


load_dotenv()

filterwarnings(action="ignore", category=UserWarning, module="pydantic")
filterwarnings(action="ignore", category=FutureWarning, module="easyocr")
# https://github.com/huggingface/transformers/issues/5486:
os.environ["TOKENIZERS_PARALLELISM"] = "false"

"""
We can now define the main parameters:
"""

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

EMBED_MODEL = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
MILVUS_URI = str(Path(mkdtemp()) / "docling.db")
GEN_MODEL = HuggingFaceInferenceAPI(
    token=_get_env_from_colab_or_os("HF_TOKEN"),
    model_name="mistralai/Mixtral-8x7B-Instruct-v0.1",
)
SOURCE = "https://arxiv.org/pdf/2408.09869"  # Docling Technical Report
QUERY = "Which are the main AI models in Docling?"

embed_dim = len(EMBED_MODEL.get_text_embedding("hi"))

"""
## Using Markdown export
"""

"""
To create a simple RAG pipeline, we can:
- define a `DoclingReader`, which by default exports to Markdown, and
- use a standard node parser for these Markdown-based docs, e.g. a `MarkdownNodeParser`
"""

from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.readers.docling import DoclingReader
from llama_index.vector_stores.milvus import MilvusVectorStore

reader = DoclingReader()
node_parser = MarkdownNodeParser()

vector_store = MilvusVectorStore(
    uri=str(Path(mkdtemp()) / "docling.db"),  # or set as needed
    dim=embed_dim,
    overwrite=True,
)
index = VectorStoreIndex.from_documents(
    documents=reader.load_data(SOURCE),
    transformations=[node_parser],
    storage_context=StorageContext.from_defaults(vector_store=vector_store),
    embed_model=EMBED_MODEL,
)
result = index.as_query_engine(llm=GEN_MODEL).query(QUERY)
print(f"Q: {QUERY}\nA: {result.response.strip()}\n\nSources:")
display([(n.text, n.metadata) for n in result.source_nodes])
# Output:
#   Q: Which are the main AI models in Docling?

#   A: The main AI models in Docling are a layout analysis model, which is an accurate object-detector for page elements, and TableFormer, a state-of-the-art table structure recognition model.

#   

#   Sources:

#   [('3.2 AI models\n\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.',

#     {'Header_2': '3.2 AI models'}),

#    ("5 Applications\n\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.",

#     {'Header_2': '5 Applications'})]

"""
## Using Docling format
"""

"""
To leverage Docling's rich native format, we:
- create a `DoclingReader` with JSON export type, and
- employ a `DoclingNodeParser` in order to appropriately parse that Docling format.

Notice how the sources now also contain document-level grounding (e.g. page number or bounding box information):
"""

from llama_index.node_parser.docling import DoclingNodeParser

reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)
node_parser = DoclingNodeParser()

vector_store = MilvusVectorStore(
    uri=str(Path(mkdtemp()) / "docling.db"),  # or set as needed
    dim=embed_dim,
    overwrite=True,
)
index = VectorStoreIndex.from_documents(
    documents=reader.load_data(SOURCE),
    transformations=[node_parser],
    storage_context=StorageContext.from_defaults(vector_store=vector_store),
    embed_model=EMBED_MODEL,
)
result = index.as_query_engine(llm=GEN_MODEL).query(QUERY)
print(f"Q: {QUERY}\nA: {result.response.strip()}\n\nSources:")
display([(n.text, n.metadata) for n in result.source_nodes])
# Output:
#   Q: Which are the main AI models in Docling?

#   A: The main AI models in Docling are a layout analysis model and TableFormer. The layout analysis model is an accurate object-detector for page elements, and TableFormer is a state-of-the-art table structure recognition model.

#   

#   Sources:

#   [('As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.',

#     {'schema_name': 'docling_core.transforms.chunker.DocMeta',

#      'version': '1.0.0',

#      'doc_items': [{'self_ref': '#/texts/34',

#        'parent': {'$ref': '#/body'},

#        'children': [],

#        'label': 'text',

#        'prov': [{'page_no': 3,

#          'bbox': {'l': 107.07593536376953,

#           't': 406.1695251464844,

#           'r': 504.1148681640625,

#           'b': 330.2677307128906,

#           'coord_origin': 'BOTTOMLEFT'},

#          'charspan': [0, 608]}]}],

#      'headings': ['3.2 AI models'],

#      'origin': {'mimetype': 'application/pdf',

#       'binary_hash': 14981478401387673002,

#       'filename': '2408.09869v3.pdf'}}),

#    ('With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.',

#     {'schema_name': 'docling_core.transforms.chunker.DocMeta',

#      'version': '1.0.0',

#      'doc_items': [{'self_ref': '#/texts/9',

#        'parent': {'$ref': '#/body'},

#        'children': [],

#        'label': 'text',

#        'prov': [{'page_no': 1,

#          'bbox': {'l': 107.0031967163086,

#           't': 136.7283935546875,

#           'r': 504.04998779296875,

#           'b': 83.30133056640625,

#           'coord_origin': 'BOTTOMLEFT'},

#          'charspan': [0, 488]}]}],

#      'headings': ['1 Introduction'],

#      'origin': {'mimetype': 'application/pdf',

#       'binary_hash': 14981478401387673002,

#       'filename': '2408.09869v3.pdf'}})]

"""
## With Simple Directory Reader
"""

"""
To demonstrate this usage pattern, we first set up a test document directory.
"""

from pathlib import Path
from tempfile import mkdtemp

import requests

tmp_dir_path = Path(mkdtemp())
r = requests.get(SOURCE)
with open(tmp_dir_path / f"{Path(SOURCE).name}.pdf", "wb") as out_file:
    out_file.write(r.content)

"""
Using the `reader` and `node_parser` definitions from any of the above variants, usage with `SimpleDirectoryReader` then looks as follows:
"""

from llama_index.core import SimpleDirectoryReader

dir_reader = SimpleDirectoryReader(
    input_dir=tmp_dir_path,
    file_extractor={".pdf": reader},
)

vector_store = MilvusVectorStore(
    uri=str(Path(mkdtemp()) / "docling.db"),  # or set as needed
    dim=embed_dim,
    overwrite=True,
)
index = VectorStoreIndex.from_documents(
    documents=dir_reader.load_data(SOURCE),
    transformations=[node_parser],
    storage_context=StorageContext.from_defaults(vector_store=vector_store),
    embed_model=EMBED_MODEL,
)
result = index.as_query_engine(llm=GEN_MODEL).query(QUERY)
print(f"Q: {QUERY}\nA: {result.response.strip()}\n\nSources:")
display([(n.text, n.metadata) for n in result.source_nodes])
# Output:
#   Loading files: 100%|██████████| 1/1 [00:11<00:00, 11.27s/file]

#   Q: Which are the main AI models in Docling?

#   A: 1. A layout analysis model, an accurate object-detector for page elements. 2. TableFormer, a state-of-the-art table structure recognition model.

#   

#   Sources:

#   [('As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.',

#     {'file_path': '/var/folders/76/4wwfs06x6835kcwj4186c0nc0000gn/T/tmp2ooyusg5/2408.09869.pdf',

#      'file_name': '2408.09869.pdf',

#      'file_type': 'application/pdf',

#      'file_size': 5566574,

#      'creation_date': '2024-10-28',

#      'last_modified_date': '2024-10-28',

#      'schema_name': 'docling_core.transforms.chunker.DocMeta',

#      'version': '1.0.0',

#      'doc_items': [{'self_ref': '#/texts/34',

#        'parent': {'$ref': '#/body'},

#        'children': [],

#        'label': 'text',

#        'prov': [{'page_no': 3,

#          'bbox': {'l': 107.07593536376953,

#           't': 406.1695251464844,

#           'r': 504.1148681640625,

#           'b': 330.2677307128906,

#           'coord_origin': 'BOTTOMLEFT'},

#          'charspan': [0, 608]}]}],

#      'headings': ['3.2 AI models'],

#      'origin': {'mimetype': 'application/pdf',

#       'binary_hash': 14981478401387673002,

#       'filename': '2408.09869.pdf'}}),

#    ('With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.',

#     {'file_path': '/var/folders/76/4wwfs06x6835kcwj4186c0nc0000gn/T/tmp2ooyusg5/2408.09869.pdf',

#      'file_name': '2408.09869.pdf',

#      'file_type': 'application/pdf',

#      'file_size': 5566574,

#      'creation_date': '2024-10-28',

#      'last_modified_date': '2024-10-28',

#      'schema_name': 'docling_core.transforms.chunker.DocMeta',

#      'version': '1.0.0',

#      'doc_items': [{'self_ref': '#/texts/9',

#        'parent': {'$ref': '#/body'},

#        'children': [],

#        'label': 'text',

#        'prov': [{'page_no': 1,

#          'bbox': {'l': 107.0031967163086,

#           't': 136.7283935546875,

#           'r': 504.04998779296875,

#           'b': 83.30133056640625,

#           'coord_origin': 'BOTTOMLEFT'},

#          'charspan': [0, 488]}]}],

#      'headings': ['1 Introduction'],

#      'origin': {'mimetype': 'application/pdf',

#       'binary_hash': 14981478401387673002,

#       'filename': '2408.09869.pdf'}})]


================================================
File: docs/examples/rag_weaviate.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/rag_weaviate.ipynb)
"""

"""
# RAG with Weaviate
"""

"""

| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | Open AI | 🌐 Remote |
| Vector store | Weavieate | 💻 Local |
| Gen AI | Open AI | 🌐 Remote |

## A recipe 🧑‍🍳 🐥 💚

This is a code recipe that uses [Weaviate](https://weaviate.io/) to perform RAG over PDF documents parsed by [Docling](https://ds4sd.github.io/docling/).

In this notebook, we accomplish the following:
* Parse the top machine learning papers on [arXiv](https://arxiv.org/) using Docling
* Perform hierarchical chunking of the documents using Docling
* Generate text embeddings with OpenAI
* Perform RAG using [Weaviate](https://weaviate.io/developers/weaviate/search/generative)

To run this notebook, you'll need:
* An [OpenAI API key](https://platform.openai.com/docs/quickstart)
* Access to GPU/s

Note: For best results, please use **GPU acceleration** to run this notebook. Here are two options for running this notebook:
1. **Locally on a MacBook with an Apple Silicon chip.** Converting all documents in the notebook takes ~2 minutes on a MacBook M2 due to Docling's usage of MPS accelerators.
2. **Run this notebook on Google Colab.** Converting all documents in the notebook takes ~8 mintutes on a Google Colab T4 GPU.
"""

"""
### Install Docling and Weaviate client

Note: If Colab prompts you to restart the session after running the cell below, click "restart" and proceed with running the rest of the notebook.
"""

%%capture
%pip install docling~="2.7.0"
%pip install -U weaviate-client~="4.9.4"
%pip install rich
%pip install torch

import warnings

warnings.filterwarnings("ignore")

import logging

# Suppress Weaviate client logs
logging.getLogger("weaviate").setLevel(logging.ERROR)

"""
## 🐥 Part 1: Docling

Part of what makes Docling so remarkable is the fact that it can run on commodity hardware. This means that this notebook can be run on a local machine with GPU acceleration. If you're using a MacBook with a silicon chip, Docling integrates seamlessly with Metal Performance Shaders (MPS). MPS provides out-of-the-box GPU acceleration for macOS, seamlessly integrating with PyTorch and TensorFlow, offering energy-efficient performance on Apple Silicon, and broad compatibility with all Metal-supported GPUs.

The code below checks to see if a GPU is available, either via CUDA or MPS.
"""

import torch

# Check if GPU or MPS is available
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"CUDA GPU is enabled: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS GPU is enabled.")
else:
    raise EnvironmentError(
        "No GPU or MPS device found. Please check your environment and ensure GPU or MPS support is configured."
    )
# Output:
#   MPS GPU is enabled.


"""
Here, we've collected 10 influential machine learning papers published as PDFs on arXiv. Because Docling does not yet have title extraction for PDFs, we manually add the titles in a corresponding list.

Note: Converting all 10 papers should take around 8 minutes with a T4 GPU.
"""

# Influential machine learning papers
source_urls = [
    "https://arxiv.org/pdf/1706.03762",
    "https://arxiv.org/pdf/1810.04805",
    "https://arxiv.org/pdf/1406.2661",
    "https://arxiv.org/pdf/1409.0473",
    "https://arxiv.org/pdf/1412.6980",
    "https://arxiv.org/pdf/1312.6114",
    "https://arxiv.org/pdf/1312.5602",
    "https://arxiv.org/pdf/1512.03385",
    "https://arxiv.org/pdf/1409.3215",
    "https://arxiv.org/pdf/1301.3781",
]

# And their corresponding titles (because Docling doesn't have title extraction yet!)
source_titles = [
    "Attention Is All You Need",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "Generative Adversarial Nets",
    "Neural Machine Translation by Jointly Learning to Align and Translate",
    "Adam: A Method for Stochastic Optimization",
    "Auto-Encoding Variational Bayes",
    "Playing Atari with Deep Reinforcement Learning",
    "Deep Residual Learning for Image Recognition",
    "Sequence to Sequence Learning with Neural Networks",
    "A Neural Probabilistic Language Model",
]

"""
### Convert PDFs to Docling documents

Here we use Docling's `.convert_all()` to parse a batch of PDFs. The result is a list of Docling documents that we can use for text extraction.

Note: Please ignore the `ERR#` message.
"""

from docling.datamodel.document import ConversionResult
from docling.document_converter import DocumentConverter

# Instantiate the doc converter
doc_converter = DocumentConverter()

# Directly pass list of files or streams to `convert_all`
conv_results_iter = doc_converter.convert_all(source_urls)  # previously `convert`

# Iterate over the generator to get a list of Docling documents
docs = [result.document for result in conv_results_iter]
# Output:
#   Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 84072.91it/s]

#   ERR#: COULD NOT CONVERT TO RS THIS TABLE TO COMPUTE SPANS


"""
### Post-process extracted document data
#### Perform hierarchical chunking on documents

We use Docling's `HierarchicalChunker()` to perform hierarchy-aware chunking of our list of documents. This is meant to preserve some of the structure and relationships within the document, which enables more accurate and relevant retrieval in our RAG pipeline.
"""

from docling_core.transforms.chunker import HierarchicalChunker

# Initialize lists for text, and titles
texts, titles = [], []

chunker = HierarchicalChunker()

# Process each document in the list
for doc, title in zip(docs, source_titles):  # Pair each document with its title
    chunks = list(
        chunker.chunk(doc)
    )  # Perform hierarchical chunking and get text from chunks
    for chunk in chunks:
        texts.append(chunk.text)
        titles.append(title)

"""
Because we're splitting the documents into chunks, we'll concatenate the article title to the beginning of each chunk for additional context.
"""

# Concatenate title and text
for i in range(len(texts)):
    texts[i] = f"{titles[i]} {texts[i]}"

"""
## 💚 Part 2: Weaviate
### Create and configure an embedded Weaviate collection
"""

"""
We'll be using the OpenAI API for both generating the text embeddings and for the generative model in our RAG pipeline. The code below dynamically fetches your API key based on whether you're running this notebook in Google Colab and running it as a regular Jupyter notebook. All you need to do is replace `openai_api_key_var` with the name of your environmental variable name or Colab secret name for the API key.

If you're running this notebook in Google Colab, make sure you [add](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) your API key as a secret.
"""

# OpenAI API key variable name
openai_api_key_var = "OPENAI_API_KEY"  # Replace with the name of your secret/env var

# Fetch OpenAI API key
try:
    # If running in Colab, fetch API key from Secrets
    import google.colab
    from google.colab import userdata

    openai_api_key = userdata.get(openai_api_key_var)
    if not openai_api_key:
        raise ValueError(f"Secret '{openai_api_key_var}' not found in Colab secrets.")
except ImportError:
    # If not running in Colab, fetch API key from environment variable
    import os

    openai_api_key = os.getenv(openai_api_key_var)
    if not openai_api_key:
        raise EnvironmentError(
            f"Environment variable '{openai_api_key_var}' is not set. "
            "Please define it before running this script."
        )

"""
[Embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) allows you to spin up a Weaviate instance directly from your application code, without having to use a Docker container. If you're interested in other deployment methods, like using Docker-Compose or Kubernetes, check out this [page](https://weaviate.io/developers/weaviate/installation) in the Weaviate docs.
"""

import weaviate

# Connect to Weaviate embedded
client = weaviate.connect_to_embedded(headers={"X-OpenAI-Api-Key": openai_api_key})

import weaviate.classes.config as wc
from weaviate.classes.config import DataType, Property

# Define the collection name
collection_name = "docling"

# Delete the collection if it already exists
if client.collections.exists(collection_name):
    client.collections.delete(collection_name)

# Create the collection
collection = client.collections.create(
    name=collection_name,
    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(
        model="text-embedding-3-large",  # Specify your embedding model here
    ),
    # Enable generative model from Cohere
    generative_config=wc.Configure.Generative.openai(
        model="gpt-4o"  # Specify your generative model for RAG here
    ),
    # Define properties of metadata
    properties=[
        wc.Property(name="text", data_type=wc.DataType.TEXT),
        wc.Property(name="title", data_type=wc.DataType.TEXT, skip_vectorization=True),
    ],
)

"""
### Wrangle data into an acceptable format for Weaviate

Transform our data from lists to a list of dictionaries for insertion into our Weaviate collection.
"""

# Initialize the data object
data = []

# Create a dictionary for each row by iterating through the corresponding lists
for text, title in zip(texts, titles):
    data_point = {
        "text": text,
        "title": title,
    }
    data.append(data_point)

"""
### Insert data into Weaviate and generate embeddings

Embeddings will be generated upon insertion to our Weaviate collection.
"""

# Insert text chunks and metadata into vector DB collection
response = collection.data.insert_many(data)

if response.has_errors:
    print(response.errors)
else:
    print("Insert complete.")

"""
### Query the data

Here, we perform a simple similarity search to return the most similar embedded chunks to our search query.
"""

from weaviate.classes.query import MetadataQuery

response = collection.query.near_text(
    query="bert",
    limit=2,
    return_metadata=MetadataQuery(distance=True),
    return_properties=["text", "title"],
)

for o in response.objects:
    print(o.properties)
    print(o.metadata.distance)
# Output:
#   {'text': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding A distinctive feature of BERT is its unified architecture across different tasks. There is mini-', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}

#   0.6578550338745117

#   {'text': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding We introduce a new language representation model called BERT , which stands for B idirectional E ncoder R epresentations from T ransformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}

#   0.6696287989616394


"""
### Perform RAG on parsed articles

Weaviate's `generate` module allows you to perform RAG over your embedded data without having to use a separate framework.

We specify a prompt that includes the field we want to search through in the database (in this case it's `text`), a query that includes our search term, and the number of retrieved results to use in the generation.
"""

from rich.console import Console
from rich.panel import Panel

# Create a prompt where context from the Weaviate collection will be injected
prompt = "Explain how {text} works, using only the retrieved context."
query = "bert"

response = collection.generate.near_text(
    query=query, limit=3, grouped_task=prompt, return_properties=["text", "title"]
)

# Prettify the output using Rich
console = Console()

console.print(
    Panel(f"{prompt}".replace("{text}", query), title="Prompt", border_style="bold red")
)
console.print(
    Panel(response.generated, title="Generated Content", border_style="bold green")
)
# Output:
#   [1;31m╭─[0m[1;31m───────────────────────────────────────────────────[0m[1;31m Prompt [0m[1;31m────────────────────────────────────────────────────[0m[1;31m─╮[0m

#   [1;31m│[0m Explain how bert works, using only the retrieved context.                                                       [1;31m│[0m

#   [1;31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m

#   [1;32m╭─[0m[1;32m──────────────────────────────────────────────[0m[1;32m Generated Content [0m[1;32m──────────────────────────────────────────────[0m[1;32m─╮[0m

#   [1;32m│[0m BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation    [1;32m│[0m

#   [1;32m│[0m model designed to pretrain deep bidirectional representations from unlabeled text. It conditions on both left   [1;32m│[0m

#   [1;32m│[0m and right context in all layers, unlike traditional left-to-right or right-to-left language models. This        [1;32m│[0m

#   [1;32m│[0m pre-training involves two unsupervised tasks. The pre-trained BERT model can then be fine-tuned with just one   [1;32m│[0m

#   [1;32m│[0m additional output layer to create state-of-the-art models for various tasks, such as question answering and     [1;32m│[0m

#   [1;32m│[0m language inference, without needing substantial task-specific architecture modifications. A distinctive feature [1;32m│[0m

#   [1;32m│[0m of BERT is its unified architecture across different tasks.                                                     [1;32m│[0m

#   [1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m


# Create a prompt where context from the Weaviate collection will be injected
prompt = "Explain how {text} works, using only the retrieved context."
query = "a generative adversarial net"

response = collection.generate.near_text(
    query=query, limit=3, grouped_task=prompt, return_properties=["text", "title"]
)

# Prettify the output using Rich
console = Console()

console.print(
    Panel(f"{prompt}".replace("{text}", query), title="Prompt", border_style="bold red")
)
console.print(
    Panel(response.generated, title="Generated Content", border_style="bold green")
)
# Output:
#   [1;31m╭─[0m[1;31m───────────────────────────────────────────────────[0m[1;31m Prompt [0m[1;31m────────────────────────────────────────────────────[0m[1;31m─╮[0m

#   [1;31m│[0m Explain how a generative adversarial net works, using only the retrieved context.                               [1;31m│[0m

#   [1;31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m

#   [1;32m╭─[0m[1;32m──────────────────────────────────────────────[0m[1;32m Generated Content [0m[1;32m──────────────────────────────────────────────[0m[1;32m─╮[0m

#   [1;32m│[0m Generative Adversarial Nets (GANs) operate within an adversarial framework where two models are trained         [1;32m│[0m

#   [1;32m│[0m simultaneously: a generative model (G) and a discriminative model (D). The generative model aims to capture the [1;32m│[0m

#   [1;32m│[0m data distribution and generate samples that mimic real data, while the discriminative model's task is to        [1;32m│[0m

#   [1;32m│[0m distinguish between samples from the real data and those generated by G. This setup is akin to a game where the [1;32m│[0m

#   [1;32m│[0m generative model acts like counterfeiters trying to produce indistinguishable fake currency, and the            [1;32m│[0m

#   [1;32m│[0m discriminative model acts like the police trying to detect these counterfeits.                                  [1;32m│[0m

#   [1;32m│[0m                                                                                                                 [1;32m│[0m

#   [1;32m│[0m The training process involves a minimax two-player game where G tries to maximize the probability of D making a [1;32m│[0m

#   [1;32m│[0m mistake, while D tries to minimize it. When both models are defined by multilayer perceptrons, they can be      [1;32m│[0m

#   [1;32m│[0m trained using backpropagation without the need for Markov chains or approximate inference networks. The         [1;32m│[0m

#   [1;32m│[0m ultimate goal is for G to perfectly replicate the training data distribution, making D's output equal to 1/2    [1;32m│[0m

#   [1;32m│[0m everywhere, indicating it cannot distinguish between real and generated data. This framework allows for         [1;32m│[0m

#   [1;32m│[0m specific training algorithms and optimization techniques, such as backpropagation and dropout, to be            [1;32m│[0m

#   [1;32m│[0m effectively utilized.                                                                                           [1;32m│[0m

#   [1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m


"""
We can see that our RAG pipeline performs relatively well for simple queries, especially given the small size of the dataset. Scaling this method for converting a larger sample of PDFs would require more compute (GPUs) and a more advanced deployment of Weaviate (like Docker, Kubernetes, or Weaviate Cloud). For more information on available Weaviate configurations, check out the [documetation](https://weaviate.io/developers/weaviate/starter-guides/which-weaviate).
"""


================================================
File: docs/examples/rapidocr_with_custom_models.py
================================================
import os

from huggingface_hub import snapshot_download

from docling.datamodel.pipeline_options import PdfPipelineOptions, RapidOcrOptions
from docling.document_converter import (
    ConversionResult,
    DocumentConverter,
    InputFormat,
    PdfFormatOption,
)


def main():
    # Source document to convert
    source = "https://arxiv.org/pdf/2408.09869v4"

    # Download RappidOCR models from HuggingFace
    print("Downloading RapidOCR models")
    download_path = snapshot_download(repo_id="SWHL/RapidOCR")

    # Setup RapidOcrOptions for english detection
    det_model_path = os.path.join(
        download_path, "PP-OCRv4", "en_PP-OCRv3_det_infer.onnx"
    )
    rec_model_path = os.path.join(
        download_path, "PP-OCRv4", "ch_PP-OCRv4_rec_server_infer.onnx"
    )
    cls_model_path = os.path.join(
        download_path, "PP-OCRv3", "ch_ppocr_mobile_v2.0_cls_train.onnx"
    )
    ocr_options = RapidOcrOptions(
        det_model_path=det_model_path,
        rec_model_path=rec_model_path,
        cls_model_path=cls_model_path,
    )

    pipeline_options = PdfPipelineOptions(
        ocr_options=ocr_options,
    )

    # Convert the document
    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            ),
        },
    )

    conversion_result: ConversionResult = converter.convert(source=source)
    doc = conversion_result.document
    md = doc.export_to_markdown()
    print(md)


if __name__ == "__main__":
    main()


================================================
File: docs/examples/retrieval_qdrant.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
<a href="https://colab.research.google.com/github/DS4SD/docling/blob/main/docs/examples/hybrid_rag_qdrant
.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

"""
# Retrieval with Qdrant
"""

"""
| Step | Tech | Execution | 
| --- | --- | --- |
| Embedding | FastEmbed | 💻 Local |
| Vector store | Qdrant | 💻 Local |
"""

"""
## Overview
"""

"""
This example demonstrates using Docling with [Qdrant](https://qdrant.tech/) to perform a hybrid search across your documents using dense and sparse vectors.

We'll chunk the documents using Docling before adding them to a Qdrant collection. By limiting the length of the chunks, we can preserve the meaning in each vector embedding.
"""

"""
## Setup
"""

"""
- 👉 Qdrant client uses [FastEmbed](https://github.com/qdrant/fastembed) to generate vector embeddings. You can install the `fastembed-gpu` package if you've got the hardware to support it.
"""

%pip install --no-warn-conflicts -q qdrant-client docling fastembed
# Output:
#   Note: you may need to restart the kernel to use updated packages.


"""
Let's import all the classes we'll be working with.
"""

from qdrant_client import QdrantClient

from docling.chunking import HybridChunker
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter

"""
- For Docling, we'll set the  allowed formats to HTML since we'll only be working with webpages in this tutorial.
- If we set a sparse model, Qdrant client will fuse the dense and sparse results using RRF. [Reference](https://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/).
"""

COLLECTION_NAME = "docling"

doc_converter = DocumentConverter(allowed_formats=[InputFormat.HTML])
client = QdrantClient(location=":memory:")
# The :memory: mode is a Python imitation of Qdrant's APIs for prototyping and CI.
# For production deployments, use the Docker image: docker run -p 6333:6333 qdrant/qdrant
# client = QdrantClient(location="http://localhost:6333")

client.set_model("sentence-transformers/all-MiniLM-L6-v2")
client.set_sparse_model("Qdrant/bm25")
# Output:
#   /Users/pva/work/github.com/DS4SD/docling/.venv/lib/python3.12/site-packages/huggingface_hub/utils/tqdm.py:155: UserWarning: Cannot enable progress bars: environment variable `HF_HUB_DISABLE_PROGRESS_BARS=1` is set and has priority.

#     warnings.warn(


"""
We can now download and chunk the document using Docling. For demonstration, we'll use an article about chunking strategies :)
"""

result = doc_converter.convert(
    "https://www.sagacify.com/news/a-guide-to-chunking-strategies-for-retrieval-augmented-generation-rag"
)
documents, metadatas = [], []
for chunk in HybridChunker().chunk(result.document):
    documents.append(chunk.text)
    metadatas.append(chunk.meta.export_json_dict())

"""
Let's now upload the documents to Qdrant.

- The `add()` method batches the documents and uses FastEmbed to generate vector embeddings on our machine.
"""

_ = client.add(
    collection_name=COLLECTION_NAME,
    documents=documents,
    metadata=metadatas,
    batch_size=64,
)

"""
## Retrieval
"""

points = client.query(
    collection_name=COLLECTION_NAME,
    query_text="Can I split documents?",
    limit=10,
)

for i, point in enumerate(points):
    print(f"=== {i} ===")
    print(point.document)
    print()
# Output:
#   === 0 ===

#   Have you ever wondered how we, humans, would chunk? Here's a breakdown of a possible way a human would process a new document:

#   1. We start at the top of the document, treating the first part as a chunk.

#      2. We continue down the document, deciding if a new sentence or piece of information belongs with the first chunk or should start a new one.

#       3. We keep this up until we reach the end of the document.

#   The ultimate dream? Having an agent do this for you. But slow down! This approach is still being tested and isn't quite ready for the big leagues due to the time it takes to process multiple LLM calls and the cost of those calls. There's no implementation available in public libraries just yet. However, Greg Kamradt has his version available here.

#   

#   === 1 ===

#   Document Specific Chunking is a strategy that respects the document's structure. Rather than using a set number of characters or a recursive process, it creates chunks that align with the logical sections of the document, like paragraphs or subsections. This approach maintains the original author's organization of content and helps keep the text coherent. It makes the retrieved information more relevant and useful, particularly for structured documents with clearly defined sections.

#   Document Specific Chunking can handle a variety of document formats, such as:

#   Markdown

#   HTML

#   Python

#   etc

#   Here we’ll take Markdown as our example and use a modified version of our first sample text:

#   ‍

#   The result is the following:

#   You can see here that with a chunk size of 105, the Markdown structure of the document is taken into account, and the chunks thus preserve the semantics of the text!

#   

#   === 2 ===

#   And there you have it! These chunking strategies are like a personal toolbox when it comes to implementing Retrieval Augmented Generation. They're a ton of ways to slice and dice text, each with its unique features and quirks. This variety gives you the freedom to pick the strategy that suits your project best, allowing you to tailor your approach to perfectly fit the unique needs of your work.

#   To put these strategies into action, there's a whole array of tools and libraries at your disposal. For example, llama_index is a fantastic tool that lets you create document indices and retrieve chunked documents. Let's not forget LangChain, another remarkable tool that makes implementing chunking strategies a breeze, particularly when dealing with multi-language data. Diving into these tools and understanding how they can work in harmony with the chunking strategies we've discussed is a crucial part of mastering Retrieval Augmented Generation.

#   By the way, if you're eager to experiment with your own examples using the chunking visualisation tool featured in this blog, feel free to give it a try! You can access it right here. Enjoy, and happy chunking! 😉

#   

#   === 3 ===

#   Retrieval Augmented Generation (RAG) has been a hot topic in understanding, interpreting, and generating text with AI for the last few months. It's like a wonderful union of retrieval-based and generative models, creating a playground for researchers, data scientists, and natural language processing enthusiasts, like you and me.

#   To truly control the results produced by our RAG, we need to understand chunking strategies and their role in the process of retrieving and generating text. Indeed, each chunking strategy enhances RAG's effectiveness in its unique way.

#   The goal of chunking is, as its name says, to chunk the information into multiple smaller pieces in order to store it in a more efficient and meaningful way. This allows the retrieval to capture pieces of information that are more related to the question at hand, and the generation to be more precise, but also less costly, as only a part of a document will be included in the LLM prompt, instead of the whole document.

#   Let's explore some chunking strategies together.

#   The methods mentioned in the article you're about to read usually make use of two key parameters. First, we have [chunk_size]— which controls the size of your text chunks. Then there's [chunk_overlap], which takes care of how much text overlaps between one chunk and the next.

#   

#   === 4 ===

#   Semantic Chunking considers the relationships within the text. It divides the text into meaningful, semantically complete chunks. This approach ensures the information's integrity during retrieval, leading to a more accurate and contextually appropriate outcome.

#   Semantic chunking involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together.

#   By focusing on the text's meaning and context, Semantic Chunking significantly enhances the quality of retrieval. It's a top-notch choice when maintaining the semantic integrity of the text is vital.

#   However, this method does require more effort and is notably slower than the previous ones.

#   On our example text, since it is quite short and does not expose varied subjects, this method would only generate a single chunk.

#   

#   === 5 ===

#   Language models used in the rest of your possible RAG pipeline have a token limit, which should not be exceeded. When dividing your text into chunks, it's advisable to count the number of tokens. Plenty of tokenizers are available. To ensure accuracy, use the same tokenizer for counting tokens as the one used in the language model.

#   Consequently, there are also splitters available for this purpose.

#   For instance, by using the [SpacyTextSplitter] from LangChain, the following chunks are created:

#   ‍

#   

#   === 6 ===

#   First things first, we have Character Chunking. This strategy divides the text into chunks based on a fixed number of characters. Its simplicity makes it a great starting point, but it can sometimes disrupt the text's flow, breaking sentences or words in unexpected places. Despite its limitations, it's a great stepping stone towards more advanced methods.

#   Now let’s see that in action with an example. Imagine a text that reads:

#   If we decide to set our chunk size to 100 and no chunk overlap, we'd end up with the following chunks. As you can see, Character Chunking can lead to some intriguing, albeit sometimes nonsensical, results, cutting some of the sentences in their middle.

#   By choosing a smaller chunk size,  we would obtain more chunks, and by setting a bigger chunk overlap, we could obtain something like this:

#   ‍

#   Also, by default this method creates chunks character by character based on the empty character [’ ’]. But you can specify a different one in order to chunk on something else, even a complete word! For instance, by specifying [' '] as the separator, you can avoid cutting words in their middle.

#   

#   === 7 ===

#   Next, let's take a look at Recursive Character Chunking. Based on the basic concept of Character Chunking, this advanced version takes it up a notch by dividing the text into chunks until a certain condition is met, such as reaching a minimum chunk size. This method ensures that the chunking process aligns with the text's structure, preserving more meaning. Its adaptability makes Recursive Character Chunking great for texts with varied structures.

#   Again, let’s use the same example in order to illustrate this method. With a chunk size of 100, and the default settings for the other parameters, we obtain the following chunks:

#   



================================================
File: docs/examples/run_md.py
================================================
import json
import logging
import os
from pathlib import Path

import yaml

from docling.backend.md_backend import MarkdownDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

_log = logging.getLogger(__name__)


def main():
    input_paths = [Path("README.md")]

    for path in input_paths:
        in_doc = InputDocument(
            path_or_stream=path,
            format=InputFormat.PDF,
            backend=MarkdownDocumentBackend,
        )
        mdb = MarkdownDocumentBackend(in_doc=in_doc, path_or_stream=path)
        document = mdb.convert()

        out_path = Path("scratch")
        print(
            f"Document {path} converted." f"\nSaved markdown output to: {str(out_path)}"
        )

        # Export Docling document format to markdowndoc:
        fn = os.path.basename(path)

        with (out_path / f"{fn}.md").open("w") as fp:
            fp.write(document.export_to_markdown())

        with (out_path / f"{fn}.json").open("w") as fp:
            fp.write(json.dumps(document.export_to_dict()))

        with (out_path / f"{fn}.yaml").open("w") as fp:
            fp.write(yaml.safe_dump(document.export_to_dict()))


if __name__ == "__main__":
    main()


================================================
File: docs/examples/run_with_accelerator.py
================================================
from pathlib import Path

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    PdfPipelineOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.datamodel.settings import settings
from docling.document_converter import DocumentConverter, PdfFormatOption


def main():
    input_doc = Path("./tests/data/pdf/2206.01062.pdf")

    # Explicitly set the accelerator
    # accelerator_options = AcceleratorOptions(
    #     num_threads=8, device=AcceleratorDevice.AUTO
    # )
    accelerator_options = AcceleratorOptions(
        num_threads=8, device=AcceleratorDevice.CPU
    )
    # accelerator_options = AcceleratorOptions(
    #     num_threads=8, device=AcceleratorDevice.MPS
    # )
    # accelerator_options = AcceleratorOptions(
    #     num_threads=8, device=AcceleratorDevice.CUDA
    # )

    # easyocr doesnt support cuda:N allocation, defaults to cuda:0
    # accelerator_options = AcceleratorOptions(num_threads=8, device="cuda:1")

    pipeline_options = PdfPipelineOptions()
    pipeline_options.accelerator_options = accelerator_options
    pipeline_options.do_ocr = True
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            )
        }
    )

    # Enable the profiling to measure the time spent
    settings.debug.profile_pipeline_timings = True

    # Convert the document
    conversion_result = converter.convert(input_doc)
    doc = conversion_result.document

    # List with total time per document
    doc_conversion_secs = conversion_result.timings["pipeline_total"].times

    md = doc.export_to_markdown()
    print(md)
    print(f"Conversion secs: {doc_conversion_secs}")


if __name__ == "__main__":
    main()


================================================
File: docs/examples/run_with_formats.py
================================================
import json
import logging
from pathlib import Path

import yaml

from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
    WordFormatOption,
)
from docling.pipeline.simple_pipeline import SimplePipeline
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline

_log = logging.getLogger(__name__)


def main():
    input_paths = [
        Path("README.md"),
        Path("tests/data/html/wiki_duck.html"),
        Path("tests/data/docx/word_sample.docx"),
        Path("tests/data/docx/lorem_ipsum.docx"),
        Path("tests/data/pptx/powerpoint_sample.pptx"),
        Path("tests/data/2305.03393v1-pg9-img.png"),
        Path("tests/data/pdf/2206.01062.pdf"),
        Path("tests/data/asciidoc/test_01.asciidoc"),
    ]

    ## for defaults use:
    # doc_converter = DocumentConverter()

    ## to customize use:

    doc_converter = (
        DocumentConverter(  # all of the below is optional, has internal defaults.
            allowed_formats=[
                InputFormat.PDF,
                InputFormat.IMAGE,
                InputFormat.DOCX,
                InputFormat.HTML,
                InputFormat.PPTX,
                InputFormat.ASCIIDOC,
                InputFormat.CSV,
                InputFormat.MD,
            ],  # whitelist formats, non-matching files are ignored.
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_cls=StandardPdfPipeline, backend=PyPdfiumDocumentBackend
                ),
                InputFormat.DOCX: WordFormatOption(
                    pipeline_cls=SimplePipeline  # , backend=MsWordDocumentBackend
                ),
            },
        )
    )

    conv_results = doc_converter.convert_all(input_paths)

    for res in conv_results:
        out_path = Path("scratch")
        print(
            f"Document {res.input.file.name} converted."
            f"\nSaved markdown output to: {str(out_path)}"
        )
        _log.debug(res.document._export_to_indented_text(max_text_len=16))
        # Export Docling document format to markdowndoc:
        with (out_path / f"{res.input.file.stem}.md").open("w") as fp:
            fp.write(res.document.export_to_markdown())

        with (out_path / f"{res.input.file.stem}.json").open("w") as fp:
            fp.write(json.dumps(res.document.export_to_dict()))

        with (out_path / f"{res.input.file.stem}.yaml").open("w") as fp:
            fp.write(yaml.safe_dump(res.document.export_to_dict()))


if __name__ == "__main__":
    main()


================================================
File: docs/examples/tesseract_lang_detection.py
================================================
from pathlib import Path

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    PdfPipelineOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption


def main():
    input_doc = Path("./tests/data/pdf/2206.01062.pdf")

    # Set lang=["auto"] with a tesseract OCR engine: TesseractOcrOptions, TesseractCliOcrOptions
    # ocr_options = TesseractOcrOptions(lang=["auto"])
    ocr_options = TesseractCliOcrOptions(lang=["auto"])

    pipeline_options = PdfPipelineOptions(
        do_ocr=True, force_full_page_ocr=True, ocr_options=ocr_options
    )

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            )
        }
    )

    doc = converter.convert(input_doc).document
    md = doc.export_to_markdown()
    print(md)


if __name__ == "__main__":
    main()


================================================
File: docs/examples/translate.py
================================================
import logging
import time
from pathlib import Path

from docling_core.types.doc import ImageRefMode, PictureItem, TableItem, TextItem

from docling.datamodel.base_models import FigureElement, InputFormat, Table
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

_log = logging.getLogger(__name__)

IMAGE_RESOLUTION_SCALE = 2.0


# FIXME: put in your favorite translation code ....
def translate(text: str, src: str = "en", dest: str = "de"):

    _log.warning("!!! IMPLEMENT HERE YOUR FAVORITE TRANSLATION CODE!!!")
    # from googletrans import Translator

    # Initialize the translator
    # translator = Translator()

    # Translate text from English to German
    # text = "Hello, how are you?"
    # translated = translator.translate(text, src="en", dest="de")

    return text


def main():
    logging.basicConfig(level=logging.INFO)

    input_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    output_dir = Path("scratch")

    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter
    # will destroy them for cleaning up memory.
    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.
    # scale=1 correspond of a standard 72 DPI image
    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched
    # with the image field
    pipeline_options = PdfPipelineOptions()
    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
    pipeline_options.generate_page_images = True
    pipeline_options.generate_picture_images = True

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    start_time = time.time()

    conv_res = doc_converter.convert(input_doc_path)
    conv_doc = conv_res.document

    # Save markdown with embedded pictures in original text
    md_filename = output_dir / f"{doc_filename}-with-images-orig.md"
    conv_doc.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)

    for element, _level in conv_res.document.iterate_items():
        if isinstance(element, TextItem):
            element.orig = element.text
            element.text = translate(text=element.text)

        elif isinstance(element, TableItem):
            for cell in element.data.table_cells:
                cell.text = translate(text=element.text)

    # Save markdown with embedded pictures in translated text
    md_filename = output_dir / f"{doc_filename}-with-images-translated.md"
    conv_doc.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)


================================================
File: docs/integrations/bee.md
================================================
Docling is available as an extraction backend in the [Bee][github] framework.

- 💻 [Bee GitHub][github]
- 📖 [Bee docs][docs]
- 📦 [Bee NPM][package]

[github]: https://github.com/i-am-bee
[docs]: https://i-am-bee.github.io/bee-agent-framework/
[package]: https://www.npmjs.com/package/bee-agent-framework


================================================
File: docs/integrations/cloudera.md
================================================
Docling is available in [Cloudera](https://www.cloudera.com/) through the *RAG Studio*
Accelerator for Machine Learning Projects (AMP).

- 💻 [RAG Studio AMP GitHub][github]

[github]: https://github.com/cloudera/CML_AMP_RAG_Studio


================================================
File: docs/integrations/crewai.md
================================================
Docling is available in [CrewAI](https://www.crewai.com/) as the `CrewDoclingSource`
knowledge source.

- 💻 [Crew AI GitHub][github]
- 📖 [Crew AI knowledge docs][docs]
- 📦 [Crew AI PyPI][package]

[github]: https://github.com/crewAIInc/crewAI/
[docs]: https://docs.crewai.com/concepts/knowledge
[package]: https://pypi.org/project/crewai/


================================================
File: docs/integrations/data_prep_kit.md
================================================
Docling is used by the [Data Prep Kit](https://ibm.github.io/data-prep-kit/) open-source toolkit for preparing unstructured data for LLM application development ranging from laptop scale to datacenter scale.

## Components
### PDF ingestion to Parquet
- 💻 [PDF-to-Parquet GitHub](https://github.com/IBM/data-prep-kit/tree/dev/transforms/language/pdf2parquet)
- 📖 [PDF-to-Parquet docs](https://ibm.github.io/data-prep-kit/transforms/language/pdf2parquet/python/)

### Document chunking
- 💻 [Doc Chunking GitHub](https://github.com/IBM/data-prep-kit/tree/dev/transforms/language/doc_chunk)
- 📖 [Doc Chunking docs](https://ibm.github.io/data-prep-kit/transforms/language/doc_chunk/python/)


================================================
File: docs/integrations/docetl.md
================================================
Docling is available as a file conversion method in [DocETL](https://github.com/ucbepic/docetl):

- 💻 [DocETL GitHub][github]
- 📖 [DocETL docs][docs]
- 📦 [DocETL PyPI][pypi]

[github]: https://github.com/ucbepic/docetl
[docs]: https://ucbepic.github.io/docetl/
[pypi]: https://pypi.org/project/docetl/


================================================
File: docs/integrations/haystack.md
================================================
Docling is available as a converter in [Haystack](https://haystack.deepset.ai/):

- 📖 [Docling Haystack integration docs][docs]
- 💻 [Docling Haystack integration GitHub][github]
- 🧑🏽‍🍳 [Docling Haystack integration example][example]
- 📦 [Docling Haystack integration PyPI][pypi]

[github]: https://github.com/DS4SD/docling-haystack
[docs]: https://haystack.deepset.ai/integrations/docling
[pypi]: https://pypi.org/project/docling-haystack
[example]: ../examples/rag_haystack.ipynb


================================================
File: docs/integrations/index.md
================================================
Use the navigation on the left to browse through Docling integrations with popular frameworks and tools.


<p align="center">
  <img loading="lazy" alt="Docling" src="../assets/docling_ecosystem.png" width="100%" />
</p>


================================================
File: docs/integrations/instructlab.md
================================================
Docling is powering document processing in [InstructLab][home],
enabling users to unlock the knowledge hidden in documents and present it to
InstructLab's fine-tuning for aligning AI models to the user's specific data.

More details can be found in this [blog post][blog].

- 🏠 [InstructLab home][home]
- 💻 [InstructLab GitHub][github]
- 🧑🏻‍💻 [InstructLab UI][ui]
- 📖 [InstructLab docs][docs]

[home]: https://instructlab.ai
[github]: https://github.com/instructlab
[ui]: https://ui.instructlab.ai/
[docs]: https://docs.instructlab.ai/
[blog]: https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai


================================================
File: docs/integrations/kotaemon.md
================================================
Docling is available in [Kotaemon](https://cinnamon.github.io/kotaemon/) as the `DoclingReader` loader:

- 💻 [Kotaemon GitHub][github]
- 📖 [DoclingReader docs][docs]
- ⚙️ [Docling setup in Kotaemon][setup]

[github]: https://github.com/Cinnamon/kotaemon
[docs]: https://cinnamon.github.io/kotaemon/reference/loaders/docling_loader/
[setup]: https://cinnamon.github.io/kotaemon/development/?h=docling#setup-multimodal-document-parsing-ocr-table-parsing-figure-extraction


================================================
File: docs/integrations/langchain.md
================================================
Docling is available as an official [LangChain](https://python.langchain.com/) extension.

To get started, check out the [step-by-step guide in LangChain][guide].

- 📖 [LangChain Docling integration docs][docs]
- 💻 [LangChain Docling integration GitHub][github]
- 🧑🏽‍🍳 [LangChain Docling integration example][example]
- 📦 [LangChain Docling integration PyPI][pypi]

[docs]: https://python.langchain.com/docs/integrations/providers/docling/
[github]: https://github.com/DS4SD/docling-langchain
[guide]: https://python.langchain.com/docs/integrations/document_loaders/docling/
[example]: ../examples/rag_langchain.ipynb
[pypi]: https://pypi.org/project/langchain-docling/


================================================
File: docs/integrations/llamaindex.md
================================================
Docling is available as an official [LlamaIndex](https://docs.llamaindex.ai/) extension.

To get started, check out the [step-by-step guide in LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/data_connectors/DoclingReaderDemo/).

## Components

### Docling Reader

Reads document files and uses Docling to populate LlamaIndex `Document` objects — either serializing Docling's data model (losslessly, e.g. as JSON) or exporting to a simplified format (lossily, e.g. as Markdown).

- 💻 [Docling Reader GitHub](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-docling)
- 📖 [Docling Reader docs](https://docs.llamaindex.ai/en/stable/api_reference/readers/docling/)
- 📦 [Docling Reader PyPI](https://pypi.org/project/llama-index-readers-docling/)

### Docling Node Parser

Reads LlamaIndex `Document` objects populated in Docling's format by Docling Reader and, using its knowledge of the Docling format, parses them to LlamaIndex `Node` objects for downstream usage in LlamaIndex applications, e.g. as chunks for embedding.

- 💻 [Docling Node Parser GitHub](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/node_parser/llama-index-node-parser-docling)
- 📖 [Docling Node Parser docs](https://docs.llamaindex.ai/en/stable/api_reference/node_parser/docling/)
- 📦 [Docling Node Parser PyPI](https://pypi.org/project/llama-index-node-parser-docling/)


================================================
File: docs/integrations/nvidia.md
================================================
Docling is powering the NVIDIA *PDF to Podcast* agentic AI blueprint:

- [🏠 PDF to Podcast home](https://build.nvidia.com/nvidia/pdf-to-podcast)
- [💻 PDF to Podcast GitHub](https://github.com/NVIDIA-AI-Blueprints/pdf-to-podcast)
- [📣 PDF to Podcast announcement](https://nvidianews.nvidia.com/news/nvidia-launches-ai-foundation-models-for-rtx-ai-pcs)
- [✍️ PDF to Podcast blog post](https://blogs.nvidia.com/blog/agentic-ai-blueprints/)


================================================
File: docs/integrations/opencontracts.md
================================================
Docling is available an ingestion engine for [OpenContracts](https://github.com/JSv4/OpenContracts), allowing you to use Docling's OCR engine(s), chunker(s), labels, etc. and load them into a platform supporting bulk data extraction, text annotating, and question-answering:

- 💻 [OpenContracts GitHub](https://github.com/JSv4/OpenContracts)
- 📖 [OpenContracts Docs](https://jsv4.github.io/OpenContracts/)
- ▶️ [OpenContracts x Docling PDF annotation screen capture](https://github.com/JSv4/OpenContracts/blob/main/docs/assets/images/gifs/PDF%20Annotation%20Flow.gif)


================================================
File: docs/integrations/prodigy.md
================================================
Docling is available in [Prodigy][home] as a [Prodigy-PDF plugin][plugin] recipe.

More details can be found in this [blog post][blog].

- 🌐 [Prodigy home][home]
- 🔌 [Prodigy-PDF plugin][plugin]
- 🧑🏽‍🍳 [pdf-spans.manual recipe][recipe]

[home]: https://prodi.gy/
[plugin]: https://prodi.gy/docs/plugins#pdf
[recipe]: https://prodi.gy/docs/plugins#pdf-spans.manual
[blog]: https://explosion.ai/blog/pdfs-nlp-structured-data


================================================
File: docs/integrations/rhel_ai.md
================================================
Docling is powering document processing in [Red Hat Enterprise Linux AI (RHEL AI)](https://rhel.ai),
enabling users to unlock the knowledge hidden in documents and present it to
InstructLab's fine-tuning for aligning AI models to the user's specific data.

- 📣 [RHEL AI 1.3 announcement](https://www.redhat.com/en/about/press-releases/red-hat-delivers-next-wave-gen-ai-innovation-new-red-hat-enterprise-linux-ai-capabilities)
- ✍️ RHEL blog posts:
    - [RHEL AI 1.3 Docling context aware chunking: What you need to know](https://www.redhat.com/en/blog/rhel-13-docling-context-aware-chunking-what-you-need-know)
    - [Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai)


================================================
File: docs/integrations/spacy.md
================================================
Docling is available in [spaCy](https://spacy.io/) as the *spaCy Layout* plugin.

More details can be found in this [blog post][blog].

- 💻 [SpacyLayout GitHub][github]
- 📖 [SpacyLayout docs][docs]
- 📦 [SpacyLayout PyPI][pypi]

[github]: https://github.com/explosion/spacy-layout
[docs]: https://github.com/explosion/spacy-layout?tab=readme-ov-file#readme
[pypi]: https://pypi.org/project/spacy-layout/
[blog]: https://explosion.ai/blog/pdfs-nlp-structured-data


================================================
File: docs/integrations/txtai.md
================================================
Docling is available as a text extraction backend for [txtai](https://neuml.github.io/txtai/).

- 💻 [txtai GitHub][github]
- 📖 [txtai docs][docs]
- 📖 [txtai Docling backend][integration_docs]

[github]: https://github.com/neuml/txtai
[docs]: https://neuml.github.io/txtai
[integration_docs]: https://neuml.github.io/txtai/pipeline/data/filetohtml/#docling


================================================
File: docs/integrations/vectara.md
================================================
Docling is available as a document parser in [Vectara](https://www.vectara.com/).

- 💻 [Vectara GitHub org](https://github.com/vectara)
    - [vectara-ingest GitHub repo](https://github.com/vectara/vectara-ingest)
- 📖 [Vectara docs](https://docs.vectara.com/)


================================================
File: docs/integrations/.template.md
================================================
Docling is available as a plugin for [EXAMPLE](https://example.com).

- 💻 [GitHub][github]
- 📖 [Docs][docs]
- 📦 [PyPI][pypi]

[github]: https://github.com/...
[docs]: https://...
[pypi]: https://pypi.org/project/...


================================================
File: docs/overrides/main.html
================================================
{% extends "base.html" %}

{#
{% block announce %}
  <p>🎉 Docling has gone v2! <a href="{{ 'v2' | url }}">Check out</a> what's new and how to get started!</p>
{% endblock %}
#}


================================================
File: docs/reference/cli.md
================================================
# CLI reference

This page provides documentation for our command line tools.

::: mkdocs-click
    :module: docling.cli.main
    :command: click_app
    :prog_name: docling
    :style: table


================================================
File: docs/reference/docling_document.md
================================================
# Docling Document

This is an automatic generated API reference of the DoclingDocument type.

::: docling_core.types.doc
    handler: python
    options:
        members:
            - DoclingDocument
            - DocumentOrigin
            - DocItem
            - DocItemLabel
            - ProvenanceItem
            - GroupItem
            - GroupLabel
            - NodeItem
            - PageItem
            - FloatingItem
            - TextItem
            - TableItem
            - TableCell
            - TableData
            - TableCellLabel
            - KeyValueItem
            - SectionHeaderItem
            - PictureItem
            - ImageRef
            - PictureClassificationClass
            - PictureClassificationData
            - RefItem
            - BoundingBox
            - CoordOrigin
            - ImageRefMode
            - Size
        docstring_style: sphinx
        show_if_no_docstring: true
        show_submodules: true
        docstring_section_style: list
        filters: ["!^_"]
        heading_level: 2
        show_root_toc_entry: true
        inherited_members: true
        merge_init_into_class: true
        separate_signature: true
        show_root_heading: true
        show_root_full_path: false
        show_signature_annotations: true
        show_source: false
        show_symbol_type_heading: true
        show_symbol_type_toc: true
        show_labels: false
        signature_crossrefs: true
        summary: true


================================================
File: docs/reference/document_converter.md
================================================
# Document converter

This is an automatic generated API reference of the main components of Docling.

::: docling.document_converter
    handler: python
    options:
        members:
            - DocumentConverter
            - ConversionResult
            - ConversionStatus
            - FormatOption
            - InputFormat
            - PdfFormatOption
            - ImageFormatOption
            - StandardPdfPipeline
            - WordFormatOption
            - PowerpointFormatOption
            - MarkdownFormatOption
            - AsciiDocFormatOption
            - HTMLFormatOption
            - SimplePipeline
        show_if_no_docstring: true
        show_submodules: true
        docstring_section_style: list
        filters: ["!^_"]
        heading_level: 2
        inherited_members: true
        merge_init_into_class: true
        separate_signature: true
        show_root_heading: true
        show_root_full_path: false
        show_signature_annotations: true
        show_source: false
        show_symbol_type_heading: true
        show_symbol_type_toc: true
        signature_crossrefs: true
        summary: true


================================================
File: docs/reference/pipeline_options.md
================================================
# Pipeline options

Pipeline options allow to customize the execution of the models during the conversion pipeline.
This includes options for the OCR engines, the table model as well as enrichment options which
can be enabled with `do_xyz = True`.


This is an automatic generated API reference of the all the pipeline options available in Docling.


::: docling.datamodel.pipeline_options
    handler: python
    options:
        show_if_no_docstring: true
        show_submodules: true
        docstring_section_style: list
        filters: ["!^_"]
        heading_level: 2
        inherited_members: true
        merge_init_into_class: true
        separate_signature: true
        show_root_heading: true
        show_root_full_path: false
        show_signature_annotations: true
        show_source: false
        show_symbol_type_heading: true
        show_symbol_type_toc: true
        signature_crossrefs: true
        summary: true

<!-- ::: docling.document_converter.DocumentConverter
    handler: python
    options:
        show_if_no_docstring: true
        show_submodules: true -->
        


================================================
File: docs/stylesheets/extra.css
================================================
[data-md-color-scheme="default"] .md-banner a {
    color: #5e8bde;
}


================================================
File: tests/test_backend_asciidoc.py
================================================
import glob
import os
from pathlib import Path

from docling.backend.asciidoc_backend import AsciiDocBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument


def _get_backend(fname):
    in_doc = InputDocument(
        path_or_stream=fname,
        format=InputFormat.ASCIIDOC,
        backend=AsciiDocBackend,
    )

    doc_backend = in_doc._backend
    return doc_backend


def test_asciidocs_examples():

    fnames = sorted(glob.glob("./tests/data/asciidoc/*.asciidoc"))

    for fname in fnames:
        print(f"reading {fname}")

        bname = os.path.basename(fname)
        gname = os.path.join("./tests/data/groundtruth/docling_v2/", bname + ".md")

        doc_backend = _get_backend(Path(fname))
        doc = doc_backend.convert()

        pred_itdoc = doc._export_to_indented_text(max_text_len=16)
        print("\n\n", pred_itdoc)

        pred_mddoc = doc.export_to_markdown()
        print("\n\n", pred_mddoc)

        if os.path.exists(gname):
            with open(gname, "r") as fr:
                true_mddoc = fr.read()

            # assert pred_mddoc == true_mddoc, "pred_mddoc!=true_mddoc for asciidoc"
        else:
            with open(gname, "w") as fw:
                fw.write(pred_mddoc)

            # print("\n\n", doc.export_to_markdown())

    assert True


================================================
File: tests/test_backend_csv.py
================================================
import json
import os
from pathlib import Path

from pytest import warns

from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult, DoclingDocument
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def get_csv_paths():

    # Define the directory you want to search
    directory = Path(f"./tests/data/csv/")

    # List all CSV files in the directory and its subdirectories
    return sorted(directory.rglob("*.csv"))


def get_csv_path(name: str):

    # Return the matching CSV file path
    return Path(f"./tests/data/csv/{name}.csv")


def get_converter():

    converter = DocumentConverter(allowed_formats=[InputFormat.CSV])

    return converter


def test_e2e_valid_csv_conversions():
    valid_csv_paths = get_csv_paths()
    converter = get_converter()

    for csv_path in valid_csv_paths:
        print(f"converting {csv_path}")

        gt_path = csv_path.parent.parent / "groundtruth" / "docling_v2" / csv_path.name

        conv_result: ConversionResult = converter.convert(csv_path)

        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(doc, str(gt_path) + ".json"), "export to json"


def test_e2e_invalid_csv_conversions():
    csv_too_few_columns = get_csv_path("csv-too-few-columns")
    csv_too_many_columns = get_csv_path("csv-too-many-columns")
    csv_inconsistent_header = get_csv_path("csv-inconsistent-header")
    converter = get_converter()

    print(f"converting {csv_too_few_columns}")
    with warns(UserWarning, match="Inconsistent column lengths"):
        converter.convert(csv_too_few_columns)

    print(f"converting {csv_too_many_columns}")
    with warns(UserWarning, match="Inconsistent column lengths"):
        converter.convert(csv_too_many_columns)

    print(f"converting {csv_inconsistent_header}")
    with warns(UserWarning, match="Inconsistent column lengths"):
        converter.convert(csv_inconsistent_header)


================================================
File: tests/test_backend_docling_json.py
================================================
"""Test methods in module docling.backend.json.docling_json_backend.py."""

from io import BytesIO
from pathlib import Path

import pytest
from pydantic import ValidationError

from docling.backend.json.docling_json_backend import DoclingJSONBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import DoclingDocument, InputDocument

GT_PATH: Path = Path("./tests/data/groundtruth/docling_v2/2206.01062.json")


def test_convert_valid_docling_json():
    """Test ingestion of valid Docling JSON."""
    cls = DoclingJSONBackend
    path_or_stream = GT_PATH
    in_doc = InputDocument(
        path_or_stream=path_or_stream,
        format=InputFormat.JSON_DOCLING,
        backend=cls,
    )
    backend = cls(
        in_doc=in_doc,
        path_or_stream=path_or_stream,
    )
    assert backend.is_valid()

    act_doc = backend.convert()
    act_data = act_doc.export_to_dict()

    exp_doc = DoclingDocument.load_from_json(GT_PATH)
    exp_data = exp_doc.export_to_dict()

    assert act_data == exp_data


def test_invalid_docling_json():
    """Test ingestion of invalid Docling JSON."""
    cls = DoclingJSONBackend
    path_or_stream = BytesIO(b"{}")
    in_doc = InputDocument(
        path_or_stream=path_or_stream,
        format=InputFormat.JSON_DOCLING,
        backend=cls,
        filename="foo",
    )
    backend = cls(
        in_doc=in_doc,
        path_or_stream=path_or_stream,
    )

    assert not backend.is_valid()

    with pytest.raises(ValidationError):
        backend.convert()


================================================
File: tests/test_backend_docling_parse.py
================================================
from pathlib import Path

import pytest
from docling_core.types.doc import BoundingBox

from docling.backend.docling_parse_backend import (
    DoclingParseDocumentBackend,
    DoclingParsePageBackend,
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument


@pytest.fixture
def test_doc_path():
    return Path("./tests/data/pdf/2206.01062.pdf")


def _get_backend(pdf_doc):
    in_doc = InputDocument(
        path_or_stream=pdf_doc,
        format=InputFormat.PDF,
        backend=DoclingParseDocumentBackend,
    )

    doc_backend = in_doc._backend
    return doc_backend


def test_text_cell_counts():
    pdf_doc = Path("./tests/data/pdf/redp5110_sampled.pdf")

    doc_backend = _get_backend(pdf_doc)

    for page_index in range(0, doc_backend.page_count()):
        last_cell_count = None
        for i in range(10):
            page_backend: DoclingParsePageBackend = doc_backend.load_page(0)
            cells = list(page_backend.get_text_cells())

            if last_cell_count is None:
                last_cell_count = len(cells)

            if len(cells) != last_cell_count:
                assert (
                    False
                ), "Loading page multiple times yielded non-identical text cell counts"
            last_cell_count = len(cells)


def test_get_text_from_rect(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: DoclingParsePageBackend = doc_backend.load_page(0)

    # Get the title text of the DocLayNet paper
    textpiece = page_backend.get_text_in_rect(
        bbox=BoundingBox(l=102, t=77, r=511, b=124)
    )
    ref = "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis"

    assert textpiece.strip() == ref


def test_crop_page_image(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: DoclingParsePageBackend = doc_backend.load_page(0)

    # Crop out "Figure 1" from the DocLayNet paper
    im = page_backend.get_page_image(
        scale=2, cropbox=BoundingBox(l=317, t=246, r=574, b=527)
    )
    # im.show()


def test_num_pages(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    doc_backend.page_count() == 9


================================================
File: tests/test_backend_docling_parse_v2.py
================================================
from pathlib import Path

import pytest

from docling.backend.docling_parse_v2_backend import (
    DoclingParseV2DocumentBackend,
    DoclingParseV2PageBackend,
)
from docling.datamodel.base_models import BoundingBox, InputFormat
from docling.datamodel.document import InputDocument


@pytest.fixture
def test_doc_path():
    return Path("./tests/data/pdf/2206.01062.pdf")


def _get_backend(pdf_doc):
    in_doc = InputDocument(
        path_or_stream=pdf_doc,
        format=InputFormat.PDF,
        backend=DoclingParseV2DocumentBackend,
    )

    doc_backend = in_doc._backend
    return doc_backend


def test_text_cell_counts():
    pdf_doc = Path("./tests/data/pdf/redp5110_sampled.pdf")

    doc_backend = _get_backend(pdf_doc)

    for page_index in range(0, doc_backend.page_count()):
        last_cell_count = None
        for i in range(10):
            page_backend: DoclingParseV2PageBackend = doc_backend.load_page(0)
            cells = list(page_backend.get_text_cells())

            if last_cell_count is None:
                last_cell_count = len(cells)

            if len(cells) != last_cell_count:
                assert (
                    False
                ), "Loading page multiple times yielded non-identical text cell counts"
            last_cell_count = len(cells)


def test_get_text_from_rect(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: DoclingParseV2PageBackend = doc_backend.load_page(0)

    # Get the title text of the DocLayNet paper
    textpiece = page_backend.get_text_in_rect(
        bbox=BoundingBox(l=102, t=77, r=511, b=124)
    )
    ref = "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis"

    assert textpiece.strip() == ref


def test_crop_page_image(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: DoclingParseV2PageBackend = doc_backend.load_page(0)

    # Crop out "Figure 1" from the DocLayNet paper
    im = page_backend.get_page_image(
        scale=2, cropbox=BoundingBox(l=317, t=246, r=574, b=527)
    )
    # im.show()


def test_num_pages(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    doc_backend.page_count() == 9


================================================
File: tests/test_backend_html.py
================================================
from io import BytesIO
from pathlib import Path

from docling.backend.html_backend import HTMLDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import (
    ConversionResult,
    DoclingDocument,
    InputDocument,
    SectionHeaderItem,
)
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def test_heading_levels():
    in_path = Path("tests/data/html/wiki_duck.html")
    in_doc = InputDocument(
        path_or_stream=in_path,
        format=InputFormat.HTML,
        backend=HTMLDocumentBackend,
    )
    backend = HTMLDocumentBackend(
        in_doc=in_doc,
        path_or_stream=in_path,
    )
    doc = backend.convert()

    found_lvl_2 = found_lvl_3 = False
    for item, _ in doc.iterate_items():
        if isinstance(item, SectionHeaderItem):
            if item.text == "Etymology":
                found_lvl_2 = True
                assert item.level == 2
            elif item.text == "Feeding":
                found_lvl_3 = True
                assert item.level == 3
    assert found_lvl_2 and found_lvl_3


def test_ordered_lists():
    test_set: list[tuple[bytes, str]] = []

    test_set.append(
        (
            b"<html><body><ol><li>1st item</li><li>2nd item</li></ol></body></html>",
            "1. 1st item\n2. 2nd item",
        )
    )
    test_set.append(
        (
            b'<html><body><ol start="1"><li>1st item</li><li>2nd item</li></ol></body></html>',
            "1. 1st item\n2. 2nd item",
        )
    )
    test_set.append(
        (
            b'<html><body><ol start="2"><li>1st item</li><li>2nd item</li></ol></body></html>',
            "2. 1st item\n3. 2nd item",
        )
    )
    test_set.append(
        (
            b'<html><body><ol start="0"><li>1st item</li><li>2nd item</li></ol></body></html>',
            "0. 1st item\n1. 2nd item",
        )
    )
    test_set.append(
        (
            b'<html><body><ol start="-5"><li>1st item</li><li>2nd item</li></ol></body></html>',
            "1. 1st item\n2. 2nd item",
        )
    )
    test_set.append(
        (
            b'<html><body><ol start="foo"><li>1st item</li><li>2nd item</li></ol></body></html>',
            "1. 1st item\n2. 2nd item",
        )
    )

    for pair in test_set:
        in_doc = InputDocument(
            path_or_stream=BytesIO(pair[0]),
            format=InputFormat.HTML,
            backend=HTMLDocumentBackend,
            filename="test",
        )
        backend = HTMLDocumentBackend(
            in_doc=in_doc,
            path_or_stream=BytesIO(pair[0]),
        )
        doc: DoclingDocument = backend.convert()
        assert doc
        assert doc.export_to_markdown() == pair[1]


def get_html_paths():

    # Define the directory you want to search
    directory = Path("./tests/data/html/")

    # List all HTML files in the directory and its subdirectories
    html_files = sorted(directory.rglob("*.html"))
    return html_files


def get_converter():

    converter = DocumentConverter(allowed_formats=[InputFormat.HTML])

    return converter


def test_e2e_html_conversions():

    html_paths = get_html_paths()
    converter = get_converter()

    for html_path in html_paths:
        # print(f"converting {html_path}")

        gt_path = (
            html_path.parent.parent / "groundtruth" / "docling_v2" / html_path.name
        )

        conv_result: ConversionResult = converter.convert(html_path)

        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(doc, str(gt_path) + ".json", GENERATE)


================================================
File: tests/test_backend_jats.py
================================================
import os
from io import BytesIO
from pathlib import Path

from docling_core.types.doc import DoclingDocument

from docling.datamodel.base_models import DocumentStream, InputFormat
from docling.datamodel.document import ConversionResult
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def get_pubmed_paths():
    directory = Path(os.path.dirname(__file__) + f"/data/pubmed/")
    xml_files = sorted(directory.rglob("*.xml"))
    return xml_files


def get_converter():
    converter = DocumentConverter(allowed_formats=[InputFormat.XML_JATS])
    return converter


def test_e2e_pubmed_conversions(use_stream=False):
    pubmed_paths = get_pubmed_paths()
    converter = get_converter()

    for pubmed_path in pubmed_paths:
        gt_path = (
            pubmed_path.parent.parent / "groundtruth" / "docling_v2" / pubmed_path.name
        )
        if use_stream:
            buf = BytesIO(pubmed_path.open("rb").read())
            stream = DocumentStream(name=pubmed_path.name, stream=buf)
            conv_result: ConversionResult = converter.convert(stream)
        else:
            conv_result: ConversionResult = converter.convert(pubmed_path)
        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(doc, str(gt_path) + ".json", GENERATE), "export to json"


def test_e2e_pubmed_conversions_stream():
    test_e2e_pubmed_conversions(use_stream=True)


def test_e2e_pubmed_conversions_no_stream():
    test_e2e_pubmed_conversions(use_stream=False)


================================================
File: tests/test_backend_markdown.py
================================================
from pathlib import Path

from docling.backend.md_backend import MarkdownDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

from .test_data_gen_flag import GEN_TEST_DATA


def test_convert_valid():
    fmt = InputFormat.MD
    cls = MarkdownDocumentBackend

    test_data_path = Path("tests") / "data"
    relevant_paths = sorted((test_data_path / "md").rglob("*.md"))
    assert len(relevant_paths) > 0

    for in_path in relevant_paths:
        gt_path = test_data_path / "groundtruth" / "docling_v2" / f"{in_path.name}.md"

        in_doc = InputDocument(
            path_or_stream=in_path,
            format=fmt,
            backend=cls,
        )
        backend = cls(
            in_doc=in_doc,
            path_or_stream=in_path,
        )
        assert backend.is_valid()

        act_doc = backend.convert()
        act_data = act_doc.export_to_markdown()

        if GEN_TEST_DATA:
            with open(gt_path, mode="w", encoding="utf-8") as f:
                f.write(f"{act_data}\n")
        else:
            with open(gt_path, encoding="utf-8") as f:
                exp_data = f.read().rstrip()
            assert exp_data == act_data


================================================
File: tests/test_backend_msexcel.py
================================================
import os
from pathlib import Path

from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult, DoclingDocument
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def get_xlsx_paths():

    # Define the directory you want to search
    directory = Path("./tests/data/xlsx/")

    # List all PDF files in the directory and its subdirectories
    pdf_files = sorted(directory.rglob("*.xlsx"))
    return pdf_files


def get_converter():

    converter = DocumentConverter(allowed_formats=[InputFormat.XLSX])

    return converter


def test_e2e_xlsx_conversions():

    xlsx_paths = get_xlsx_paths()
    converter = get_converter()

    for xlsx_path in xlsx_paths:
        print(f"converting {xlsx_path}")

        gt_path = (
            xlsx_path.parent.parent / "groundtruth" / "docling_v2" / xlsx_path.name
        )

        conv_result: ConversionResult = converter.convert(xlsx_path)

        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(
            doc, str(gt_path) + ".json", GENERATE
        ), "document document"


================================================
File: tests/test_backend_msword.py
================================================
import os
from pathlib import Path

from docling.backend.msword_backend import MsWordDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import (
    ConversionResult,
    DoclingDocument,
    InputDocument,
    SectionHeaderItem,
)
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def test_heading_levels():
    in_path = Path("tests/data/docx/word_sample.docx")
    in_doc = InputDocument(
        path_or_stream=in_path,
        format=InputFormat.DOCX,
        backend=MsWordDocumentBackend,
    )
    backend = MsWordDocumentBackend(
        in_doc=in_doc,
        path_or_stream=in_path,
    )
    doc = backend.convert()

    found_lvl_1 = found_lvl_2 = False
    for item, _ in doc.iterate_items():
        if isinstance(item, SectionHeaderItem):
            if item.text == "Let\u2019s swim!":
                found_lvl_1 = True
                assert item.level == 1
            elif item.text == "Let\u2019s eat":
                found_lvl_2 = True
                assert item.level == 2
    assert found_lvl_1 and found_lvl_2


def get_docx_paths():

    # Define the directory you want to search
    directory = Path("./tests/data/docx/")

    # List all PDF files in the directory and its subdirectories
    pdf_files = sorted(directory.rglob("*.docx"))
    return pdf_files


def get_converter():

    converter = DocumentConverter(allowed_formats=[InputFormat.DOCX])

    return converter


def test_e2e_docx_conversions():

    docx_paths = get_docx_paths()
    converter = get_converter()

    for docx_path in docx_paths:
        # print(f"converting {docx_path}")

        gt_path = (
            docx_path.parent.parent / "groundtruth" / "docling_v2" / docx_path.name
        )

        conv_result: ConversionResult = converter.convert(docx_path)

        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(
            doc, str(gt_path) + ".json", GENERATE
        ), "document document"

        if docx_path.name == "word_tables.docx":
            pred_html: str = doc.export_to_html()
            assert verify_export(pred_html, str(gt_path) + ".html"), "export to html"


================================================
File: tests/test_backend_patent_uspto.py
================================================
"""Test methods in module docling.backend.patent_uspto_backend.py."""

import logging
import os
from pathlib import Path
from tempfile import NamedTemporaryFile

import pytest
from docling_core.types import DoclingDocument
from docling_core.types.doc import DocItemLabel, TableData, TextItem

from docling.backend.xml.uspto_backend import PatentUsptoDocumentBackend, XmlTable
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

from .verify_utils import verify_document

GENERATE: bool = False
DATA_PATH: Path = Path("./tests/data/uspto/")
GT_PATH: Path = Path("./tests/data/groundtruth/docling_v2/")


def _generate_groundtruth(doc: DoclingDocument, file_stem: str) -> None:
    with open(GT_PATH / f"{file_stem}.itxt", "w", encoding="utf-8") as file_obj:
        file_obj.write(doc._export_to_indented_text())
    doc.save_as_json(GT_PATH / f"{file_stem}.json")
    doc.save_as_markdown(GT_PATH / f"{file_stem}.md")


@pytest.fixture(scope="module")
def patents() -> list[tuple[Path, DoclingDocument]]:
    patent_paths = (
        sorted(DATA_PATH.glob("ip*.xml"))
        + sorted(DATA_PATH.glob("pg*.xml"))
        + sorted(DATA_PATH.glob("pa*.xml"))
        + sorted(DATA_PATH.glob("pftaps*.txt"))
    )
    patents: list[dict[Path, DoclingDocument]] = []
    for in_path in patent_paths:
        in_doc = InputDocument(
            path_or_stream=in_path,
            format=InputFormat.XML_USPTO,
            backend=PatentUsptoDocumentBackend,
        )
        backend = PatentUsptoDocumentBackend(in_doc=in_doc, path_or_stream=in_path)
        logging.info(f"Converting patent from file {in_path}")
        doc = backend.convert()
        assert doc, f"Failed to parse document {in_path}"
        patents.append((in_path, doc))

    return patents


@pytest.fixture(scope="module")
def groundtruth() -> list[tuple[Path, str]]:
    patent_paths = (
        sorted(GT_PATH.glob("ip*"))
        + sorted(GT_PATH.glob("pg*"))
        + sorted(GT_PATH.glob("pa*"))
        + sorted(GT_PATH.glob("pftaps*"))
    )
    groundtruth: list[tuple[Path, str]] = []
    for in_path in patent_paths:
        with open(in_path, encoding="utf-8") as file_obj:
            content = file_obj.read()
            groundtruth.append((in_path, content))

    return groundtruth


@pytest.fixture(scope="module")
def tables() -> list[tuple[Path, TableData]]:
    table_paths = sorted(DATA_PATH.glob("tables*.xml"))
    tables: list[tuple[Path, TableData]] = []
    for in_path in table_paths:
        with open(in_path, encoding="utf-8") as file_obj:
            content = file_obj.read()
            parser = XmlTable(content)
            parsed_table = parser.parse()
            assert parsed_table
            tables.append((in_path, parsed_table))

    return tables


@pytest.mark.skip("Slow test")
def test_patent_export(patents):
    for _, doc in patents:
        with NamedTemporaryFile(suffix=".yaml", delete=False) as tmp_file:
            doc.save_as_yaml(Path(tmp_file.name))
            assert os.path.getsize(tmp_file.name) > 0

        with NamedTemporaryFile(suffix=".html", delete=False) as tmp_file:
            doc.save_as_html(Path(tmp_file.name))
            assert os.path.getsize(tmp_file.name) > 0

        with NamedTemporaryFile(suffix=".md", delete=False) as tmp_file:
            doc.save_as_markdown(Path(tmp_file.name))
            assert os.path.getsize(tmp_file.name) > 0


def test_patent_groundtruth(patents, groundtruth):
    gt_stems: list[str] = [item[0].stem for item in groundtruth]
    gt_names: dict[str, str] = {item[0].name: item[1] for item in groundtruth}
    for path, doc in patents:
        if path.stem not in gt_stems:
            continue
        md_name = path.stem + ".md"
        if md_name in gt_names:
            pred_md = doc.export_to_markdown()
            assert (
                pred_md == gt_names[md_name]
            ), f"Markdown file mismatch against groundtruth {md_name}"
        json_path = path.with_suffix(".json")
        if json_path.stem in gt_names:
            assert verify_document(
                doc, str(json_path), GENERATE
            ), f"JSON file mismatch against groundtruth {json_path}"
        itxt_name = path.stem + ".itxt"
        if itxt_name in gt_names:
            pred_itxt = doc._export_to_indented_text()
            assert (
                pred_itxt == gt_names[itxt_name]
            ), f"Indented text file mismatch against groundtruth {itxt_name}"


def test_tables(tables):
    """Test the table parser."""
    # CHECK table in file tables_20180000016.xml
    file_name = "tables_ipa20180000016.xml"
    file_table = [item[1] for item in tables if item[0].name == file_name][0]
    assert file_table.num_rows == 13
    assert file_table.num_cols == 10
    assert len(file_table.table_cells) == 130


def test_patent_uspto_ice(patents):
    """Test applications and grants Full Text Data/XML Version 4.x ICE."""

    # CHECK application doc number 20200022300
    file_name = "ipa20200022300.xml"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    if GENERATE:
        _generate_groundtruth(doc, Path(file_name).stem)

    assert doc.name == file_name
    texts = doc.texts
    assert len(texts) == 78
    assert isinstance(texts[0], TextItem)
    assert (
        texts[0].text
        == "SYSTEM FOR CONTROLLING THE OPERATION OF AN ACTUATOR MOUNTED ON A SEED PLANTING IMPLEMENT"
    )
    assert texts[0].label == DocItemLabel.TITLE
    assert texts[0].parent.cref == "#/body"
    assert isinstance(texts[1], TextItem)
    assert texts[1].text == "ABSTRACT"
    assert texts[1].label == DocItemLabel.SECTION_HEADER
    assert texts[1].parent.cref == "#/texts/0"
    assert isinstance(texts[2], TextItem)
    assert texts[2].text == (
        "In one aspect, a system for controlling an operation of an actuator mounted "
        "on a seed planting implement may include an actuator configured to adjust a "
        "position of a row unit of the seed planting implement relative to a toolbar "
        "of the seed planting implement. The system may also include a flow restrictor"
        " fluidly coupled to a fluid chamber of the actuator, with the flow restrictor"
        " being configured to reduce a rate at which fluid is permitted to exit the "
        "fluid chamber in a manner that provides damping to the row unit. Furthermore,"
        " the system may include a valve fluidly coupled to the flow restrictor in a "
        "parallel relationship such that the valve is configured to permit the fluid "
        "exiting the fluid chamber to flow through the flow restrictor and the fluid "
        "entering the fluid chamber to bypass the flow restrictor."
    )
    assert texts[2].label == DocItemLabel.PARAGRAPH
    assert texts[2].parent.cref == "#/texts/1"
    assert isinstance(texts[3], TextItem)
    assert texts[3].text == "FIELD"
    assert texts[3].label == DocItemLabel.SECTION_HEADER
    assert texts[3].parent.cref == "#/texts/0"
    assert isinstance(texts[4], TextItem)
    assert texts[4].text == (
        "The present disclosure generally relates to seed planting implements and, "
        "more particularly, to systems for controlling the operation of an actuator "
        "mounted on a seed planting implement in a manner that provides damping to "
        "one or more components of the seed planting implement."
    )
    assert texts[4].label == DocItemLabel.PARAGRAPH
    assert texts[4].parent.cref == "#/texts/3"
    assert isinstance(texts[5], TextItem)
    assert texts[5].text == "BACKGROUND"
    assert texts[5].label == DocItemLabel.SECTION_HEADER
    assert texts[5].parent.cref == "#/texts/0"
    assert isinstance(texts[6], TextItem)
    assert texts[6].text == (
        "Modern farming practices strive to increase yields of agricultural fields. In"
        " this respect, seed planting implements are towed behind a tractor or other "
        "work vehicle to deposit seeds in a field. For example, seed planting "
        "implements typically include one or more ground engaging tools or openers "
        "that form a furrow or trench in the soil. One or more dispensing devices of "
        "the seed planting implement may, in turn, deposit seeds into the furrow(s). "
        "After deposition of the seeds, a packer wheel may pack the soil on top of the"
        " deposited seeds."
    )
    assert texts[6].label == DocItemLabel.PARAGRAPH
    assert texts[6].parent.cref == "#/texts/5"
    assert isinstance(texts[7], TextItem)
    assert texts[7].text == (
        "In certain instances, the packer wheel may also control the penetration depth"
        " of the furrow. In this regard, the position of the packer wheel may be moved"
        " vertically relative to the associated opener(s) to adjust the depth of the "
        "furrow. Additionally, the seed planting implement includes an actuator "
        "configured to exert a downward force on the opener(s) to ensure that the "
        "opener(s) is able to penetrate the soil to the depth set by the packer wheel."
        " However, the seed planting implement may bounce or chatter when traveling at"
        " high speeds and/or when the opener(s) encounters hard or compacted soil. As "
        "such, operators generally operate the seed planting implement with the "
        "actuator exerting more downward force on the opener(s) than is necessary in "
        "order to prevent such bouncing or chatter. Operation of the seed planting "
        "implement with excessive down pressure applied to the opener(s), however, "
        "reduces the overall stability of the seed planting implement."
    )
    assert texts[7].label == DocItemLabel.PARAGRAPH
    assert texts[7].parent.cref == "#/texts/5"
    assert isinstance(texts[8], TextItem)
    assert texts[8].text == (
        "Accordingly, an improved system for controlling the operation of an actuator "
        "mounted on s seed planting implement to enhance the overall operation of the "
        "implement would be welcomed in the technology."
    )
    assert texts[8].label == DocItemLabel.PARAGRAPH
    assert texts[8].parent.cref == "#/texts/5"
    assert isinstance(texts[9], TextItem)
    assert texts[9].text == "BRIEF DESCRIPTION"
    assert texts[9].label == DocItemLabel.SECTION_HEADER
    assert texts[9].parent.cref == "#/texts/0"
    assert isinstance(texts[15], TextItem)
    assert texts[15].text == "BRIEF DESCRIPTION OF THE DRAWINGS"
    assert texts[15].label == DocItemLabel.SECTION_HEADER
    assert texts[15].parent.cref == "#/texts/0"
    assert isinstance(texts[17], TextItem)
    assert texts[17].text == (
        "FIG. 1 illustrates a perspective view of one embodiment of a seed planting "
        "implement in accordance with aspects of the present subject matter;"
    )
    assert texts[17].label == DocItemLabel.PARAGRAPH
    assert texts[17].parent.cref == "#/texts/15"
    assert isinstance(texts[27], TextItem)
    assert texts[27].text == "DETAILED DESCRIPTION"
    assert texts[27].label == DocItemLabel.SECTION_HEADER
    assert texts[27].parent.cref == "#/texts/0"
    assert isinstance(texts[57], TextItem)
    assert texts[57].text == (
        "This written description uses examples to disclose the technology, including "
        "the best mode, and also to enable any person skilled in the art to practice "
        "the technology, including making and using any devices or systems and "
        "performing any incorporated methods. The patentable scope of the technology "
        "is defined by the claims, and may include other examples that occur to those "
        "skilled in the art. Such other examples are intended to be within the scope "
        "of the claims if they include structural elements that do not differ from the"
        " literal language of the claims, or if they include equivalent structural "
        "elements with insubstantial differences from the literal language of the "
        "claims."
    )
    assert texts[57].label == DocItemLabel.PARAGRAPH
    assert texts[57].parent.cref == "#/texts/27"
    assert isinstance(texts[58], TextItem)
    assert texts[58].text == "CLAIMS"
    assert texts[58].label == DocItemLabel.SECTION_HEADER
    assert texts[58].parent.cref == "#/texts/0"
    assert isinstance(texts[77], TextItem)
    assert texts[77].text == (
        "19. The system of claim 18, wherein the flow restrictor and the valve are "
        "fluidly coupled in a parallel relationship."
    )
    assert texts[77].label == DocItemLabel.PARAGRAPH
    assert texts[77].parent.cref == "#/texts/58"

    # CHECK application doc number 20180000016 for HTML entities, level 2 headings, tables
    file_name = "ipa20180000016.xml"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    if GENERATE:
        _generate_groundtruth(doc, Path(file_name).stem)

    assert doc.name == file_name
    texts = doc.texts
    assert len(texts) == 183
    assert isinstance(texts[0], TextItem)
    assert texts[0].text == "LIGHT EMITTING DEVICE AND PLANT CULTIVATION METHOD"
    assert texts[0].label == DocItemLabel.TITLE
    assert texts[0].parent.cref == "#/body"
    assert isinstance(texts[1], TextItem)
    assert texts[1].text == "ABSTRACT"
    assert texts[1].label == DocItemLabel.SECTION_HEADER
    assert texts[1].parent.cref == "#/texts/0"
    assert isinstance(texts[2], TextItem)
    assert texts[2].text == (
        "Provided is a light emitting device that includes a light emitting element "
        "having a light emission peak wavelength ranging from 380 nm to 490 nm, and a "
        "fluorescent material excited by light from the light emitting element and "
        "emitting light having at a light emission peak wavelength ranging from 580 nm"
        " or more to less than 680 nm. The light emitting device emits light having a "
        "ratio R/B of a photon flux density R to a photon flux density B ranging from "
        "2.0 to 4.0 and a ratio R/FR of the photon flux density R to a photon flux "
        "density FR ranging from 0.7 to 13.0, the photon flux density R being in a "
        "wavelength range of 620 nm or more and less than 700 nm, the photon flux "
        "density B being in a wavelength range of 380 nm or more and 490 nm or less, "
        "and the photon flux density FR being in a wavelength range of 700 nm or more "
        "and 780 nm or less."
    )
    assert isinstance(texts[3], TextItem)
    assert texts[3].text == "CROSS-REFERENCE TO RELATED APPLICATION"
    assert texts[3].label == DocItemLabel.SECTION_HEADER
    assert texts[3].parent.cref == "#/texts/0"
    assert isinstance(texts[4], TextItem)
    assert texts[5].text == "BACKGROUND"
    assert texts[5].label == DocItemLabel.SECTION_HEADER
    assert texts[5].parent.cref == "#/texts/0"
    assert isinstance(texts[6], TextItem)
    assert texts[6].text == "Technical Field"
    assert texts[6].label == DocItemLabel.SECTION_HEADER
    assert texts[6].parent.cref == "#/texts/0"
    assert isinstance(texts[7], TextItem)
    assert texts[7].text == (
        "The present disclosure relates to a light emitting device and a plant "
        "cultivation method."
    )
    assert texts[7].label == DocItemLabel.PARAGRAPH
    assert texts[7].parent.cref == "#/texts/6"
    assert isinstance(texts[8], TextItem)
    assert texts[8].text == "Description of Related Art"
    assert texts[8].label == DocItemLabel.SECTION_HEADER
    assert texts[8].parent.cref == "#/texts/0"
    assert isinstance(texts[63], TextItem)
    assert texts[63].text == (
        "wherein r, s, and t are numbers satisfying 0≦r≦1.0, 0≦s≦1.0, 0<t<1.0, and "
        "r+s+t≦1.0."
    )
    assert texts[63].label == DocItemLabel.PARAGRAPH
    assert texts[63].parent.cref == "#/texts/51"
    assert isinstance(texts[89], TextItem)
    assert texts[89].text == (
        "Examples of the compound containing Al, Ga, or In specifically include Al₂O₃, "
        "Ga₂O₃, and In₂O₃."
    )
    assert texts[89].label == DocItemLabel.PARAGRAPH
    assert texts[89].parent.cref == "#/texts/87"

    # CHECK application doc number 20110039701 for complex long tables
    file_name = "ipa20110039701.xml"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    assert doc.name == file_name
    assert len(doc.tables) == 17


def test_patent_uspto_grant_v2(patents):
    """Test applications and grants Full Text Data/APS."""

    # CHECK application doc number 06442728
    file_name = "pg06442728.xml"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    if GENERATE:
        _generate_groundtruth(doc, Path(file_name).stem)

    assert doc.name == file_name
    texts = doc.texts
    assert len(texts) == 108
    assert isinstance(texts[0], TextItem)
    assert texts[0].text == "Methods and apparatus for turbo code"
    assert texts[0].label == DocItemLabel.TITLE
    assert texts[0].parent.cref == "#/body"
    assert isinstance(texts[1], TextItem)
    assert texts[1].text == "ABSTRACT"
    assert texts[1].label == DocItemLabel.SECTION_HEADER
    assert texts[1].parent.cref == "#/texts/0"
    assert isinstance(texts[2], TextItem)
    assert texts[2].text == (
        "An interleaver receives incoming data frames of size N. The interleaver "
        "indexes the elements of the frame with an N₁×N₂ index array. The interleaver "
        "then effectively rearranges (permutes) the data by permuting the rows of the "
        "index array. The interleaver employs the equation I(j,k)=I(j,αjk+βj)modP) to "
        "permute the columns (indexed by k) of each row (indexed by j). P is at least "
        "equal to N₂, βj is a constant which may be different for each row, and each "
        "αj is a relative prime number relative to P. After permuting, the "
        "interleaver outputs the data in a different order than received (e.g., "
        "receives sequentially row by row, outputs sequentially each column by column)."
    )
    # check that the formula has been skipped
    assert texts[43].text == (
        "Calculating the specified equation with the specified values for permuting "
        "row 0 of array D 350 into row 0 of array D₁ 360 proceeds as:"
    )
    assert texts[44].text == (
        "and the permuted data frame is contained in array D₁ 360 shown in FIG. 3. "
        "Outputting the array column by column outputs the frame elements in the "
        "order:"
    )


def test_patent_uspto_app_v1(patents):
    """Test applications Full Text Data/XML Version 1.x."""

    # CHECK application doc number 20010031492
    file_name = "pa20010031492.xml"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    if GENERATE:
        _generate_groundtruth(doc, Path(file_name).stem)

    assert doc.name == file_name
    texts = doc.texts
    assert len(texts) == 103
    assert isinstance(texts[0], TextItem)
    assert texts[0].text == "Assay reagent"
    assert texts[0].label == DocItemLabel.TITLE
    assert texts[0].parent.cref == "#/body"
    assert isinstance(texts[1], TextItem)
    assert texts[1].text == "ABSTRACT"
    assert texts[1].label == DocItemLabel.SECTION_HEADER
    assert texts[1].parent.cref == "#/texts/0"
    # check that the formula has been skipped
    assert texts[62].text == (
        "5. The % toxic effect for each sample was calculated as follows:"
    )
    assert texts[63].text == "where: Cₒ=light in control at time zero"
    assert len(doc.tables) == 1
    assert doc.tables[0].data.num_rows == 6
    assert doc.tables[0].data.num_cols == 3


def test_patent_uspto_grant_aps(patents):
    """Test applications Full Text Data/APS."""

    # CHECK application doc number 057006474
    file_name = "pftaps057006474.txt"
    doc = [item[1] for item in patents if item[0].name == file_name][0]
    if GENERATE:
        _generate_groundtruth(doc, Path(file_name).stem)

    assert doc.name == file_name
    texts = doc.texts
    assert len(texts) == 75
    assert isinstance(texts[0], TextItem)
    assert texts[0].text == "Carbocation containing cyanine-type dye"
    assert texts[0].label == DocItemLabel.TITLE
    assert texts[0].parent.cref == "#/body"
    assert isinstance(texts[1], TextItem)
    assert texts[1].text == "ABSTRACT"
    assert texts[1].label == DocItemLabel.SECTION_HEADER
    assert texts[1].parent.cref == "#/texts/0"
    assert isinstance(texts[2], TextItem)
    assert texts[2].text == (
        "To provide a reagent with excellent stability under storage, which can detect"
        " a subject compound to be measured with higher specificity and sensitibity. "
        "Complexes of a compound represented by the general formula (IV):"
    )
    assert len(doc.tables) == 0
    for item in texts:
        assert "##STR1##" not in item.text


================================================
File: tests/test_backend_pdfium.py
================================================
from pathlib import Path

import pytest
from docling_core.types.doc import BoundingBox

from docling.backend.pypdfium2_backend import (
    PyPdfiumDocumentBackend,
    PyPdfiumPageBackend,
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument


@pytest.fixture
def test_doc_path():
    return Path("./tests/data/pdf/2206.01062.pdf")


def _get_backend(pdf_doc):
    in_doc = InputDocument(
        path_or_stream=pdf_doc,
        format=InputFormat.PDF,
        backend=PyPdfiumDocumentBackend,
    )

    doc_backend = in_doc._backend
    return doc_backend


def test_text_cell_counts():
    pdf_doc = Path("./tests/data/pdf/redp5110_sampled.pdf")

    doc_backend = _get_backend(pdf_doc)

    for page_index in range(0, doc_backend.page_count()):
        last_cell_count = None
        for i in range(10):
            page_backend: PyPdfiumPageBackend = doc_backend.load_page(0)
            cells = list(page_backend.get_text_cells())

            if last_cell_count is None:
                last_cell_count = len(cells)

            if len(cells) != last_cell_count:
                assert (
                    False
                ), "Loading page multiple times yielded non-identical text cell counts"
            last_cell_count = len(cells)


def test_get_text_from_rect(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: PyPdfiumPageBackend = doc_backend.load_page(0)

    # Get the title text of the DocLayNet paper
    textpiece = page_backend.get_text_in_rect(
        bbox=BoundingBox(l=102, t=77, r=511, b=124)
    )
    ref = "DocLayNet: A Large Human-Annotated Dataset for\r\nDocument-Layout Analysis"

    assert textpiece.strip() == ref


def test_crop_page_image(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    page_backend: PyPdfiumPageBackend = doc_backend.load_page(0)

    # Crop out "Figure 1" from the DocLayNet paper
    im = page_backend.get_page_image(
        scale=2, cropbox=BoundingBox(l=317, t=246, r=574, b=527)
    )
    # im.show()


def test_num_pages(test_doc_path):
    doc_backend = _get_backend(test_doc_path)
    doc_backend.page_count() == 9


================================================
File: tests/test_backend_pptx.py
================================================
import os
from pathlib import Path

from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult, DoclingDocument
from docling.document_converter import DocumentConverter

from .verify_utils import verify_document, verify_export

GENERATE = False


def get_pptx_paths():

    # Define the directory you want to search
    directory = Path("./tests/data/pptx/")

    # List all PPTX files in the directory and its subdirectories
    pptx_files = sorted(directory.rglob("*.pptx"))
    return pptx_files


def get_converter():

    converter = DocumentConverter(allowed_formats=[InputFormat.PPTX])

    return converter


def test_e2e_pptx_conversions():

    pptx_paths = get_pptx_paths()
    converter = get_converter()

    for pptx_path in pptx_paths:
        # print(f"converting {pptx_path}")

        gt_path = (
            pptx_path.parent.parent / "groundtruth" / "docling_v2" / pptx_path.name
        )

        conv_result: ConversionResult = converter.convert(pptx_path)

        doc: DoclingDocument = conv_result.document

        pred_md: str = doc.export_to_markdown()
        assert verify_export(pred_md, str(gt_path) + ".md"), "export to md"

        pred_itxt: str = doc._export_to_indented_text(
            max_text_len=70, explicit_tables=False
        )
        assert verify_export(
            pred_itxt, str(gt_path) + ".itxt"
        ), "export to indented-text"

        assert verify_document(
            doc, str(gt_path) + ".json", GENERATE
        ), "document document"


================================================
File: tests/test_cli.py
================================================
from pathlib import Path

from typer.testing import CliRunner

from docling.cli.main import app

runner = CliRunner()


def test_cli_help():
    result = runner.invoke(app, ["--help"])
    assert result.exit_code == 0


def test_cli_version():
    result = runner.invoke(app, ["--version"])
    assert result.exit_code == 0


def test_cli_convert(tmp_path):
    source = "./tests/data/pdf/2305.03393v1-pg9.pdf"
    output = tmp_path / "out"
    output.mkdir()
    result = runner.invoke(app, [source, "--output", str(output)])
    assert result.exit_code == 0
    converted = output / f"{Path(source).stem}.md"
    assert converted.exists()


================================================
File: tests/test_code_formula.py
================================================
from pathlib import Path

from docling_core.types.doc import CodeItem, TextItem
from docling_core.types.doc.labels import CodeLanguageLabel, DocItemLabel

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline


def get_converter():

    pipeline_options = PdfPipelineOptions()
    pipeline_options.generate_page_images = True

    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = False
    pipeline_options.do_code_enrichment = True
    pipeline_options.do_formula_enrichment = True

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                backend=DoclingParseV2DocumentBackend,
                pipeline_cls=StandardPdfPipeline,
                pipeline_options=pipeline_options,
            )
        }
    )

    return converter


def test_code_and_formula_conversion():
    pdf_path = Path("tests/data/pdf/code_and_formula.pdf")
    converter = get_converter()

    print(f"converting {pdf_path}")

    doc_result: ConversionResult = converter.convert(pdf_path)

    results = doc_result.document.texts

    code_blocks = [el for el in results if isinstance(el, CodeItem)]
    assert len(code_blocks) == 1

    gt = "function add(a, b) {\n    return a + b;\n}\nconsole.log(add(3, 5));"

    predicted = code_blocks[0].text.strip()
    assert predicted == gt, f"mismatch in text {predicted=}, {gt=}"
    assert code_blocks[0].code_language == CodeLanguageLabel.JAVASCRIPT

    formula_blocks = [
        el
        for el in results
        if isinstance(el, TextItem) and el.label == DocItemLabel.FORMULA
    ]
    assert len(formula_blocks) == 1

    gt = "a ^ { 2 } + 8 = 1 2"
    predicted = formula_blocks[0].text
    assert predicted == gt, f"mismatch in text {predicted=}, {gt=}"


================================================
File: tests/test_data_gen_flag.py
================================================
import os

from pydantic import TypeAdapter

GEN_TEST_DATA = TypeAdapter(bool).validate_python(os.getenv("DOCLING_GEN_TEST_DATA", 0))


def test_gen_test_data_flag():
    assert not GEN_TEST_DATA


================================================
File: tests/test_document_picture_classifier.py
================================================
from pathlib import Path

from docling_core.types.doc import PictureClassificationData

from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline


def get_converter():

    pipeline_options = PdfPipelineOptions()
    pipeline_options.generate_page_images = True

    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = False
    pipeline_options.do_code_enrichment = False
    pipeline_options.do_formula_enrichment = False
    pipeline_options.do_picture_classification = True
    pipeline_options.generate_picture_images = True
    pipeline_options.images_scale = 2

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                backend=DoclingParseV2DocumentBackend,
                pipeline_cls=StandardPdfPipeline,
                pipeline_options=pipeline_options,
            )
        }
    )

    return converter


def test_picture_classifier():
    pdf_path = Path("tests/data/pdf/picture_classification.pdf")
    converter = get_converter()

    print(f"converting {pdf_path}")

    doc_result: ConversionResult = converter.convert(pdf_path)

    results = doc_result.document.pictures

    assert len(results) == 2

    res = results[0]
    assert len(res.annotations) == 1
    assert type(res.annotations[0]) == PictureClassificationData
    classification_data = res.annotations[0]
    assert classification_data.provenance == "DocumentPictureClassifier"
    assert (
        len(classification_data.predicted_classes) == 16
    ), "Number of predicted classes is not equal to 16"
    confidences = [pred.confidence for pred in classification_data.predicted_classes]
    assert confidences == sorted(
        confidences, reverse=True
    ), "Predictions are not sorted in descending order of confidence"
    assert (
        classification_data.predicted_classes[0].class_name == "bar_chart"
    ), "The prediction is wrong for the bar chart image."

    res = results[1]
    assert len(res.annotations) == 1
    assert type(res.annotations[0]) == PictureClassificationData
    classification_data = res.annotations[0]
    assert classification_data.provenance == "DocumentPictureClassifier"
    assert (
        len(classification_data.predicted_classes) == 16
    ), "Number of predicted classes is not equal to 16"
    confidences = [pred.confidence for pred in classification_data.predicted_classes]
    assert confidences == sorted(
        confidences, reverse=True
    ), "Predictions are not sorted in descending order of confidence"
    assert (
        classification_data.predicted_classes[0].class_name == "map"
    ), "The prediction is wrong for the bar chart image."


================================================
File: tests/test_e2e_conversion.py
================================================
from pathlib import Path

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import AcceleratorDevice, PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

from .verify_utils import verify_conversion_result_v1, verify_conversion_result_v2

GENERATE_V1 = False
GENERATE_V2 = False


def get_pdf_paths():

    # Define the directory you want to search
    directory = Path("./tests/data/pdf/")

    # List all PDF files in the directory and its subdirectories
    pdf_files = sorted(directory.rglob("*.pdf"))
    return pdf_files


def get_converter():

    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True
    pipeline_options.accelerator_options.device = AcceleratorDevice.CPU

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options, backend=DoclingParseDocumentBackend
            )
        }
    )

    return converter


def test_e2e_pdfs_conversions():

    pdf_paths = get_pdf_paths()
    converter = get_converter()

    for pdf_path in pdf_paths:
        print(f"converting {pdf_path}")

        doc_result: ConversionResult = converter.convert(pdf_path)

        verify_conversion_result_v1(
            input_path=pdf_path, doc_result=doc_result, generate=GENERATE_V1
        )

        verify_conversion_result_v2(
            input_path=pdf_path, doc_result=doc_result, generate=GENERATE_V2
        )


================================================
File: tests/test_e2e_ocr_conversion.py
================================================
import sys
from pathlib import Path
from typing import List

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    EasyOcrOptions,
    OcrMacOptions,
    OcrOptions,
    PdfPipelineOptions,
    RapidOcrOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption

from .verify_utils import verify_conversion_result_v1, verify_conversion_result_v2

GENERATE_V1 = False
GENERATE_V2 = False


def get_pdf_paths():
    # Define the directory you want to search
    directory = Path("./tests/data_scanned")

    # List all PDF files in the directory and its subdirectories
    pdf_files = sorted(directory.rglob("*.pdf"))
    return pdf_files


def get_converter(ocr_options: OcrOptions):
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = True
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True
    pipeline_options.ocr_options = ocr_options
    pipeline_options.accelerator_options.device = AcceleratorDevice.CPU

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
                backend=DoclingParseDocumentBackend,
            )
        }
    )

    return converter


def test_e2e_conversions():
    pdf_paths = get_pdf_paths()

    engines: List[OcrOptions] = [
        EasyOcrOptions(),
        TesseractOcrOptions(),
        TesseractCliOcrOptions(),
        EasyOcrOptions(force_full_page_ocr=True),
        TesseractOcrOptions(force_full_page_ocr=True),
        TesseractOcrOptions(force_full_page_ocr=True, lang=["auto"]),
        TesseractCliOcrOptions(force_full_page_ocr=True),
        TesseractCliOcrOptions(force_full_page_ocr=True, lang=["auto"]),
    ]

    # rapidocr is only available for Python >=3.6,<3.13
    if sys.version_info < (3, 13):
        engines.append(RapidOcrOptions())
        engines.append(RapidOcrOptions(force_full_page_ocr=True))

    # only works on mac
    if "darwin" == sys.platform:
        engines.append(OcrMacOptions())
        engines.append(OcrMacOptions(force_full_page_ocr=True))

    for ocr_options in engines:
        print(
            f"Converting with ocr_engine: {ocr_options.kind}, language: {ocr_options.lang}"
        )
        converter = get_converter(ocr_options=ocr_options)
        for pdf_path in pdf_paths:
            print(f"converting {pdf_path}")

            doc_result: ConversionResult = converter.convert(pdf_path)

            verify_conversion_result_v1(
                input_path=pdf_path,
                doc_result=doc_result,
                generate=GENERATE_V1,
                fuzzy=True,
            )

            verify_conversion_result_v2(
                input_path=pdf_path,
                doc_result=doc_result,
                generate=GENERATE_V2,
                fuzzy=True,
            )


================================================
File: tests/test_input_doc.py
================================================
from io import BytesIO
from pathlib import Path

from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling.datamodel.base_models import DocumentStream, InputFormat
from docling.datamodel.document import InputDocument, _DocumentConversionInput
from docling.datamodel.settings import DocumentLimits


def test_in_doc_from_valid_path():

    test_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    doc = _make_input_doc(test_doc_path)
    assert doc.valid == True


def test_in_doc_from_invalid_path():
    test_doc_path = Path("./tests/does/not/exist.pdf")

    doc = _make_input_doc(test_doc_path)

    assert doc.valid == False


def test_in_doc_from_valid_buf():

    buf = BytesIO(Path("./tests/data/pdf/2206.01062.pdf").open("rb").read())
    stream = DocumentStream(name="my_doc.pdf", stream=buf)

    doc = _make_input_doc_from_stream(stream)
    assert doc.valid == True


def test_in_doc_from_invalid_buf():

    buf = BytesIO(b"")
    stream = DocumentStream(name="my_doc.pdf", stream=buf)

    doc = _make_input_doc_from_stream(stream)
    assert doc.valid == False


def test_in_doc_with_page_range():
    test_doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    limits = DocumentLimits()
    limits.page_range = (1, 10)

    doc = InputDocument(
        path_or_stream=test_doc_path,
        format=InputFormat.PDF,
        backend=PyPdfiumDocumentBackend,
        limits=limits,
    )
    assert doc.valid == True

    limits.page_range = (9, 9)

    doc = InputDocument(
        path_or_stream=test_doc_path,
        format=InputFormat.PDF,
        backend=PyPdfiumDocumentBackend,
        limits=limits,
    )
    assert doc.valid == True

    limits.page_range = (11, 12)

    doc = InputDocument(
        path_or_stream=test_doc_path,
        format=InputFormat.PDF,
        backend=PyPdfiumDocumentBackend,
        limits=limits,
    )
    assert doc.valid == False


def test_guess_format(tmp_path):
    """Test docling.datamodel.document._DocumentConversionInput.__guess_format"""
    dci = _DocumentConversionInput(path_or_stream_iterator=[])
    temp_dir = tmp_path / "test_guess_format"
    temp_dir.mkdir()

    # Valid PDF
    buf = BytesIO(Path("./tests/data/pdf/2206.01062.pdf").open("rb").read())
    stream = DocumentStream(name="my_doc.pdf", stream=buf)
    assert dci._guess_format(stream) == InputFormat.PDF
    doc_path = Path("./tests/data/pdf/2206.01062.pdf")
    assert dci._guess_format(doc_path) == InputFormat.PDF

    # Valid MS Office
    buf = BytesIO(Path("./tests/data/docx/lorem_ipsum.docx").open("rb").read())
    stream = DocumentStream(name="lorem_ipsum.docx", stream=buf)
    assert dci._guess_format(stream) == InputFormat.DOCX
    doc_path = Path("./tests/data/docx/lorem_ipsum.docx")
    assert dci._guess_format(doc_path) == InputFormat.DOCX

    # Valid HTML
    buf = BytesIO(Path("./tests/data/html/wiki_duck.html").open("rb").read())
    stream = DocumentStream(name="wiki_duck.html", stream=buf)
    assert dci._guess_format(stream) == InputFormat.HTML
    doc_path = Path("./tests/data/html/wiki_duck.html")
    assert dci._guess_format(doc_path) == InputFormat.HTML

    # Valid MD
    buf = BytesIO(Path("./tests/data/md/wiki.md").open("rb").read())
    stream = DocumentStream(name="wiki.md", stream=buf)
    assert dci._guess_format(stream) == InputFormat.MD
    doc_path = Path("./tests/data/md/wiki.md")
    assert dci._guess_format(doc_path) == InputFormat.MD

    # Valid CSV
    buf = BytesIO(Path("./tests/data/csv/csv-comma.csv").open("rb").read())
    stream = DocumentStream(name="csv-comma.csv", stream=buf)
    assert dci._guess_format(stream) == InputFormat.CSV
    stream = DocumentStream(name="test-comma", stream=buf)
    assert dci._guess_format(stream) == InputFormat.CSV
    doc_path = Path("./tests/data/csv/csv-comma.csv")
    assert dci._guess_format(doc_path) == InputFormat.CSV

    # Valid XML USPTO patent
    buf = BytesIO(Path("./tests/data/uspto/ipa20110039701.xml").open("rb").read())
    stream = DocumentStream(name="ipa20110039701.xml", stream=buf)
    assert dci._guess_format(stream) == InputFormat.XML_USPTO
    doc_path = Path("./tests/data/uspto/ipa20110039701.xml")
    assert dci._guess_format(doc_path) == InputFormat.XML_USPTO

    buf = BytesIO(Path("./tests/data/uspto/pftaps057006474.txt").open("rb").read())
    stream = DocumentStream(name="pftaps057006474.txt", stream=buf)
    assert dci._guess_format(stream) == InputFormat.XML_USPTO
    doc_path = Path("./tests/data/uspto/pftaps057006474.txt")
    assert dci._guess_format(doc_path) == InputFormat.XML_USPTO

    # Valid XML JATS
    buf = BytesIO(Path("./tests/data/jats/elife-56337.xml").open("rb").read())
    stream = DocumentStream(name="elife-56337.xml", stream=buf)
    assert dci._guess_format(stream) == InputFormat.XML_JATS
    doc_path = Path("./tests/data/jats/elife-56337.xml")
    assert dci._guess_format(doc_path) == InputFormat.XML_JATS

    buf = BytesIO(Path("./tests/data/jats/elife-56337.nxml").open("rb").read())
    stream = DocumentStream(name="elife-56337.nxml", stream=buf)
    assert dci._guess_format(stream) == InputFormat.XML_JATS
    doc_path = Path("./tests/data/jats/elife-56337.nxml")
    assert dci._guess_format(doc_path) == InputFormat.XML_JATS

    buf = BytesIO(Path("./tests/data/jats/elife-56337.txt").open("rb").read())
    stream = DocumentStream(name="elife-56337.txt", stream=buf)
    assert dci._guess_format(stream) == InputFormat.XML_JATS
    doc_path = Path("./tests/data/jats/elife-56337.txt")
    assert dci._guess_format(doc_path) == InputFormat.XML_JATS

    # Valid XML, non-supported flavor
    xml_content = (
        '<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE docling_test SYSTEM '
        '"test.dtd"><docling>Docling parses documents</docling>'
    )
    doc_path = temp_dir / "docling_test.xml"
    doc_path.write_text(xml_content, encoding="utf-8")
    assert dci._guess_format(doc_path) == None
    buf = BytesIO(Path(doc_path).open("rb").read())
    stream = DocumentStream(name="docling_test.xml", stream=buf)
    assert dci._guess_format(stream) == None

    # Invalid USPTO patent (as plain text)
    stream = DocumentStream(name="pftaps057006474.txt", stream=BytesIO(b"xyz"))
    assert dci._guess_format(stream) == None
    doc_path = temp_dir / "pftaps_wrong.txt"
    doc_path.write_text("xyz", encoding="utf-8")
    assert dci._guess_format(doc_path) == None

    # Valid Docling JSON
    test_str = '{"name": ""}'
    stream = DocumentStream(name="test.json", stream=BytesIO(f"{test_str}".encode()))
    assert dci._guess_format(stream) == InputFormat.JSON_DOCLING
    doc_path = temp_dir / "test.json"
    doc_path.write_text(test_str, encoding="utf-8")
    assert dci._guess_format(doc_path) == InputFormat.JSON_DOCLING

    # Non-Docling JSON
    # TODO: Docling JSON is currently the single supported JSON flavor and the pipeline
    # will try to validate *any* JSON (based on suffix/MIME) as Docling JSON; proper
    # disambiguation seen as part of https://github.com/DS4SD/docling/issues/802
    test_str = "{}"
    stream = DocumentStream(name="test.json", stream=BytesIO(f"{test_str}".encode()))
    assert dci._guess_format(stream) == InputFormat.JSON_DOCLING
    doc_path = temp_dir / "test.json"
    doc_path.write_text(test_str, encoding="utf-8")
    assert dci._guess_format(doc_path) == InputFormat.JSON_DOCLING


def _make_input_doc(path):
    in_doc = InputDocument(
        path_or_stream=path,
        format=InputFormat.PDF,
        backend=PyPdfiumDocumentBackend,
    )
    return in_doc


def _make_input_doc_from_stream(doc_stream):
    in_doc = InputDocument(
        path_or_stream=doc_stream.stream,
        format=InputFormat.PDF,
        filename=doc_stream.name,
        backend=PyPdfiumDocumentBackend,
    )
    return in_doc


================================================
File: tests/test_interfaces.py
================================================
from io import BytesIO
from pathlib import Path

import pytest

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import DocumentStream, InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

from .verify_utils import verify_conversion_result_v1, verify_conversion_result_v2

GENERATE = False


def get_pdf_path():

    pdf_path = Path("./tests/data/pdf/2305.03393v1-pg9.pdf")
    return pdf_path


@pytest.fixture
def converter():

    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = True

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options, backend=DoclingParseDocumentBackend
            )
        }
    )

    return converter


def test_convert_path(converter: DocumentConverter):

    pdf_path = get_pdf_path()
    print(f"converting {pdf_path}")

    doc_result = converter.convert(pdf_path)
    verify_conversion_result_v1(
        input_path=pdf_path, doc_result=doc_result, generate=GENERATE
    )
    verify_conversion_result_v2(
        input_path=pdf_path, doc_result=doc_result, generate=GENERATE
    )


def test_convert_stream(converter: DocumentConverter):

    pdf_path = get_pdf_path()
    print(f"converting {pdf_path}")

    buf = BytesIO(pdf_path.open("rb").read())
    stream = DocumentStream(name=pdf_path.name, stream=buf)

    doc_result = converter.convert(stream)
    verify_conversion_result_v1(
        input_path=pdf_path, doc_result=doc_result, generate=GENERATE
    )
    verify_conversion_result_v2(
        input_path=pdf_path, doc_result=doc_result, generate=GENERATE
    )


================================================
File: tests/test_invalid_input.py
================================================
from io import BytesIO
from pathlib import Path

import pytest

from docling.datamodel.base_models import ConversionStatus, DocumentStream
from docling.document_converter import ConversionError, DocumentConverter


def get_pdf_path():

    pdf_path = Path("./tests/data/pdf/2305.03393v1-pg9.pdf")
    return pdf_path


@pytest.fixture
def converter():
    converter = DocumentConverter()

    return converter


def test_convert_unsupported_doc_format_wout_exception(converter: DocumentConverter):
    result = converter.convert(
        DocumentStream(name="input.xyz", stream=BytesIO(b"xyz")), raises_on_error=False
    )
    assert result.status == ConversionStatus.SKIPPED


def test_convert_unsupported_doc_format_with_exception(converter: DocumentConverter):
    with pytest.raises(ConversionError):
        converter.convert(
            DocumentStream(name="input.xyz", stream=BytesIO(b"xyz")),
            raises_on_error=True,
        )


def test_convert_too_small_filesize_limit_wout_exception(converter: DocumentConverter):
    result = converter.convert(get_pdf_path(), max_file_size=1, raises_on_error=False)
    assert result.status == ConversionStatus.FAILURE


def test_convert_too_small_filesize_limit_with_exception(converter: DocumentConverter):
    with pytest.raises(ConversionError):
        converter.convert(get_pdf_path(), max_file_size=1, raises_on_error=True)


================================================
File: tests/test_legacy_format_transform.py
================================================
import json
from pathlib import Path

import pytest

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption


@pytest.fixture
def test_doc_paths():
    return [
        Path("tests/data/html/wiki_duck.html"),
        Path("tests/data/docx/word_sample.docx"),
        Path("tests/data/docx/lorem_ipsum.docx"),
        Path("tests/data/pptx/powerpoint_sample.pptx"),
        Path("tests/data/2305.03393v1-pg9-img.png"),
        Path("tests/data/pdf/2206.01062.pdf"),
    ]


def get_converter():

    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    return converter


def test_compare_legacy_output(test_doc_paths):
    converter = get_converter()

    res = converter.convert_all(test_doc_paths, raises_on_error=True)

    for conv_res in res:
        print(f"Results for {conv_res.input.file}")
        print(
            json.dumps(
                conv_res.legacy_document.model_dump(
                    mode="json", by_alias=True, exclude_none=True
                )
            )
        )

    # assert res.legacy_output == res.legacy_output_transformed


================================================
File: tests/test_options.py
================================================
import os
from pathlib import Path

import pytest

from docling.backend.docling_parse_backend import DoclingParseDocumentBackend
from docling.datamodel.base_models import ConversionStatus, InputFormat
from docling.datamodel.document import ConversionResult
from docling.datamodel.pipeline_options import (
    AcceleratorDevice,
    AcceleratorOptions,
    PdfPipelineOptions,
    TableFormerMode,
)
from docling.document_converter import DocumentConverter, PdfFormatOption


@pytest.fixture
def test_doc_path():
    return Path("./tests/data/pdf/2206.01062.pdf")


def get_converters_with_table_options():
    for cell_matching in [True, False]:
        for mode in [TableFormerMode.FAST, TableFormerMode.ACCURATE]:
            pipeline_options = PdfPipelineOptions()
            pipeline_options.do_ocr = False
            pipeline_options.do_table_structure = True
            pipeline_options.table_structure_options.do_cell_matching = cell_matching
            pipeline_options.table_structure_options.mode = mode

            converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pipeline_options,
                        backend=DoclingParseDocumentBackend,
                    )
                }
            )

            yield converter


def test_accelerator_options():
    # Check the default options
    ao = AcceleratorOptions()
    assert ao.num_threads == 4, "Wrong default num_threads"
    assert ao.device == AcceleratorDevice.AUTO, "Wrong default device"

    # Use API
    ao2 = AcceleratorOptions(num_threads=2, device=AcceleratorDevice.MPS)
    ao3 = AcceleratorOptions(num_threads=3, device=AcceleratorDevice.CUDA)
    assert ao2.num_threads == 2
    assert ao2.device == AcceleratorDevice.MPS
    assert ao3.num_threads == 3
    assert ao3.device == AcceleratorDevice.CUDA

    # Use envvars (regular + alternative) and default values
    os.environ["OMP_NUM_THREADS"] = "1"
    ao.__init__()
    assert ao.num_threads == 1
    assert ao.device == AcceleratorDevice.AUTO
    os.environ["DOCLING_DEVICE"] = "cpu"
    ao.__init__()
    assert ao.device == AcceleratorDevice.CPU
    assert ao.num_threads == 1

    # Use envvars and override in init
    os.environ["DOCLING_DEVICE"] = "cpu"
    ao4 = AcceleratorOptions(num_threads=5, device=AcceleratorDevice.MPS)
    assert ao4.num_threads == 5
    assert ao4.device == AcceleratorDevice.MPS

    # Use regular and alternative envvar
    os.environ["DOCLING_NUM_THREADS"] = "2"
    ao5 = AcceleratorOptions()
    assert ao5.num_threads == 2
    assert ao5.device == AcceleratorDevice.CPU

    # Use wrong values
    is_exception = False
    try:
        os.environ["DOCLING_DEVICE"] = "wrong"
        ao5.__init__()
    except Exception as ex:
        print(ex)
        is_exception = True
    assert is_exception

    # Use misformatted alternative envvar
    del os.environ["DOCLING_NUM_THREADS"]
    del os.environ["DOCLING_DEVICE"]
    os.environ["OMP_NUM_THREADS"] = "wrong"
    ao6 = AcceleratorOptions()
    assert ao6.num_threads == 4
    assert ao6.device == AcceleratorDevice.AUTO


def test_e2e_conversions(test_doc_path):
    for converter in get_converters_with_table_options():
        print(f"converting {test_doc_path}")

        doc_result: ConversionResult = converter.convert(test_doc_path)

        assert doc_result.status == ConversionStatus.SUCCESS


def test_page_range(test_doc_path):
    converter = DocumentConverter()
    doc_result: ConversionResult = converter.convert(test_doc_path, page_range=(9, 9))

    assert doc_result.status == ConversionStatus.SUCCESS
    assert doc_result.input.page_count == 9
    assert doc_result.document.num_pages() == 1

    doc_result: ConversionResult = converter.convert(
        test_doc_path, page_range=(10, 10), raises_on_error=False
    )
    assert doc_result.status == ConversionStatus.FAILURE


def test_ocr_coverage_threshold(test_doc_path):
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = True
    pipeline_options.ocr_options.bitmap_area_threshold = 1.1

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
            )
        }
    )

    test_doc_path = Path("./tests/data_scanned/ocr_test.pdf")
    doc_result: ConversionResult = converter.convert(test_doc_path)

    # this should have generated no results, since we set a very high threshold
    assert len(doc_result.document.texts) == 0


================================================
File: tests/verify_utils.py
================================================
import json
import os
import warnings
from pathlib import Path
from typing import List, Optional

from docling_core.types.doc import (
    DocItem,
    DoclingDocument,
    PictureItem,
    TableItem,
    TextItem,
)
from docling_core.types.legacy_doc.document import ExportedCCSDocument as DsDocument
from PIL import Image as PILImage
from pydantic import TypeAdapter
from pydantic.json import pydantic_encoder

from docling.datamodel.base_models import ConversionStatus, Page
from docling.datamodel.document import ConversionResult


def levenshtein(str1: str, str2: str) -> int:

    # Ensure str1 is the shorter string to optimize memory usage
    if len(str1) > len(str2):
        str1, str2 = str2, str1

    # Previous and current row buffers
    previous_row = list(range(len(str2) + 1))
    current_row = [0] * (len(str2) + 1)

    # Compute the Levenshtein distance row by row
    for i, c1 in enumerate(str1, start=1):
        current_row[0] = i
        for j, c2 in enumerate(str2, start=1):
            insertions = previous_row[j] + 1
            deletions = current_row[j - 1] + 1
            substitutions = previous_row[j - 1] + (c1 != c2)
            current_row[j] = min(insertions, deletions, substitutions)
        # Swap rows for the next iteration
        previous_row, current_row = current_row, previous_row

    # The result is in the last element of the previous row
    return previous_row[-1]


def verify_text(gt: str, pred: str, fuzzy: bool, fuzzy_threshold: float = 0.4):

    if len(gt) == 0 or not fuzzy:
        assert gt == pred, f"{gt}!={pred}"
    else:
        dist = levenshtein(gt, pred)
        diff = dist / len(gt)
        assert diff < fuzzy_threshold, f"{gt}!~{pred}"
    return True


def verify_cells(doc_pred_pages: List[Page], doc_true_pages: List[Page]):

    assert len(doc_pred_pages) == len(
        doc_true_pages
    ), "pred- and true-doc do not have the same number of pages"

    for pid, page_true_item in enumerate(doc_true_pages):

        num_true_cells = len(page_true_item.cells)
        num_pred_cells = len(doc_pred_pages[pid].cells)

        assert (
            num_true_cells == num_pred_cells
        ), f"num_true_cells!=num_pred_cells {num_true_cells}!={num_pred_cells}"

        for cid, cell_true_item in enumerate(page_true_item.cells):

            cell_pred_item = doc_pred_pages[pid].cells[cid]

            true_text = cell_true_item.text
            pred_text = cell_pred_item.text
            assert true_text == pred_text, f"{true_text}!={pred_text}"

            true_bbox = cell_true_item.bbox.as_tuple()
            pred_bbox = cell_pred_item.bbox.as_tuple()
            assert (
                true_bbox == pred_bbox
            ), f"bbox is not the same: {true_bbox} != {pred_bbox}"

    return True


# def verify_maintext(doc_pred: DsDocument, doc_true: DsDocument):
#     assert doc_true.main_text is not None, "doc_true cannot be None"
#     assert doc_pred.main_text is not None, "doc_true cannot be None"
#
#     assert len(doc_true.main_text) == len(
#         doc_pred.main_text
#     ), f"document has different length of main-text than expected. {len(doc_true.main_text)}!={len(doc_pred.main_text)}"
#
#     for l, true_item in enumerate(doc_true.main_text):
#         pred_item = doc_pred.main_text[l]
#         # Validate type
#         assert (
#             true_item.obj_type == pred_item.obj_type
#         ), f"Item[{l}] type does not match. expected[{true_item.obj_type}] != predicted [{pred_item.obj_type}]"
#
#         # Validate text ceels
#         if isinstance(true_item, BaseText):
#             assert isinstance(
#                 pred_item, BaseText
#             ), f"{pred_item} is not a BaseText element, but {true_item} is."
#             assert true_item.text == pred_item.text
#
#     return True


def verify_tables_v1(doc_pred: DsDocument, doc_true: DsDocument, fuzzy: bool):
    if doc_true.tables is None:
        # No tables to check
        assert doc_pred.tables is None, "not expecting any table on this document"
        return True

    assert doc_pred.tables is not None, "no tables predicted, but expected in doc_true"

    # print("Expected number of tables: {}, result: {}".format(len(doc_true.tables), len(doc_pred.tables)))

    assert len(doc_true.tables) == len(
        doc_pred.tables
    ), "document has different count of tables than expected."

    for l, true_item in enumerate(doc_true.tables):
        pred_item = doc_pred.tables[l]

        assert (
            true_item.num_rows == pred_item.num_rows
        ), "table does not have the same #-rows"
        assert (
            true_item.num_cols == pred_item.num_cols
        ), "table does not have the same #-cols"

        assert true_item.data is not None, "documents are expected to have table data"
        assert pred_item.data is not None, "documents are expected to have table data"

        print("True: \n", true_item.export_to_dataframe().to_markdown())
        print("Pred: \n", true_item.export_to_dataframe().to_markdown())

        for i, row in enumerate(true_item.data):
            for j, col in enumerate(true_item.data[i]):

                # print("true: ", true_item.data[i][j].text)
                # print("pred: ", pred_item.data[i][j].text)
                # print("")

                verify_text(
                    true_item.data[i][j].text, pred_item.data[i][j].text, fuzzy=fuzzy
                )

                assert (
                    true_item.data[i][j].obj_type == pred_item.data[i][j].obj_type
                ), "table-cell does not have the same type"

    return True


def verify_table_v2(true_item: TableItem, pred_item: TableItem, fuzzy: bool):
    assert (
        true_item.data.num_rows == pred_item.data.num_rows
    ), "table does not have the same #-rows"
    assert (
        true_item.data.num_cols == pred_item.data.num_cols
    ), "table does not have the same #-cols"

    assert true_item.data is not None, "documents are expected to have table data"
    assert pred_item.data is not None, "documents are expected to have table data"

    # print("True: \n", true_item.export_to_dataframe().to_markdown())
    # print("Pred: \n", true_item.export_to_dataframe().to_markdown())

    for i, row in enumerate(true_item.data.grid):
        for j, col in enumerate(true_item.data.grid[i]):

            # print("true: ", true_item.data[i][j].text)
            # print("pred: ", pred_item.data[i][j].text)
            # print("")

            verify_text(
                true_item.data.grid[i][j].text,
                pred_item.data.grid[i][j].text,
                fuzzy=fuzzy,
            )

            assert (
                true_item.data.grid[i][j].column_header
                == pred_item.data.grid[i][j].column_header
            ), "table-cell should be a column_header but prediction isn't"

            assert (
                true_item.data.grid[i][j].row_header
                == pred_item.data.grid[i][j].row_header
            ), "table-cell should be a row_header but prediction isn't"

            assert (
                true_item.data.grid[i][j].row_section
                == pred_item.data.grid[i][j].row_section
            ), "table-cell should be a row_section but prediction isn't"

    return True


def verify_picture_image_v2(
    true_image: PILImage.Image, pred_item: Optional[PILImage.Image]
):
    assert pred_item is not None, "predicted image is None"
    assert true_image.size == pred_item.size
    assert true_image.mode == pred_item.mode
    # assert true_image.tobytes() == pred_item.tobytes()
    return True


# def verify_output(doc_pred: DsDocument, doc_true: DsDocument):
#     #assert verify_maintext(doc_pred, doc_true), "verify_maintext(doc_pred, doc_true)"
#     assert verify_tables_v1(doc_pred, doc_true), "verify_tables(doc_pred, doc_true)"
#     return True


def verify_docitems(doc_pred: DoclingDocument, doc_true: DoclingDocument, fuzzy: bool):
    assert len(doc_pred.texts) == len(doc_true.texts), f"Text lengths do not match."

    assert len(doc_true.tables) == len(
        doc_pred.tables
    ), "document has different count of tables than expected."

    for (true_item, _true_level), (pred_item, _pred_level) in zip(
        doc_true.iterate_items(), doc_pred.iterate_items()
    ):
        if not isinstance(true_item, DocItem):
            continue
        assert isinstance(pred_item, DocItem), "Test item is not a DocItem"

        # Validate type
        assert true_item.label == pred_item.label, f"Object label does not match."

        # Validate provenance
        assert len(true_item.prov) == len(pred_item.prov), "Length of prov mismatch"
        if len(true_item.prov) > 0:
            true_prov = true_item.prov[0]
            pred_prov = pred_item.prov[0]

            assert true_prov.page_no == pred_prov.page_no, "Page provenance mistmatch"

            # TODO: add bbox check with tolerance

        # Validate text content
        if isinstance(true_item, TextItem):
            assert isinstance(pred_item, TextItem), (
                "Test item is not a TextItem as the expected one "
                f"{true_item=} "
                f"{pred_item=} "
            )

            assert verify_text(true_item.text, pred_item.text, fuzzy=fuzzy)

        # Validate table content
        if isinstance(true_item, TableItem):
            assert isinstance(
                pred_item, TableItem
            ), "Test item is not a TableItem as the expected one"
            assert verify_table_v2(
                true_item, pred_item, fuzzy=fuzzy
            ), "Tables not matching"

        # Validate picture content
        if isinstance(true_item, PictureItem):
            assert isinstance(
                pred_item, PictureItem
            ), "Test item is not a PictureItem as the expected one"

            true_image = true_item.get_image(doc=doc_true)
            pred_image = true_item.get_image(doc=doc_pred)
            if true_image is not None:
                assert verify_picture_image_v2(
                    true_image, pred_image
                ), "Picture image mismatch"

            # TODO: check picture annotations

    return True


def verify_md(doc_pred_md: str, doc_true_md: str, fuzzy: bool):
    return verify_text(doc_true_md, doc_pred_md, fuzzy)


def verify_dt(doc_pred_dt: str, doc_true_dt: str, fuzzy: bool):
    return verify_text(doc_true_dt, doc_pred_dt, fuzzy)


def verify_conversion_result_v1(
    input_path: Path,
    doc_result: ConversionResult,
    generate: bool = False,
    ocr_engine: str = None,
    fuzzy: bool = False,
):
    PageList = TypeAdapter(List[Page])

    assert (
        doc_result.status == ConversionStatus.SUCCESS
    ), f"Doc {input_path} did not convert successfully."

    doc_pred_pages: List[Page] = doc_result.pages
    doc_pred: DsDocument = doc_result.legacy_document
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", DeprecationWarning)
        doc_pred_md = doc_result.legacy_document.export_to_markdown()
        doc_pred_dt = doc_result.legacy_document.export_to_document_tokens()

    engine_suffix = "" if ocr_engine is None else f".{ocr_engine}"

    gt_subpath = input_path.parent / "groundtruth" / "docling_v1" / input_path.name
    if str(input_path.parent).endswith("pdf"):
        gt_subpath = (
            input_path.parent.parent / "groundtruth" / "docling_v1" / input_path.name
        )

    pages_path = gt_subpath.with_suffix(f"{engine_suffix}.pages.json")
    json_path = gt_subpath.with_suffix(f"{engine_suffix}.json")
    md_path = gt_subpath.with_suffix(f"{engine_suffix}.md")
    dt_path = gt_subpath.with_suffix(f"{engine_suffix}.doctags.txt")

    if generate:  # only used when re-generating truth
        pages_path.parent.mkdir(parents=True, exist_ok=True)
        with open(pages_path, "w") as fw:
            fw.write(json.dumps(doc_pred_pages, default=pydantic_encoder))

        json_path.parent.mkdir(parents=True, exist_ok=True)
        with open(json_path, "w") as fw:
            fw.write(json.dumps(doc_pred, default=pydantic_encoder))

        md_path.parent.mkdir(parents=True, exist_ok=True)
        with open(md_path, "w") as fw:
            fw.write(doc_pred_md)

        dt_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dt_path, "w") as fw:
            fw.write(doc_pred_dt)
    else:  # default branch in test
        with open(pages_path, "r") as fr:
            doc_true_pages = PageList.validate_json(fr.read())

        with open(json_path, "r") as fr:
            doc_true: DsDocument = DsDocument.model_validate_json(fr.read())

        with open(md_path, "r") as fr:
            doc_true_md = fr.read()

        with open(dt_path, "r") as fr:
            doc_true_dt = fr.read()

        if not fuzzy:
            assert verify_cells(
                doc_pred_pages, doc_true_pages
            ), f"Mismatch in PDF cell prediction for {input_path}"

        # assert verify_output(
        #    doc_pred, doc_true
        # ), f"Mismatch in JSON prediction for {input_path}"

        assert verify_tables_v1(
            doc_pred, doc_true, fuzzy=fuzzy
        ), f"verify_tables(doc_pred, doc_true) mismatch for {input_path}"

        assert verify_md(
            doc_pred_md, doc_true_md, fuzzy=fuzzy
        ), f"Mismatch in Markdown prediction for {input_path}"

        assert verify_dt(
            doc_pred_dt, doc_true_dt, fuzzy=fuzzy
        ), f"Mismatch in DocTags prediction for {input_path}"


def verify_conversion_result_v2(
    input_path: Path,
    doc_result: ConversionResult,
    generate: bool = False,
    ocr_engine: str = None,
    fuzzy: bool = False,
):
    PageList = TypeAdapter(List[Page])

    assert (
        doc_result.status == ConversionStatus.SUCCESS
    ), f"Doc {input_path} did not convert successfully."

    doc_pred_pages: List[Page] = doc_result.pages
    doc_pred: DoclingDocument = doc_result.document
    doc_pred_md = doc_result.document.export_to_markdown()
    doc_pred_dt = doc_result.document.export_to_document_tokens()

    engine_suffix = "" if ocr_engine is None else f".{ocr_engine}"

    gt_subpath = input_path.parent / "groundtruth" / "docling_v2" / input_path.name
    if str(input_path.parent).endswith("pdf"):
        gt_subpath = (
            input_path.parent.parent / "groundtruth" / "docling_v2" / input_path.name
        )

    pages_path = gt_subpath.with_suffix(f"{engine_suffix}.pages.json")
    json_path = gt_subpath.with_suffix(f"{engine_suffix}.json")
    md_path = gt_subpath.with_suffix(f"{engine_suffix}.md")
    dt_path = gt_subpath.with_suffix(f"{engine_suffix}.doctags.txt")

    if generate:  # only used when re-generating truth
        pages_path.parent.mkdir(parents=True, exist_ok=True)
        with open(pages_path, "w") as fw:
            fw.write(json.dumps(doc_pred_pages, default=pydantic_encoder))

        json_path.parent.mkdir(parents=True, exist_ok=True)
        with open(json_path, "w") as fw:
            fw.write(json.dumps(doc_pred, default=pydantic_encoder))

        md_path.parent.mkdir(parents=True, exist_ok=True)
        with open(md_path, "w") as fw:
            fw.write(doc_pred_md)

        dt_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dt_path, "w") as fw:
            fw.write(doc_pred_dt)
    else:  # default branch in test
        with open(pages_path, "r") as fr:
            doc_true_pages = PageList.validate_json(fr.read())

        with open(json_path, "r") as fr:
            doc_true: DoclingDocument = DoclingDocument.model_validate_json(fr.read())

        with open(md_path, "r") as fr:
            doc_true_md = fr.read()

        with open(dt_path, "r") as fr:
            doc_true_dt = fr.read()

        if not fuzzy:
            assert verify_cells(
                doc_pred_pages, doc_true_pages
            ), f"Mismatch in PDF cell prediction for {input_path}"

        # assert verify_output(
        #    doc_pred, doc_true
        # ), f"Mismatch in JSON prediction for {input_path}"

        assert verify_docitems(
            doc_pred, doc_true, fuzzy=fuzzy
        ), f"verify_docling_document(doc_pred, doc_true) mismatch for {input_path}"

        assert verify_md(
            doc_pred_md, doc_true_md, fuzzy=fuzzy
        ), f"Mismatch in Markdown prediction for {input_path}"

        assert verify_dt(
            doc_pred_dt, doc_true_dt, fuzzy=fuzzy
        ), f"Mismatch in DocTags prediction for {input_path}"


def verify_document(pred_doc: DoclingDocument, gtfile: str, generate: bool = False):

    if not os.path.exists(gtfile) or generate:
        with open(gtfile, "w") as fw:
            json.dump(pred_doc.export_to_dict(), fw, indent=2)

        return True
    else:
        with open(gtfile) as fr:
            true_doc = DoclingDocument.model_validate_json(fr.read())

        return verify_docitems(pred_doc, true_doc, fuzzy=False)


def verify_export(pred_text: str, gtfile: str, generate: bool = False) -> bool:
    file = Path(gtfile)

    if not file.exists() or generate:
        with file.open("w") as fw:
            fw.write(pred_text)
        return True

    with file.open("r") as fr:
        true_text = fr.read()

    return pred_text == true_text


================================================
File: tests/data/asciidoc/test_01.asciidoc
================================================
= 1st Sample Document Title

This is an abstract.

== Section 1

This is some introductory text in section 1.

This spans multiple lines but should be treated
as a single paragraph.
    
=== Subsection 1.1
* First list item
* Second list item

This is some introductory text in section 1.1.

- A dash list item
    
== Section 2
This is some text in section 2.
    
|Header 1|Header 2|
|Value 1|Value 2|
|Value 3|Value 4|

================================================
File: tests/data/asciidoc/test_02.asciidoc
================================================
= 2nd Sample Document Title

This is an abstract.

== Section 1: Testing nestedlists

* First item
  * Nested item 1
  * Nested item 2
* Second item
  1. Nested ordered item 1
  2. Nested ordered item 2
    * Deeper nested unordered item
* Third item
  1. Nested ordered item 1
  2. Nested ordered item 2
    * Deeper nested unordered item
  3. Nested ordered item 2

== Section 2

bla bla

==== SubSubSection 2.1.1

bla bla bla
bli bla ble

== Section 3: test image

image::images/example1.png[Example Image, width=200, height=150, align=center]

.An example caption for the image
image::images/example2.png[Example Image, width=200, height=150, align=center]

== Section 4: test tables

|Header 1|Header 2|
|Value 1|Value 2|
|Value 3|Value 4|

.Caption for the table 1
|===
|Header 1 |Header 2
|Value 1  |Value 2
|Value 3  |Value 4
|===

.Caption for the table 2
|=== 
|Column 1 Heading |Column 2 Heading |Column 3 Heading
|Cell 1 |Cell 2 |Cell 3
|Cell 4 |Cell 5 colspan=2|Cell spans two columns
|===

.Caption for the table 3
|===
|Column 1 Heading |Column 2 Heading |Column 3 Heading
|Rowspan=2 |Cell 2 |Cell 3
| |Cell 5 |Cell 6
|===

.Caption for the table 4
|===
|Col 1 |Col 2 |Col 3 |Col 4
|Rowspan=2.Colspan=2|Cell spanning 2 rows and 2 columns |Col 3 |Col 4
|   |   |Col 3 |Col 4
|Col 1 |Col 2 |Col 3 |Col 4
|===

================================================
File: tests/data/csv/csv-comma-in-cell.csv
================================================
1,2,3,4
a,b,c,d
a,",",c,d
a,b,c,d
a,b,c,d

================================================
File: tests/data/csv/csv-comma.csv
================================================
Index,Customer Id,First Name,Last Name,Company,City,Country,Phone 1,Phone 2,Email,Subscription Date,Website
1,DD37Cf93aecA6Dc,Sheryl,Baxter,Rasmussen Group,East Leonard,Chile,229.077.5154,397.884.0519x718,zunigavanessa@smith.info,2020-08-24,http://www.stephenson.com/
2,1Ef7b82A4CAAD10,Preston,"Lozano, Dr",Vega-Gentry,East Jimmychester,Djibouti,5153435776,686-620-1820x944,vmata@colon.com,2021-04-23,http://www.hobbs.com/
3,6F94879bDAfE5a6,Roy,Berry,Murillo-Perry,Isabelborough,Antigua and Barbuda,+1-539-402-0259,(496)978-3969x58947,beckycarr@hogan.com,2020-03-25,http://www.lawrence.com/
4,5Cef8BFA16c5e3c,Linda,Olsen,"Dominguez, Mcmillan and Donovan",Bensonview,Dominican Republic,001-808-617-6467x12895,+1-813-324-8756,stanleyblackwell@benson.org,2020-06-02,http://www.good-lyons.com/
5,053d585Ab6b3159,Joanna,Bender,"Martin, Lang and Andrade",West Priscilla,Slovakia (Slovak Republic),001-234-203-0635x76146,001-199-446-3860x3486,colinalvarado@miles.net,2021-04-17,https://goodwin-ingram.com/


================================================
File: tests/data/csv/csv-inconsistent-header.csv
================================================
1,2,3
a,b,c,d
a,b,c,d
a,b,c,d
a,b,c,d

================================================
File: tests/data/csv/csv-pipe.csv
================================================
Index|Customer Id|First Name|Last Name|Company|City|Country|Phone 1|Phone 2|Email|Subscription Date|Website
1|DD37Cf93aecA6Dc|Sheryl|Baxter|Rasmussen Group|East Leonard|Chile|229.077.5154|397.884.0519x718|zunigavanessa@smith.info|2020-08-24|http://www.stephenson.com/
2|1Ef7b82A4CAAD10|Preston|Lozano|Vega-Gentry|East Jimmychester|Djibouti|5153435776|686-620-1820x944|vmata@colon.com|2021-04-23|http://www.hobbs.com/
3|6F94879bDAfE5a6|Roy|Berry|Murillo-Perry|Isabelborough|Antigua and Barbuda|+1-539-402-0259|(496)978-3969x58947|beckycarr@hogan.com|2020-03-25|http://www.lawrence.com/
4|5Cef8BFA16c5e3c|Linda|Olsen|"Dominguez|Mcmillan and Donovan"|Bensonview|Dominican Republic|001-808-617-6467x12895|+1-813-324-8756|stanleyblackwell@benson.org|2020-06-02|http://www.good-lyons.com/
5|053d585Ab6b3159|Joanna|Bender|"Martin|Lang and Andrade"|West Priscilla|Slovakia (Slovak Republic)|001-234-203-0635x76146|001-199-446-3860x3486|colinalvarado@miles.net|2021-04-17|https://goodwin-ingram.com/


================================================
File: tests/data/csv/csv-semicolon.csv
================================================
Index;Customer Id;First Name;Last Name;Company;City;Country;Phone 1;Phone 2;Email;Subscription Date;Website
1;DD37Cf93aecA6Dc;Sheryl;Baxter;Rasmussen Group;East Leonard;Chile;229.077.5154;397.884.0519x718;zunigavanessa@smith.info;2020-08-24;http://www.stephenson.com/
2;1Ef7b82A4CAAD10;Preston;Lozano;Vega-Gentry;East Jimmychester;Djibouti;5153435776;686-620-1820x944;vmata@colon.com;2021-04-23;http://www.hobbs.com/
3;6F94879bDAfE5a6;Roy;Berry;Murillo-Perry;Isabelborough;Antigua and Barbuda;+1-539-402-0259;(496)978-3969x58947;beckycarr@hogan.com;2020-03-25;http://www.lawrence.com/
4;5Cef8BFA16c5e3c;Linda;Olsen;"Dominguez;Mcmillan and Donovan";Bensonview;Dominican Republic;001-808-617-6467x12895;+1-813-324-8756;stanleyblackwell@benson.org;2020-06-02;http://www.good-lyons.com/
5;053d585Ab6b3159;Joanna;Bender;"Martin;Lang and Andrade";West Priscilla;Slovakia (Slovak Republic);001-234-203-0635x76146;001-199-446-3860x3486;colinalvarado@miles.net;2021-04-17;https://goodwin-ingram.com/


================================================
File: tests/data/csv/csv-tab.csv
================================================
Index	Customer Id	First Name	Last Name	Company	City	Country	Phone 1	Phone 2	Email	Subscription Date	Website
1	DD37Cf93aecA6Dc	Sheryl	Baxter	Rasmussen Group	East Leonard	Chile	229.077.5154	397.884.0519x718	zunigavanessa@smith.info	2020-08-24	http://www.stephenson.com/
2	1Ef7b82A4CAAD10	Preston	Lozano	Vega-Gentry	East Jimmychester	Djibouti	5153435776	686-620-1820x944	vmata@colon.com	2021-04-23	http://www.hobbs.com/
3	6F94879bDAfE5a6	Roy	Berry	Murillo-Perry	Isabelborough	Antigua and Barbuda	+1-539-402-0259	(496)978-3969x58947	beckycarr@hogan.com	2020-03-25	http://www.lawrence.com/
4	5Cef8BFA16c5e3c	Linda	Olsen	"Dominguez	Mcmillan and Donovan"	Bensonview	Dominican Republic	001-808-617-6467x12895	+1-813-324-8756	stanleyblackwell@benson.org	2020-06-02	http://www.good-lyons.com/
5	053d585Ab6b3159	Joanna	Bender	"Martin	Lang and Andrade"	West Priscilla	Slovakia (Slovak Republic)	001-234-203-0635x76146	001-199-446-3860x3486	colinalvarado@miles.net	2021-04-17	https://goodwin-ingram.com/


================================================
File: tests/data/csv/csv-too-few-columns.csv
================================================
1,2,3,4
a,'b',c,d
a,b,c
a,b,c,d
a,b,c,d

================================================
File: tests/data/csv/csv-too-many-columns.csv
================================================
1,2,3,4
a,b,c,d
a,b,c,d,e
a,b,c,d
a,b,c,d

================================================
File: tests/data/groundtruth/docling_v1/2206.01062.md
================================================
## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis

Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com

Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com

Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com

Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com

Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com

## ABSTRACT

Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.

## CCS CONCEPTS

· Information systems → Document structure ; · Applied computing → Document analysis ; · Computing methodologies → Machine learning ; Computer vision ; Object detection ;

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

KDD ’22, August 14-18, 2022, Washington, DC, USA

© 2022 Copyright held by the owner/author(s).

ACM ISBN 978-1-4503-9385-0/22/08.

https://doi.org/10.1145/3534678.3539043

Figure 1: Four examples of complex page layouts across different document categories
<!-- image -->

## KEYWORDS

PDF document conversion, layout segmentation, object-detection, data set, Machine Learning

## ACM Reference Format:

Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043

## 1 INTRODUCTION

Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.

A key problem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.

In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:

- (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.

- (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.

- (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.

- (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.

This enables experimentation with annotation uncertainty and quality control analysis.

- (5) Pre-defined Train-, Test- & Validation-set : Like DocBank, we provide fixed train-, test- & validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.

All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.

In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.

## 2 RELATED WORK

While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].

Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.

## 3 THE DOCLAYNET DATASET

DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.

In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents

Figure 2: Distribution of DocLayNet pages across document categories.
<!-- image -->

to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( > 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing "text in the wild".

The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws & Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.

We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.

To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.

Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.

In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.

Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, "invisible" tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as "invisible" list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a "natural" upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.

## 4 ANNOTATION CAMPAIGN

The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,

Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row "Total") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.

|                |         | % of Total   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   |
|----------------|---------|--------------|--------------|--------------|--------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|
| class label    | Count   | Train        | Test         | Val          | All          | Fin                                         | Man                                         | Sci                                         | Law                                         | Pat                                         | Ten                                         |
| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89        | 40-61                                       | 86-92                                       | 94-99                                       | 95-99                                       | 69-78                                       | n/a                                         |
| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91        | n/a                                         | 100                                         | 62-88                                       | 85-94                                       | n/a                                         | 82-97                                       |
| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85        | n/a                                         | n/a                                         | 84-87                                       | 86-96                                       | n/a                                         | n/a                                         |
| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88        | 74-83                                       | 90-92                                       | 97-97                                       | 81-85                                       | 75-88                                       | 93-95                                       |
| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94        | 88-90                                       | 95-96                                       | 100                                         | 92-97                                       | 100                                         | 96-98                                       |
| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89        | 66-76                                       | 90-94                                       | 98-100                                      | 91-92                                       | 97-99                                       | 81-86                                       |
| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71        | 56-59                                       | 82-86                                       | 69-82                                       | 80-95                                       | 66-71                                       | 59-76                                       |
| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84        | 76-81                                       | 90-92                                       | 94-95                                       | 87-94                                       | 69-73                                       | 78-86                                       |
| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81        | 75-80                                       | 83-86                                       | 98-99                                       | 58-80                                       | 79-84                                       | 70-85                                       |
| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86        | 81-86                                       | 88-93                                       | 89-93                                       | 87-92                                       | 71-79                                       | 87-95                                       |
| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72        | 24-63                                       | 50-63                                       | 94-100                                      | 82-96                                       | 68-79                                       | 24-56                                       |
| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83        | 71-74                                       | 79-81                                       | 89-94                                       | 86-91                                       | 71-76                                       | 68-85                                       |

Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
<!-- image -->

we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.

Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv$^{3}$, government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.

Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.

Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on

the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.

At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.

Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:

- (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.

- (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.

- (3) For every Caption , there must be exactly one corresponding Picture or Table .

- (4) Connected sub-pictures are grouped together in one Picture object.

- (5) Formula numbers are included in a Formula object.

- (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.

The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.

Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations

<!-- image -->

05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0

Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.

were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.

Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted

Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.

|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
|----------------|---------|---------|---------|---------|--------|
|                | human   | R50     | R101    | R101    | v5x6   |
| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |

to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.

## 5 EXPERIMENTS

The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this

Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
<!-- image -->

paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.

In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].

## Baselines for Object Detection

In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.

Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.

| Class-count    |   11 | 6       | 5       | 4       |
|----------------|------|---------|---------|---------|
| Caption        |   68 | Text    | Text    | Text    |
| Footnote       |   71 | Text    | Text    | Text    |
| Formula        |   60 | Text    | Text    | Text    |
| List-item      |   81 | Text    | 82      | Text    |
| Page-footer    |   62 | 62      | -       | -       |
| Page-header    |   72 | 68      | -       | -       |
| Picture        |   72 | 72      | 72      | 72      |
| Section-header |   68 | 67      | 69      | 68      |
| Table          |   82 | 83      | 82      | 82      |
| Text           |   85 | 84      | 84      | 84      |
| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
| Overall        |   72 | 73      | 78      | 77      |

## Learning Curve

One of the fundamental questions related to any dataset is if it is "large enough". To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.

## Impact of Class Labels

The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of

Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in GLYPH<tildelow> 10% point improvement.

| Class-count    | 11   | 11   | 5   | 5    |
|----------------|------|------|-----|------|
| Split          | Doc  | Page | Doc | Page |
| Caption        | 68   | 83   |     |      |
| Footnote       | 71   | 84   |     |      |
| Formula        | 60   | 66   |     |      |
| List-item      | 81   | 88   | 82  | 88   |
| Page-footer    | 62   | 89   |     |      |
| Page-header    | 72   | 90   |     |      |
| Picture        | 72   | 82   | 72  | 82   |
| Section-header | 68   | 83   | 69  | 83   |
| Table          | 82   | 89   | 82  | 90   |
| Text           | 85   | 91   | 84  | 90   |
| Title          | 77   | 81   |     |      |
| All            | 72   | 84   | 78  | 87   |

lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.

## Impact of Document Split in Train and Test Set

Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.

## Dataset Comparison

Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,

Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank & DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.

|                 |            | Testing on   | Testing on   | Testing on   |
|-----------------|------------|--------------|--------------|--------------|
| Training on     | labels     | PLN          | DB           | DLN          |
| PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
| PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
| PubLayNet (PLN) | Table      | 95           | 24           | 49           |
| PubLayNet (PLN) | Text       | 96           | -            | 42           |
| PubLayNet (PLN) | total      | 93           | 34           | 30           |
| DocBank (DB)    | Figure     | 77           | 71           | 31           |
| DocBank (DB)    | Table      | 19           | 65           | 22           |
| DocBank (DB)    | total      | 48           | 68           | 27           |
| DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
| DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
| DocLayNet (DLN) | Table      | 87           | 43           | 82           |
| DocLayNet (DLN) | Text       | 77           | -            | 84           |
| DocLayNet (DLN) | total      | 59           | 47           | 78           |

Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .

For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.

## Example Predictions

To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.

## 6 CONCLUSION

In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.

From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.

To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.

## REFERENCES

- [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.

- [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.

- [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.

- [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.

- [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.

- [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.

- [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.

- [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.

- [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.

- [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.

- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.

- [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.

- [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu

Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title
<!-- image -->

Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.

Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.

- [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.

- [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.

- [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.

- [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.

- [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.

- [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.

- [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.

- [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.

- [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.

- [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.

================================================
File: tests/data/groundtruth/docling_v1/2305.03393v1-pg9.doctags.txt
================================================
<document>
<paragraph><location><page_1><loc_22><loc_81><loc_79><loc_85></location>order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.</paragraph>
<subtitle-level-1><location><page_1><loc_22><loc_77><loc_52><loc_79></location>5.1 Hyper Parameter Optimization</subtitle-level-1>
<paragraph><location><page_1><loc_22><loc_68><loc_79><loc_77></location>We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.</paragraph>
<table>
<location><page_1><loc_23><loc_41><loc_78><loc_57></location>
<caption>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption>
<row_0><col_0><col_header>#</col_0><col_1><col_header>#</col_1><col_2><col_header>Language</col_2><col_3><col_header>TEDs</col_3><col_4><col_header>TEDs</col_4><col_5><col_header>TEDs</col_5><col_6><col_header>mAP</col_6><col_7><col_header>Inference</col_7></row_0>
<row_1><col_0><col_header>enc-layers</col_0><col_1><col_header>dec-layers</col_1><col_2><col_header>Language</col_2><col_3><col_header>simple</col_3><col_4><col_header>complex</col_4><col_5><col_header>all</col_5><col_6><col_header>(0.75)</col_6><col_7><col_header>time (secs)</col_7></row_1>
<row_2><col_0><body>6</col_0><col_1><body>6</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.965 0.969</col_3><col_4><body>0.934 0.927</col_4><col_5><body>0.955 0.955</col_5><col_6><body>0.88 0.857</col_6><col_7><body>2.73 5.39</col_7></row_2>
<row_3><col_0><body>4</col_0><col_1><body>4</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.938</col_3><col_4><body>0.904</col_4><col_5><body>0.927</col_5><col_6><body>0.853</col_6><col_7><body>1.97</col_7></row_3>
<row_4><col_0><body></col_0><col_1><body></col_1><col_2><body>OTSL</col_2><col_3><body>0.952 0.923</col_3><col_4><body>0.909</col_4><col_5><body>0.938</col_5><col_6><body>0.843</col_6><col_7><body>3.77</col_7></row_4>
<row_5><col_0><body>2</col_0><col_1><body>4</col_1><col_2><body>HTML</col_2><col_3><body>0.945</col_3><col_4><body>0.897 0.901</col_4><col_5><body>0.915 0.931</col_5><col_6><body>0.859 0.834</col_6><col_7><body>1.91 3.81</col_7></row_5>
<row_6><col_0><body>4</col_0><col_1><body>2</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.952 0.944</col_3><col_4><body>0.92 0.903</col_4><col_5><body>0.942 0.931</col_5><col_6><body>0.857 0.824</col_6><col_7><body>1.22 2</col_7></row_6>
</table>
<caption><location><page_1><loc_22><loc_59><loc_79><loc_66></location>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption>
<subtitle-level-1><location><page_1><loc_22><loc_35><loc_43><loc_36></location>5.2 Quantitative Results</subtitle-level-1>
<paragraph><location><page_1><loc_22><loc_22><loc_79><loc_34></location>We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.</paragraph>
<paragraph><location><page_1><loc_22><loc_16><loc_79><loc_22></location>Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/2305.03393v1-pg9.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "2305.03393v1-pg9.pdf", "filename-prov": null, "document-hash": "1a36870a3e6aa062b563b50c1eaed40685b651ee03e0538453de65e7013b742f", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "8a5a8d9a1ae6cbd1dcedcad02ed10195aa71d1ac3e4d56be4ab72c858d7f543e", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [134.765, 639.09302, 480.59665, 675.53699], "page": 1, "span": [0, 163], "__ref_s3_data": null}], "text": "order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 612.79181, 318.45145, 625.29486], "page": 1, "span": [0, 32], "__ref_s3_data": null}], "text": "5.1 Hyper Parameter Optimization", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 536.57599, 480.59567, 608.88495], "page": 1, "span": [0, 423], "__ref_s3_data": null}], "text": "We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Table", "type": "table", "$ref": "#/tables/0"}, {"prov": [{"bbox": [134.765, 464.01782, 480.59890999999993, 519.20526], "page": 1, "span": [0, 398], "__ref_s3_data": null}], "text": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.", "type": "caption", "payload": null, "name": "Caption", "font": null}, {"prov": [{"bbox": [134.765, 273.82581000000005, 264.40829, 286.32889], "page": 1, "span": [0, 24], "__ref_s3_data": null}], "text": "5.2 Quantitative Results", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [134.765, 173.70000000000005, 480.72003, 269.91995], "page": 1, "span": [0, 555], "__ref_s3_data": null}], "text": "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.765, 125.88, 480.59857000000005, 174.27795000000003], "page": 1, "span": [0, 289], "__ref_s3_data": null}], "text": "Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [], "tables": [{"prov": [{"bbox": [139.66741943359375, 322.5054626464844, 475.00927734375, 454.45458984375], "page": 1, "span": [0, 0], "__ref_s3_data": null}], "text": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.", "type": "table", "payload": null, "#-cols": 8, "#-rows": 7, "data": [[{"bbox": [160.37, 339.45749, 168.04523, 350.74619], "spans": [[0, 0]], "text": "#", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [207.974, 339.45749, 215.64923000000002, 350.74619], "spans": [[0, 1]], "text": "#", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [239.79799999999997, 344.93649, 278.3338, 356.22519000000005], "spans": [[0, 2], [1, 2]], "text": "Language", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 3], "row": 0, "row-header": false, "row-span": [0, 2]}, {"bbox": [324.67001, 339.45749, 348.26419, 350.74619], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 3, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [324.67001, 339.45749, 348.26419, 350.74619], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 4, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [324.67001, 339.45749, 348.26419, 350.74619], "spans": [[0, 3], [0, 4], [0, 5]], "text": "TEDs", "type": "col_header", "col": 5, "col-header": true, "col-span": [3, 6], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [396.271, 339.45749, 417.12595, 350.74619], "spans": [[0, 6]], "text": "mAP", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 0, "row-header": false, "row-span": [0, 1]}, {"bbox": [430.771, 339.45749, 467.14142000000004, 350.74619], "spans": [[0, 7]], "text": "Inference", "type": "col_header", "col": 7, "col-header": true, "col-span": [7, 8], "row": 0, "row-header": false, "row-span": [0, 1]}], [{"bbox": [144.592, 352.40848, 183.82895, 363.69717], "spans": [[1, 0]], "text": "enc-layers", "type": "col_header", "col": 0, "col-header": true, "col-span": [0, 1], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [192.19501, 352.40848, 231.42303, 363.69717], "spans": [[1, 1]], "text": "dec-layers", "type": "col_header", "col": 1, "col-header": true, "col-span": [1, 2], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [239.79799999999997, 344.93649, 278.3338, 356.22519000000005], "spans": [[0, 2], [1, 2]], "text": "Language", "type": "col_header", "col": 2, "col-header": true, "col-span": [2, 3], "row": 1, "row-header": false, "row-span": [0, 2]}, {"bbox": [286.686, 352.40848, 312.32812, 363.69717], "spans": [[1, 3]], "text": "simple", "type": "col_header", "col": 3, "col-header": true, "col-span": [3, 4], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [320.702, 352.40848, 353.71539, 363.69717], "spans": [[1, 4]], "text": "complex", "type": "col_header", "col": 4, "col-header": true, "col-span": [4, 5], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [369.306, 352.40848, 379.02914, 363.69717], "spans": [[1, 5]], "text": "all", "type": "col_header", "col": 5, "col-header": true, "col-span": [5, 6], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [394.927, 350.41647, 418.46921, 361.70517], "spans": [[1, 6]], "text": "(0.75)", "type": "col_header", "col": 6, "col-header": true, "col-span": [6, 7], "row": 1, "row-header": false, "row-span": [1, 2]}, {"bbox": [427.14801, 350.41647, 470.76955999999996, 361.70517], "spans": [[1, 7]], "text": "time (secs)", "type": "col_header", "col": 7, "col-header": true, "col-span": [7, 8], "row": 1, "row-header": false, "row-span": [1, 2]}], [{"bbox": [161.90601, 371.23849, 166.51474, 382.52719], "spans": [[2, 0]], "text": "6", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [209.509, 371.23849, 214.11774, 382.52719], "spans": [[2, 1]], "text": "6", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [245.17598999999998, 365.75848, 272.94495, 389.99917999999997], "spans": [[2, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [289.017, 365.75848, 310.00732, 389.99917999999997], "spans": [[2, 3]], "text": "0.965 0.969", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [326.71701, 365.75848, 347.70734, 389.99917999999997], "spans": [[2, 4]], "text": "0.934 0.927", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [363.67599, 365.75848, 384.66632, 389.99917999999997], "spans": [[2, 5]], "text": "0.955 0.955", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [396.20599, 365.69571, 417.19632, 389.99917999999997], "spans": [[2, 6]], "text": "0.88 0.857", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 2, "row-header": false, "row-span": [2, 3]}, {"bbox": [439.52701, 365.69571, 458.38336, 389.99917999999997], "spans": [[2, 7]], "text": "2.73 5.39", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 2, "row-header": false, "row-span": [2, 3]}], [{"bbox": [161.90601, 397.53949, 166.51474, 408.82819], "spans": [[3, 0]], "text": "4", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [209.509, 397.53949, 214.11774, 408.82819], "spans": [[3, 1]], "text": "4", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [245.17598999999998, 392.06049, 272.94495, 416.30017], "spans": [[3, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [289.017, 392.06049, 310.00732, 403.34918], "spans": [[3, 3]], "text": "0.938", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [326.71701, 392.06049, 347.70734, 403.34918], "spans": [[3, 4]], "text": "0.904", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [363.67599, 392.06049, 384.66632, 403.34918], "spans": [[3, 5]], "text": "0.927", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [394.61801, 391.99771, 418.77798, 403.40298], "spans": [[3, 6]], "text": "0.853", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 3, "row-header": false, "row-span": [3, 4]}, {"bbox": [439.52701, 391.99771, 458.38336, 403.40298], "spans": [[3, 7]], "text": "1.97", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 3, "row-header": false, "row-span": [3, 4]}], [{"bbox": null, "spans": [[4, 0]], "text": "", "type": "body"}, {"bbox": null, "spans": [[4, 1]], "text": "", "type": "body"}, {"bbox": [246.71000999999998, 418.3614799999999, 271.41064, 429.65018], "spans": [[4, 2]], "text": "OTSL", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [289.017, 405.01147, 310.00732, 429.65018], "spans": [[4, 3]], "text": "0.952 0.923", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [326.71701, 405.01147, 347.70734, 416.30017], "spans": [[4, 4]], "text": "0.909", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [362.08801, 404.9486999999999, 386.24799, 416.35397], "spans": [[4, 5]], "text": "0.938", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [396.20599, 405.01147, 417.19632, 416.30017], "spans": [[4, 6]], "text": "0.843", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 4, "row-header": false, "row-span": [4, 5]}, {"bbox": [440.767, 405.01147, 457.15039, 416.30017], "spans": [[4, 7]], "text": "3.77", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 4, "row-header": false, "row-span": [4, 5]}], [{"bbox": [161.90601, 423.84048, 166.51474, 435.12918], "spans": [[5, 0]], "text": "2", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [209.509, 423.84048, 214.11774, 435.12918], "spans": [[5, 1]], "text": "4", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [245.17598999999998, 431.31246999999996, 272.94495, 442.60117], "spans": [[5, 2]], "text": "HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [289.017, 431.31246999999996, 310.00732, 442.60117], "spans": [[5, 3]], "text": "0.945", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [326.71701, 418.3614799999999, 347.70734, 442.60117], "spans": [[5, 4]], "text": "0.897 0.901", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [362.08801, 418.3614799999999, 386.24799, 442.65497], "spans": [[5, 5]], "text": "0.915 0.931", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [394.61801, 418.29871, 418.77798, 442.60117], "spans": [[5, 6]], "text": "0.859 0.834", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 5, "row-header": false, "row-span": [5, 6]}, {"bbox": [439.52701, 418.29871, 458.38336, 442.60117], "spans": [[5, 7]], "text": "1.91 3.81", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 5, "row-header": false, "row-span": [5, 6]}], [{"bbox": [161.90601, 450.14248999999995, 166.51474, 461.43118], "spans": [[6, 0]], "text": "4", "type": "body", "col": 0, "col-header": false, "col-span": [0, 1], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [209.509, 450.14248999999995, 214.11774, 461.43118], "spans": [[6, 1]], "text": "2", "type": "body", "col": 1, "col-header": false, "col-span": [1, 2], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [245.17598999999998, 444.66248, 272.94495, 468.90317], "spans": [[6, 2]], "text": "OTSL HTML", "type": "body", "col": 2, "col-header": false, "col-span": [2, 3], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [289.017, 444.66248, 310.00732, 468.90317], "spans": [[6, 3]], "text": "0.952 0.944", "type": "body", "col": 3, "col-header": false, "col-span": [3, 4], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [326.71701, 444.66248, 347.70734, 468.90317], "spans": [[6, 4]], "text": "0.92 0.903", "type": "body", "col": 4, "col-header": false, "col-span": [4, 5], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [362.08801, 444.5996999999999, 386.24799, 468.90317], "spans": [[6, 5]], "text": "0.942 0.931", "type": "body", "col": 5, "col-header": false, "col-span": [5, 6], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [394.61801, 444.5996999999999, 418.77798, 468.90317], "spans": [[6, 6]], "text": "0.857 0.824", "type": "body", "col": 6, "col-header": false, "col-span": [6, 7], "row": 6, "row-header": false, "row-span": [6, 7]}, {"bbox": [439.52701, 444.5996999999999, 458.38336, 468.90317], "spans": [[6, 7]], "text": "1.22 2", "type": "body", "col": 7, "col-header": false, "col-span": [7, 8], "row": 6, "row-header": false, "row-span": [6, 7]}]], "model": null, "bounding-box": null}], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 792.0, "page": 1, "width": 612.0}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/2305.03393v1-pg9.md
================================================
order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.

## 5.1 Hyper Parameter Optimization

We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.

Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.

| #          | #          | Language   | TEDs        | TEDs        | TEDs        | mAP         | Inference   |
|------------|------------|------------|-------------|-------------|-------------|-------------|-------------|
| enc-layers | dec-layers | Language   | simple      | complex     | all         | (0.75)      | time (secs) |
| 6          | 6          | OTSL HTML  | 0.965 0.969 | 0.934 0.927 | 0.955 0.955 | 0.88 0.857  | 2.73 5.39   |
| 4          | 4          | OTSL HTML  | 0.938       | 0.904       | 0.927       | 0.853       | 1.97        |
|            |            | OTSL       | 0.952 0.923 | 0.909       | 0.938       | 0.843       | 3.77        |
| 2          | 4          | HTML       | 0.945       | 0.897 0.901 | 0.915 0.931 | 0.859 0.834 | 1.91 3.81   |
| 4          | 2          | OTSL HTML  | 0.952 0.944 | 0.92 0.903  | 0.942 0.931 | 0.857 0.824 | 1.22 2      |

## 5.2 Quantitative Results

We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.

Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.

================================================
File: tests/data/groundtruth/docling_v1/2305.03393v1.doctags.txt
================================================
<document>
<subtitle-level-1><location><page_1><loc_22><loc_82><loc_79><loc_85></location>Optimized Table Tokenization for Table Structure Recognition</subtitle-level-1>
<paragraph><location><page_1><loc_23><loc_75><loc_78><loc_79></location>Maksym Lysak [0000 − 0002 − 3723 − $^{6960]}$, Ahmed Nassar[0000 − 0002 − 9468 − $^{0822]}$, Nikolaos Livathinos [0000 − 0001 − 8513 − $^{3491]}$, Christoph Auer[0000 − 0001 − 5761 − $^{0422]}$, [0000 − 0002 − 8088 − 0823]</paragraph>
<paragraph><location><page_1><loc_38><loc_74><loc_49><loc_75></location>and Peter Staar</paragraph>
<paragraph><location><page_1><loc_46><loc_72><loc_55><loc_73></location>IBM Research</paragraph>
<paragraph><location><page_1><loc_36><loc_70><loc_64><loc_71></location>{mly,ahn,nli,cau,taa}@zurich.ibm.com</paragraph>
<paragraph><location><page_1><loc_27><loc_41><loc_74><loc_66></location>Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.</paragraph>
<paragraph><location><page_1><loc_27><loc_37><loc_74><loc_40></location>Keywords: Table Structure Recognition · Data Representation · Transformers · Optimization.</paragraph>
<subtitle-level-1><location><page_1><loc_22><loc_33><loc_37><loc_34></location>1 Introduction</subtitle-level-1>
<paragraph><location><page_1><loc_22><loc_21><loc_79><loc_31></location>Tables are ubiquitous in documents such as scientific papers, patents, reports, manuals, specification sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and structure, making it difficult to recover their correct structure with simple analytical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods.</paragraph>
<paragraph><location><page_1><loc_22><loc_16><loc_79><loc_20></location>In modern document understanding systems [1,15], table extraction is typically a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of</paragraph>
<figure>
<location><page_2><loc_24><loc_46><loc_76><loc_74></location>
<caption>Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a fixed length of rows in OTSL).</caption>
</figure>
<paragraph><location><page_2><loc_22><loc_34><loc_79><loc_43></location>today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].</paragraph>
<paragraph><location><page_2><loc_22><loc_16><loc_79><loc_34></location>Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the network in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various configurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.</paragraph>
<paragraph><location><page_3><loc_22><loc_73><loc_79><loc_85></location>While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure representation language with an otherwise unmodified Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments.</paragraph>
<paragraph><location><page_3><loc_22><loc_58><loc_79><loc_73></location>The main contribution of this paper is the introduction of a new optimised table structure language (OTSL), specifically designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Specifically, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectangular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML.</paragraph>
<paragraph><location><page_3><loc_22><loc_45><loc_79><loc_58></location>The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws. Subsequently, we introduce OTSL in section 4, which includes the language definition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to TableFormer models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps.</paragraph>
<subtitle-level-1><location><page_3><loc_22><loc_40><loc_39><loc_42></location>2 Related Work</subtitle-level-1>
<paragraph><location><page_3><loc_22><loc_16><loc_79><loc_38></location>Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the problem of table structure recognition, i.e. the correct identification of columns, rows and spanning cells in a given table. Broadly speaking, the current deeplearning based approaches fall into three categories: object detection (OD) methods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geometric coordinates of the table cell. The edges of the graph define the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.</paragraph>
<paragraph><location><page_4><loc_22><loc_67><loc_79><loc_85></location>Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.</paragraph>
<paragraph><location><page_4><loc_22><loc_39><loc_79><loc_67></location>Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( <td> ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.</paragraph>
<paragraph><location><page_4><loc_22><loc_26><loc_79><loc_38></location>Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.</paragraph>
<subtitle-level-1><location><page_4><loc_22><loc_22><loc_44><loc_24></location>3 Problem Statement</subtitle-level-1>
<paragraph><location><page_4><loc_22><loc_16><loc_79><loc_20></location>All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-</paragraph>
<paragraph><location><page_5><loc_22><loc_76><loc_79><loc_85></location>ulary and can be interpreted as a table structure. For example, with the HTML tokens <table> , </table> , <tr> , </tr> , <td> and </td> , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.</paragraph>
<figure>
<location><page_5><loc_22><loc_57><loc_78><loc_71></location>
<caption>Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.</caption>
</figure>
<paragraph><location><page_5><loc_22><loc_33><loc_79><loc_54></location>Obviously, HTML and other general-purpose markup languages were not designed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artificially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( <td> and </td> ). Furthermore, when tokenizing the HTML structure, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 different HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure.</paragraph>
<paragraph><location><page_5><loc_22><loc_27><loc_79><loc_32></location>Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the verification of incomplete sequences is non-trivial or even impossible.</paragraph>
<paragraph><location><page_5><loc_22><loc_16><loc_79><loc_26></location>In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierarchical manner. Since the number of tokens for each table row and column can vary significantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence</paragraph>
<paragraph><location><page_6><loc_22><loc_82><loc_79><loc_85></location>generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output.</paragraph>
<paragraph><location><page_6><loc_22><loc_63><loc_79><loc_82></location>In practice, we observe two major issues with prediction quality when training Im2Seq models on HTML table structure generation from images. On the one hand, we find that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we find many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Figure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reflect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content.</paragraph>
<subtitle-level-1><location><page_6><loc_22><loc_58><loc_61><loc_60></location>4 Optimised Table Structure Language</subtitle-level-1>
<paragraph><location><page_6><loc_22><loc_44><loc_79><loc_56></location>To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both significantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture.</paragraph>
<subtitle-level-1><location><page_6><loc_22><loc_40><loc_43><loc_41></location>4.1 Language Definition</subtitle-level-1>
<paragraph><location><page_6><loc_22><loc_34><loc_79><loc_38></location>In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.</paragraph>
<paragraph><location><page_6><loc_24><loc_33><loc_67><loc_34></location>The OTSL vocabulary is comprised of the following tokens:</paragraph>
<paragraph><location><page_6><loc_23><loc_30><loc_75><loc_31></location>- -"C" cell a new table cell that either has or does not have cell content</paragraph>
<paragraph><location><page_6><loc_23><loc_27><loc_79><loc_29></location>- -"L" cell left-looking cell , merging with the left neighbor cell to create a span</paragraph>
<paragraph><location><page_6><loc_23><loc_24><loc_79><loc_26></location>- -"U" cell up-looking cell , merging with the upper neighbor cell to create a span</paragraph>
<paragraph><location><page_6><loc_23><loc_22><loc_74><loc_23></location>- -"X" cell cross cell , to merge with both left and upper neighbor cells</paragraph>
<paragraph><location><page_6><loc_23><loc_20><loc_54><loc_21></location>- -"NL" new-line , switch to the next row.</paragraph>
<paragraph><location><page_6><loc_22><loc_16><loc_79><loc_19></location>A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.</paragraph>
<figure>
<location><page_7><loc_27><loc_65><loc_73><loc_79></location>
<caption>Fig. 3. OTSL description of table structure: A - table example; B - graphical representation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding</caption>
</figure>
<subtitle-level-1><location><page_7><loc_22><loc_60><loc_40><loc_61></location>4.2 Language Syntax</subtitle-level-1>
<paragraph><location><page_7><loc_22><loc_58><loc_59><loc_59></location>The OTSL representation follows these syntax rules:</paragraph>
<paragraph><location><page_7><loc_23><loc_54><loc_79><loc_56></location>- 1. Left-looking cell rule : The left neighbour of an "L" cell must be either another "L" cell or a "C" cell.</paragraph>
<paragraph><location><page_7><loc_23><loc_51><loc_79><loc_53></location>- 2. Up-looking cell rule : The upper neighbour of a "U" cell must be either another "U" cell or a "C" cell.</paragraph>
<subtitle-level-1><location><page_7><loc_23><loc_49><loc_37><loc_50></location>3. Cross cell rule :</subtitle-level-1>
<paragraph><location><page_7><loc_25><loc_44><loc_79><loc_49></location>- The left neighbour of an "X" cell must be either another "X" cell or a "U" cell, and the upper neighbour of an "X" cell must be either another "X" cell or an "L" cell.</paragraph>
<paragraph><location><page_7><loc_23><loc_43><loc_78><loc_44></location>- 4. First row rule : Only "L" cells and "C" cells are allowed in the first row.</paragraph>
<paragraph><location><page_7><loc_23><loc_40><loc_79><loc_43></location>- 5. First column rule : Only "U" cells and "C" cells are allowed in the first column.</paragraph>
<paragraph><location><page_7><loc_23><loc_37><loc_79><loc_40></location>- 6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with "NL" token.</paragraph>
<paragraph><location><page_7><loc_22><loc_19><loc_79><loc_35></location>The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a "C"-cell token, which in case of spans is always located in the top-left corner of the table cell definition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid.</paragraph>
<paragraph><location><page_7><loc_22><loc_16><loc_79><loc_19></location>These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We find strong indications that this pattern</paragraph>
<paragraph><location><page_8><loc_22><loc_82><loc_79><loc_85></location>reduces significantly the column drift seen in the HTML based models (see Figure 5).</paragraph>
<subtitle-level-1><location><page_8><loc_22><loc_78><loc_52><loc_80></location>4.3 Error-detection and -mitigation</subtitle-level-1>
<paragraph><location><page_8><loc_22><loc_62><loc_79><loc_77></location>The design of OTSL allows to validate a table structure easily on an unfinished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee prediction correctness. Different heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can correct the predicted OTSL sequence on-the-fly is to verify if the token with the highest prediction confidence invalidates the predicted sequence, and replace it by the token with the next highest confidence until OTSL rules are satisfied.</paragraph>
<subtitle-level-1><location><page_8><loc_22><loc_58><loc_37><loc_59></location>5 Experiments</subtitle-level-1>
<paragraph><location><page_8><loc_22><loc_43><loc_79><loc_56></location>To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been converted into OTSL format for this purpose, and will be made publicly available.</paragraph>
<figure>
<location><page_8><loc_23><loc_25><loc_77><loc_36></location>
<caption>Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach.</caption>
</figure>
<paragraph><location><page_8><loc_22><loc_16><loc_79><loc_22></location>We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in</paragraph>
<paragraph><location><page_9><loc_22><loc_81><loc_79><loc_85></location>order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.</paragraph>
<subtitle-level-1><location><page_9><loc_22><loc_78><loc_52><loc_79></location>5.1 Hyper Parameter Optimization</subtitle-level-1>
<paragraph><location><page_9><loc_22><loc_68><loc_79><loc_77></location>We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.</paragraph>
<table>
<location><page_9><loc_23><loc_41><loc_78><loc_57></location>
<caption>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption>
<row_0><col_0><col_header>#</col_0><col_1><col_header>#</col_1><col_2><col_header>Language</col_2><col_3><col_header>TEDs</col_3><col_4><col_header>TEDs</col_4><col_5><col_header>TEDs</col_5><col_6><col_header>mAP</col_6><col_7><col_header>Inference</col_7></row_0>
<row_1><col_0><col_header>enc-layers</col_0><col_1><col_header>dec-layers</col_1><col_2><col_header>Language</col_2><col_3><col_header>simple</col_3><col_4><col_header>complex</col_4><col_5><col_header>all</col_5><col_6><col_header>(0.75)</col_6><col_7><col_header>time (secs)</col_7></row_1>
<row_2><col_0><body>6</col_0><col_1><body>6</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.965 0.969</col_3><col_4><body>0.934 0.927</col_4><col_5><body>0.955 0.955</col_5><col_6><body>0.88 0.857</col_6><col_7><body>2.73 5.39</col_7></row_2>
<row_3><col_0><body>4</col_0><col_1><body>4</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.938 0.952</col_3><col_4><body>0.904</col_4><col_5><body>0.927</col_5><col_6><body>0.853</col_6><col_7><body>1.97</col_7></row_3>
<row_4><col_0><body>2</col_0><col_1><body>4</col_1><col_2><body>OTSL</col_2><col_3><body>0.923 0.945</col_3><col_4><body>0.909 0.897</col_4><col_5><body>0.938</col_5><col_6><body>0.843</col_6><col_7><body>3.77</col_7></row_4>
<row_5><col_0><body></col_0><col_1><body></col_1><col_2><body>HTML</col_2><col_3><body></col_3><col_4><body>0.901</col_4><col_5><body>0.915 0.931</col_5><col_6><body>0.859 0.834</col_6><col_7><body>1.91 3.81</col_7></row_5>
<row_6><col_0><body>4</col_0><col_1><body>2</col_1><col_2><body>OTSL HTML</col_2><col_3><body>0.952 0.944</col_3><col_4><body>0.92 0.903</col_4><col_5><body>0.942 0.931</col_5><col_6><body>0.857 0.824</col_6><col_7><body>1.22 2</col_7></row_6>
</table>
<caption><location><page_9><loc_22><loc_59><loc_79><loc_65></location>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption>
<subtitle-level-1><location><page_9><loc_22><loc_35><loc_43><loc_36></location>5.2 Quantitative Results</subtitle-level-1>
<paragraph><location><page_9><loc_22><loc_22><loc_79><loc_34></location>We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.</paragraph>
<paragraph><location><page_9><loc_22><loc_16><loc_79><loc_22></location>Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.</paragraph>
<table>
<location><page_10><loc_23><loc_67><loc_77><loc_80></location>
<caption>Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).</caption>
<row_0><col_0><body></col_0><col_1><col_header>Language</col_1><col_2><col_header>TEDs</col_2><col_3><col_header>TEDs</col_3><col_4><col_header>TEDs</col_4><col_5><col_header>mAP(0.75)</col_5><col_6><col_header>Inference time (secs)</col_6></row_0>
<row_1><col_0><body></col_0><col_1><col_header>Language</col_1><col_2><col_header>simple</col_2><col_3><col_header>complex</col_3><col_4><col_header>all</col_4><col_5><col_header>mAP(0.75)</col_5><col_6><col_header>Inference time (secs)</col_6></row_1>
<row_2><col_0><row_header>PubTabNet</col_0><col_1><row_header>OTSL</col_1><col_2><body>0.965</col_2><col_3><body>0.934</col_3><col_4><body>0.955</col_4><col_5><body>0.88</col_5><col_6><body>2.73</col_6></row_2>
<row_3><col_0><row_header>PubTabNet</col_0><col_1><row_header>HTML</col_1><col_2><body>0.969</col_2><col_3><body>0.927</col_3><col_4><body>0.955</col_4><col_5><body>0.857</col_5><col_6><body>5.39</col_6></row_3>
<row_4><col_0><row_header>FinTabNet</col_0><col_1><row_header>OTSL</col_1><col_2><body>0.955</col_2><col_3><body>0.961</col_3><col_4><body>0.959</col_4><col_5><body>0.862</col_5><col_6><body>1.85</col_6></row_4>
<row_5><col_0><row_header>FinTabNet</col_0><col_1><row_header>HTML</col_1><col_2><body>0.917</col_2><col_3><body>0.922</col_3><col_4><body>0.92</col_4><col_5><body>0.722</col_5><col_6><body>3.26</col_6></row_5>
<row_6><col_0><row_header>PubTables-1M</col_0><col_1><row_header>OTSL</col_1><col_2><body>0.987</col_2><col_3><body>0.964</col_3><col_4><body>0.977</col_4><col_5><body>0.896</col_5><col_6><body>1.79</col_6></row_6>
<row_7><col_0><row_header>PubTables-1M</col_0><col_1><row_header>HTML</col_1><col_2><body>0.983</col_2><col_3><body>0.944</col_3><col_4><body>0.966</col_4><col_5><body>0.889</col_5><col_6><body>3.26</col_6></row_7>
</table>
<caption><location><page_10><loc_22><loc_82><loc_79><loc_85></location>Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).</caption>
<subtitle-level-1><location><page_10><loc_22><loc_62><loc_42><loc_64></location>5.3 Qualitative Results</subtitle-level-1>
<paragraph><location><page_10><loc_22><loc_54><loc_79><loc_61></location>To illustrate the qualitative differences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more effective in handling tables with longer token sequences, resulting in even more precise structure prediction and bounding boxes.</paragraph>
<figure>
<location><page_10><loc_27><loc_16><loc_74><loc_44></location>
<caption>Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap (E) than the HTML model (D), when predicting the structure of a sparse table (A), at twice the inference speed because of shorter sequence length (B),(C). "PMC2807444_006_00.png" PubTabNet. μ</caption>
</figure>
<paragraph><location><page_10><loc_37><loc_15><loc_38><loc_16></location>μ</paragraph>
<paragraph><location><page_10><loc_49><loc_12><loc_49><loc_14></location>≥</paragraph>
<figure>
<location><page_11><loc_28><loc_20><loc_73><loc_77></location>
<caption>Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn't complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. "PMC5406406_003_01.png" PubTabNet.</caption>
</figure>
<subtitle-level-1><location><page_12><loc_22><loc_84><loc_36><loc_85></location>6 Conclusion</subtitle-level-1>
<paragraph><location><page_12><loc_22><loc_74><loc_79><loc_81></location>We demonstrated that representing tables in HTML for the task of table structure recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key benefits.</paragraph>
<paragraph><location><page_12><loc_22><loc_59><loc_79><loc_74></location>First and foremost, given the same network configuration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance benefits can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be significantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1).</paragraph>
<paragraph><location><page_12><loc_22><loc_41><loc_79><loc_59></location>Secondly, OTSL has more inherent structure and a significantly restricted vocabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bounding boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a significant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Significant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-fly during sequence generation.</paragraph>
<subtitle-level-1><location><page_12><loc_22><loc_36><loc_32><loc_38></location>References</subtitle-level-1>
<paragraph><location><page_12><loc_23><loc_29><loc_79><loc_34></location>- 1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering document conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785</paragraph>
<paragraph><location><page_12><loc_23><loc_23><loc_79><loc_28></location>- 2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Fornés, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545561. Springer International Publishing, Cham (2022)</paragraph>
<paragraph><location><page_12><loc_23><loc_20><loc_79><loc_23></location>- 3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019)</paragraph>
<paragraph><location><page_12><loc_23><loc_16><loc_79><loc_20></location>- 4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894-901. IEEE (2019)</paragraph>
<paragraph><location><page_13><loc_23><loc_81><loc_79><loc_85></location>- 5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1-10 (2022)</paragraph>
<paragraph><location><page_13><loc_23><loc_76><loc_79><loc_81></location>- 6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 18681873. IEEE (2022)</paragraph>
<paragraph><location><page_13><loc_23><loc_73><loc_79><loc_75></location>- 7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019)</paragraph>
<paragraph><location><page_13><loc_23><loc_66><loc_79><loc_72></location>- 8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777</paragraph>
<paragraph><location><page_13><loc_23><loc_62><loc_79><loc_66></location>- 9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)</paragraph>
<paragraph><location><page_13><loc_22><loc_53><loc_79><loc_61></location>- 10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743-3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043</paragraph>
<paragraph><location><page_13><loc_22><loc_48><loc_79><loc_53></location>- 11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572-573 (2020)</paragraph>
<paragraph><location><page_13><loc_22><loc_42><loc_79><loc_48></location>- 12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)</paragraph>
<paragraph><location><page_13><loc_22><loc_37><loc_79><loc_42></location>- 13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226</paragraph>
<paragraph><location><page_13><loc_22><loc_31><loc_79><loc_36></location>- 14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive table extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June 2022)</paragraph>
<paragraph><location><page_13><loc_22><loc_23><loc_79><loc_31></location>- 15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service: A machine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 774-782. KDD '18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834</paragraph>
<paragraph><location><page_13><loc_22><loc_20><loc_79><loc_23></location>- 16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397</paragraph>
<paragraph><location><page_13><loc_22><loc_16><loc_79><loc_20></location>- 17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749-755. IEEE (2019)</paragraph>
<paragraph><location><page_14><loc_22><loc_81><loc_79><loc_85></location>- 18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295-1304 (2021)</paragraph>
<paragraph><location><page_14><loc_22><loc_76><loc_79><loc_81></location>- 19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup's solution for icdar 2021 competition on scientific literature parsing task b: Table recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848</paragraph>
<paragraph><location><page_14><loc_22><loc_73><loc_79><loc_75></location>- 20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022)</paragraph>
<paragraph><location><page_14><loc_22><loc_66><loc_79><loc_72></location>- 21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074</paragraph>
<paragraph><location><page_14><loc_22><loc_60><loc_79><loc_66></location>- 22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision - ECCV 2020. pp. 564-580. Springer International Publishing, Cham (2020)</paragraph>
<paragraph><location><page_14><loc_22><loc_56><loc_79><loc_60></location>- 23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/2305.03393v1.md
================================================
## Optimized Table Tokenization for Table Structure Recognition

Maksym Lysak [0000 − 0002 − 3723 − $^{6960]}$, Ahmed Nassar[0000 − 0002 − 9468 − $^{0822]}$, Nikolaos Livathinos [0000 − 0001 − 8513 − $^{3491]}$, Christoph Auer[0000 − 0001 − 5761 − $^{0422]}$, [0000 − 0002 − 8088 − 0823]

and Peter Staar

IBM Research

{mly,ahn,nli,cau,taa}@zurich.ibm.com

Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.

Keywords: Table Structure Recognition · Data Representation · Transformers · Optimization.

## 1 Introduction

Tables are ubiquitous in documents such as scientific papers, patents, reports, manuals, specification sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and structure, making it difficult to recover their correct structure with simple analytical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods.

In modern document understanding systems [1,15], table extraction is typically a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of

Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a fixed length of rows in OTSL).
<!-- image -->

today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].

Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the network in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various configurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.

While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure representation language with an otherwise unmodified Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments.

The main contribution of this paper is the introduction of a new optimised table structure language (OTSL), specifically designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Specifically, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectangular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML.

The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws. Subsequently, we introduce OTSL in section 4, which includes the language definition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to TableFormer models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps.

## 2 Related Work

Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the problem of table structure recognition, i.e. the correct identification of columns, rows and spanning cells in a given table. Broadly speaking, the current deeplearning based approaches fall into three categories: object detection (OD) methods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geometric coordinates of the table cell. The edges of the graph define the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.

Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.

Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( <td> ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.

Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.

## 3 Problem Statement

All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-

ulary and can be interpreted as a table structure. For example, with the HTML tokens <table> , </table> , <tr> , </tr> , <td> and </td> , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.

Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.
<!-- image -->

Obviously, HTML and other general-purpose markup languages were not designed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artificially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( <td> and </td> ). Furthermore, when tokenizing the HTML structure, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 different HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure.

Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the verification of incomplete sequences is non-trivial or even impossible.

In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierarchical manner. Since the number of tokens for each table row and column can vary significantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence

generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output.

In practice, we observe two major issues with prediction quality when training Im2Seq models on HTML table structure generation from images. On the one hand, we find that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we find many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Figure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reflect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content.

## 4 Optimised Table Structure Language

To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both significantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture.

## 4.1 Language Definition

In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.

The OTSL vocabulary is comprised of the following tokens:

- -"C" cell a new table cell that either has or does not have cell content

- -"L" cell left-looking cell , merging with the left neighbor cell to create a span

- -"U" cell up-looking cell , merging with the upper neighbor cell to create a span

- -"X" cell cross cell , to merge with both left and upper neighbor cells

- -"NL" new-line , switch to the next row.

A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.

Fig. 3. OTSL description of table structure: A - table example; B - graphical representation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding
<!-- image -->

## 4.2 Language Syntax

The OTSL representation follows these syntax rules:

- 1. Left-looking cell rule : The left neighbour of an "L" cell must be either another "L" cell or a "C" cell.

- 2. Up-looking cell rule : The upper neighbour of a "U" cell must be either another "U" cell or a "C" cell.

## 3. Cross cell rule :

- The left neighbour of an "X" cell must be either another "X" cell or a "U" cell, and the upper neighbour of an "X" cell must be either another "X" cell or an "L" cell.

- 4. First row rule : Only "L" cells and "C" cells are allowed in the first row.

- 5. First column rule : Only "U" cells and "C" cells are allowed in the first column.

- 6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with "NL" token.

The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a "C"-cell token, which in case of spans is always located in the top-left corner of the table cell definition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid.

These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We find strong indications that this pattern

reduces significantly the column drift seen in the HTML based models (see Figure 5).

## 4.3 Error-detection and -mitigation

The design of OTSL allows to validate a table structure easily on an unfinished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee prediction correctness. Different heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can correct the predicted OTSL sequence on-the-fly is to verify if the token with the highest prediction confidence invalidates the predicted sequence, and replace it by the token with the next highest confidence until OTSL rules are satisfied.

## 5 Experiments

To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been converted into OTSL format for this purpose, and will be made publicly available.

Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach.
<!-- image -->

We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in

order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.

## 5.1 Hyper Parameter Optimization

We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.

Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.

| #          | #          | Language   | TEDs        | TEDs        | TEDs        | mAP         | Inference   |
|------------|------------|------------|-------------|-------------|-------------|-------------|-------------|
| enc-layers | dec-layers | Language   | simple      | complex     | all         | (0.75)      | time (secs) |
| 6          | 6          | OTSL HTML  | 0.965 0.969 | 0.934 0.927 | 0.955 0.955 | 0.88 0.857  | 2.73 5.39   |
| 4          | 4          | OTSL HTML  | 0.938 0.952 | 0.904       | 0.927       | 0.853       | 1.97        |
| 2          | 4          | OTSL       | 0.923 0.945 | 0.909 0.897 | 0.938       | 0.843       | 3.77        |
|            |            | HTML       |             | 0.901       | 0.915 0.931 | 0.859 0.834 | 1.91 3.81   |
| 4          | 2          | OTSL HTML  | 0.952 0.944 | 0.92 0.903  | 0.942 0.931 | 0.857 0.824 | 1.22 2      |

## 5.2 Quantitative Results

We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.

Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.

Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).

|              | Language   | TEDs   | TEDs    | TEDs   | mAP(0.75)   | Inference time (secs)   |
|--------------|------------|--------|---------|--------|-------------|-------------------------|
|              | Language   | simple | complex | all    | mAP(0.75)   | Inference time (secs)   |
| PubTabNet    | OTSL       | 0.965  | 0.934   | 0.955  | 0.88        | 2.73                    |
| PubTabNet    | HTML       | 0.969  | 0.927   | 0.955  | 0.857       | 5.39                    |
| FinTabNet    | OTSL       | 0.955  | 0.961   | 0.959  | 0.862       | 1.85                    |
| FinTabNet    | HTML       | 0.917  | 0.922   | 0.92   | 0.722       | 3.26                    |
| PubTables-1M | OTSL       | 0.987  | 0.964   | 0.977  | 0.896       | 1.79                    |
| PubTables-1M | HTML       | 0.983  | 0.944   | 0.966  | 0.889       | 3.26                    |

## 5.3 Qualitative Results

To illustrate the qualitative differences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more effective in handling tables with longer token sequences, resulting in even more precise structure prediction and bounding boxes.

Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap (E) than the HTML model (D), when predicting the structure of a sparse table (A), at twice the inference speed because of shorter sequence length (B),(C). "PMC2807444_006_00.png" PubTabNet. μ
<!-- image -->

μ

≥

Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn't complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. "PMC5406406_003_01.png" PubTabNet.
<!-- image -->

## 6 Conclusion

We demonstrated that representing tables in HTML for the task of table structure recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key benefits.

First and foremost, given the same network configuration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance benefits can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be significantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1).

Secondly, OTSL has more inherent structure and a significantly restricted vocabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bounding boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a significant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Significant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-fly during sequence generation.

## References

- 1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering document conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785

- 2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Fornés, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545561. Springer International Publishing, Cham (2022)

- 3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019)

- 4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894-901. IEEE (2019)

- 5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1-10 (2022)

- 6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 18681873. IEEE (2022)

- 7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019)

- 8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777

- 9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)

- 10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743-3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043

- 11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572-573 (2020)

- 12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)

- 13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226

- 14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive table extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June 2022)

- 15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service: A machine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 774-782. KDD '18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834

- 16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397

- 17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749-755. IEEE (2019)

- 18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295-1304 (2021)

- 19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup's solution for icdar 2021 competition on scientific literature parsing task b: Table recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848

- 20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022)

- 21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074

- 22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision - ECCV 2020. pp. 564-580. Springer International Publishing, Cham (2020)

- 23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)

================================================
File: tests/data/groundtruth/docling_v1/amt_handbook_sample.doctags.txt
================================================
<document>
<paragraph><location><page_1><loc_12><loc_88><loc_53><loc_94></location>pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.</paragraph>
<paragraph><location><page_1><loc_12><loc_77><loc_53><loc_86></location>The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.</paragraph>
<subtitle-level-1><location><page_1><loc_12><loc_73><loc_28><loc_75></location>Boots Self-Locking Nut</subtitle-level-1>
<paragraph><location><page_1><loc_12><loc_64><loc_54><loc_73></location>The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.</paragraph>
<paragraph><location><page_1><loc_12><loc_52><loc_53><loc_62></location>The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.</paragraph>
<paragraph><location><page_1><loc_12><loc_38><loc_54><loc_50></location>The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.</paragraph>
<paragraph><location><page_1><loc_12><loc_33><loc_53><loc_36></location>Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is</paragraph>
<figure>
<location><page_1><loc_12><loc_10><loc_52><loc_31></location>
<caption>Figure 7-26. Self-locking nuts.</caption>
</figure>
<paragraph><location><page_1><loc_54><loc_85><loc_95><loc_94></location>the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.</paragraph>
<paragraph><location><page_1><loc_54><loc_83><loc_55><loc_85></location>.</paragraph>
<subtitle-level-1><location><page_1><loc_54><loc_82><loc_76><loc_83></location>Stainless Steel Self-Locking Nut</subtitle-level-1>
<paragraph><location><page_1><loc_54><loc_54><loc_96><loc_81></location>The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.</paragraph>
<subtitle-level-1><location><page_1><loc_54><loc_51><loc_65><loc_52></location>Elastic Stop Nut</subtitle-level-1>
<paragraph><location><page_1><loc_54><loc_47><loc_93><loc_50></location>The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This</paragraph>
<figure>
<location><page_1><loc_54><loc_11><loc_94><loc_46></location>
<caption>Figure 7-27. Stainless steel self-locking nut.</caption>
</figure>
</document>

================================================
File: tests/data/groundtruth/docling_v1/amt_handbook_sample.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "amt_handbook_sample.pdf", "filename-prov": null, "document-hash": "4ba7cdbd9ce8155d692d8f477f88bb3ec1acc2a463cf1e0209d1e624e58ebce9", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "f31706a847734c62e1e41f9f792c756283d1d4955552c1cc7f5e23c351bdd7cb", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [71.992126, 681.34637, 314.11212, 730.31635], "page": 1, "span": [0, 244], "__ref_s3_data": null}], "text": "pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [71.992302, 593.84637, 313.1546, 667.81635], "page": 1, "span": [0, 376], "__ref_s3_data": null}], "text": "The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [71.992302, 568.84637, 167.27231, 580.1864], "page": 1, "span": [0, 22], "__ref_s3_data": null}], "text": "Boots Self-Locking Nut", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [71.992294, 491.84637000000004, 318.49225, 565.81635], "page": 1, "span": [0, 319], "__ref_s3_data": null}], "text": "The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [71.992294, 404.34637000000004, 316.65729, 478.31638000000004], "page": 1, "span": [0, 332], "__ref_s3_data": null}], "text": "The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [71.992294, 291.84637, 318.81229, 390.81638000000004], "page": 1, "span": [0, 477], "__ref_s3_data": null}], "text": "The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [71.992294, 254.34636999999998, 313.91229, 278.31638], "page": 1, "span": [0, 122], "__ref_s3_data": null}], "text": "Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/0"}, {"prov": [{"bbox": [320.99231, 656.34637, 561.80835, 730.31635], "page": 1, "span": [0, 368], "__ref_s3_data": null}], "text": "the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [320.99542, 643.84637, 325.99542, 655.31635], "page": 1, "span": [0, 1], "__ref_s3_data": null}], "text": ".", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [320.99542, 631.34637, 450.99542, 642.6864], "page": 1, "span": [0, 32], "__ref_s3_data": null}], "text": "Stainless Steel Self-Locking Nut", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [320.99542, 416.84637000000004, 568.00439, 628.31635], "page": 1, "span": [0, 1015], "__ref_s3_data": null}], "text": "The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [320.99542, 391.84637000000004, 388.50543, 403.18636999999995], "page": 1, "span": [0, 16], "__ref_s3_data": null}], "text": "Elastic Stop Nut", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [320.99542, 364.84637000000004, 552.35132, 388.81638000000004], "page": 1, "span": [0, 108], "__ref_s3_data": null}], "text": "The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/1"}], "figures": [{"prov": [{"bbox": [70.59269714355469, 79.6090087890625, 309.863037109375, 242.77777099609375], "page": 1, "span": [0, 31], "__ref_s3_data": null}], "text": "Figure 7-26. Self-locking nuts.", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [320.4467468261719, 81.689208984375, 558.8576049804688, 352.359375], "page": 1, "span": [0, 46], "__ref_s3_data": null}], "text": "Figure 7-27. Stainless steel self-locking nut.", "type": "figure", "payload": null, "bounding-box": null}], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 774.0, "page": 1, "width": 594.0}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/amt_handbook_sample.md
================================================
pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.

The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.

## Boots Self-Locking Nut

The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.

The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.

The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.

Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is

Figure 7-26. Self-locking nuts.
<!-- image -->

the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.

.

## Stainless Steel Self-Locking Nut

The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.

## Elastic Stop Nut

The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This

Figure 7-27. Stainless steel self-locking nut.
<!-- image -->

================================================
File: tests/data/groundtruth/docling_v1/code_and_formula.doctags.txt
================================================
<document>
<subtitle-level-1><location><page_1><loc_22><loc_83><loc_52><loc_84></location>JavaScript Code Example</subtitle-level-1>
<paragraph><location><page_1><loc_22><loc_63><loc_78><loc_81></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<paragraph><location><page_1><loc_22><loc_57><loc_78><loc_63></location>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,</paragraph>
<paragraph><location><page_1><loc_22><loc_49><loc_43><loc_54></location>function add(a, b) { return a + b; } console.log(add(3, 5));</paragraph>
<caption><location><page_1><loc_36><loc_55><loc_63><loc_56></location>Listing 1: Simple JavaScript Program</caption>
<paragraph><location><page_1><loc_22><loc_29><loc_78><loc_47></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<paragraph><location><page_1><loc_22><loc_23><loc_78><loc_29></location>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,</paragraph>
<subtitle-level-1><location><page_2><loc_22><loc_84><loc_32><loc_85></location>Formula</subtitle-level-1>
<paragraph><location><page_2><loc_22><loc_66><loc_80><loc_82></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<paragraph><location><page_2><loc_22><loc_58><loc_80><loc_65></location>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.</paragraph>
<paragraph><location><page_2><loc_22><loc_38><loc_80><loc_55></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<paragraph><location><page_2><loc_22><loc_29><loc_80><loc_37></location>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.</paragraph>
<paragraph><location><page_2><loc_22><loc_21><loc_80><loc_29></location>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/code_and_formula.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "code_and_formula.pdf", "filename-prov": null, "document-hash": "821fdb0aa6d749c0adf24279b59d8030f6725a82e6566b5710c69c635d6a5e5f", "#-pages": 2, "collection-name": null, "description": null, "page-hashes": [{"hash": "321a2b1c88480306c4b1861c6db6764166f689017eb00dc38cfd50b526a68274", "model": "default", "page": 1}, {"hash": "6b116ec9598ebf64cc6de54e9ab2e896990e68d68a5e0fdf6b6bd314b29cdd5d", "model": "default", "page": 2}]}, "main-text": [{"prov": [{"bbox": [133.76801, 654.45184, 315.91595, 667.19122], "page": 1, "span": [0, 23], "__ref_s3_data": null}], "text": "JavaScript Code Example", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [133.76801, 501.97412, 477.48276, 642.32806], "page": 1, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76801, 454.15417, 477.47876, 498.86591], "page": 1, "span": [0, 298], "__ref_s3_data": null}], "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [134.239, 385.25446, 263.22409, 425.6004899999999], "page": 1, "span": [0, 60], "__ref_s3_data": null}], "text": "function add(a, b) { return a + b; } console.log(add(3, 5));", "type": "paragraph", "payload": null, "name": "Code", "font": null}, {"prov": [{"bbox": [223.15500000000003, 433.23218, 388.09375, 442.07895], "page": 1, "span": [0, 36], "__ref_s3_data": null}], "text": "Listing 1: Simple JavaScript Program", "type": "caption", "payload": null, "name": "Caption", "font": null}, {"prov": [{"bbox": [133.76801, 232.58536000000004, 477.48172000000005, 372.93902999999995], "page": 1, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76801, 184.76436, 477.47876, 229.47713999999996], "page": 1, "span": [0, 298], "__ref_s3_data": null}], "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76801021944917, 704.341863888975, 191.5272403142044, 717.0812439593145], "page": 2, "span": [0, 7], "__ref_s3_data": null}], "text": "Formula", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [133.76801021944917, 551.8641430470798, 477.48276078332026, 692.2180838220343], "page": 2, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76801021944917, 492.0881027170305, 477.48163078331845, 548.7559230299179], "page": 2, "span": [0, 369], "__ref_s3_data": null}], "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [280.5540204602546, 468.178102585013, 330.6965605425145, 479.06467264512247], "page": 2, "span": [0, 0], "__ref_s3_data": null}], "text": "", "type": "equation", "payload": null, "name": "Formula", "font": null}, {"prov": [{"bbox": [133.76799021944913, 318.7382217598911, 477.4816907833186, 459.091862534844], "page": 2, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76799021944913, 247.0072913638337, 477.48370078332186, 315.6300017427293], "page": 2, "span": [0, 415], "__ref_s3_data": null}], "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76799021944913, 175.27629096777594, 477.48370078332186, 243.8990813466719], "page": 2, "span": [0, 415], "__ref_s3_data": null}], "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 792.0, "page": 1, "width": 612.0}, {"height": 841.8900146484375, "page": 2, "width": 595.2760009765625}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/code_and_formula.md
================================================
## JavaScript Code Example

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,

function add(a, b) { return a + b; } console.log(add(3, 5));

Listing 1: Simple JavaScript Program

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,

## Formula

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.

================================================
File: tests/data/groundtruth/docling_v1/picture_classification.doctags.txt
================================================
<document>
<subtitle-level-1><location><page_1><loc_22><loc_83><loc_41><loc_84></location>Figures Example</subtitle-level-1>
<paragraph><location><page_1><loc_22><loc_63><loc_78><loc_81></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<figure>
<location><page_1><loc_22><loc_36><loc_78><loc_62></location>
<caption>Figure 1: This is an example image.</caption>
</figure>
<paragraph><location><page_1><loc_22><loc_15><loc_78><loc_30></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.</paragraph>
<paragraph><location><page_2><loc_22><loc_66><loc_78><loc_84></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</paragraph>
<figure>
<location><page_2><loc_36><loc_36><loc_64><loc_65></location>
<caption>Figure 2: This is an example image.</caption>
</figure>
<paragraph><location><page_2><loc_22><loc_15><loc_78><loc_31></location>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/picture_classification.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "picture_classification.pdf", "filename-prov": null, "document-hash": "959854dff729acaa22404d629a45cefcad8d942e595961185fc03a80d9fcc3a1", "#-pages": 2, "collection-name": null, "description": null, "page-hashes": [{"hash": "d9e3fc1226356b30c66012f05ad14089b00c59ea129195cd6ff8a0c68bda6f39", "model": "default", "page": 1}, {"hash": "9386884e13a97ce9662210a7e4258bbbb4f2e0e00663636160918e55b2806575", "model": "default", "page": 2}]}, "main-text": [{"prov": [{"bbox": [133.76801, 654.45184, 252.35513, 667.19122], "page": 1, "span": [0, 15], "__ref_s3_data": null}], "text": "Figures Example", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [133.76801, 501.97412, 477.48276, 642.32806], "page": 1, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/0"}, {"prov": [{"bbox": [133.76801, 122.51225, 477.48172000000005, 238.95505000000003], "page": 1, "span": [0, 747], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [133.76801, 523.7951, 477.48172000000005, 664.1490499999999], "page": 2, "span": [0, 887], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/1"}, {"prov": [{"bbox": [133.76801, 117.32024000000001, 477.48172000000005, 245.71804999999995], "page": 2, "span": [0, 804], "__ref_s3_data": null}], "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [{"prov": [{"bbox": [134.9200439453125, 281.78173828125, 475.6635437011719, 487.109375], "page": 1, "span": [0, 35], "__ref_s3_data": null}], "text": "Figure 1: This is an example image.", "type": "figure", "payload": null, "bounding-box": null}, {"prov": [{"bbox": [218.8155517578125, 283.10589599609375, 391.96246337890625, 513.9846496582031], "page": 2, "span": [0, 35], "__ref_s3_data": null}], "text": "Figure 2: This is an example image.", "type": "figure", "payload": null, "bounding-box": null}], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 792.0, "page": 1, "width": 612.0}, {"height": 792.0, "page": 2, "width": 612.0}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/picture_classification.md
================================================
## Figures Example

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Figure 1: This is an example image.
<!-- image -->

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Figure 2: This is an example image.
<!-- image -->

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.

================================================
File: tests/data/groundtruth/docling_v1/redp5110_sampled.doctags.txt
================================================
<document>
<paragraph><location><page_1><loc_47><loc_94><loc_68><loc_96></location>Front cover</paragraph>
<figure>
<location><page_1><loc_84><loc_93><loc_96><loc_97></location>
</figure>
<subtitle-level-1><location><page_1><loc_6><loc_79><loc_96><loc_89></location>Row and Column Access Control Support in IBM DB2 for i</subtitle-level-1>
<figure>
<location><page_1><loc_5><loc_11><loc_96><loc_63></location>
</figure>
<paragraph><location><page_1><loc_47><loc_94><loc_68><loc_96></location>Front cover</paragraph>
<subtitle-level-1><location><page_2><loc_11><loc_88><loc_28><loc_91></location>Contents</subtitle-level-1>
<paragraph><location><page_3><loc_11><loc_89><loc_39><loc_91></location>DB2 for i Center of Excellence</paragraph>
<paragraph><location><page_3><loc_15><loc_80><loc_38><loc_83></location>Solution Brief IBM Systems Lab Services and Training</paragraph>
<figure>
<location><page_3><loc_23><loc_64><loc_29><loc_66></location>
</figure>
<subtitle-level-1><location><page_3><loc_24><loc_57><loc_31><loc_59></location>Highlights</subtitle-level-1>
<paragraph><location><page_3><loc_24><loc_55><loc_40><loc_56></location>- GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g81>GLYPH<g75>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g72>GLYPH<g3> GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g73>GLYPH<g82>GLYPH<g85>GLYPH<g80>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g92>GLYPH<g82>GLYPH<g88>GLYPH<g85> GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86></paragraph>
<paragraph><location><page_3><loc_24><loc_51><loc_42><loc_54></location>- GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g68>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g74>GLYPH<g85>GLYPH<g72>GLYPH<g68>GLYPH<g87>GLYPH<g72>GLYPH<g85>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g87>GLYPH<g88>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g55>GLYPH<g3> GLYPH<g83>GLYPH<g85>GLYPH<g82>GLYPH<g77>GLYPH<g72>GLYPH<g70>GLYPH<g87>GLYPH<g86> GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g85>GLYPH<g82>GLYPH<g88>GLYPH<g74>GLYPH<g75>GLYPH<g3> GLYPH<g80>GLYPH<g82>GLYPH<g71>GLYPH<g72>GLYPH<g85> GLYPH<g81>GLYPH<g76>GLYPH<g93>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71> GLYPH<g3> GLYPH<g68>GLYPH<g83>GLYPH<g83>GLYPH<g79>GLYPH<g76>GLYPH<g70>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86></paragraph>
<paragraph><location><page_3><loc_24><loc_48><loc_41><loc_50></location>- GLYPH<g115>GLYPH<g3> GLYPH<g53>GLYPH<g72>GLYPH<g79>GLYPH<g92>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g37>GLYPH<g48>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g3> GLYPH<g70>GLYPH<g82>GLYPH<g81>GLYPH<g86>GLYPH<g88>GLYPH<g79>GLYPH<g87>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g15>GLYPH<g3> GLYPH<g86>GLYPH<g78>GLYPH<g76>GLYPH<g79>GLYPH<g79>GLYPH<g86> GLYPH<g3> GLYPH<g86>GLYPH<g75>GLYPH<g68>GLYPH<g85>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g81>GLYPH<g82>GLYPH<g90>GLYPH<g81>GLYPH<g3> GLYPH<g86>GLYPH<g72>GLYPH<g85>GLYPH<g89>GLYPH<g76>GLYPH<g70>GLYPH<g72>GLYPH<g86></paragraph>
<paragraph><location><page_3><loc_24><loc_45><loc_38><loc_47></location>- GLYPH<g115>GLYPH<g3> GLYPH<g55> GLYPH<g68>GLYPH<g78>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g71>GLYPH<g89>GLYPH<g68>GLYPH<g81>GLYPH<g87>GLYPH<g68>GLYPH<g74>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g68>GLYPH<g70>GLYPH<g70>GLYPH<g72>GLYPH<g86>GLYPH<g86>GLYPH<g3> GLYPH<g87>GLYPH<g82>GLYPH<g3> GLYPH<g68> GLYPH<g3> GLYPH<g90>GLYPH<g82>GLYPH<g85>GLYPH<g79>GLYPH<g71>GLYPH<g90>GLYPH<g76>GLYPH<g71>GLYPH<g72>GLYPH<g3> GLYPH<g86>GLYPH<g82>GLYPH<g88>GLYPH<g85>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g76>GLYPH<g86>GLYPH<g72></paragraph>
<figure>
<location><page_3><loc_10><loc_13><loc_42><loc_24></location>
</figure>
<paragraph><location><page_3><loc_75><loc_82><loc_83><loc_83></location>Power Services</paragraph>
<subtitle-level-1><location><page_3><loc_46><loc_65><loc_76><loc_70></location>DB2 for i Center of Excellence</subtitle-level-1>
<paragraph><location><page_3><loc_46><loc_64><loc_79><loc_65></location>Expert help to achieve your business requirements</paragraph>
<subtitle-level-1><location><page_3><loc_46><loc_59><loc_72><loc_60></location>We build confident, satisfied clients</subtitle-level-1>
<paragraph><location><page_3><loc_46><loc_56><loc_80><loc_59></location>No one else has the vast consulting experiences, skills sharing and renown service offerings to do what we can do for you.</paragraph>
<paragraph><location><page_3><loc_46><loc_54><loc_60><loc_55></location>Because no one else is IBM.</paragraph>
<paragraph><location><page_3><loc_46><loc_46><loc_82><loc_52></location>With combined experiences and direct access to development groups, we're the experts in IBM DB2® for i. The DB2 for i Center of Excellence (CoE) can help you achieve-perhaps reexamine and exceed-your business requirements and gain more confidence and satisfaction in IBM product data management products and solutions.</paragraph>
<subtitle-level-1><location><page_3><loc_46><loc_44><loc_71><loc_45></location>Who we are, some of what we do</subtitle-level-1>
<paragraph><location><page_3><loc_46><loc_42><loc_71><loc_43></location>Global CoE engagements cover topics including:</paragraph>
<paragraph><location><page_3><loc_46><loc_40><loc_66><loc_41></location>- r Database performance and scalability</paragraph>
<paragraph><location><page_3><loc_46><loc_39><loc_69><loc_39></location>- r Advanced SQL knowledge and skills transfer</paragraph>
<paragraph><location><page_3><loc_46><loc_37><loc_64><loc_38></location>- r Business intelligence and analytics</paragraph>
<paragraph><location><page_3><loc_46><loc_36><loc_56><loc_37></location>- r DB2 Web Query</paragraph>
<paragraph><location><page_3><loc_46><loc_35><loc_82><loc_36></location>- r Query/400 modernization for better reporting and analysis capabilities</paragraph>
<paragraph><location><page_3><loc_46><loc_33><loc_69><loc_34></location>- r Database modernization and re-engineering</paragraph>
<paragraph><location><page_3><loc_46><loc_32><loc_65><loc_33></location>- r Data-centric architecture and design</paragraph>
<paragraph><location><page_3><loc_46><loc_31><loc_76><loc_32></location>- r Extremely large database and overcoming limits to growth</paragraph>
<paragraph><location><page_3><loc_46><loc_30><loc_62><loc_30></location>- r ISV education and enablement</paragraph>
<subtitle-level-1><location><page_4><loc_11><loc_88><loc_25><loc_91></location>Preface</subtitle-level-1>
<paragraph><location><page_4><loc_22><loc_75><loc_89><loc_83></location>This IBMfi Redpaper™ publication provides information about the IBM i 7.2 feature of IBM DB2fi for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.</paragraph>
<paragraph><location><page_4><loc_22><loc_67><loc_89><loc_73></location>This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.</paragraph>
<paragraph><location><page_4><loc_22><loc_57><loc_89><loc_60></location>This paper was produced by the IBM DB2 for i Center of Excellence team in partnership with the International Technical Support Organization (ITSO), Rochester, Minnesota US.</paragraph>
<figure>
<location><page_4><loc_23><loc_36><loc_41><loc_53></location>
</figure>
<paragraph><location><page_4><loc_43><loc_35><loc_88><loc_53></location>Jim Bainbridge is a senior DB2 consultant on the DB2 for i Center of Excellence team in the IBM Lab Services and Training organization. His primary role is training and implementation services for IBM DB2 Web Query for i and business analytics. Jim began his career with IBM 30 years ago in the IBM Rochester Development Lab, where he developed cooperative processing products that paired IBM PCs with IBM S/36 and AS/.400 systems. In the years since, Jim has held numerous technical roles, including independent software vendors technical support on a broad range of IBM technologies and products, and supporting customers in the IBM Executive Briefing Center and IBM Project Office.</paragraph>
<figure>
<location><page_4><loc_24><loc_20><loc_41><loc_33></location>
</figure>
<paragraph><location><page_4><loc_43><loc_14><loc_88><loc_33></location>Hernando Bedoya is a Senior IT Specialist at STG Lab Services and Training in Rochester, Minnesota. He writes extensively and teaches IBM classes worldwide in all areas of DB2 for i. Before joining STG Lab Services, he worked in the ITSO for nine years writing multiple IBM Redbooksfi publications. He also worked for IBM Colombia as an IBM AS/400fi IT Specialist doing presales support for the Andean countries. He has 28 years of experience in the computing field and has taught database classes in Colombian universities. He holds a Master's degree in Computer Science from EAFIT, Colombia. His areas of expertise are database technology, performance, and data warehousing. Hernando can be contacted at hbedoya@us.ibm.com .</paragraph>
<subtitle-level-1><location><page_4><loc_11><loc_62><loc_20><loc_64></location>Authors</subtitle-level-1>
<figure>
<location><page_5><loc_5><loc_70><loc_39><loc_91></location>
</figure>
<paragraph><location><page_5><loc_82><loc_84><loc_85><loc_88></location>1</paragraph>
<paragraph><location><page_5><loc_13><loc_65><loc_19><loc_66></location>Chapter 1.</paragraph>
<subtitle-level-1><location><page_5><loc_22><loc_61><loc_89><loc_68></location>Securing and protecting IBM DB2 data</subtitle-level-1>
<paragraph><location><page_5><loc_22><loc_46><loc_89><loc_56></location>Recent news headlines are filled with reports of data breaches and cyber-attacks impacting global businesses of all sizes. The Identity Theft Resource Center$^{1}$ reports that almost 5000 data breaches have occurred since 2005, exposing over 600 million records of data. The financial cost of these data breaches is skyrocketing. Studies from the Ponemon Institute$^{2}$ revealed that the average cost of a data breach increased in 2013 by 15% globally and resulted in a brand equity loss of $9.4 million per attack. The average cost that is incurred for each lost record containing sensitive information increased more than 9% to $145 per record.</paragraph>
<paragraph><location><page_5><loc_22><loc_38><loc_86><loc_44></location>Businesses must make a serious effort to secure their data and recognize that securing information assets is a cost of doing business. In many parts of the world and in many industries, securing the data is required by law and subject to audits. Data security is no longer an option; it is a requirement.</paragraph>
<paragraph><location><page_5><loc_22><loc_34><loc_89><loc_37></location>This chapter describes how you can secure and protect data in DB2 for i. The following topics are covered in this chapter:</paragraph>
<paragraph><location><page_5><loc_22><loc_32><loc_41><loc_33></location>- GLYPH<SM590000> Security fundamentals</paragraph>
<paragraph><location><page_5><loc_22><loc_30><loc_46><loc_32></location>- GLYPH<SM590000> Current state of IBM i security</paragraph>
<paragraph><location><page_5><loc_22><loc_29><loc_43><loc_30></location>- GLYPH<SM590000> DB2 for i security controls</paragraph>
<subtitle-level-1><location><page_6><loc_11><loc_89><loc_44><loc_91></location>1.1 Security fundamentals</subtitle-level-1>
<paragraph><location><page_6><loc_22><loc_84><loc_89><loc_87></location>Before reviewing database security techniques, there are two fundamental steps in securing information assets that must be described:</paragraph>
<paragraph><location><page_6><loc_22><loc_77><loc_89><loc_83></location>- GLYPH<SM590000> First, and most important, is the definition of a company's security policy . Without a security policy, there is no definition of what are acceptable practices for using, accessing, and storing information by who, what, when, where, and how. A security policy should minimally address three things: confidentiality, integrity, and availability.</paragraph>
<paragraph><location><page_6><loc_25><loc_66><loc_89><loc_76></location>- The monitoring and assessment of adherence to the security policy determines whether your security strategy is working. Often, IBM security consultants are asked to perform security assessments for companies without regard to the security policy. Although these assessments can be useful for observing how the system is defined and how data is being accessed, they cannot determine the level of security without a security policy. Without a security policy, it really is not an assessment as much as it is a baseline for monitoring the changes in the security settings that are captured.</paragraph>
<paragraph><location><page_6><loc_25><loc_64><loc_89><loc_65></location>A security policy is what defines whether the system and its settings are secure (or not).</paragraph>
<paragraph><location><page_6><loc_22><loc_53><loc_89><loc_63></location>- GLYPH<SM590000> The second fundamental in securing data assets is the use of resource security . If implemented properly, resource security prevents data breaches from both internal and external intrusions. Resource security controls are closely tied to the part of the security policy that defines who should have access to what information resources. A hacker might be good enough to get through your company firewalls and sift his way through to your system, but if they do not have explicit access to your database, the hacker cannot compromise your information assets.</paragraph>
<paragraph><location><page_6><loc_22><loc_48><loc_87><loc_51></location>With your eyes now open to the importance of securing information assets, the rest of this chapter reviews the methods that are available for securing database resources on IBM i.</paragraph>
<subtitle-level-1><location><page_6><loc_11><loc_43><loc_53><loc_45></location>1.2 Current state of IBM i security</subtitle-level-1>
<paragraph><location><page_6><loc_22><loc_35><loc_89><loc_41></location>Because of the inherently secure nature of IBM i, many clients rely on the default system settings to protect their business data that is stored in DB2 for i. In most cases, this means no data protection because the default setting for the Create default public authority (QCRTAUT) system value is *CHANGE.</paragraph>
<paragraph><location><page_6><loc_22><loc_26><loc_89><loc_33></location>Even more disturbing is that many IBM i clients remain in this state, despite the news headlines and the significant costs that are involved with databases being compromised. This default security configuration makes it quite challenging to implement basic security policies. A tighter implementation is required if you really want to protect one of your company's most valuable assets, which is the data.</paragraph>
<paragraph><location><page_6><loc_22><loc_14><loc_89><loc_24></location>Traditionally, IBM i applications have employed menu-based security to counteract this default configuration that gives all users access to the data. The theory is that data is protected by the menu options controlling what database operations that the user can perform. This approach is ineffective, even if the user profile is restricted from running interactive commands. The reason is that in today's connected world there are a multitude of interfaces into the system, from web browsers to PC clients, that bypass application menus. If there are no object-level controls, users of these newer interfaces have an open door to your data.</paragraph>
<paragraph><location><page_7><loc_22><loc_81><loc_89><loc_91></location>Many businesses are trying to limit data access to a need-to-know basis. This security goal means that users should be given access only to the minimum set of data that is required to perform their job. Often, users with object-level access are given access to row and column values that are beyond what their business task requires because that object-level security provides an all-or-nothing solution. For example, object-level controls allow a manager to access data about all employees. Most security policies limit a manager to accessing data only for the employees that they manage.</paragraph>
<subtitle-level-1><location><page_7><loc_11><loc_77><loc_49><loc_78></location>1.3.1 Existing row and column control</subtitle-level-1>
<paragraph><location><page_7><loc_22><loc_68><loc_88><loc_75></location>Some IBM i clients have tried augmenting the all-or-nothing object-level security with SQL views (or logical files) and application logic, as shown in Figure 1-2. However, application-based logic is easy to bypass with all of the different data access interfaces that are provided by the IBM i operating system, such as Open Database Connectivity (ODBC) and System i Navigator.</paragraph>
<paragraph><location><page_7><loc_22><loc_60><loc_89><loc_66></location>Using SQL views to limit access to a subset of the data in a table also has its own set of challenges. First, there is the complexity of managing all of the SQL view objects that are used for securing data access. Second, scaling a view-based security solution can be difficult as the amount of data grows and the number of users increases.</paragraph>
<paragraph><location><page_7><loc_22><loc_54><loc_89><loc_59></location>Even if you are willing to live with these performance and management issues, a user with *ALLOBJ access still can directly access all of the data in the underlying DB2 table and easily bypass the security controls that are built into an SQL view.</paragraph>
<figure>
<location><page_7><loc_22><loc_13><loc_89><loc_53></location>
<caption>Figure 1-2 Existing row and column controls</caption>
</figure>
<subtitle-level-1><location><page_8><loc_11><loc_89><loc_55><loc_91></location>2.1.6 Change Function Usage CL command</subtitle-level-1>
<paragraph><location><page_8><loc_22><loc_87><loc_89><loc_88></location>The following CL commands can be used to work with, display, or change function usage IDs:</paragraph>
<paragraph><location><page_8><loc_22><loc_84><loc_49><loc_86></location>- GLYPH<SM590000> Work Function Usage ( WRKFCNUSG )</paragraph>
<paragraph><location><page_8><loc_22><loc_83><loc_51><loc_84></location>- GLYPH<SM590000> Change Function Usage ( CHGFCNUSG )</paragraph>
<paragraph><location><page_8><loc_22><loc_81><loc_51><loc_83></location>- GLYPH<SM590000> Display Function Usage ( DSPFCNUSG )</paragraph>
<paragraph><location><page_8><loc_22><loc_77><loc_84><loc_80></location>For example, the following CHGFCNUSG command shows granting authorization to user HBEDOYA to administer and manage RCAC rules:</paragraph>
<paragraph><location><page_8><loc_22><loc_75><loc_72><loc_76></location>CHGFCNUSG FCNID(QIBM_DB_SECADM) USER(HBEDOYA) USAGE(*ALLOWED)</paragraph>
<subtitle-level-1><location><page_8><loc_11><loc_71><loc_89><loc_72></location>2.1.7 Verifying function usage IDs for RCAC with the FUNCTION_USAGE view</subtitle-level-1>
<paragraph><location><page_8><loc_22><loc_66><loc_85><loc_69></location>The FUNCTION_USAGE view contains function usage configuration details. Table 2-1 describes the columns in the FUNCTION_USAGE view.</paragraph>
<table>
<location><page_8><loc_22><loc_44><loc_89><loc_63></location>
<caption>Table 2-1 FUNCTION_USAGE view</caption>
<row_0><col_0><col_header>Column name</col_0><col_1><col_header>Data type</col_1><col_2><col_header>Description</col_2></row_0>
<row_1><col_0><body>FUNCTION_ID</col_0><col_1><body>VARCHAR(30)</col_1><col_2><body>ID of the function.</col_2></row_1>
<row_2><col_0><body>USER_NAME</col_0><col_1><body>VARCHAR(10)</col_1><col_2><body>Name of the user profile that has a usage setting for this  function.</col_2></row_2>
<row_3><col_0><body>USAGE</col_0><col_1><body>VARCHAR(7)</col_1><col_2><body>Usage setting: GLYPH<SM590000> ALLOWED: The user profile is allowed to use the function. GLYPH<SM590000> DENIED: The user profile is not allowed to use the function.</col_2></row_3>
<row_4><col_0><body>USER_TYPE</col_0><col_1><body>VARCHAR(5)</col_1><col_2><body>Type of user profile: GLYPH<SM590000> USER: The user profile is a user. GLYPH<SM590000> GROUP: The user profile is a group.</col_2></row_4>
</table>
<caption><location><page_8><loc_22><loc_64><loc_46><loc_65></location>Table 2-1 FUNCTION_USAGE view</caption>
<paragraph><location><page_8><loc_22><loc_40><loc_89><loc_43></location>To discover who has authorization to define and manage RCAC, you can use the query that is shown in Example 2-1.</paragraph>
<caption><location><page_8><loc_22><loc_38><loc_76><loc_39></location>Example 2-1 Query to determine who has authority to define and manage RCAC</caption>
<paragraph><location><page_8><loc_22><loc_35><loc_28><loc_36></location>SELECT</paragraph>
<paragraph><location><page_8><loc_30><loc_35><loc_41><loc_36></location>function_id,</paragraph>
<paragraph><location><page_8><loc_27><loc_34><loc_39><loc_35></location>user_name,</paragraph>
<paragraph><location><page_8><loc_28><loc_32><loc_36><loc_33></location>usage,</paragraph>
<paragraph><location><page_8><loc_27><loc_31><loc_39><loc_32></location>user_type</paragraph>
<paragraph><location><page_8><loc_22><loc_29><loc_26><loc_30></location>FROM</paragraph>
<paragraph><location><page_8><loc_29><loc_29><loc_43><loc_30></location>function_usage</paragraph>
<paragraph><location><page_8><loc_22><loc_28><loc_27><loc_29></location>WHERE</paragraph>
<paragraph><location><page_8><loc_29><loc_28><loc_54><loc_29></location>function_id=’QIBM_DB_SECADM’</paragraph>
<paragraph><location><page_8><loc_22><loc_26><loc_29><loc_27></location>ORDER BY</paragraph>
<paragraph><location><page_8><loc_31><loc_26><loc_39><loc_27></location>user_name;</paragraph>
<subtitle-level-1><location><page_8><loc_11><loc_20><loc_41><loc_22></location>2.2 Separation of duties</subtitle-level-1>
<paragraph><location><page_8><loc_22><loc_10><loc_89><loc_18></location>Separation of duties helps businesses comply with industry regulations or organizational requirements and simplifies the management of authorities. Separation of duties is commonly used to prevent fraudulent activities or errors by a single person. It provides the ability for administrative functions to be divided across individuals without overlapping responsibilities, so that one user does not possess unlimited authority, such as with the *ALLOBJ authority.</paragraph>
<paragraph><location><page_9><loc_22><loc_82><loc_89><loc_91></location>For example, assume that a business has assigned the duty to manage security on IBM i to Theresa. Before release IBM i 7.2, to grant privileges, Theresa had to have the same privileges Theresa was granting to others. Therefore, to grant *USE privileges to the PAYROLL table, Theresa had to have *OBJMGT and *USE authority (or a higher level of authority, such as *ALLOBJ). This requirement allowed Theresa to access the data in the PAYROLL table even though Theresa's job description was only to manage its security.</paragraph>
<paragraph><location><page_9><loc_22><loc_75><loc_89><loc_81></location>In IBM i 7.2, the QIBM_DB_SECADM function usage grants authorities, revokes authorities, changes ownership, or changes the primary group without giving access to the object or, in the case of a database table, to the data that is in the table or allowing other operations on the table.</paragraph>
<paragraph><location><page_9><loc_22><loc_71><loc_88><loc_73></location>QIBM_DB_SECADM function usage can be granted only by a user with *SECADM special authority and can be given to a user or a group.</paragraph>
<paragraph><location><page_9><loc_22><loc_65><loc_89><loc_69></location>QIBM_DB_SECADM also is responsible for administering RCAC, which restricts which rows a user is allowed to access in a table and whether a user is allowed to see information in certain columns of a table.</paragraph>
<paragraph><location><page_9><loc_22><loc_57><loc_88><loc_63></location>A preferred practice is that the RCAC administrator has the QIBM_DB_SECADM function usage ID, but absolutely no other data privileges. The result is that the RCAC administrator can deploy and maintain the RCAC constructs, but cannot grant themselves unauthorized access to data itself.</paragraph>
<paragraph><location><page_9><loc_22><loc_53><loc_89><loc_56></location>Table 2-2 shows a comparison of the different function usage IDs and *JOBCTL authority to the different CL commands and DB2 for i tools.</paragraph>
<table>
<location><page_9><loc_11><loc_9><loc_89><loc_50></location>
<caption>Table 2-2 Comparison of the different function usage IDs and *JOBCTL authority</caption>
<row_0><col_0><row_header>User action</col_0><col_1><body>*JOBCTL</col_1><col_2><body>QIBM_DB_SECADM</col_2><col_3><body>QIBM_DB_SQLADM</col_3><col_4><body>QIBM_DB_SYSMON</col_4><col_5><body>No Authority</col_5></row_0>
<row_1><col_0><row_header>SET CURRENT DEGREE  (SQL statement)</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_1>
<row_2><col_0><row_header>CHGQRYA  command targeting a different user’s job</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_2>
<row_3><col_0><row_header>STRDBMON  or  ENDDBMON  commands targeting a different user’s job</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_3>
<row_4><col_0><row_header>STRDBMON  or  ENDDBMON  commands targeting a job that matches the current user</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body>X</col_4><col_5><body>X</col_5></row_4>
<row_5><col_0><row_header>QUSRJOBI() API format 900 or System i Navigator’s SQL Details for Job</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body>X</col_4><col_5><body></col_5></row_5>
<row_6><col_0><row_header>Visual Explain within Run SQL scripts</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body>X</col_4><col_5><body>X</col_5></row_6>
<row_7><col_0><row_header>Visual Explain outside of Run SQL scripts</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_7>
<row_8><col_0><row_header>ANALYZE PLAN CACHE procedure</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_8>
<row_9><col_0><row_header>DUMP PLAN CACHE procedure</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_9>
<row_10><col_0><row_header>MODIFY PLAN CACHE procedure</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_10>
<row_11><col_0><row_header>MODIFY PLAN CACHE PROPERTIES procedure (currently does not check authority)</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_11>
<row_12><col_0><row_header>CHANGE PLAN CACHE SIZE procedure (currently does not check authority)</col_0><col_1><body>X</col_1><col_2><body></col_2><col_3><body>X</col_3><col_4><body></col_4><col_5><body></col_5></row_12>
</table>
<caption><location><page_9><loc_11><loc_51><loc_64><loc_52></location>Table 2-2 Comparison of the different function usage IDs and *JOBCTL authority</caption>
<caption><location><page_10><loc_22><loc_88><loc_86><loc_91></location>The SQL CREATE PERMISSION statement that is shown in Figure 3-1 is used to define and initially enable or disable the row access rules.</caption>
<figure>
<location><page_10><loc_22><loc_48><loc_89><loc_86></location>
<caption>Figure 3-1 CREATE PERMISSION SQL statement</caption>
</figure>
<subtitle-level-1><location><page_10><loc_22><loc_43><loc_35><loc_44></location>Column mask</subtitle-level-1>
<paragraph><location><page_10><loc_22><loc_37><loc_89><loc_43></location>A column mask is a database object that manifests a column value access control rule for a specific column in a specific table. It uses a CASE expression that describes what you see when you access the column. For example, a teller can see only the last four digits of a tax identification number.</paragraph>
<caption><location><page_11><loc_22><loc_90><loc_67><loc_91></location>Table 3-1 summarizes these special registers and their values.</caption>
<table>
<location><page_11><loc_22><loc_74><loc_89><loc_87></location>
<caption>Table 3-1 Special registers and their corresponding values</caption>
<row_0><col_0><col_header>Special register</col_0><col_1><col_header>Corresponding value</col_1></row_0>
<row_1><col_0><body>USER or SESSION_USER</col_0><col_1><body>The effective user of the thread excluding adopted authority.</col_1></row_1>
<row_2><col_0><body>CURRENT_USER</col_0><col_1><body>The effective user of the thread including adopted authority. When no adopted  authority is present, this has the same value as USER.</col_1></row_2>
<row_3><col_0><body>SYSTEM_USER</col_0><col_1><body>The authorization ID that initiated the connection.</col_1></row_3>
</table>
<caption><location><page_11><loc_22><loc_87><loc_61><loc_88></location>Table 3-1 Special registers and their corresponding values</caption>
<paragraph><location><page_11><loc_22><loc_70><loc_88><loc_73></location>Figure 3-5 shows the difference in the special register values when an adopted authority is used:</paragraph>
<paragraph><location><page_11><loc_22><loc_68><loc_67><loc_69></location>- GLYPH<SM590000> A user connects to the server using the user profile ALICE.</paragraph>
<paragraph><location><page_11><loc_22><loc_66><loc_74><loc_67></location>- GLYPH<SM590000> USER and CURRENT USER initially have the same value of ALICE.</paragraph>
<paragraph><location><page_11><loc_22><loc_62><loc_88><loc_65></location>- GLYPH<SM590000> ALICE calls an SQL procedure that is named proc1, which is owned by user profile JOE and was created to adopt JOE's authority when it is called.</paragraph>
<paragraph><location><page_11><loc_22><loc_57><loc_89><loc_61></location>- GLYPH<SM590000> While the procedure is running, the special register USER still contains the value of ALICE because it excludes any adopted authority. The special register CURRENT USER contains the value of JOE because it includes any adopted authority.</paragraph>
<paragraph><location><page_11><loc_22><loc_53><loc_89><loc_56></location>- GLYPH<SM590000> When proc1 ends, the session reverts to its original state with both USER and CURRENT USER having the value of ALICE.</paragraph>
<figure>
<location><page_11><loc_22><loc_25><loc_49><loc_51></location>
<caption>Figure 3-5 Special registers and adopted authority</caption>
</figure>
<subtitle-level-1><location><page_11><loc_11><loc_20><loc_40><loc_21></location>3.2.2 Built-in global variables</subtitle-level-1>
<paragraph><location><page_11><loc_22><loc_15><loc_85><loc_18></location>Built-in global variables are provided with the database manager and are used in SQL statements to retrieve scalar values that are associated with the variables.</paragraph>
<paragraph><location><page_11><loc_22><loc_9><loc_87><loc_13></location>IBM DB2 for i supports nine different built-in global variables that are read only and maintained by the system. These global variables can be used to identify attributes of the database connection and used as part of the RCAC logic.</paragraph>
<paragraph><location><page_12><loc_22><loc_90><loc_56><loc_91></location>Table 3-2 lists the nine built-in global variables.</paragraph>
<table>
<location><page_12><loc_10><loc_63><loc_90><loc_87></location>
<caption>Table 3-2 Built-in global variables</caption>
<row_0><col_0><col_header>Global variable</col_0><col_1><col_header>Type</col_1><col_2><col_header>Description</col_2></row_0>
<row_1><col_0><body>CLIENT_HOST</col_0><col_1><body>VARCHAR(255)</col_1><col_2><body>Host name of the current client as returned by the system</col_2></row_1>
<row_2><col_0><body>CLIENT_IPADDR</col_0><col_1><body>VARCHAR(128)</col_1><col_2><body>IP address of the current client as returned by the system</col_2></row_2>
<row_3><col_0><body>CLIENT_PORT</col_0><col_1><body>INTEGER</col_1><col_2><body>Port used by the current client to communicate with the server</col_2></row_3>
<row_4><col_0><body>PACKAGE_NAME</col_0><col_1><body>VARCHAR(128)</col_1><col_2><body>Name of the currently running package</col_2></row_4>
<row_5><col_0><body>PACKAGE_SCHEMA</col_0><col_1><body>VARCHAR(128)</col_1><col_2><body>Schema name of the currently running package</col_2></row_5>
<row_6><col_0><body>PACKAGE_VERSION</col_0><col_1><body>VARCHAR(64)</col_1><col_2><body>Version identifier of the currently running package</col_2></row_6>
<row_7><col_0><body>ROUTINE_SCHEMA</col_0><col_1><body>VARCHAR(128)</col_1><col_2><body>Schema name of the currently running routine</col_2></row_7>
<row_8><col_0><body>ROUTINE_SPECIFIC_NAME</col_0><col_1><body>VARCHAR(128)</col_1><col_2><body>Name of the currently running routine</col_2></row_8>
<row_9><col_0><body>ROUTINE_TYPE</col_0><col_1><body>CHAR(1)</col_1><col_2><body>Type of the currently running routine</col_2></row_9>
</table>
<caption><location><page_12><loc_11><loc_87><loc_33><loc_88></location>Table 3-2 Built-in global variables</caption>
<subtitle-level-1><location><page_12><loc_11><loc_57><loc_63><loc_59></location>3.3 VERIFY_GROUP_FOR_USER function</subtitle-level-1>
<paragraph><location><page_12><loc_22><loc_45><loc_89><loc_55></location>The VERIFY_GROUP_FOR_USER function was added in IBM i 7.2. Although it is primarily intended for use with RCAC permissions and masks, it can be used in other SQL statements. The first parameter must be one of these three special registers: SESSION_USER, USER, or CURRENT_USER. The second and subsequent parameters are a list of user or group profiles. Each of these values must be 1 - 10 characters in length. These values are not validated for their existence, which means that you can specify the names of user profiles that do not exist without receiving any kind of error.</paragraph>
<paragraph><location><page_12><loc_22><loc_39><loc_89><loc_43></location>If a special register value is in the list of user profiles or it is a member of a group profile included in the list, the function returns a long integer value of 1. Otherwise, it returns a value of 0. It never returns the null value.</paragraph>
<paragraph><location><page_12><loc_22><loc_36><loc_75><loc_38></location>Here is an example of using the VERIFY_GROUP_FOR_USER function:</paragraph>
<paragraph><location><page_12><loc_22><loc_34><loc_66><loc_35></location>- 1. There are user profiles for MGR, JANE, JUDY, and TONY.</paragraph>
<paragraph><location><page_12><loc_22><loc_32><loc_65><loc_33></location>- 2. The user profile JANE specifies a group profile of MGR.</paragraph>
<paragraph><location><page_12><loc_22><loc_28><loc_88><loc_31></location>- 3. If a user is connected to the server using user profile JANE, all of the following function invocations return a value of 1:</paragraph>
<paragraph><location><page_12><loc_25><loc_19><loc_74><loc_27></location>VERIFY_GROUP_FOR_USER (CURRENT_USER, 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR', 'STEVE') The following function invocation returns a value of 0: VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JUDY', 'TONY')</paragraph>
<paragraph><location><page_13><loc_22><loc_90><loc_27><loc_91></location>RETURN</paragraph>
<paragraph><location><page_13><loc_22><loc_88><loc_26><loc_89></location>CASE</paragraph>
<paragraph><location><page_13><loc_22><loc_67><loc_85><loc_88></location>WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR', 'EMP' ) = 1 THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 9999 || '-' || MONTH ( EMPLOYEES . DATE_OF_BIRTH ) || '-' || DAY (EMPLOYEES.DATE_OF_BIRTH )) ELSE NULL END ENABLE ;</paragraph>
<paragraph><location><page_13><loc_22><loc_63><loc_89><loc_65></location>- 2. The other column to mask in this example is the TAX_ID information. In this example, the rules to enforce include the following ones:</paragraph>
<paragraph><location><page_13><loc_25><loc_60><loc_77><loc_62></location>- -Human Resources can see the unmasked TAX_ID of the employees.</paragraph>
<paragraph><location><page_13><loc_25><loc_58><loc_66><loc_59></location>- -Employees can see only their own unmasked TAX_ID.</paragraph>
<paragraph><location><page_13><loc_25><loc_55><loc_89><loc_57></location>- -Managers see a masked version of TAX_ID with the first five characters replaced with the X character (for example, XXX-XX-1234).</paragraph>
<paragraph><location><page_13><loc_25><loc_52><loc_87><loc_54></location>- -Any other person sees the entire TAX_ID as masked, for example, XXX-XX-XXXX.</paragraph>
<paragraph><location><page_13><loc_25><loc_50><loc_87><loc_51></location>- To implement this column mask, run the SQL statement that is shown in Example 3-9.</paragraph>
<paragraph><location><page_13><loc_22><loc_14><loc_86><loc_47></location>CREATE MASK HR_SCHEMA.MASK_TAX_ID_ON_EMPLOYEES ON HR_SCHEMA.EMPLOYEES AS EMPLOYEES FOR COLUMN TAX_ID RETURN CASE WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR' ) = 1 THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( EMPLOYEES . TAX_ID , 8 , 4 ) ) WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'EMP' ) = 1 THEN EMPLOYEES . TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ;</paragraph>
<caption><location><page_13><loc_22><loc_48><loc_58><loc_49></location>Example 3-9 Creating a mask on the TAX_ID column</caption>
<paragraph><location><page_14><loc_22><loc_90><loc_74><loc_91></location>- 3. Figure 3-10 shows the masks that are created in the HR_SCHEMA.</paragraph>
<figure>
<location><page_14><loc_10><loc_79><loc_89><loc_88></location>
<caption>Figure 3-10 Column masks shown in System i Navigator</caption>
</figure>
<subtitle-level-1><location><page_14><loc_11><loc_73><loc_33><loc_74></location>3.6.6 Activating RCAC</subtitle-level-1>
<paragraph><location><page_14><loc_22><loc_67><loc_89><loc_71></location>Now that you have created the row permission and the two column masks, RCAC must be activated. The row permission and the two column masks are enabled (last clause in the scripts), but now you must activate RCAC on the table. To do so, complete the following steps:</paragraph>
<paragraph><location><page_14><loc_22><loc_65><loc_67><loc_66></location>- 1. Run the SQL statements that are shown in Example 3-10.</paragraph>
<subtitle-level-1><location><page_14><loc_22><loc_62><loc_61><loc_63></location>Example 3-10 Activating RCAC on the EMPLOYEES table</subtitle-level-1>
<paragraph><location><page_14><loc_22><loc_60><loc_62><loc_61></location>- /* Active Row Access Control (permissions) */</paragraph>
<paragraph><location><page_14><loc_22><loc_58><loc_58><loc_60></location>- /* Active Column Access Control (masks)</paragraph>
<paragraph><location><page_14><loc_60><loc_58><loc_62><loc_60></location>*/</paragraph>
<paragraph><location><page_14><loc_22><loc_57><loc_48><loc_58></location>ALTER TABLE HR_SCHEMA.EMPLOYEES</paragraph>
<paragraph><location><page_14><loc_22><loc_55><loc_44><loc_56></location>ACTIVATE ROW ACCESS CONTROL</paragraph>
<paragraph><location><page_14><loc_22><loc_54><loc_48><loc_55></location>ACTIVATE COLUMN ACCESS CONTROL;</paragraph>
<paragraph><location><page_14><loc_22><loc_48><loc_88><loc_52></location>- 2. Look at the definition of the EMPLOYEE table, as shown in Figure 3-11. To do this, from the main navigation pane of System i Navigator, click Schemas  HR_SCHEMA  Tables , right-click the EMPLOYEES table, and click Definition .</paragraph>
<figure>
<location><page_14><loc_10><loc_18><loc_87><loc_46></location>
<caption>Figure 3-11 Selecting the EMPLOYEES table from System i Navigator</caption>
</figure>
<paragraph><location><page_15><loc_22><loc_87><loc_84><loc_91></location>- 2. Figure 4-68 shows the Visual Explain of the same SQL statement, but with RCAC enabled. It is clear that the implementation of the SQL statement is more complex because the row permission rule becomes part of the WHERE clause.</paragraph>
<paragraph><location><page_15><loc_22><loc_32><loc_89><loc_36></location>- 3. Compare the advised indexes that are provided by the Optimizer without RCAC and with RCAC enabled. Figure 4-69 shows the index advice for the SQL statement without RCAC enabled. The index being advised is for the ORDER BY clause.</paragraph>
<figure>
<location><page_15><loc_22><loc_40><loc_89><loc_85></location>
<caption>Figure 4-68 Visual Explain with RCAC enabled</caption>
</figure>
<figure>
<location><page_15><loc_11><loc_16><loc_83><loc_30></location>
<caption>Figure 4-69 Index advice with no RCAC</caption>
</figure>
<paragraph><location><page_16><loc_11><loc_11><loc_82><loc_91></location>THEN C . CUSTOMER_TAX_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( C . CUSTOMER_TAX_ID , 8 , 4 ) ) WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_DRIVERS_LICENSE_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_DRIVERS_LICENSE_NUMBER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER ELSE '*************' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_LOGIN_ID_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_LOGIN_ID RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_LOGIN_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_LOGIN_ID ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ANSWER_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION_ANSWER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER ELSE '*****' END ENABLE ; ALTER TABLE BANK_SCHEMA.CUSTOMERS ACTIVATE ROW ACCESS CONTROL ACTIVATE COLUMN ACCESS CONTROL ;</paragraph>
<paragraph><location><page_18><loc_47><loc_94><loc_68><loc_96></location>Back cover</paragraph>
<subtitle-level-1><location><page_18><loc_4><loc_82><loc_73><loc_91></location>Row and Column Access Control Support in IBM DB2 for i</subtitle-level-1>
<paragraph><location><page_18><loc_4><loc_66><loc_21><loc_69></location>Implement roles and separation of duties</paragraph>
<paragraph><location><page_18><loc_4><loc_59><loc_20><loc_64></location>Leverage row permissions on the database</paragraph>
<paragraph><location><page_18><loc_25><loc_59><loc_68><loc_69></location>This IBM Redpaper publication provides information about the IBM i 7.2 feature of IBM DB2 for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.</paragraph>
<paragraph><location><page_18><loc_4><loc_52><loc_20><loc_57></location>Protect columns by defining column masks</paragraph>
<paragraph><location><page_18><loc_25><loc_51><loc_68><loc_58></location>This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.</paragraph>
<figure>
<location><page_18><loc_79><loc_93><loc_93><loc_97></location>
</figure>
<figure>
<location><page_18><loc_78><loc_76><loc_97><loc_90></location>
</figure>
<paragraph><location><page_18><loc_76><loc_62><loc_91><loc_69></location>INTERNATIONAL TECHNICAL SUPPORT ORGANIZATION</paragraph>
<paragraph><location><page_18><loc_76><loc_51><loc_96><loc_56></location>BUILDING TECHNICAL INFORMATION BASED ON PRACTICAL EXPERIENCE</paragraph>
<paragraph><location><page_18><loc_76><loc_32><loc_96><loc_50></location>IBM Redbooks are developed by the IBM International Technical Support Organization. Experts from IBM, Customers and Partners from around the world create timely technical information based on realistic scenarios. Specific recommendations are provided to help you implement IT solutions more effectively in your environment.</paragraph>
<paragraph><location><page_18><loc_76><loc_24><loc_93><loc_27></location>For more information: ibm.com /redbooks</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/redp5110_sampled.md
================================================
Front cover

<!-- image -->

## Row and Column Access Control Support in IBM DB2 for i

<!-- image -->

Front cover

## Contents

DB2 for i Center of Excellence

Solution Brief IBM Systems Lab Services and Training

<!-- image -->

## Highlights

- GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g81>GLYPH<g75>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g72>GLYPH<g3> GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g73>GLYPH<g82>GLYPH<g85>GLYPH<g80>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g92>GLYPH<g82>GLYPH<g88>GLYPH<g85> GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86>

- GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g68>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g74>GLYPH<g85>GLYPH<g72>GLYPH<g68>GLYPH<g87>GLYPH<g72>GLYPH<g85>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g87>GLYPH<g88>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g55>GLYPH<g3> GLYPH<g83>GLYPH<g85>GLYPH<g82>GLYPH<g77>GLYPH<g72>GLYPH<g70>GLYPH<g87>GLYPH<g86> GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g85>GLYPH<g82>GLYPH<g88>GLYPH<g74>GLYPH<g75>GLYPH<g3> GLYPH<g80>GLYPH<g82>GLYPH<g71>GLYPH<g72>GLYPH<g85> GLYPH<g81>GLYPH<g76>GLYPH<g93>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71> GLYPH<g3> GLYPH<g68>GLYPH<g83>GLYPH<g83>GLYPH<g79>GLYPH<g76>GLYPH<g70>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86>

- GLYPH<g115>GLYPH<g3> GLYPH<g53>GLYPH<g72>GLYPH<g79>GLYPH<g92>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g37>GLYPH<g48>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g3> GLYPH<g70>GLYPH<g82>GLYPH<g81>GLYPH<g86>GLYPH<g88>GLYPH<g79>GLYPH<g87>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g15>GLYPH<g3> GLYPH<g86>GLYPH<g78>GLYPH<g76>GLYPH<g79>GLYPH<g79>GLYPH<g86> GLYPH<g3> GLYPH<g86>GLYPH<g75>GLYPH<g68>GLYPH<g85>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g81>GLYPH<g82>GLYPH<g90>GLYPH<g81>GLYPH<g3> GLYPH<g86>GLYPH<g72>GLYPH<g85>GLYPH<g89>GLYPH<g76>GLYPH<g70>GLYPH<g72>GLYPH<g86>

- GLYPH<g115>GLYPH<g3> GLYPH<g55> GLYPH<g68>GLYPH<g78>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g71>GLYPH<g89>GLYPH<g68>GLYPH<g81>GLYPH<g87>GLYPH<g68>GLYPH<g74>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g68>GLYPH<g70>GLYPH<g70>GLYPH<g72>GLYPH<g86>GLYPH<g86>GLYPH<g3> GLYPH<g87>GLYPH<g82>GLYPH<g3> GLYPH<g68> GLYPH<g3> GLYPH<g90>GLYPH<g82>GLYPH<g85>GLYPH<g79>GLYPH<g71>GLYPH<g90>GLYPH<g76>GLYPH<g71>GLYPH<g72>GLYPH<g3> GLYPH<g86>GLYPH<g82>GLYPH<g88>GLYPH<g85>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g76>GLYPH<g86>GLYPH<g72>

<!-- image -->

Power Services

## DB2 for i Center of Excellence

Expert help to achieve your business requirements

## We build confident, satisfied clients

No one else has the vast consulting experiences, skills sharing and renown service offerings to do what we can do for you.

Because no one else is IBM.

With combined experiences and direct access to development groups, we're the experts in IBM DB2® for i. The DB2 for i Center of Excellence (CoE) can help you achieve-perhaps reexamine and exceed-your business requirements and gain more confidence and satisfaction in IBM product data management products and solutions.

## Who we are, some of what we do

Global CoE engagements cover topics including:

- r Database performance and scalability

- r Advanced SQL knowledge and skills transfer

- r Business intelligence and analytics

- r DB2 Web Query

- r Query/400 modernization for better reporting and analysis capabilities

- r Database modernization and re-engineering

- r Data-centric architecture and design

- r Extremely large database and overcoming limits to growth

- r ISV education and enablement

## Preface

This IBMfi Redpaper™ publication provides information about the IBM i 7.2 feature of IBM DB2fi for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.

This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.

This paper was produced by the IBM DB2 for i Center of Excellence team in partnership with the International Technical Support Organization (ITSO), Rochester, Minnesota US.

<!-- image -->

Jim Bainbridge is a senior DB2 consultant on the DB2 for i Center of Excellence team in the IBM Lab Services and Training organization. His primary role is training and implementation services for IBM DB2 Web Query for i and business analytics. Jim began his career with IBM 30 years ago in the IBM Rochester Development Lab, where he developed cooperative processing products that paired IBM PCs with IBM S/36 and AS/.400 systems. In the years since, Jim has held numerous technical roles, including independent software vendors technical support on a broad range of IBM technologies and products, and supporting customers in the IBM Executive Briefing Center and IBM Project Office.

<!-- image -->

Hernando Bedoya is a Senior IT Specialist at STG Lab Services and Training in Rochester, Minnesota. He writes extensively and teaches IBM classes worldwide in all areas of DB2 for i. Before joining STG Lab Services, he worked in the ITSO for nine years writing multiple IBM Redbooksfi publications. He also worked for IBM Colombia as an IBM AS/400fi IT Specialist doing presales support for the Andean countries. He has 28 years of experience in the computing field and has taught database classes in Colombian universities. He holds a Master's degree in Computer Science from EAFIT, Colombia. His areas of expertise are database technology, performance, and data warehousing. Hernando can be contacted at hbedoya@us.ibm.com .

## Authors

<!-- image -->

1

Chapter 1.

## Securing and protecting IBM DB2 data

Recent news headlines are filled with reports of data breaches and cyber-attacks impacting global businesses of all sizes. The Identity Theft Resource Center$^{1}$ reports that almost 5000 data breaches have occurred since 2005, exposing over 600 million records of data. The financial cost of these data breaches is skyrocketing. Studies from the Ponemon Institute$^{2}$ revealed that the average cost of a data breach increased in 2013 by 15% globally and resulted in a brand equity loss of $9.4 million per attack. The average cost that is incurred for each lost record containing sensitive information increased more than 9% to $145 per record.

Businesses must make a serious effort to secure their data and recognize that securing information assets is a cost of doing business. In many parts of the world and in many industries, securing the data is required by law and subject to audits. Data security is no longer an option; it is a requirement.

This chapter describes how you can secure and protect data in DB2 for i. The following topics are covered in this chapter:

- GLYPH<SM590000> Security fundamentals

- GLYPH<SM590000> Current state of IBM i security

- GLYPH<SM590000> DB2 for i security controls

## 1.1 Security fundamentals

Before reviewing database security techniques, there are two fundamental steps in securing information assets that must be described:

- GLYPH<SM590000> First, and most important, is the definition of a company's security policy . Without a security policy, there is no definition of what are acceptable practices for using, accessing, and storing information by who, what, when, where, and how. A security policy should minimally address three things: confidentiality, integrity, and availability.

- The monitoring and assessment of adherence to the security policy determines whether your security strategy is working. Often, IBM security consultants are asked to perform security assessments for companies without regard to the security policy. Although these assessments can be useful for observing how the system is defined and how data is being accessed, they cannot determine the level of security without a security policy. Without a security policy, it really is not an assessment as much as it is a baseline for monitoring the changes in the security settings that are captured.

A security policy is what defines whether the system and its settings are secure (or not).

- GLYPH<SM590000> The second fundamental in securing data assets is the use of resource security . If implemented properly, resource security prevents data breaches from both internal and external intrusions. Resource security controls are closely tied to the part of the security policy that defines who should have access to what information resources. A hacker might be good enough to get through your company firewalls and sift his way through to your system, but if they do not have explicit access to your database, the hacker cannot compromise your information assets.

With your eyes now open to the importance of securing information assets, the rest of this chapter reviews the methods that are available for securing database resources on IBM i.

## 1.2 Current state of IBM i security

Because of the inherently secure nature of IBM i, many clients rely on the default system settings to protect their business data that is stored in DB2 for i. In most cases, this means no data protection because the default setting for the Create default public authority (QCRTAUT) system value is *CHANGE.

Even more disturbing is that many IBM i clients remain in this state, despite the news headlines and the significant costs that are involved with databases being compromised. This default security configuration makes it quite challenging to implement basic security policies. A tighter implementation is required if you really want to protect one of your company's most valuable assets, which is the data.

Traditionally, IBM i applications have employed menu-based security to counteract this default configuration that gives all users access to the data. The theory is that data is protected by the menu options controlling what database operations that the user can perform. This approach is ineffective, even if the user profile is restricted from running interactive commands. The reason is that in today's connected world there are a multitude of interfaces into the system, from web browsers to PC clients, that bypass application menus. If there are no object-level controls, users of these newer interfaces have an open door to your data.

Many businesses are trying to limit data access to a need-to-know basis. This security goal means that users should be given access only to the minimum set of data that is required to perform their job. Often, users with object-level access are given access to row and column values that are beyond what their business task requires because that object-level security provides an all-or-nothing solution. For example, object-level controls allow a manager to access data about all employees. Most security policies limit a manager to accessing data only for the employees that they manage.

## 1.3.1 Existing row and column control

Some IBM i clients have tried augmenting the all-or-nothing object-level security with SQL views (or logical files) and application logic, as shown in Figure 1-2. However, application-based logic is easy to bypass with all of the different data access interfaces that are provided by the IBM i operating system, such as Open Database Connectivity (ODBC) and System i Navigator.

Using SQL views to limit access to a subset of the data in a table also has its own set of challenges. First, there is the complexity of managing all of the SQL view objects that are used for securing data access. Second, scaling a view-based security solution can be difficult as the amount of data grows and the number of users increases.

Even if you are willing to live with these performance and management issues, a user with *ALLOBJ access still can directly access all of the data in the underlying DB2 table and easily bypass the security controls that are built into an SQL view.

Figure 1-2 Existing row and column controls
<!-- image -->

## 2.1.6 Change Function Usage CL command

The following CL commands can be used to work with, display, or change function usage IDs:

- GLYPH<SM590000> Work Function Usage ( WRKFCNUSG )

- GLYPH<SM590000> Change Function Usage ( CHGFCNUSG )

- GLYPH<SM590000> Display Function Usage ( DSPFCNUSG )

For example, the following CHGFCNUSG command shows granting authorization to user HBEDOYA to administer and manage RCAC rules:

CHGFCNUSG FCNID(QIBM_DB_SECADM) USER(HBEDOYA) USAGE(*ALLOWED)

## 2.1.7 Verifying function usage IDs for RCAC with the FUNCTION_USAGE view

The FUNCTION_USAGE view contains function usage configuration details. Table 2-1 describes the columns in the FUNCTION_USAGE view.

Table 2-1 FUNCTION_USAGE view

| Column name   | Data type   | Description                                                                                                                                                           |
|---------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| FUNCTION_ID   | VARCHAR(30) | ID of the function.                                                                                                                                                   |
| USER_NAME     | VARCHAR(10) | Name of the user profile that has a usage setting for this  function.                                                                                                 |
| USAGE         | VARCHAR(7)  | Usage setting: GLYPH<SM590000> ALLOWED: The user profile is allowed to use the function. GLYPH<SM590000> DENIED: The user profile is not allowed to use the function. |
| USER_TYPE     | VARCHAR(5)  | Type of user profile: GLYPH<SM590000> USER: The user profile is a user. GLYPH<SM590000> GROUP: The user profile is a group.                                           |

To discover who has authorization to define and manage RCAC, you can use the query that is shown in Example 2-1.

Example 2-1 Query to determine who has authority to define and manage RCAC

SELECT

function_id,

user_name,

usage,

user_type

FROM

function_usage

WHERE

function_id=’QIBM_DB_SECADM’

ORDER BY

user_name;

## 2.2 Separation of duties

Separation of duties helps businesses comply with industry regulations or organizational requirements and simplifies the management of authorities. Separation of duties is commonly used to prevent fraudulent activities or errors by a single person. It provides the ability for administrative functions to be divided across individuals without overlapping responsibilities, so that one user does not possess unlimited authority, such as with the *ALLOBJ authority.

For example, assume that a business has assigned the duty to manage security on IBM i to Theresa. Before release IBM i 7.2, to grant privileges, Theresa had to have the same privileges Theresa was granting to others. Therefore, to grant *USE privileges to the PAYROLL table, Theresa had to have *OBJMGT and *USE authority (or a higher level of authority, such as *ALLOBJ). This requirement allowed Theresa to access the data in the PAYROLL table even though Theresa's job description was only to manage its security.

In IBM i 7.2, the QIBM_DB_SECADM function usage grants authorities, revokes authorities, changes ownership, or changes the primary group without giving access to the object or, in the case of a database table, to the data that is in the table or allowing other operations on the table.

QIBM_DB_SECADM function usage can be granted only by a user with *SECADM special authority and can be given to a user or a group.

QIBM_DB_SECADM also is responsible for administering RCAC, which restricts which rows a user is allowed to access in a table and whether a user is allowed to see information in certain columns of a table.

A preferred practice is that the RCAC administrator has the QIBM_DB_SECADM function usage ID, but absolutely no other data privileges. The result is that the RCAC administrator can deploy and maintain the RCAC constructs, but cannot grant themselves unauthorized access to data itself.

Table 2-2 shows a comparison of the different function usage IDs and *JOBCTL authority to the different CL commands and DB2 for i tools.

Table 2-2 Comparison of the different function usage IDs and *JOBCTL authority

| User action                                                                    | *JOBCTL   | QIBM_DB_SECADM   | QIBM_DB_SQLADM   | QIBM_DB_SYSMON   | No Authority   |
|--------------------------------------------------------------------------------|-----------|------------------|------------------|------------------|----------------|
| SET CURRENT DEGREE  (SQL statement)                                            | X         |                  | X                |                  |                |
| CHGQRYA  command targeting a different user’s job                              | X         |                  | X                |                  |                |
| STRDBMON  or  ENDDBMON  commands targeting a different user’s job              | X         |                  | X                |                  |                |
| STRDBMON  or  ENDDBMON  commands targeting a job that matches the current user | X         |                  | X                | X                | X              |
| QUSRJOBI() API format 900 or System i Navigator’s SQL Details for Job          | X         |                  | X                | X                |                |
| Visual Explain within Run SQL scripts                                          | X         |                  | X                | X                | X              |
| Visual Explain outside of Run SQL scripts                                      | X         |                  | X                |                  |                |
| ANALYZE PLAN CACHE procedure                                                   | X         |                  | X                |                  |                |
| DUMP PLAN CACHE procedure                                                      | X         |                  | X                |                  |                |
| MODIFY PLAN CACHE procedure                                                    | X         |                  | X                |                  |                |
| MODIFY PLAN CACHE PROPERTIES procedure (currently does not check authority)    | X         |                  | X                |                  |                |
| CHANGE PLAN CACHE SIZE procedure (currently does not check authority)          | X         |                  | X                |                  |                |

The SQL CREATE PERMISSION statement that is shown in Figure 3-1 is used to define and initially enable or disable the row access rules.

Figure 3-1 CREATE PERMISSION SQL statement
<!-- image -->

## Column mask

A column mask is a database object that manifests a column value access control rule for a specific column in a specific table. It uses a CASE expression that describes what you see when you access the column. For example, a teller can see only the last four digits of a tax identification number.

Table 3-1 summarizes these special registers and their values.

Table 3-1 Special registers and their corresponding values

| Special register     | Corresponding value                                                                                                                   |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------|
| USER or SESSION_USER | The effective user of the thread excluding adopted authority.                                                                         |
| CURRENT_USER         | The effective user of the thread including adopted authority. When no adopted  authority is present, this has the same value as USER. |
| SYSTEM_USER          | The authorization ID that initiated the connection.                                                                                   |

Figure 3-5 shows the difference in the special register values when an adopted authority is used:

- GLYPH<SM590000> A user connects to the server using the user profile ALICE.

- GLYPH<SM590000> USER and CURRENT USER initially have the same value of ALICE.

- GLYPH<SM590000> ALICE calls an SQL procedure that is named proc1, which is owned by user profile JOE and was created to adopt JOE's authority when it is called.

- GLYPH<SM590000> While the procedure is running, the special register USER still contains the value of ALICE because it excludes any adopted authority. The special register CURRENT USER contains the value of JOE because it includes any adopted authority.

- GLYPH<SM590000> When proc1 ends, the session reverts to its original state with both USER and CURRENT USER having the value of ALICE.

Figure 3-5 Special registers and adopted authority
<!-- image -->

## 3.2.2 Built-in global variables

Built-in global variables are provided with the database manager and are used in SQL statements to retrieve scalar values that are associated with the variables.

IBM DB2 for i supports nine different built-in global variables that are read only and maintained by the system. These global variables can be used to identify attributes of the database connection and used as part of the RCAC logic.

Table 3-2 lists the nine built-in global variables.

Table 3-2 Built-in global variables

| Global variable       | Type         | Description                                                    |
|-----------------------|--------------|----------------------------------------------------------------|
| CLIENT_HOST           | VARCHAR(255) | Host name of the current client as returned by the system      |
| CLIENT_IPADDR         | VARCHAR(128) | IP address of the current client as returned by the system     |
| CLIENT_PORT           | INTEGER      | Port used by the current client to communicate with the server |
| PACKAGE_NAME          | VARCHAR(128) | Name of the currently running package                          |
| PACKAGE_SCHEMA        | VARCHAR(128) | Schema name of the currently running package                   |
| PACKAGE_VERSION       | VARCHAR(64)  | Version identifier of the currently running package            |
| ROUTINE_SCHEMA        | VARCHAR(128) | Schema name of the currently running routine                   |
| ROUTINE_SPECIFIC_NAME | VARCHAR(128) | Name of the currently running routine                          |
| ROUTINE_TYPE          | CHAR(1)      | Type of the currently running routine                          |

## 3.3 VERIFY_GROUP_FOR_USER function

The VERIFY_GROUP_FOR_USER function was added in IBM i 7.2. Although it is primarily intended for use with RCAC permissions and masks, it can be used in other SQL statements. The first parameter must be one of these three special registers: SESSION_USER, USER, or CURRENT_USER. The second and subsequent parameters are a list of user or group profiles. Each of these values must be 1 - 10 characters in length. These values are not validated for their existence, which means that you can specify the names of user profiles that do not exist without receiving any kind of error.

If a special register value is in the list of user profiles or it is a member of a group profile included in the list, the function returns a long integer value of 1. Otherwise, it returns a value of 0. It never returns the null value.

Here is an example of using the VERIFY_GROUP_FOR_USER function:

- 1. There are user profiles for MGR, JANE, JUDY, and TONY.

- 2. The user profile JANE specifies a group profile of MGR.

- 3. If a user is connected to the server using user profile JANE, all of the following function invocations return a value of 1:

VERIFY_GROUP_FOR_USER (CURRENT_USER, 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR', 'STEVE') The following function invocation returns a value of 0: VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JUDY', 'TONY')

RETURN

CASE

WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR', 'EMP' ) = 1 THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 9999 || '-' || MONTH ( EMPLOYEES . DATE_OF_BIRTH ) || '-' || DAY (EMPLOYEES.DATE_OF_BIRTH )) ELSE NULL END ENABLE ;

- 2. The other column to mask in this example is the TAX_ID information. In this example, the rules to enforce include the following ones:

- -Human Resources can see the unmasked TAX_ID of the employees.

- -Employees can see only their own unmasked TAX_ID.

- -Managers see a masked version of TAX_ID with the first five characters replaced with the X character (for example, XXX-XX-1234).

- -Any other person sees the entire TAX_ID as masked, for example, XXX-XX-XXXX.

- To implement this column mask, run the SQL statement that is shown in Example 3-9.

CREATE MASK HR_SCHEMA.MASK_TAX_ID_ON_EMPLOYEES ON HR_SCHEMA.EMPLOYEES AS EMPLOYEES FOR COLUMN TAX_ID RETURN CASE WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR' ) = 1 THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( EMPLOYEES . TAX_ID , 8 , 4 ) ) WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'EMP' ) = 1 THEN EMPLOYEES . TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ;

Example 3-9 Creating a mask on the TAX_ID column

- 3. Figure 3-10 shows the masks that are created in the HR_SCHEMA.

Figure 3-10 Column masks shown in System i Navigator
<!-- image -->

## 3.6.6 Activating RCAC

Now that you have created the row permission and the two column masks, RCAC must be activated. The row permission and the two column masks are enabled (last clause in the scripts), but now you must activate RCAC on the table. To do so, complete the following steps:

- 1. Run the SQL statements that are shown in Example 3-10.

## Example 3-10 Activating RCAC on the EMPLOYEES table

- /* Active Row Access Control (permissions) */

- /* Active Column Access Control (masks)

*/

ALTER TABLE HR_SCHEMA.EMPLOYEES

ACTIVATE ROW ACCESS CONTROL

ACTIVATE COLUMN ACCESS CONTROL;

- 2. Look at the definition of the EMPLOYEE table, as shown in Figure 3-11. To do this, from the main navigation pane of System i Navigator, click Schemas  HR_SCHEMA  Tables , right-click the EMPLOYEES table, and click Definition .

Figure 3-11 Selecting the EMPLOYEES table from System i Navigator
<!-- image -->

- 2. Figure 4-68 shows the Visual Explain of the same SQL statement, but with RCAC enabled. It is clear that the implementation of the SQL statement is more complex because the row permission rule becomes part of the WHERE clause.

- 3. Compare the advised indexes that are provided by the Optimizer without RCAC and with RCAC enabled. Figure 4-69 shows the index advice for the SQL statement without RCAC enabled. The index being advised is for the ORDER BY clause.

Figure 4-68 Visual Explain with RCAC enabled
<!-- image -->

Figure 4-69 Index advice with no RCAC
<!-- image -->

THEN C . CUSTOMER_TAX_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( C . CUSTOMER_TAX_ID , 8 , 4 ) ) WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_DRIVERS_LICENSE_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_DRIVERS_LICENSE_NUMBER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER ELSE '*************' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_LOGIN_ID_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_LOGIN_ID RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_LOGIN_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_LOGIN_ID ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ANSWER_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION_ANSWER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER ELSE '*****' END ENABLE ; ALTER TABLE BANK_SCHEMA.CUSTOMERS ACTIVATE ROW ACCESS CONTROL ACTIVATE COLUMN ACCESS CONTROL ;

Back cover

## Row and Column Access Control Support in IBM DB2 for i

Implement roles and separation of duties

Leverage row permissions on the database

This IBM Redpaper publication provides information about the IBM i 7.2 feature of IBM DB2 for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.

Protect columns by defining column masks

This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.

<!-- image -->

<!-- image -->

INTERNATIONAL TECHNICAL SUPPORT ORGANIZATION

BUILDING TECHNICAL INFORMATION BASED ON PRACTICAL EXPERIENCE

IBM Redbooks are developed by the IBM International Technical Support Organization. Experts from IBM, Customers and Partners from around the world create timely technical information based on realistic scenarios. Specific recommendations are provided to help you implement IT solutions more effectively in your environment.

For more information: ibm.com /redbooks

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_01.doctags.txt
================================================
<document>
<subtitle-level-1><location><page_1><loc_37><loc_89><loc_85><loc_91></location>Pythonو R ةغلب ةجمربلا للاخ نم تلاكشملا لحو ةيجاتنلإا نيسحت</subtitle-level-1>
<paragraph><location><page_1><loc_15><loc_80><loc_85><loc_87></location>Python و R ةغلب ةجمربلا ربتعت ةلاعف لولح داجيإ يف دعاستو ةيجاتنلإا ززعت نأ نكمي يتلا ةيوقلا تاودلأا نم ءاملعلاو نيللحملا ىلع لهسي امم ،تانايبلا ليلحتل ةيلاثم اهلعجت ةديرف تازيمPython و R نم لك كلتمي .تلاكشملل ناك اذإ .ةلاعفو ةعيرس ةقيرطب ةدقعم تلايلحت ءارجإ مهسي نأ نكمي تاغللا هذه مادختسا نإف ،ةيليلحت ةيلقع كيدل .لمعلا جئاتن نيسحت يف ريبك لكشب</paragraph>
<paragraph><location><page_1><loc_34><loc_73><loc_34><loc_75></location>ً</paragraph>
<paragraph><location><page_1><loc_83><loc_71><loc_83><loc_73></location>ً</paragraph>
<paragraph><location><page_1><loc_16><loc_71><loc_85><loc_78></location>جارختساو تانايبلا نم ةلئاه تايمك ةجلاعم نكمملا نم حبصي ،ةجمربلا تاراهم عم يليلحتلا ريكفتلا عمتجي امدنع ذيفنتلPython و R مادختسا نيجمربملل نكمي .اهنم تاهجوتلاو طامنلأا ةجذمنلا لثم ،ةمدقتم ةيليلحت تايلمع ةقد رثكأ تارارق ذاختا ىلإ ا ضيأ يدؤي نأ نكمي لب ،تقولا رفوي طقف سيل اذه .ةريبكلا تانايبلا ليلحتو ةيئاصحلإا تانايبلا ىلع ةمئاق تاجاتنتسا ىلع ءانب .</paragraph>
<paragraph><location><page_1><loc_15><loc_63><loc_85><loc_70></location>ليلحتلا نم ،تاقيبطتلا نم ةعساو ةعومجم معدت ةينغ تاودأو تابتكمPython و R نم لك رفوت ،كلذ ىلع ةولاع ىلع .ةفلتخملا تلاكشملل ةركتبم لولح ريوطتل تابتكملا هذه نم ةدافتسلاا نيمدختسملل نكمي .يللآا ملعتلا ىلإ ينايبلا R رفوت امنيب ،ةءافكب تانايبلا ةرادلإ Python يف pandas ةبتكم مادختسا نكمي ،لاثملا ليبس مسرلل ةيوق تاودأ .نيللحملاو نيثحابلل ةيلاثم اهلعجي امم ،يئاصحلإا ليلحتلاو ينايبلا</paragraph>
<paragraph><location><page_1><loc_16><loc_56><loc_85><loc_61></location>Python و R ةغلب ةجمربلا يدؤت نأ نكمي ،ةياهنلا يف ةركتبم لولح ريفوتو ةيجاتنلإا نيسحت ىلإ ةيليلحت ةيلقع عم اهل نوكت نأ نكمي ةبسانملا ةيجمربلا بيلاسلأا قيبطتو لاعف لكشب تانايبلا ليلحت ىلع ةردقلا نإ .ةدقعملا تلاكشملل .ينهملاو يصخشلا ءادلأا ىلع ىدملا ةديعب ةيباجيإ تاريثأت</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_01.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "right_to_left_01.pdf", "filename-prov": null, "document-hash": "85c9c0772fa51fd26f16eaae6abd522c96a4d169ceb7b72cbcfe3444ce22db79", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "6400df9d1750f707e1e0b310224d0b988ed99457bd230029715def0a6030dd06", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [223.85000999999997, 704.4510500000001, 521.98181, 719.4619800000002], "page": 1, "span": [0, 59], "__ref_s3_data": null}], "text": "Python\u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0644\u0644\u0627\u062e \u0646\u0645 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0627 \u0644\u062d\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [90.744003, 635.30804, 522.19, 689.992], "page": 1, "span": [0, 345], "__ref_s3_data": null}], "text": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0631\u0628\u062a\u0639\u062a \u0629\u0644\u0627\u0639\u0641 \u0644\u0648\u0644\u062d \u062f\u0627\u062c\u064a\u0625 \u064a\u0641 \u062f\u0639\u0627\u0633\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0632\u0632\u0639\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u064a\u062a\u0644\u0627 \u0629\u064a\u0648\u0642\u0644\u0627 \u062a\u0627\u0648\u062f\u0644\u0623\u0627 \u0646\u0645 \u0621\u0627\u0645\u0644\u0639\u0644\u0627\u0648 \u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627 \u0649\u0644\u0639 \u0644\u0647\u0633\u064a \u0627\u0645\u0645 \u060c\u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u062a \u0629\u062f\u064a\u0631\u0641 \u062a\u0627\u0632\u064a\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0643\u0644\u062a\u0645\u064a .\u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0646\u0627\u0643 \u0627\u0630\u0625 .\u0629\u0644\u0627\u0639\u0641\u0648 \u0629\u0639\u064a\u0631\u0633 \u0629\u0642\u064a\u0631\u0637\u0628 \u0629\u062f\u0642\u0639\u0645 \u062a\u0644\u0627\u064a\u0644\u062d\u062a \u0621\u0627\u0631\u062c\u0625 \u0645\u0647\u0633\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u062a\u0627\u063a\u0644\u0644\u0627 \u0647\u0630\u0647 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0625\u0641 \u060c\u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0643\u064a\u062f\u0644 .\u0644\u0645\u0639\u0644\u0627 \u062c\u0626\u0627\u062a\u0646 \u0646\u064a\u0633\u062d\u062a \u064a\u0641 \u0631\u064a\u0628\u0643 \u0644\u0643\u0634\u0628", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [208.10402, 579.38806, 208.10402, 592.67206], "page": 1, "span": [0, 1], "__ref_s3_data": null}], "text": "\u064b", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [509.34990999999997, 564.74799, 509.34990999999997, 578.03198], "page": 1, "span": [0, 1], "__ref_s3_data": null}], "text": "\u064b", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [99.863998, 566.06799, 522.23792, 620.75201], "page": 1, "span": [0, 348], "__ref_s3_data": null}], "text": "\u062c\u0627\u0631\u062e\u062a\u0633\u0627\u0648 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0646\u0645 \u0629\u0644\u0626\u0627\u0647 \u062a\u0627\u064a\u0645\u0643 \u0629\u062c\u0644\u0627\u0639\u0645 \u0646\u0643\u0645\u0645\u0644\u0627 \u0646\u0645 \u062d\u0628\u0635\u064a \u060c\u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u062a\u0627\u0631\u0627\u0647\u0645 \u0639\u0645 \u064a\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0631\u064a\u0643\u0641\u062a\u0644\u0627 \u0639\u0645\u062a\u062c\u064a \u0627\u0645\u062f\u0646\u0639 \u0630\u064a\u0641\u0646\u062a\u0644Python \u0648 R \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u064a\u062c\u0645\u0631\u0628\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u0627\u0647\u0646\u0645 \u062a\u0627\u0647\u062c\u0648\u062a\u0644\u0627\u0648 \u0637\u0627\u0645\u0646\u0644\u0623\u0627 \u0629\u062c\u0630\u0645\u0646\u0644\u0627 \u0644\u062b\u0645 \u060c\u0629\u0645\u062f\u0642\u062a\u0645 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u062a\u0627\u064a\u0644\u0645\u0639 \u0629\u0642\u062f \u0631\u062b\u0643\u0623 \u062a\u0627\u0631\u0627\u0631\u0642 \u0630\u0627\u062e\u062a\u0627 \u0649\u0644\u0625 \u0627 \u0636\u064a\u0623 \u064a\u062f\u0624\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u0644\u0628 \u060c\u062a\u0642\u0648\u0644\u0627 \u0631\u0641\u0648\u064a \u0637\u0642\u0641 \u0633\u064a\u0644 \u0627\u0630\u0647 .\u0629\u0631\u064a\u0628\u0643\u0644\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0648 \u0629\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0649\u0644\u0639 \u0629\u0645\u0626\u0627\u0642 \u062a\u0627\u062c\u0627\u062a\u0646\u062a\u0633\u0627 \u0649\u0644\u0639 \u0621\u0627\u0646\u0628 .", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [92.903999, 496.91799999999995, 522.10596, 551.63202], "page": 1, "span": [0, 375], "__ref_s3_data": null}], "text": "\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0646\u0645 \u060c\u062a\u0627\u0642\u064a\u0628\u0637\u062a\u0644\u0627 \u0646\u0645 \u0629\u0639\u0633\u0627\u0648 \u0629\u0639\u0648\u0645\u062c\u0645 \u0645\u0639\u062f\u062a \u0629\u064a\u0646\u063a \u062a\u0627\u0648\u062f\u0623\u0648 \u062a\u0627\u0628\u062a\u0643\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0631\u0641\u0648\u062a \u060c\u0643\u0644\u0630 \u0649\u0644\u0639 \u0629\u0648\u0644\u0627\u0639 \u0649\u0644\u0639 .\u0629\u0641\u0644\u062a\u062e\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0648\u0637\u062a\u0644 \u062a\u0627\u0628\u062a\u0643\u0645\u0644\u0627 \u0647\u0630\u0647 \u0646\u0645 \u0629\u062f\u0627\u0641\u062a\u0633\u0644\u0627\u0627 \u0646\u064a\u0645\u062f\u062e\u062a\u0633\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u064a\u0644\u0644\u0622\u0627 \u0645\u0644\u0639\u062a\u0644\u0627 \u0649\u0644\u0625 \u064a\u0646\u0627\u064a\u0628\u0644\u0627 R \u0631\u0641\u0648\u062a \u0627\u0645\u0646\u064a\u0628 \u060c\u0629\u0621\u0627\u0641\u0643\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0629\u0631\u0627\u062f\u0644\u0625 Python \u064a\u0641 pandas \u0629\u0628\u062a\u0643\u0645 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0643\u0645\u064a \u060c\u0644\u0627\u062b\u0645\u0644\u0627 \u0644\u064a\u0628\u0633 \u0645\u0633\u0631\u0644\u0644 \u0629\u064a\u0648\u0642 \u062a\u0627\u0648\u062f\u0623 .\u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627\u0648 \u0646\u064a\u062b\u062d\u0627\u0628\u0644\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u064a \u0627\u0645\u0645 \u060c\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u0644\u064a\u0644\u062d\u062a\u0644\u0627\u0648 \u064a\u0646\u0627\u064a\u0628\u0644\u0627", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [96.863998, 441.478, 522.07404, 482.362], "page": 1, "span": [0, 267], "__ref_s3_data": null}], "text": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u064a\u062f\u0624\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u060c\u0629\u064a\u0627\u0647\u0646\u0644\u0627 \u064a\u0641 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0641\u0648\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a \u0649\u0644\u0625 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0639\u0645 \u0627\u0647\u0644 \u0646\u0648\u0643\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u0629\u0628\u0633\u0627\u0646\u0645\u0644\u0627 \u0629\u064a\u062c\u0645\u0631\u0628\u0644\u0627 \u0628\u064a\u0644\u0627\u0633\u0644\u0623\u0627 \u0642\u064a\u0628\u0637\u062a\u0648 \u0644\u0627\u0639\u0641 \u0644\u0643\u0634\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a \u0649\u0644\u0639 \u0629\u0631\u062f\u0642\u0644\u0627 \u0646\u0625 .\u0629\u062f\u0642\u0639\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 .\u064a\u0646\u0647\u0645\u0644\u0627\u0648 \u064a\u0635\u062e\u0634\u0644\u0627 \u0621\u0627\u062f\u0644\u0623\u0627 \u0649\u0644\u0639 \u0649\u062f\u0645\u0644\u0627 \u0629\u062f\u064a\u0639\u0628 \u0629\u064a\u0628\u0627\u062c\u064a\u0625 \u062a\u0627\u0631\u064a\u062b\u0623\u062a", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 792.0, "page": 1, "width": 612.0}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_01.md
================================================
## Pythonو R ةغلب ةجمربلا للاخ نم تلاكشملا لحو ةيجاتنلإا نيسحت

Python و R ةغلب ةجمربلا ربتعت ةلاعف لولح داجيإ يف دعاستو ةيجاتنلإا ززعت نأ نكمي يتلا ةيوقلا تاودلأا نم ءاملعلاو نيللحملا ىلع لهسي امم ،تانايبلا ليلحتل ةيلاثم اهلعجت ةديرف تازيمPython و R نم لك كلتمي .تلاكشملل ناك اذإ .ةلاعفو ةعيرس ةقيرطب ةدقعم تلايلحت ءارجإ مهسي نأ نكمي تاغللا هذه مادختسا نإف ،ةيليلحت ةيلقع كيدل .لمعلا جئاتن نيسحت يف ريبك لكشب

ً

جارختساو تانايبلا نم ةلئاه تايمك ةجلاعم نكمملا نم حبصي ،ةجمربلا تاراهم عم يليلحتلا ريكفتلا عمتجي امدنع ذيفنتلPython و R مادختسا نيجمربملل نكمي .اهنم تاهجوتلاو طامنلأا ةجذمنلا لثم ،ةمدقتم ةيليلحت تايلمع ةقد رثكأ تارارق ذاختا ىلإ ا ضيأ يدؤي نأ نكمي لب ،تقولا رفوي طقف سيل اذه .ةريبكلا تانايبلا ليلحتو ةيئاصحلإا تانايبلا ىلع ةمئاق تاجاتنتسا ىلع ءانب .

ليلحتلا نم ،تاقيبطتلا نم ةعساو ةعومجم معدت ةينغ تاودأو تابتكمPython و R نم لك رفوت ،كلذ ىلع ةولاع ىلع .ةفلتخملا تلاكشملل ةركتبم لولح ريوطتل تابتكملا هذه نم ةدافتسلاا نيمدختسملل نكمي .يللآا ملعتلا ىلإ ينايبلا R رفوت امنيب ،ةءافكب تانايبلا ةرادلإ Python يف pandas ةبتكم مادختسا نكمي ،لاثملا ليبس مسرلل ةيوق تاودأ .نيللحملاو نيثحابلل ةيلاثم اهلعجي امم ،يئاصحلإا ليلحتلاو ينايبلا

Python و R ةغلب ةجمربلا يدؤت نأ نكمي ،ةياهنلا يف ةركتبم لولح ريفوتو ةيجاتنلإا نيسحت ىلإ ةيليلحت ةيلقع عم اهل نوكت نأ نكمي ةبسانملا ةيجمربلا بيلاسلأا قيبطتو لاعف لكشب تانايبلا ليلحت ىلع ةردقلا نإ .ةدقعملا تلاكشملل .ينهملاو يصخشلا ءادلأا ىلع ىدملا ةديعب ةيباجيإ تاريثأت

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_02.doctags.txt
================================================
<document>
<paragraph><location><page_1><loc_8><loc_3><loc_10><loc_4></location>11</paragraph>
<paragraph><location><page_1><loc_11><loc_50><loc_73><loc_75></location>،هيلعو ملا ةوا رملا لاول خواهييع ووص عضت ةيرص م لا ةموكح لا نإف ةو اب لأا نم ددي قي حت ىاي لمعلخب خال ةير وام جلا سي ئر د يسلا فياكت ا دو ه :خاسعر ىاي ويولولأا ةومئخق سعر ىا ي يرصملا نخسنلإا ءخهب فام عضو ، تخ ووومن تحدووعم قووي حت ىوو اي لو وم علا ،ليوواعللاو ةحووصلا تحخووجم اووف ةووصخل ىوووواي خوووو حلا ا وووو و ،تخوووو ي خل لا فوووواذع اووووف ةامخوووو و ةمادلووووسمو ةوووويوق وو يلودلاو ةوويمياقلإا تخيدوو حلل ا ءوووض اووف يرووصملا امووو لا نووملأا تاددووحم ،ة وو ام ةووعبخلم رارملووساو ،ةيووسخيسلا ة رخوواملا ر ي وو و لت د ووواو ةاووصاومو تخ ايوووو لاو ةوووفخ لا تخووو ام ريوووولت ، خوووهرلإا ةوووحفخ كمو ر ار لوووسحاو نوووملأا لي هخووو م وووسري ي ووولا وووو حهل ا ىووواي لدووولعملا اهيدووو لا خووولبلاو ،اه،وووولا .اعملجملا ماسلاو ةه،اوملا</paragraph>
<paragraph><location><page_1><loc_13><loc_45><loc_74><loc_48></location>رول لا لاول ةيرو ص م لا ةو موكحلا امخونرب دالوسي ،قبس خمل خً فوو 2024( -)2026 اتلآا وحهلا ىاي اهو ،ةسيئر ةيجيتارلسا اد هع ةعبرع قي حت :</paragraph>
<paragraph><location><page_1><loc_12><loc_37><loc_73><loc_40></location>نــــــــم ما ةــــــــيا م رـ صم لا يم وقل ا اــــسن ا ءاــــ نب رــــــــــــــــــــصم لا عاـــــصت ا ءاـــــ نب يــــــــــــــــــــــسبا نت قتسظا ق يقحت را ر يــــــــــــــــــــــــساي سلا</paragraph>
<paragraph><location><page_1><loc_12><loc_23><loc_73><loc_31></location>خهلوسحخب امخونرب لا ت خفدالوسم ديدحت لت دق هن ع ىلإ رخ لإا ردجت لكواب د روووصم ةو ووي ر تخ فدال ووو س م ىووو اي سيوووئر 2023 ر اوو وو حلا تخووو ساو تخيوووصوتو ، كيال ا تخ اووصيل اه،ووولا امخوونربلاو ،تارا ووو لا ت خ فدا لوو سمو ،اه،ووولا ،ةوو ي ا ةيه، ولا تخ ي جيتا رلسحا فالبمو .</paragraph>
<figure>
<location><page_1><loc_75><loc_23><loc_100><loc_76></location>
</figure>
</document>

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_02.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "right_to_left_02.pdf", "filename-prov": null, "document-hash": "7c10c86372b57e92ef859a9beeafaba13793e29f0d91c6cac47cf7aaf67c9c13", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "6bbaeb8317aa6c61f11969884caceb4a282d52c4d6a9fabaae058bf53003e511", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [47.9520001778084, 23.787720082223927, 58.751999217855335, 37.827721130754185], "page": 1, "span": [0, 2], "__ref_s3_data": null}], "text": "11", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [67.9919972521177, 422.5932914607237, 437.42722162200187, 632.2632421854628], "page": 1, "span": [0, 745], "__ref_s3_data": null}], "text": "\u060c\u0647\u064a\u0644\u0639\u0648 \u0645\u0644\u0627 \u0629\u0648\u0627 \u0631\u0645\u0644\u0627 \u0644\u0627\u0648\u0644 \u062e\u0648\u0627\u0647\u064a\u064a\u0639 \u0648\u0648\u0635 \u0639\u0636\u062a \u0629\u064a\u0631\u0635 \u0645 \u0644\u0627 \u0629\u0645\u0648\u0643\u062d \u0644\u0627 \u0646\u0625\u0641 \u0629\u0648 \u0627\u0628 \u0644\u0623\u0627 \u0646\u0645 \u062f\u062f\u064a \u0642\u064a \u062d\u062a \u0649\u0627\u064a \u0644\u0645\u0639\u0644\u062e\u0628 \u062e\u0627\u0644 \u0629\u064a\u0631 \u0648\u0627\u0645 \u062c\u0644\u0627 \u0633\u064a \u0626\u0631 \u062f \u064a\u0633\u0644\u0627 \u0641\u064a\u0627\u0643\u062a \u0627 \u062f\u0648 \u0647 :\u062e\u0627\u0633\u0639\u0631 \u0649\u0627\u064a \u0648\u064a\u0648\u0644\u0648\u0644\u0623\u0627 \u0629\u0648\u0645\u0626\u062e\u0642 \u0633\u0639\u0631 \u0649\u0627 \u064a \u064a\u0631\u0635\u0645\u0644\u0627 \u0646\u062e\u0633\u0646\u0644\u0625\u0627 \u0621\u062e\u0647\u0628 \u0641\u0627\u0645 \u0639\u0636\u0648 \u060c \u062a\u062e \u0648\u0648\u0648\u0645\u0646 \u062a\u062d\u062f\u0648\u0648\u0639\u0645 \u0642\u0648\u0648\u064a \u062d\u062a \u0649\u0648\u0648 \u0627\u064a \u0644\u0648 \u0648\u0645 \u0639\u0644\u0627 \u060c\u0644\u064a\u0648\u0648\u0627\u0639\u0644\u0644\u0627\u0648 \u0629\u062d\u0648\u0648\u0635\u0644\u0627 \u062a\u062d\u062e\u0648\u0648\u062c\u0645 \u0627\u0648\u0648\u0641 \u0629\u0648\u0648\u0635\u062e\u0644 \u0649\u0648\u0648\u0648\u0648\u0627\u064a \u062e\u0648\u0648\u0648\u0648 \u062d\u0644\u0627 \u0627 \u0648\u0648\u0648\u0648 \u0648 \u060c\u062a\u062e\u0648\u0648\u0648\u0648 \u064a \u062e\u0644 \u0644\u0627 \u0641\u0648\u0648\u0648\u0648\u0627\u0630\u0639 \u0627\u0648\u0648\u0648\u0648\u0641 \u0629\u0627\u0645\u062e\u0648\u0648\u0648\u0648 \u0648 \u0629\u0645\u0627\u062f\u0644\u0648\u0648\u0648\u0648\u0633\u0645\u0648 \u0629\u0648\u0648\u0648\u0648\u064a\u0648\u0642 \u0648\u0648 \u064a\u0644\u0648\u062f\u0644\u0627\u0648 \u0629\u0648\u0648\u064a\u0645\u064a\u0627\u0642\u0644\u0625\u0627 \u062a\u062e\u064a\u062f\u0648\u0648 \u062d\u0644\u0644 \u0627 \u0621\u0648\u0648\u0648\u0636 \u0627\u0648\u0648\u0641 \u064a\u0631\u0648\u0648\u0635\u0645\u0644\u0627 \u0627\u0645\u0648\u0648\u0648 \u0644\u0627 \u0646\u0648\u0648\u0645\u0644\u0623\u0627 \u062a\u0627\u062f\u062f\u0648\u0648\u062d\u0645 \u060c\u0629 \u0648\u0648 \u0627\u0645 \u0629\u0648\u0648\u0639\u0628\u062e\u0644\u0645 \u0631\u0627\u0631\u0645\u0644\u0648\u0648\u0633\u0627\u0648 \u060c\u0629\u064a\u0648\u0648\u0633\u062e\u064a\u0633\u0644\u0627 \u0629 \u0631\u062e\u0648\u0648\u0627\u0645\u0644\u0627 \u0631 \u064a \u0648\u0648 \u0648 \u0644\u062a \u062f \u0648\u0648\u0648\u0627\u0648 \u0629\u0627\u0648\u0648\u0635\u0627\u0648\u0645\u0648 \u062a\u062e \u0627\u064a\u0648\u0648\u0648\u0648 \u0644\u0627\u0648 \u0629\u0648\u0648\u0648\u0641\u062e \u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0627\u0645 \u0631\u064a\u0648\u0648\u0648\u0648\u0644\u062a \u060c \u062e\u0648\u0648\u0648\u0647\u0631\u0644\u0625\u0627 \u0629\u0648\u0648\u0648\u062d\u0641\u062e \u0643\u0645\u0648 \u0631 \u0627\u0631 \u0644\u0648\u0648\u0648\u0633\u062d\u0627\u0648 \u0646\u0648\u0648\u0648\u0645\u0644\u0623\u0627 \u0644\u064a \u0647\u062e\u0648\u0648\u0648 \u0645 \u0648\u0648\u0648\u0633\u0631\u064a \u064a \u0648\u0648\u0648\u0644\u0627 \u0648\u0648\u0648\u0648 \u062d\u0647\u0644 \u0627 \u0649\u0648\u0648\u0648\u0627\u064a \u0644\u062f\u0648\u0648\u0648\u0644\u0639\u0645\u0644\u0627 \u0627\u0647\u064a\u062f\u0648\u0648\u0648 \u0644\u0627 \u062e\u0648\u0648\u0648\u0644\u0628\u0644\u0627\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0648\u0644\u0627 .\u0627\u0639\u0645\u0644\u062c\u0645\u0644\u0627 \u0645\u0627\u0633\u0644\u0627\u0648 \u0629\u0647\u060c\u0627\u0648\u0645\u0644\u0627", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [74.51999727632386, 376.3233013007883, 440.883241634817, 408.00330141029247], "page": 1, "span": [0, 135], "__ref_s3_data": null}], "text": "\u0631\u0648\u0644 \u0644\u0627 \u0644\u0627\u0648\u0644 \u0629\u064a\u0631\u0648 \u0635 \u0645 \u0644\u0627 \u0629\u0648 \u0645\u0648\u0643\u062d\u0644\u0627 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u062f\u0627\u0644\u0648\u0633\u064a \u060c\u0642\u0628\u0633 \u062e\u0645\u0644 \u062e\u064b \u0641\u0648\u0648 2024( -)2026 \u0627\u062a\u0644\u0622\u0627 \u0648\u062d\u0647\u0644\u0627 \u0649\u0627\u064a \u0627\u0647\u0648 \u060c\u0629\u0633\u064a\u0626\u0631 \u0629\u064a\u062c\u064a\u062a\u0627\u0631\u0644\u0633\u0627 \u0627\u062f \u0647\u0639 \u0629\u0639\u0628\u0631\u0639 \u0642\u064a \u062d\u062a :", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [69.09600125621141, 307.8032810639438, 437.3132016215791, 334.49329115619986], "page": 1, "span": [0, 196], "__ref_s3_data": null}], "text": "\u0646\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0645 \u0645\u0627 \u0629\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u064a\u0627 \u0645 \u0631\u0640 \u0635\u0645 \u0644\u0627 \u064a\u0645 \u0648\u0642\u0644 \u0627 \u0627\u0640\u0640\u0640\u0640\u0633\u0646 \u0627 \u0621\u0627\u0640\u0640\u0640\u0640 \u0646\u0628 \u0631\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0635\u0645 \u0644\u0627 \u0639\u0627\u0640\u0640\u0640\u0640\u0640\u0635\u062a \u0627 \u0621\u0627\u0640\u0640\u0640\u0640\u0640 \u0646\u0628 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0628\u0627 \u0646\u062a \u0642\u062a\u0633\u0638\u0627 \u0642 \u064a\u0642\u062d\u062a \u0631\u0627 \u0631 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0627\u064a \u0633\u0644\u0627", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [72.94919627049924, 193.95328067041328, 437.29059162149525, 263.09326090940056], "page": 1, "span": [0, 280], "__ref_s3_data": null}], "text": "\u062e\u0647\u0644\u0648\u0633\u062d\u062e\u0628 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u0644\u0627 \u062a \u062e\u0641\u062f\u0627\u0644\u0648\u0633\u0645 \u062f\u064a\u062f\u062d\u062a \u0644\u062a \u062f\u0642 \u0647\u0646 \u0639 \u0649\u0644\u0625 \u0631\u062e \u0644\u0625\u0627 \u0631\u062f\u062c\u062a \u0644\u0643\u0648\u0627\u0628 \u062f \u0631\u0648\u0648\u0648\u0635\u0645 \u0629\u0648 \u0648\u0648\u064a \u0631 \u062a\u062e \u0641\u062f\u0627\u0644 \u0648\u0648\u0648 \u0633 \u0645 \u0649\u0648\u0648\u0648 \u0627\u064a \u0633\u064a\u0648\u0648\u0648\u0626\u0631 2023 \u0631 \u0627\u0648\u0648 \u0648\u0648 \u062d\u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0633\u0627\u0648 \u062a\u062e\u064a\u0648\u0648\u0648\u0635\u0648\u062a\u0648 \u060c \u0643\u064a\u0627\u0644 \u0627 \u062a\u062e \u0627\u0648\u0648\u0635\u064a\u0644 \u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u0627\u0645\u062e\u0648\u0648\u0646\u0631\u0628\u0644\u0627\u0648 \u060c\u062a\u0627\u0631\u0627 \u0648\u0648\u0648 \u0644\u0627 \u062a \u062e \u0641\u062f\u0627 \u0644\u0648\u0648 \u0633\u0645\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u060c\u0629\u0648\u0648 \u064a \u0627 \u0629\u064a\u0647\u060c \u0648\u0644\u0627 \u062a\u062e \u064a \u062c\u064a\u062a\u0627 \u0631\u0644\u0633\u062d\u0627 \u0641\u0627\u0644\u0628\u0645\u0648 .", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/0"}], "figures": [{"prov": [{"bbox": [446.4657287597656, 191.27679443359375, 595.0, 641.2087554931641], "page": 1, "span": [0, 0], "__ref_s3_data": null}], "text": "", "type": "figure", "payload": null, "bounding-box": null}], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 841.9199829101562, "page": 1, "width": 595.2000122070312}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_02.md
================================================
11

،هيلعو ملا ةوا رملا لاول خواهييع ووص عضت ةيرص م لا ةموكح لا نإف ةو اب لأا نم ددي قي حت ىاي لمعلخب خال ةير وام جلا سي ئر د يسلا فياكت ا دو ه :خاسعر ىاي ويولولأا ةومئخق سعر ىا ي يرصملا نخسنلإا ءخهب فام عضو ، تخ ووومن تحدووعم قووي حت ىوو اي لو وم علا ،ليوواعللاو ةحووصلا تحخووجم اووف ةووصخل ىوووواي خوووو حلا ا وووو و ،تخوووو ي خل لا فوووواذع اووووف ةامخوووو و ةمادلووووسمو ةوووويوق وو يلودلاو ةوويمياقلإا تخيدوو حلل ا ءوووض اووف يرووصملا امووو لا نووملأا تاددووحم ،ة وو ام ةووعبخلم رارملووساو ،ةيووسخيسلا ة رخوواملا ر ي وو و لت د ووواو ةاووصاومو تخ ايوووو لاو ةوووفخ لا تخووو ام ريوووولت ، خوووهرلإا ةوووحفخ كمو ر ار لوووسحاو نوووملأا لي هخووو م وووسري ي ووولا وووو حهل ا ىووواي لدووولعملا اهيدووو لا خووولبلاو ،اه،وووولا .اعملجملا ماسلاو ةه،اوملا

رول لا لاول ةيرو ص م لا ةو موكحلا امخونرب دالوسي ،قبس خمل خً فوو 2024( -)2026 اتلآا وحهلا ىاي اهو ،ةسيئر ةيجيتارلسا اد هع ةعبرع قي حت :

نــــــــم ما ةــــــــيا م رـ صم لا يم وقل ا اــــسن ا ءاــــ نب رــــــــــــــــــــصم لا عاـــــصت ا ءاـــــ نب يــــــــــــــــــــــسبا نت قتسظا ق يقحت را ر يــــــــــــــــــــــــساي سلا

خهلوسحخب امخونرب لا ت خفدالوسم ديدحت لت دق هن ع ىلإ رخ لإا ردجت لكواب د روووصم ةو ووي ر تخ فدال ووو س م ىووو اي سيوووئر 2023 ر اوو وو حلا تخووو ساو تخيوووصوتو ، كيال ا تخ اووصيل اه،ووولا امخوونربلاو ،تارا ووو لا ت خ فدا لوو سمو ،اه،ووولا ،ةوو ي ا ةيه، ولا تخ ي جيتا رلسحا فالبمو .

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_03.doctags.txt
================================================
<document>
<subtitle-level-1><location><page_1><loc_12><loc_90><loc_45><loc_93></location>یلخاد یلااک - یلصا رازاب رد شريذپ همانديما</subtitle-level-1>
<figure>
<location><page_1><loc_65><loc_88><loc_81><loc_96></location>
</figure>
<subtitle-level-1><location><page_1><loc_63><loc_81><loc_81><loc_84></location>لااک درادناتسا -2-5</subtitle-level-1>
<paragraph><location><page_1><loc_77><loc_79><loc_87><loc_81></location>درادناتسا مان</paragraph>
<paragraph><location><page_1><loc_11><loc_75><loc_44><loc_81></location>یرگ هتخير شور هب هدش ديلوت لاشمش و هشمش فرصم دروم هتسويپ یا هزاس یاهدلاوف رد - قباطم تسويپ زيلانآ</paragraph>
<paragraph><location><page_1><loc_71><loc_72><loc_87><loc_74></location>یلم درادناتسا هرامش</paragraph>
<paragraph><location><page_1><loc_40><loc_73><loc_45><loc_74></location>20300</paragraph>
<paragraph><location><page_1><loc_68><loc_70><loc_87><loc_72></location>؟تسا یرابجا درادناتسا</paragraph>
<paragraph><location><page_1><loc_65><loc_67><loc_87><loc_69></location>درادناتسا هدننکرداص عجرم</paragraph>
<paragraph><location><page_1><loc_28><loc_67><loc_44><loc_69></location>ناريا درادناتسا یلم نامزاس</paragraph>
<paragraph><location><page_1><loc_49><loc_62><loc_87><loc_66></location>ذخا ار روکذم درادناتسا ،لوصحم هدننکديلوت ايآ ؟تسا هدومن</paragraph>
<subtitle-level-1><location><page_1><loc_69><loc_56><loc_85><loc_58></location>سروب رد شريذپ -3</subtitle-level-1>
<paragraph><location><page_1><loc_68><loc_54><loc_83><loc_56></location>کرادم هئارا خيرات</paragraph>
<paragraph><location><page_1><loc_23><loc_54><loc_32><loc_56></location>1403/09/19</paragraph>
<paragraph><location><page_1><loc_72><loc_51><loc_83><loc_53></location>شريذپ خيرات</paragraph>
<paragraph><location><page_1><loc_23><loc_51><loc_32><loc_53></location>1403/10/04</paragraph>
<paragraph><location><page_1><loc_62><loc_48><loc_83><loc_50></location>هضرع هتيمک هسلج هرامش</paragraph>
<paragraph><location><page_1><loc_26><loc_49><loc_29><loc_50></location>436</paragraph>
<paragraph><location><page_1><loc_67><loc_45><loc_83><loc_47></location>همانديما جرد خيرات</paragraph>
<paragraph><location><page_1><loc_23><loc_46><loc_32><loc_48></location>1403/10/05</paragraph>
<paragraph><location><page_1><loc_71><loc_43><loc_83><loc_45></location>شريذپ رواشم</paragraph>
<paragraph><location><page_1><loc_21><loc_43><loc_34><loc_45></location>سروب نومرآ یرازگراک</paragraph>
<paragraph><location><page_1><loc_47><loc_37><loc_83><loc_42></location>رد لااک شريذپ زا سپ هياپ تميق نييعت ةوحن سروب</paragraph>
<paragraph><location><page_1><loc_18><loc_40><loc_36><loc_42></location>یناهج  یاه تميق ساسا رب</paragraph>
<paragraph><location><page_1><loc_45><loc_32><loc_83><loc_37></location>شورف /شورف لک /ديلوت زا هضرع دصرد لقادح یلخاد</paragraph>
<paragraph><location><page_1><loc_14><loc_35><loc_40><loc_37></location>نت 47.500 اي هنايلاس ديلوت زا %50 لقادح</paragraph>
<paragraph><location><page_1><loc_68><loc_29><loc_83><loc_31></location>ليوحت زاجم یاطخ</paragraph>
<paragraph><location><page_1><loc_18><loc_30><loc_37><loc_31></location>ليوحت لباق هلومحم نيرخآ 5%</paragraph>
</document>

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_03.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "right_to_left_03.pdf", "filename-prov": null, "document-hash": "367cb9ca8606ce5676164d44f08ba7e28b794379a2124402672712e12a160bee", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "c13f4c78e4268264071589d2e5620246a5c3b3bf286522a5fed5edb9b6fdc1bc", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [68.78399669083697, 761.0098882171737, 267.65960879695194, 779.3882381741187], "page": 1, "span": [0, 42], "__ref_s3_data": null}], "text": "\u06cc\u0644\u062e\u0627\u062f \u06cc\u0644\u0627\u0627\u06a9 - \u06cc\u0644\u0635\u0627 \u0631\u0627\u0632\u0627\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e \u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"name": "Picture", "type": "figure", "$ref": "#/figures/0"}, {"prov": [{"bbox": [373.9899883190294, 685.3749983943645, 479.52999784465936, 703.4050283521253], "page": 1, "span": [0, 19], "__ref_s3_data": null}], "text": "\u0644\u0627\u0627\u06a9 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 -2-5", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [458.7399879381041, 662.7401084473915, 519.2383976661823, 679.6162084078558], "page": 1, "span": [0, 13], "__ref_s3_data": null}], "text": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0645\u0627\u0646", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [66.26399970216359, 631.5399785204845, 264.81795880972436, 681.171998404211], "page": 1, "span": [0, 97], "__ref_s3_data": null}], "text": "\u06cc\u0631\u06af \u0647\u062a\u062e\u064a\u0631 \u0634\u0648\u0631 \u0647\u0628 \u0647\u062f\u0634 \u062f\u064a\u0644\u0648\u062a \u0644\u0627\u0634\u0645\u0634 \u0648 \u0647\u0634\u0645\u0634 \u0641\u0631\u0635\u0645 \u062f\u0631\u0648\u0645 \u0647\u062a\u0633\u0648\u064a\u067e \u06cc\u0627 \u0647\u0632\u0627\u0633 \u06cc\u0627\u0647\u062f\u0644\u0627\u0648\u0641 \u0631\u062f - \u0642\u0628\u0627\u0637\u0645 \u062a\u0633\u0648\u064a\u067e \u0632\u064a\u0644\u0627\u0646\u0622", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [420.9099981081384, 608.8601085736167, 519.1619876665258, 625.7362085340809], "page": 1, "span": [0, 19], "__ref_s3_data": null}], "text": "\u06cc\u0644\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u0631\u0627\u0645\u0634", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [236.80999893561153, 613.2999885632154, 265.01000880886113, 627.2919885304362], "page": 1, "span": [0, 5], "__ref_s3_data": null}], "text": "20300", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [406.9899881707045, 586.1501486268197, 519.1415376666176, 603.0262485872838], "page": 1, "span": [0, 21], "__ref_s3_data": null}], "text": "\u061f\u062a\u0633\u0627 \u06cc\u0631\u0627\u0628\u062c\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [197.32999911306206, 590.5900286164182, 264.91399880929265, 604.5820285836392], "page": 1, "span": [0, 13], "__ref_s3_data": null}], "text": "\u0631\u064a\u062e       \u06cc\u0644\u0628", "type": "checkbox-unselected", "payload": null, "name": "Checkbox-Unselected", "font": null}, {"prov": [{"bbox": [389.4699982494516, 563.4701486799523, 519.2136776662934, 580.3462486404165], "page": 1, "span": [0, 24], "__ref_s3_data": null}], "text": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u062f\u0646\u0646\u06a9\u0631\u062f\u0627\u0635 \u0639\u062c\u0631\u0645", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [166.5799992512739, 567.9100286695509, 264.77599880991295, 581.9020386367717], "page": 1, "span": [0, 26], "__ref_s3_data": null}], "text": "\u0646\u0627\u0631\u064a\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u06cc\u0644\u0645 \u0646\u0627\u0645\u0632\u0627\u0633", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [292.129998686965, 518.5901487850932, 519.2351676661968, 557.6661986935493], "page": 1, "span": [0, 55], "__ref_s3_data": null}], "text": "\u0630\u062e\u0627 \u0627\u0631 \u0631\u0648\u06a9\u0630\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u060c\u0644\u0648\u0635\u062d\u0645 \u0647\u062f\u0646\u0646\u06a9\u062f\u064a\u0644\u0648\u062a \u0627\u064a\u0622 \u061f\u062a\u0633\u0627 \u0647\u062f\u0648\u0645\u0646", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [197.32999911306206, 545.2299787226838, 208.04769906488926, 559.2219786899045], "page": 1, "span": [0, 3], "__ref_s3_data": null}], "text": "\u0631\u064a\u062e", "type": "checkbox-selected", "payload": null, "name": "Checkbox-Selected", "font": null}, {"prov": [{"bbox": [236.62821893642857, 545.2299787226838, 247.34591888825577, 559.2219786899045], "page": 1, "span": [0, 3], "__ref_s3_data": null}], "text": "\u06cc\u0644\u0628", "type": "checkbox-unselected", "payload": null, "name": "Checkbox-Unselected", "font": null}, {"prov": [{"bbox": [409.0299981615353, 473.71013889023413, 505.7644977267433, 490.58620885069837], "page": 1, "span": [0, 16], "__ref_s3_data": null}], "text": "\u0633\u0631\u0648\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e -3", "type": "subtitle-level-1", "payload": null, "name": "Section-header", "font": null}, {"prov": [{"bbox": [405.30999817825557, 451.01012894341363, 492.6107177858655, 467.88619890387787], "page": 1, "span": [0, 17], "__ref_s3_data": null}], "text": "\u06a9\u0631\u0627\u062f\u0645 \u0647\u0626\u0627\u0631\u0627 \u062e\u064a\u0631\u0627\u062a", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [137.89998938018175, 455.4699989329655, 187.8199891558066, 469.4620089001862], "page": 1, "span": [0, 10], "__ref_s3_data": null}], "text": "1403/09/19", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [426.309998083867, 428.3301389965463, 492.59463778593783, 445.2062089570106], "page": 1, "span": [0, 11], "__ref_s3_data": null}], "text": "\u0634\u0631\u064a\u0630\u067e \u062e\u064a\u0631\u0627\u062a", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [137.89998938018175, 432.7700189861449, 187.8199891558066, 446.7620189533657], "page": 1, "span": [0, 10], "__ref_s3_data": null}], "text": "1403/10/04", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [367.14998834977314, 405.65011904967906, 492.68526778553047, 422.5261790101433], "page": 1, "span": [0, 21], "__ref_s3_data": null}], "text": "\u0647\u0636\u0631\u0639 \u0647\u062a\u064a\u0645\u06a9 \u0647\u0633\u0644\u062c \u0647\u0631\u0627\u0645\u0634", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [154.69999930467083, 409.96999903955884, 171.19999923050838, 423.96200900677957], "page": 1, "span": [0, 3], "__ref_s3_data": null}], "text": "436", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [399.42998820468443, 382.8501291030928, 492.62752778578994, 399.72619906355703], "page": 1, "span": [0, 18], "__ref_s3_data": null}], "text": "\u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627 \u062c\u0631\u062f \u062e\u064a\u0631\u0627\u062a", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [137.89998938018175, 387.29000909269143, 187.8199891558066, 401.2820090599123], "page": 1, "span": [0, 10], "__ref_s3_data": null}], "text": "1403/10/05", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [422.82998809950857, 360.17013915622545, 492.6789577855588, 377.04619911668976], "page": 1, "span": [0, 11], "__ref_s3_data": null}], "text": "\u0634\u0631\u064a\u0630\u067e \u0631\u0648\u0627\u0634\u0645", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [122.05999945137766, 364.6100191458242, 203.6480090846645, 378.6020191130449], "page": 1, "span": [0, 19], "__ref_s3_data": null}], "text": "\u0633\u0631\u0648\u0628 \u0646\u0648\u0645\u0631\u0622 \u06cc\u0631\u0627\u0632\u06af\u0631\u0627\u06a9", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [281.3299887355078, 313.730129265021, 492.70525778544066, 352.6861891737582], "page": 1, "span": [0, 45], "__ref_s3_data": null}], "text": "\u0631\u062f \u0644\u0627\u0627\u06a9 \u0634\u0631\u064a\u0630\u067e \u0632\u0627 \u0633\u067e \u0647\u064a\u0627\u067e \u062a\u0645\u064a\u0642 \u0646\u064a\u064a\u0639\u062a \u0629\u0648\u062d\u0646 \u0633\u0631\u0648\u0628", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [109.21999950908952, 340.2499992028926, 213.67396903960088, 354.24199917011344], "page": 1, "span": [0, 23], "__ref_s3_data": null}], "text": "\u06cc\u0646\u0627\u0647\u062c  \u06cc\u0627\u0647 \u062a\u0645\u064a\u0642 \u0633\u0627\u0633\u0627 \u0631\u0628", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [266.5700088018494, 268.82012937023217, 492.7008677854604, 307.7761792789694], "page": 1, "span": [0, 45], "__ref_s3_data": null}], "text": "\u0634\u0648\u0631\u0641 /\u0634\u0648\u0631\u0641 \u0644\u06a9 /\u062f\u064a\u0644\u0648\u062a \u0632\u0627 \u0647\u0636\u0631\u0639 \u062f\u0635\u0631\u062f \u0644\u0642\u0627\u062f\u062d \u06cc\u0644\u062e\u0627\u062f", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [85.4639966158655, 295.33999930810376, 240.36199891964634, 309.3319992753245], "page": 1, "span": [0, 39], "__ref_s3_data": null}], "text": "\u0646\u062a 47.500 \u0627\u064a \u0647\u0646\u0627\u064a\u0644\u0627\u0633 \u062f\u064a\u0644\u0648\u062a \u0632\u0627 %50 \u0644\u0642\u0627\u062f\u062d", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [404.2300081831098, 246.02010942364598, 492.6399177857343, 262.8962093841102], "page": 1, "span": [0, 15], "__ref_s3_data": null}], "text": "\u0644\u064a\u0648\u062d\u062a \u0632\u0627\u062c\u0645 \u06cc\u0627\u0637\u062e", "type": "paragraph", "payload": null, "name": "Text", "font": null}, {"prov": [{"bbox": [106.93999951933742, 250.45998941324467, 218.89399901613845, 264.4519993804654], "page": 1, "span": [0, 26], "__ref_s3_data": null}], "text": "\u0644\u064a\u0648\u062d\u062a \u0644\u0628\u0627\u0642 \u0647\u0644\u0648\u0645\u062d\u0645 \u0646\u064a\u0631\u062e\u0622 5%", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [{"prov": [{"bbox": [388.5767822265625, 739.034423828125, 482.4759216308594, 806.0041046142578], "page": 1, "span": [0, 0], "__ref_s3_data": null}], "text": "", "type": "figure", "payload": null, "bounding-box": null}], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 842.0399780273438, "page": 1, "width": 595.3200073242188}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data/groundtruth/docling_v1/right_to_left_03.md
================================================
## یلخاد یلااک - یلصا رازاب رد شريذپ همانديما

<!-- image -->

## لااک درادناتسا -2-5

درادناتسا مان

یرگ هتخير شور هب هدش ديلوت لاشمش و هشمش فرصم دروم هتسويپ یا هزاس یاهدلاوف رد - قباطم تسويپ زيلانآ

یلم درادناتسا هرامش

20300

؟تسا یرابجا درادناتسا

درادناتسا هدننکرداص عجرم

ناريا درادناتسا یلم نامزاس

ذخا ار روکذم درادناتسا ،لوصحم هدننکديلوت ايآ ؟تسا هدومن

## سروب رد شريذپ -3

کرادم هئارا خيرات

1403/09/19

شريذپ خيرات

1403/10/04

هضرع هتيمک هسلج هرامش

436

همانديما جرد خيرات

1403/10/05

شريذپ رواشم

سروب نومرآ یرازگراک

رد لااک شريذپ زا سپ هياپ تميق نييعت ةوحن سروب

یناهج  یاه تميق ساسا رب

شورف /شورف لک /ديلوت زا هضرع دصرد لقادح یلخاد

نت 47.500 اي هنايلاس ديلوت زا %50 لقادح

ليوحت زاجم یاطخ

ليوحت لباق هلومحم نيرخآ 5%

================================================
File: tests/data/groundtruth/docling_v2/2206.01062.md
================================================
## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis

Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com

Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com

Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com

Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com

Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com

## ABSTRACT

Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.

## CCS CONCEPTS

· Information systems → Document structure ; · Applied computing → Document analysis ; · Computing methodologies → Machine learning ; Computer vision ; Object detection ;

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

KDD ’22, August 14-18, 2022, Washington, DC, USA

© 2022 Copyright held by the owner/author(s).

ACM ISBN 978-1-4503-9385-0/22/08.

https://doi.org/10.1145/3534678.3539043

Figure 1: Four examples of complex page layouts across different document categories

<!-- image -->

## KEYWORDS

PDF document conversion, layout segmentation, object-detection, data set, Machine Learning

## ACM Reference Format:

Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043

## 1 INTRODUCTION

Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.

A key problem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.

In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:

- (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
- (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.
- (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
- (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.

This enables experimentation with annotation uncertainty and quality control analysis.

- (5) Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.

All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.

In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.

## 2 RELATED WORK

While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].

Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.

## 3 THE DOCLAYNET DATASET

DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.

In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents

Figure 2: Distribution of DocLayNet pages across document categories.

<!-- image -->

to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing "text in the wild".

The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals , Scientific Articles , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.

We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.

To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.

Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.

In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.

Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, "invisible" tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as "invisible" list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a "natural" upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.

## 4 ANNOTATION CAMPAIGN

The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,

Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row "Total") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.

|                |         | % of Total   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   | triple inter-annotator mAP @ 0.5-0.95 (%)   |
|----------------|---------|--------------|--------------|--------------|--------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|
| class label    | Count   | Train        | Test         | Val          | All          | Fin                                         | Man                                         | Sci                                         | Law                                         | Pat                                         | Ten                                         |
| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89        | 40-61                                       | 86-92                                       | 94-99                                       | 95-99                                       | 69-78                                       | n/a                                         |
| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91        | n/a                                         | 100                                         | 62-88                                       | 85-94                                       | n/a                                         | 82-97                                       |
| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85        | n/a                                         | n/a                                         | 84-87                                       | 86-96                                       | n/a                                         | n/a                                         |
| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88        | 74-83                                       | 90-92                                       | 97-97                                       | 81-85                                       | 75-88                                       | 93-95                                       |
| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94        | 88-90                                       | 95-96                                       | 100                                         | 92-97                                       | 100                                         | 96-98                                       |
| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89        | 66-76                                       | 90-94                                       | 98-100                                      | 91-92                                       | 97-99                                       | 81-86                                       |
| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71        | 56-59                                       | 82-86                                       | 69-82                                       | 80-95                                       | 66-71                                       | 59-76                                       |
| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84        | 76-81                                       | 90-92                                       | 94-95                                       | 87-94                                       | 69-73                                       | 78-86                                       |
| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81        | 75-80                                       | 83-86                                       | 98-99                                       | 58-80                                       | 79-84                                       | 70-85                                       |
| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86        | 81-86                                       | 88-93                                       | 89-93                                       | 87-92                                       | 71-79                                       | 87-95                                       |
| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72        | 24-63                                       | 50-63                                       | 94-100                                      | 82-96                                       | 68-79                                       | 24-56                                       |
| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83        | 71-74                                       | 79-81                                       | 89-94                                       | 86-91                                       | 71-76                                       | 68-85                                       |

Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.

<!-- image -->

we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.

Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv$^{3}$, government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.

Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.

Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula , List-item , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on

the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.

At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.

Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:

- (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
- (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
- (3) For every Caption , there must be exactly one corresponding Picture or Table .
- (4) Connected sub-pictures are grouped together in one Picture object.
- (5) Formula numbers are included in a Formula object.
- (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.

The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.

Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations

<!-- image -->

05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0

were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.

Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted

Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.

|                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
|----------------|---------|---------|---------|---------|--------|
|                | human   | R50     | R101    | R101    | v5x6   |
| Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
| Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
| Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
| List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
| Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
| Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
| Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
| Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
| Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
| Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
| Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
| All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |

to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.

## 5 EXPERIMENTS

The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this

Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.

<!-- image -->

paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.

In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].

## Baselines for Object Detection

In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.

Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.

| Class-count    |   11 | 6       | 5       | 4       |
|----------------|------|---------|---------|---------|
| Caption        |   68 | Text    | Text    | Text    |
| Footnote       |   71 | Text    | Text    | Text    |
| Formula        |   60 | Text    | Text    | Text    |
| List-item      |   81 | Text    | 82      | Text    |
| Page-footer    |   62 | 62      | -       | -       |
| Page-header    |   72 | 68      | -       | -       |
| Picture        |   72 | 72      | 72      | 72      |
| Section-header |   68 | 67      | 69      | 68      |
| Table          |   82 | 83      | 82      | 82      |
| Text           |   85 | 84      | 84      | 84      |
| Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
| Overall        |   72 | 73      | 78      | 77      |

## Learning Curve

One of the fundamental questions related to any dataset is if it is "large enough". To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.

## Impact of Class Labels

The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of

Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in GLYPH&lt;tildelow&gt; 10% point improvement.

| Class-count    | 11   | 11   | 5   | 5    |
|----------------|------|------|-----|------|
| Split          | Doc  | Page | Doc | Page |
| Caption        | 68   | 83   |     |      |
| Footnote       | 71   | 84   |     |      |
| Formula        | 60   | 66   |     |      |
| List-item      | 81   | 88   | 82  | 88   |
| Page-footer    | 62   | 89   |     |      |
| Page-header    | 72   | 90   |     |      |
| Picture        | 72   | 82   | 72  | 82   |
| Section-header | 68   | 83   | 69  | 83   |
| Table          | 82   | 89   | 82  | 90   |
| Text           | 85   | 91   | 84  | 90   |
| Title          | 77   | 81   |     |      |
| All            | 72   | 84   | 78  | 87   |

lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.

## Impact of Document Split in Train and Test Set

Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.

## Dataset Comparison

Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,

Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.

|                 |            | Testing on   | Testing on   | Testing on   |
|-----------------|------------|--------------|--------------|--------------|
| Training on     | labels     | PLN          | DB           | DLN          |
| PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
| PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
| PubLayNet (PLN) | Table      | 95           | 24           | 49           |
| PubLayNet (PLN) | Text       | 96           | -            | 42           |
| PubLayNet (PLN) | total      | 93           | 34           | 30           |
| DocBank (DB)    | Figure     | 77           | 71           | 31           |
| DocBank (DB)    | Table      | 19           | 65           | 22           |
| DocBank (DB)    | total      | 48           | 68           | 27           |
| DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
| DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
| DocLayNet (DLN) | Table      | 87           | 43           | 82           |
| DocLayNet (DLN) | Text       | 77           | -            | 84           |
| DocLayNet (DLN) | total      | 59           | 47           | 78           |

Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .

For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.

## Example Predictions

To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.

## 6 CONCLUSION

In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.

From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.

To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.

## REFERENCES

- [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.
- [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.
- [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
- [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
- [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.
- [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.
- [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
- [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.
- [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.
- [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.
- [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
- [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu

Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title

<!-- image -->

Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.

Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.

- [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
- [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.
- [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.
- [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
- [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
- [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.
- [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
- [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
- [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.
- [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.

================================================
File: tests/data/groundtruth/docling_v2/2305.03393v1-pg9.doctags.txt
================================================
<doctag><page_header><loc_159><loc_58><loc_366><loc_65>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_389><loc_58><loc_393><loc_65>9</page_header>
<text><loc_110><loc_74><loc_393><loc_97>order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.</text>
<section_header_level_1><loc_110><loc_105><loc_260><loc_113>5.1 Hyper Parameter Optimization</section_header_level_1>
<text><loc_110><loc_116><loc_393><loc_161>We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.</text>
<otsl><loc_114><loc_213><loc_388><loc_296><ched>#<ched>#<ched>Language<ched>TEDs<lcel><lcel><ched>mAP<ched>Inference<nl><ched>enc-layers<ched>dec-layers<ucel><ched>simple<ched>complex<ched>all<ched>(0.75)<ched>time (secs)<nl><fcel>6<fcel>6<fcel>OTSL HTML<fcel>0.965 0.969<fcel>0.934 0.927<fcel>0.955 0.955<fcel>0.88 0.857<fcel>2.73 5.39<nl><fcel>4<fcel>4<fcel>OTSL HTML<fcel>0.938<fcel>0.904<fcel>0.927<fcel>0.853<fcel>1.97<nl><ecel><ecel><fcel>OTSL<fcel>0.952 0.923<fcel>0.909<fcel>0.938<fcel>0.843<fcel>3.77<nl><fcel>2<fcel>4<fcel>HTML<fcel>0.945<fcel>0.897 0.901<fcel>0.915 0.931<fcel>0.859 0.834<fcel>1.91 3.81<nl><fcel>4<fcel>2<fcel>OTSL HTML<fcel>0.952 0.944<fcel>0.92 0.903<fcel>0.942 0.931<fcel>0.857 0.824<fcel>1.22 2<nl><caption><loc_110><loc_172><loc_393><loc_207>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption></otsl>
<section_header_level_1><loc_110><loc_319><loc_216><loc_327>5.2 Quantitative Results</section_header_level_1>
<text><loc_110><loc_330><loc_393><loc_390>We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.</text>
<text><loc_110><loc_390><loc_393><loc_421>Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.</text>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/2305.03393v1-pg9.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "2305.03393v1-pg9", "origin": {"mimetype": "application/pdf", "binary_hash": 3463920545297462180, "filename": "2305.03393v1-pg9.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/texts/2"}, {"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/tables/0"}, {"cref": "#/texts/6"}, {"cref": "#/texts/7"}, {"cref": "#/texts/8"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_header", "prov": [{"page_no": 1, "bbox": {"l": 194.478, "t": 700.50647, "r": 447.54476999999997, "b": 689.21777, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 60]}], "orig": "Optimized Table Tokenization for Table Structure Recognition", "text": "Optimized Table Tokenization for Table Structure Recognition"}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_header", "prov": [{"page_no": 1, "bbox": {"l": 475.98441, "t": 700.50647, "r": 480.59314, "b": 689.21777, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "9", "text": "9"}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 675.53699, "r": 480.59665, "b": 639.09302, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 163]}], "orig": "order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.", "text": "order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz."}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 625.29486, "r": 318.45145, "b": 612.79181, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}], "orig": "5.1 Hyper Parameter Optimization", "text": "5.1 Hyper Parameter Optimization", "level": 1}, {"self_ref": "#/texts/4", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 608.88495, "r": 480.59567, "b": 536.57599, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 423]}], "orig": "We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.", "text": "We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML."}, {"self_ref": "#/texts/5", "parent": {"cref": "#/tables/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 519.20526, "r": 480.59890999999993, "b": 464.01782, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 398]}], "orig": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.", "text": "Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart."}, {"self_ref": "#/texts/6", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 286.32889, "r": 264.40829, "b": 273.82581000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 24]}], "orig": "5.2 Quantitative Results", "text": "5.2 Quantitative Results", "level": 1}, {"self_ref": "#/texts/7", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 269.91995, "r": 480.72003, "b": 173.70000000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 555]}], "orig": "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.", "text": "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables."}, {"self_ref": "#/texts/8", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 134.765, "t": 174.27795000000003, "r": 480.59857000000005, "b": 125.88, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 289]}], "orig": "Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.", "text": "Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation."}], "pictures": [], "tables": [{"self_ref": "#/tables/0", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/5"}], "content_layer": "body", "label": "table", "prov": [{"page_no": 1, "bbox": {"l": 139.66741943359375, "t": 454.45458984375, "r": 475.00927734375, "b": 322.5054626464844, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"cref": "#/texts/5"}], "references": [], "footnotes": [], "image": null, "data": {"table_cells": [{"bbox": {"l": 160.37, "t": 339.45749, "r": 168.04523, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "#", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 144.592, "t": 352.40848, "r": 183.82895, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "enc-layers", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 207.974, "t": 339.45749, "r": 215.64923000000002, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "#", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 192.19501, "t": 352.40848, "r": 231.42303, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "dec-layers", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 239.79799999999997, "t": 344.93649, "r": 278.3338, "b": 356.22519000000005, "coord_origin": "TOPLEFT"}, "row_span": 2, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Language", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 324.67001, "t": 339.45749, "r": 348.26419, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 3, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 6, "text": "TEDs", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 396.271, "t": 339.45749, "r": 417.12595, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "mAP", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 394.927, "t": 350.41647, "r": 418.46921, "b": 361.70517, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "(0.75)", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 430.771, "t": 339.45749, "r": 467.14142000000004, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "Inference", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 427.14801, "t": 350.41647, "r": 470.76955999999996, "b": 361.70517, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "time (secs)", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 286.686, "t": 352.40848, "r": 312.32812, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "simple", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 320.702, "t": 352.40848, "r": 353.71539, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "complex", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 369.306, "t": 352.40848, "r": 379.02914, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "all", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 161.90601, "t": 371.23849, "r": 166.51474, "b": 382.52719, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "6", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 371.23849, "r": 214.11774, "b": 382.52719, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "6", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 365.75848, "r": 272.94495, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 365.75848, "r": 310.00732, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.965 0.969", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 365.75848, "r": 347.70734, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.934 0.927", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 363.67599, "t": 365.75848, "r": 384.66632, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.955 0.955", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 396.20599, "t": 365.69571, "r": 417.19632, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.88 0.857", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 365.69571, "r": 458.38336, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "2.73 5.39", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 161.90601, "t": 397.53949, "r": 166.51474, "b": 408.82819, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 397.53949, "r": 214.11774, "b": 408.82819, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 392.06049, "r": 272.94495, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 392.06049, "r": 310.00732, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.938", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 392.06049, "r": 347.70734, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.904", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 363.67599, "t": 392.06049, "r": 384.66632, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.927", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 391.99771, "r": 418.77798, "b": 403.40298, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.853", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 391.99771, "r": 458.38336, "b": 403.40298, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.97", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 405.01147, "r": 310.00732, "b": 429.65018, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.952 0.923", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 405.01147, "r": 347.70734, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.909", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 404.9486999999999, "r": 386.24799, "b": 416.35397, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.938", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 396.20599, "t": 405.01147, "r": 417.19632, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.843", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 440.767, "t": 405.01147, "r": 457.15039, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "3.77", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 161.90601, "t": 423.84048, "r": 166.51474, "b": 435.12918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "2", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 423.84048, "r": 214.11774, "b": 435.12918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 246.71000999999998, "t": 418.3614799999999, "r": 271.41064, "b": 429.65018, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 418.3614799999999, "r": 347.70734, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.897 0.901", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 418.3614799999999, "r": 386.24799, "b": 442.65497, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.915 0.931", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 418.29871, "r": 418.77798, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.859 0.834", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 418.29871, "r": 458.38336, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.91 3.81", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 431.31246999999996, "r": 272.94495, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 431.31246999999996, "r": 310.00732, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.945", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 161.90601, "t": 450.14248999999995, "r": 166.51474, "b": 461.43118, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 450.14248999999995, "r": 214.11774, "b": 461.43118, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "2", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 444.66248, "r": 272.94495, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 444.66248, "r": 310.00732, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.952 0.944", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 444.66248, "r": 347.70734, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.92 0.903", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 444.5996999999999, "r": 386.24799, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.942 0.931", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 444.5996999999999, "r": 418.77798, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.857 0.824", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 444.5996999999999, "r": 458.38336, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.22 2", "column_header": false, "row_header": false, "row_section": false}], "num_rows": 7, "num_cols": 8, "grid": [[{"bbox": {"l": 160.37, "t": 339.45749, "r": 168.04523, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "#", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 207.974, "t": 339.45749, "r": 215.64923000000002, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "#", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 239.79799999999997, "t": 344.93649, "r": 278.3338, "b": 356.22519000000005, "coord_origin": "TOPLEFT"}, "row_span": 2, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Language", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 324.67001, "t": 339.45749, "r": 348.26419, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 3, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 6, "text": "TEDs", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 324.67001, "t": 339.45749, "r": 348.26419, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 3, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 6, "text": "TEDs", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 324.67001, "t": 339.45749, "r": 348.26419, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 3, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 6, "text": "TEDs", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 396.271, "t": 339.45749, "r": 417.12595, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "mAP", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 430.771, "t": 339.45749, "r": 467.14142000000004, "b": 350.74619, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "Inference", "column_header": true, "row_header": false, "row_section": false}], [{"bbox": {"l": 144.592, "t": 352.40848, "r": 183.82895, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "enc-layers", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 192.19501, "t": 352.40848, "r": 231.42303, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "dec-layers", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 239.79799999999997, "t": 344.93649, "r": 278.3338, "b": 356.22519000000005, "coord_origin": "TOPLEFT"}, "row_span": 2, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Language", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 286.686, "t": 352.40848, "r": 312.32812, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "simple", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 320.702, "t": 352.40848, "r": 353.71539, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "complex", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 369.306, "t": 352.40848, "r": 379.02914, "b": 363.69717, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "all", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 394.927, "t": 350.41647, "r": 418.46921, "b": 361.70517, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "(0.75)", "column_header": true, "row_header": false, "row_section": false}, {"bbox": {"l": 427.14801, "t": 350.41647, "r": 470.76955999999996, "b": 361.70517, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "time (secs)", "column_header": true, "row_header": false, "row_section": false}], [{"bbox": {"l": 161.90601, "t": 371.23849, "r": 166.51474, "b": 382.52719, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "6", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 371.23849, "r": 214.11774, "b": 382.52719, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "6", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 365.75848, "r": 272.94495, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 365.75848, "r": 310.00732, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.965 0.969", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 365.75848, "r": 347.70734, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.934 0.927", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 363.67599, "t": 365.75848, "r": 384.66632, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.955 0.955", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 396.20599, "t": 365.69571, "r": 417.19632, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.88 0.857", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 365.69571, "r": 458.38336, "b": 389.99917999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "2.73 5.39", "column_header": false, "row_header": false, "row_section": false}], [{"bbox": {"l": 161.90601, "t": 397.53949, "r": 166.51474, "b": 408.82819, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 397.53949, "r": 214.11774, "b": 408.82819, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 392.06049, "r": 272.94495, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 392.06049, "r": 310.00732, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.938", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 392.06049, "r": 347.70734, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.904", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 363.67599, "t": 392.06049, "r": 384.66632, "b": 403.34918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.927", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 391.99771, "r": 418.77798, "b": 403.40298, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.853", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 391.99771, "r": 458.38336, "b": 403.40298, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.97", "column_header": false, "row_header": false, "row_section": false}], [{"bbox": null, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "", "column_header": false, "row_header": false, "row_section": false}, {"bbox": null, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 246.71000999999998, "t": 418.3614799999999, "r": 271.41064, "b": 429.65018, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 405.01147, "r": 310.00732, "b": 429.65018, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.952 0.923", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 405.01147, "r": 347.70734, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.909", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 404.9486999999999, "r": 386.24799, "b": 416.35397, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.938", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 396.20599, "t": 405.01147, "r": 417.19632, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.843", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 440.767, "t": 405.01147, "r": 457.15039, "b": 416.30017, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "3.77", "column_header": false, "row_header": false, "row_section": false}], [{"bbox": {"l": 161.90601, "t": 423.84048, "r": 166.51474, "b": 435.12918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "2", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 423.84048, "r": 214.11774, "b": 435.12918, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 431.31246999999996, "r": 272.94495, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 431.31246999999996, "r": 310.00732, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.945", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 418.3614799999999, "r": 347.70734, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.897 0.901", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 418.3614799999999, "r": 386.24799, "b": 442.65497, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.915 0.931", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 418.29871, "r": 418.77798, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.859 0.834", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 418.29871, "r": 458.38336, "b": 442.60117, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.91 3.81", "column_header": false, "row_header": false, "row_section": false}], [{"bbox": {"l": 161.90601, "t": 450.14248999999995, "r": 166.51474, "b": 461.43118, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "4", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 209.509, "t": 450.14248999999995, "r": 214.11774, "b": 461.43118, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "2", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 245.17598999999998, "t": 444.66248, "r": 272.94495, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "OTSL HTML", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 289.017, "t": 444.66248, "r": 310.00732, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "0.952 0.944", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 326.71701, "t": 444.66248, "r": 347.70734, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "0.92 0.903", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 362.08801, "t": 444.5996999999999, "r": 386.24799, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 5, "end_col_offset_idx": 6, "text": "0.942 0.931", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 394.61801, "t": 444.5996999999999, "r": 418.77798, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 6, "end_col_offset_idx": 7, "text": "0.857 0.824", "column_header": false, "row_header": false, "row_section": false}, {"bbox": {"l": 439.52701, "t": 444.5996999999999, "r": 458.38336, "b": 468.90317, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 6, "end_row_offset_idx": 7, "start_col_offset_idx": 7, "end_col_offset_idx": 8, "text": "1.22 2", "column_header": false, "row_header": false, "row_section": false}]]}}], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 612.0, "height": 792.0}, "image": null, "page_no": 1}}}

================================================
File: tests/data/groundtruth/docling_v2/2305.03393v1-pg9.md
================================================
order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.

## 5.1 Hyper Parameter Optimization

We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.

Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.

| #          | #          | Language   | TEDs        | TEDs        | TEDs        | mAP         | Inference   |
|------------|------------|------------|-------------|-------------|-------------|-------------|-------------|
| enc-layers | dec-layers | Language   | simple      | complex     | all         | (0.75)      | time (secs) |
| 6          | 6          | OTSL HTML  | 0.965 0.969 | 0.934 0.927 | 0.955 0.955 | 0.88 0.857  | 2.73 5.39   |
| 4          | 4          | OTSL HTML  | 0.938       | 0.904       | 0.927       | 0.853       | 1.97        |
|            |            | OTSL       | 0.952 0.923 | 0.909       | 0.938       | 0.843       | 3.77        |
| 2          | 4          | HTML       | 0.945       | 0.897 0.901 | 0.915 0.931 | 0.859 0.834 | 1.91 3.81   |
| 4          | 2          | OTSL HTML  | 0.952 0.944 | 0.92 0.903  | 0.942 0.931 | 0.857 0.824 | 1.22 2      |

## 5.2 Quantitative Results

We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.

Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.

================================================
File: tests/data/groundtruth/docling_v2/2305.03393v1.doctags.txt
================================================
<doctag><page_header><loc_15><loc_132><loc_30><loc_350>arXiv:2305.03393v1 [cs.CV] 5 May 2023</page_header>
<section_header_level_1><loc_110><loc_73><loc_393><loc_92>Optimized Table Tokenization for Table Structure Recognition</section_header_level_1>
<text><loc_114><loc_107><loc_389><loc_126>Maksym Lysak [0000 − 0002 − 3723 − $^{6960]}$, Ahmed Nassar[0000 − 0002 − 9468 − $^{0822]}$, Nikolaos Livathinos [0000 − 0001 − 8513 − $^{3491]}$, Christoph Auer[0000 − 0001 − 5761 − $^{0422]}$, [0000 − 0002 − 8088 − 0823]</text>
<text><loc_188><loc_123><loc_244><loc_129>and Peter Staar</text>
<text><loc_228><loc_137><loc_275><loc_142>IBM Research</text>
<text><loc_182><loc_144><loc_321><loc_149>{mly,ahn,nli,cau,taa}@zurich.ibm.com</text>
<text><loc_133><loc_171><loc_369><loc_293>Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.</text>
<text><loc_133><loc_302><loc_369><loc_314>Keywords: Table Structure Recognition · Data Representation · Transformers · Optimization.</text>
<section_header_level_1><loc_110><loc_330><loc_187><loc_336>1 Introduction</section_header_level_1>
<text><loc_110><loc_346><loc_393><loc_397>Tables are ubiquitous in documents such as scientific papers, patents, reports, manuals, specification sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and structure, making it difficult to recover their correct structure with simple analytical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods.</text>
<text><loc_110><loc_399><loc_393><loc_420>In modern document understanding systems [1,15], table extraction is typically a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of</text>
<page_break>
<page_header><loc_110><loc_59><loc_114><loc_64>2</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<picture><loc_121><loc_132><loc_379><loc_269><caption><loc_110><loc_80><loc_393><loc_126>Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a fixed length of rows in OTSL).</caption></picture>
<text><loc_110><loc_286><loc_393><loc_329>today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].</text>
<text><loc_110><loc_331><loc_393><loc_420>Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the network in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various configurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.</text>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_389><loc_59><loc_393><loc_64>3</page_header>
<text><loc_110><loc_75><loc_393><loc_133>While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure representation language with an otherwise unmodified Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments.</text>
<text><loc_110><loc_136><loc_393><loc_209>The main contribution of this paper is the introduction of a new optimised table structure language (OTSL), specifically designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Specifically, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectangular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML.</text>
<text><loc_110><loc_211><loc_393><loc_277>The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws. Subsequently, we introduce OTSL in section 4, which includes the language definition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to TableFormer models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps.</text>
<section_header_level_1><loc_110><loc_292><loc_193><loc_298>2 Related Work</section_header_level_1>
<text><loc_110><loc_309><loc_396><loc_420>Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the problem of table structure recognition, i.e. the correct identification of columns, rows and spanning cells in a given table. Broadly speaking, the current deeplearning based approaches fall into three categories: object detection (OD) methods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geometric coordinates of the table cell. The edges of the graph define the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.</text>
<page_break>
<page_header><loc_110><loc_59><loc_114><loc_64>4</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<text><loc_110><loc_75><loc_393><loc_164>Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.</text>
<text><loc_110><loc_166><loc_393><loc_307>Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( <td> ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.</text>
<text><loc_110><loc_309><loc_393><loc_368>Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.</text>
<section_header_level_1><loc_110><loc_382><loc_220><loc_389>3 Problem Statement</section_header_level_1>
<text><loc_110><loc_399><loc_393><loc_420>All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-</text>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_389><loc_59><loc_393><loc_64>5</page_header>
<text><loc_110><loc_75><loc_393><loc_118>ulary and can be interpreted as a table structure. For example, with the HTML tokens <table> , </table> , <tr> , </tr> , <td> and </td> , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.</text>
<picture><loc_112><loc_147><loc_389><loc_215><caption><loc_119><loc_140><loc_384><loc_145>Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.</caption></picture>
<text><loc_110><loc_232><loc_393><loc_336>Obviously, HTML and other general-purpose markup languages were not designed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artificially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( <td> and </td> ). Furthermore, when tokenizing the HTML structure, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 different HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure.</text>
<text><loc_110><loc_338><loc_393><loc_367>Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the verification of incomplete sequences is non-trivial or even impossible.</text>
<text><loc_110><loc_369><loc_393><loc_420>In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierarchical manner. Since the number of tokens for each table row and column can vary significantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence</text>
<page_break>
<page_header><loc_110><loc_59><loc_114><loc_64>6</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<text><loc_110><loc_75><loc_393><loc_88>generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output.</text>
<text><loc_110><loc_91><loc_393><loc_187>In practice, we observe two major issues with prediction quality when training Im2Seq models on HTML table structure generation from images. On the one hand, we find that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we find many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Figure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reflect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content.</text>
<section_header_level_1><loc_110><loc_202><loc_304><loc_209>4 Optimised Table Structure Language</section_header_level_1>
<text><loc_110><loc_220><loc_393><loc_279>To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both significantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture.</text>
<section_header_level_1><loc_110><loc_294><loc_214><loc_300>4.1 Language Definition</section_header_level_1>
<text><loc_110><loc_309><loc_393><loc_329>In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.</text>
<text><loc_122><loc_332><loc_334><loc_337>The OTSL vocabulary is comprised of the following tokens:</text>
<unordered_list><list_item><loc_115><loc_346><loc_376><loc_352>-"C" cell a new table cell that either has or does not have cell content</list_item>
<list_item><loc_115><loc_354><loc_393><loc_367>-"L" cell left-looking cell , merging with the left neighbor cell to create a span</list_item>
<list_item><loc_115><loc_369><loc_393><loc_382>-"U" cell up-looking cell , merging with the upper neighbor cell to create a span</list_item>
<list_item><loc_115><loc_385><loc_371><loc_390>-"X" cell cross cell , to merge with both left and upper neighbor cells</list_item>
<list_item><loc_115><loc_393><loc_268><loc_398>-"NL" new-line , switch to the next row.</list_item>
</unordered_list>
<text><loc_110><loc_407><loc_393><loc_420>A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.</text>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_389><loc_59><loc_393><loc_64>7</page_header>
<picture><loc_135><loc_103><loc_367><loc_177><caption><loc_110><loc_79><loc_393><loc_98>Fig. 3. OTSL description of table structure: A - table example; B - graphical representation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding</caption></picture>
<section_header_level_1><loc_110><loc_193><loc_202><loc_198>4.2 Language Syntax</section_header_level_1>
<text><loc_110><loc_205><loc_297><loc_211>The OTSL representation follows these syntax rules:</text>
<unordered_list><list_item><loc_114><loc_219><loc_393><loc_232>1. Left-looking cell rule : The left neighbour of an "L" cell must be either another "L" cell or a "C" cell.</list_item>
<list_item><loc_114><loc_234><loc_393><loc_247>2. Up-looking cell rule : The upper neighbour of a "U" cell must be either another "U" cell or a "C" cell.</list_item>
</unordered_list>
<section_header_level_1><loc_114><loc_249><loc_185><loc_255>3. Cross cell rule :</section_header_level_1>
<unordered_list><list_item><loc_124><loc_257><loc_393><loc_278>The left neighbour of an "X" cell must be either another "X" cell or a "U" cell, and the upper neighbour of an "X" cell must be either another "X" cell or an "L" cell.</list_item>
<list_item><loc_114><loc_280><loc_388><loc_285>4. First row rule : Only "L" cells and "C" cells are allowed in the first row.</list_item>
<list_item><loc_114><loc_287><loc_393><loc_300>5. First column rule : Only "U" cells and "C" cells are allowed in the first column.</list_item>
<list_item><loc_114><loc_302><loc_393><loc_315>6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with "NL" token.</list_item>
</unordered_list>
<text><loc_110><loc_324><loc_393><loc_405>The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a "C"-cell token, which in case of spans is always located in the top-left corner of the table cell definition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid.</text>
<text><loc_110><loc_407><loc_393><loc_420>These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We find strong indications that this pattern</text>
<page_break>
<page_header><loc_110><loc_59><loc_114><loc_64>8</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<text><loc_110><loc_75><loc_393><loc_88>reduces significantly the column drift seen in the HTML based models (see Figure 5).</text>
<section_header_level_1><loc_110><loc_102><loc_261><loc_108>4.3 Error-detection and -mitigation</section_header_level_1>
<text><loc_110><loc_115><loc_393><loc_189>The design of OTSL allows to validate a table structure easily on an unfinished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee prediction correctness. Different heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can correct the predicted OTSL sequence on-the-fly is to verify if the token with the highest prediction confidence invalidates the predicted sequence, and replace it by the token with the next highest confidence until OTSL rules are satisfied.</text>
<section_header_level_1><loc_110><loc_203><loc_187><loc_209>5 Experiments</section_header_level_1>
<text><loc_110><loc_219><loc_393><loc_285>To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been converted into OTSL format for this purpose, and will be made publicly available.</text>
<picture><loc_115><loc_321><loc_386><loc_375><caption><loc_110><loc_306><loc_393><loc_318>Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach.</caption></picture>
<text><loc_110><loc_392><loc_393><loc_420>We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in</text>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_389><loc_59><loc_393><loc_64>9</page_header>
<text><loc_110><loc_75><loc_393><loc_96>order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.</text>
<section_header_level_1><loc_110><loc_107><loc_260><loc_112>5.1 Hyper Parameter Optimization</section_header_level_1>
<text><loc_110><loc_117><loc_393><loc_160>We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.</text>
<otsl><loc_114><loc_213><loc_388><loc_296><ched>#<ched>#<ched>Language<ched>TEDs<lcel><lcel><ched>mAP<ched>Inference<nl><ched>enc-layers<ched>dec-layers<ucel><ched>simple<ched>complex<ched>all<ched>(0.75)<ched>time (secs)<nl><fcel>6<fcel>6<fcel>OTSL HTML<fcel>0.965 0.969<fcel>0.934 0.927<fcel>0.955 0.955<fcel>0.88 0.857<fcel>2.73 5.39<nl><fcel>4<fcel>4<fcel>OTSL HTML<fcel>0.938 0.952<fcel>0.904<fcel>0.927<fcel>0.853<fcel>1.97<nl><fcel>2<fcel>4<fcel>OTSL<fcel>0.923 0.945<fcel>0.909 0.897<fcel>0.938<fcel>0.843<fcel>3.77<nl><ecel><ecel><fcel>HTML<ecel><fcel>0.901<fcel>0.915 0.931<fcel>0.859 0.834<fcel>1.91 3.81<nl><fcel>4<fcel>2<fcel>OTSL HTML<fcel>0.952 0.944<fcel>0.92 0.903<fcel>0.942 0.931<fcel>0.857 0.824<fcel>1.22 2<nl><caption><loc_110><loc_174><loc_393><loc_206>Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.</caption></otsl>
<section_header_level_1><loc_110><loc_321><loc_216><loc_326>5.2 Quantitative Results</section_header_level_1>
<text><loc_110><loc_331><loc_393><loc_390>We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.</text>
<text><loc_110><loc_392><loc_393><loc_420>Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.</text>
<page_break>
<page_header><loc_110><loc_59><loc_118><loc_64>10</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<otsl><loc_117><loc_99><loc_385><loc_166><ecel><ched>Language<ched>TEDs<lcel><lcel><ched>mAP(0.75)<ched>Inference time (secs)<nl><ecel><ucel><ched>simple<ched>complex<ched>all<ucel><ucel><nl><rhed>PubTabNet<rhed>OTSL<fcel>0.965<fcel>0.934<fcel>0.955<fcel>0.88<fcel>2.73<nl><ucel><rhed>HTML<fcel>0.969<fcel>0.927<fcel>0.955<fcel>0.857<fcel>5.39<nl><rhed>FinTabNet<rhed>OTSL<fcel>0.955<fcel>0.961<fcel>0.959<fcel>0.862<fcel>1.85<nl><ucel><rhed>HTML<fcel>0.917<fcel>0.922<fcel>0.92<fcel>0.722<fcel>3.26<nl><rhed>PubTables-1M<rhed>OTSL<fcel>0.987<fcel>0.964<fcel>0.977<fcel>0.896<fcel>1.79<nl><ucel><rhed>HTML<fcel>0.983<fcel>0.944<fcel>0.966<fcel>0.889<fcel>3.26<nl><caption><loc_110><loc_73><loc_393><loc_92>Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).</caption></otsl>
<section_header_level_1><loc_110><loc_182><loc_210><loc_188>5.3 Qualitative Results</section_header_level_1>
<text><loc_110><loc_196><loc_393><loc_231>To illustrate the qualitative differences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more effective in handling tables with longer token sequences, resulting in even more precise structure prediction and bounding boxes.</text>
<picture><loc_133><loc_281><loc_369><loc_419><caption><loc_110><loc_251><loc_393><loc_278>Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap (E) than the HTML model (D), when predicting the structure of a sparse table (A), at twice the inference speed because of shorter sequence length (B),(C). "PMC2807444_006_00.png" PubTabNet. μ</caption></picture>
<text><loc_186><loc_420><loc_188><loc_426>μ</text>
<text><loc_246><loc_432><loc_247><loc_438>≥</text>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_385><loc_59><loc_393><loc_64>11</page_header>
<picture><loc_138><loc_115><loc_365><loc_400><caption><loc_110><loc_79><loc_393><loc_112>Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn't complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. "PMC5406406_003_01.png" PubTabNet.</caption></picture>
<page_break>
<page_header><loc_110><loc_59><loc_118><loc_64>12</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<section_header_level_1><loc_110><loc_74><loc_179><loc_81>6 Conclusion</section_header_level_1>
<text><loc_110><loc_93><loc_393><loc_128>We demonstrated that representing tables in HTML for the task of table structure recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key benefits.</text>
<text><loc_110><loc_131><loc_393><loc_204>First and foremost, given the same network configuration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance benefits can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be significantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1).</text>
<text><loc_110><loc_207><loc_393><loc_296>Secondly, OTSL has more inherent structure and a significantly restricted vocabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bounding boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a significant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Significant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-fly during sequence generation.</text>
<section_header_level_1><loc_110><loc_312><loc_162><loc_318>References</section_header_level_1>
<unordered_list><list_item><loc_114><loc_330><loc_393><loc_356>1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering document conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785</list_item>
<list_item><loc_114><loc_358><loc_393><loc_384>2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Fornés, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545561. Springer International Publishing, Cham (2022)</list_item>
<list_item><loc_114><loc_386><loc_393><loc_398>3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019)</list_item>
<list_item><loc_114><loc_401><loc_393><loc_420>4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894-901. IEEE (2019)</list_item>
</unordered_list>
<page_break>
<page_header><loc_159><loc_59><loc_366><loc_64>Optimized Table Tokenization for Table Structure Recognition</page_header>
<page_header><loc_385><loc_59><loc_393><loc_64>13</page_header>
<unordered_list><list_item><loc_114><loc_76><loc_393><loc_94>5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1-10 (2022)</list_item>
<list_item><loc_114><loc_96><loc_393><loc_122>6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 18681873. IEEE (2022)</list_item>
<list_item><loc_114><loc_124><loc_393><loc_136>7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019)</list_item>
<list_item><loc_114><loc_138><loc_393><loc_171>8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777</list_item>
<list_item><loc_114><loc_172><loc_393><loc_191>9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)</list_item>
<list_item><loc_110><loc_193><loc_393><loc_233>10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743-3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043</list_item>
<list_item><loc_110><loc_235><loc_393><loc_261>11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572-573 (2020)</list_item>
<list_item><loc_110><loc_262><loc_393><loc_288>12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)</list_item>
<list_item><loc_110><loc_290><loc_393><loc_316>13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226</list_item>
<list_item><loc_110><loc_318><loc_393><loc_344>14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive table extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June 2022)</list_item>
<list_item><loc_110><loc_345><loc_393><loc_385>15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service: A machine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 774-782. KDD '18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834</list_item>
<list_item><loc_110><loc_387><loc_393><loc_399>16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397</list_item>
<list_item><loc_110><loc_401><loc_393><loc_420>17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749-755. IEEE (2019)</list_item>
</unordered_list>
<page_break>
<page_header><loc_110><loc_59><loc_118><loc_64>14</page_header>
<page_header><loc_137><loc_59><loc_189><loc_64>M. Lysak, et al.</page_header>
<unordered_list><list_item><loc_110><loc_76><loc_393><loc_94>18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295-1304 (2021)</list_item>
<list_item><loc_110><loc_96><loc_393><loc_122>19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup's solution for icdar 2021 competition on scientific literature parsing task b: Table recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848</list_item>
<list_item><loc_110><loc_124><loc_393><loc_136>20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022)</list_item>
<list_item><loc_110><loc_138><loc_393><loc_171>21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074</list_item>
<list_item><loc_110><loc_172><loc_393><loc_198>22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision - ECCV 2020. pp. 564-580. Springer International Publishing, Cham (2020)</list_item>
<list_item><loc_110><loc_200><loc_393><loc_219>23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)</list_item>
</unordered_list>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/2305.03393v1.md
================================================
## Optimized Table Tokenization for Table Structure Recognition

Maksym Lysak [0000 − 0002 − 3723 − $^{6960]}$, Ahmed Nassar[0000 − 0002 − 9468 − $^{0822]}$, Nikolaos Livathinos [0000 − 0001 − 8513 − $^{3491]}$, Christoph Auer[0000 − 0001 − 5761 − $^{0422]}$, [0000 − 0002 − 8088 − 0823]

and Peter Staar

IBM Research

{mly,ahn,nli,cau,taa}@zurich.ibm.com

Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.

Keywords: Table Structure Recognition · Data Representation · Transformers · Optimization.

## 1 Introduction

Tables are ubiquitous in documents such as scientific papers, patents, reports, manuals, specification sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and structure, making it difficult to recover their correct structure with simple analytical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods.

In modern document understanding systems [1,15], table extraction is typically a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of

Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a fixed length of rows in OTSL).

<!-- image -->

today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].

Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the network in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various configurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.

While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure representation language with an otherwise unmodified Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments.

The main contribution of this paper is the introduction of a new optimised table structure language (OTSL), specifically designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Specifically, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectangular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML.

The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws. Subsequently, we introduce OTSL in section 4, which includes the language definition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to TableFormer models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps.

## 2 Related Work

Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the problem of table structure recognition, i.e. the correct identification of columns, rows and spanning cells in a given table. Broadly speaking, the current deeplearning based approaches fall into three categories: object detection (OD) methods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geometric coordinates of the table cell. The edges of the graph define the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.

Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.

Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( &lt;td&gt; ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.

Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.

## 3 Problem Statement

All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-

ulary and can be interpreted as a table structure. For example, with the HTML tokens &lt;table&gt; , &lt;/table&gt; , &lt;tr&gt; , &lt;/tr&gt; , &lt;td&gt; and &lt;/td&gt; , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.

Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.

<!-- image -->

Obviously, HTML and other general-purpose markup languages were not designed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artificially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( &lt;td&gt; and &lt;/td&gt; ). Furthermore, when tokenizing the HTML structure, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 different HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure.

Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the verification of incomplete sequences is non-trivial or even impossible.

In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierarchical manner. Since the number of tokens for each table row and column can vary significantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence

generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output.

In practice, we observe two major issues with prediction quality when training Im2Seq models on HTML table structure generation from images. On the one hand, we find that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we find many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Figure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reflect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content.

## 4 Optimised Table Structure Language

To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both significantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture.

## 4.1 Language Definition

In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.

The OTSL vocabulary is comprised of the following tokens:

- -"C" cell a new table cell that either has or does not have cell content
- -"L" cell left-looking cell , merging with the left neighbor cell to create a span
- -"U" cell up-looking cell , merging with the upper neighbor cell to create a span
- -"X" cell cross cell , to merge with both left and upper neighbor cells
- -"NL" new-line , switch to the next row.

A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.

Fig. 3. OTSL description of table structure: A - table example; B - graphical representation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding

<!-- image -->

## 4.2 Language Syntax

The OTSL representation follows these syntax rules:

- 1. Left-looking cell rule : The left neighbour of an "L" cell must be either another "L" cell or a "C" cell.
- 2. Up-looking cell rule : The upper neighbour of a "U" cell must be either another "U" cell or a "C" cell.

## 3. Cross cell rule :

- The left neighbour of an "X" cell must be either another "X" cell or a "U" cell, and the upper neighbour of an "X" cell must be either another "X" cell or an "L" cell.
- 4. First row rule : Only "L" cells and "C" cells are allowed in the first row.
- 5. First column rule : Only "U" cells and "C" cells are allowed in the first column.
- 6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with "NL" token.

The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a "C"-cell token, which in case of spans is always located in the top-left corner of the table cell definition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid.

These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We find strong indications that this pattern

reduces significantly the column drift seen in the HTML based models (see Figure 5).

## 4.3 Error-detection and -mitigation

The design of OTSL allows to validate a table structure easily on an unfinished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee prediction correctness. Different heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can correct the predicted OTSL sequence on-the-fly is to verify if the token with the highest prediction confidence invalidates the predicted sequence, and replace it by the token with the next highest confidence until OTSL rules are satisfied.

## 5 Experiments

To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been converted into OTSL format for this purpose, and will be made publicly available.

Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach.

<!-- image -->

We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in

order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz.

## 5.1 Hyper Parameter Optimization

We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML.

Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart.

| #          | #          | Language   | TEDs        | TEDs        | TEDs        | mAP         | Inference   |
|------------|------------|------------|-------------|-------------|-------------|-------------|-------------|
| enc-layers | dec-layers | Language   | simple      | complex     | all         | (0.75)      | time (secs) |
| 6          | 6          | OTSL HTML  | 0.965 0.969 | 0.934 0.927 | 0.955 0.955 | 0.88 0.857  | 2.73 5.39   |
| 4          | 4          | OTSL HTML  | 0.938 0.952 | 0.904       | 0.927       | 0.853       | 1.97        |
| 2          | 4          | OTSL       | 0.923 0.945 | 0.909 0.897 | 0.938       | 0.843       | 3.77        |
|            |            | HTML       |             | 0.901       | 0.915 0.931 | 0.859 0.834 | 1.91 3.81   |
| 4          | 2          | OTSL HTML  | 0.952 0.944 | 0.92 0.903  | 0.942 0.931 | 0.857 0.824 | 1.22 2      |

## 5.2 Quantitative Results

We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.

Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves significantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.

Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).

|              | Language   | TEDs   | TEDs    | TEDs   | mAP(0.75)   | Inference time (secs)   |
|--------------|------------|--------|---------|--------|-------------|-------------------------|
|              | Language   | simple | complex | all    | mAP(0.75)   | Inference time (secs)   |
| PubTabNet    | OTSL       | 0.965  | 0.934   | 0.955  | 0.88        | 2.73                    |
| PubTabNet    | HTML       | 0.969  | 0.927   | 0.955  | 0.857       | 5.39                    |
| FinTabNet    | OTSL       | 0.955  | 0.961   | 0.959  | 0.862       | 1.85                    |
| FinTabNet    | HTML       | 0.917  | 0.922   | 0.92   | 0.722       | 3.26                    |
| PubTables-1M | OTSL       | 0.987  | 0.964   | 0.977  | 0.896       | 1.79                    |
| PubTables-1M | HTML       | 0.983  | 0.944   | 0.966  | 0.889       | 3.26                    |

## 5.3 Qualitative Results

To illustrate the qualitative differences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more effective in handling tables with longer token sequences, resulting in even more precise structure prediction and bounding boxes.

Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap (E) than the HTML model (D), when predicting the structure of a sparse table (A), at twice the inference speed because of shorter sequence length (B),(C). "PMC2807444\_006\_00.png" PubTabNet. μ

<!-- image -->

μ

≥

Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn't complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. "PMC5406406\_003\_01.png" PubTabNet.

<!-- image -->

## 6 Conclusion

We demonstrated that representing tables in HTML for the task of table structure recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key benefits.

First and foremost, given the same network configuration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance benefits can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be significantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1).

Secondly, OTSL has more inherent structure and a significantly restricted vocabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bounding boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a significant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Significant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-fly during sequence generation.

## References

- 1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering document conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785
- 2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Fornés, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545561. Springer International Publishing, Cham (2022)
- 3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019)
- 4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894-901. IEEE (2019)

- 5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1-10 (2022)
- 6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 18681873. IEEE (2022)
- 7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019)
- 8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777
- 9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)
- 10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743-3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043
- 11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from imagebased documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572-573 (2020)
- 12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)
- 13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226
- 14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive table extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June 2022)
- 15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service: A machine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. pp. 774-782. KDD '18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834
- 16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397
- 17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749-755. IEEE (2019)

- 18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295-1304 (2021)
- 19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup's solution for icdar 2021 competition on scientific literature parsing task b: Table recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848
- 20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022)
- 21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074
- 22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision - ECCV 2020. pp. 564-580. Springer International Publishing, Cham (2020)
- 23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)

================================================
File: tests/data/groundtruth/docling_v2/amt_handbook_sample.doctags.txt
================================================
<doctag><text><loc_61><loc_28><loc_264><loc_60>pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.</text>
<text><loc_61><loc_69><loc_264><loc_116>The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.</text>
<section_header_level_1><loc_61><loc_125><loc_141><loc_133>Boots Self-Locking Nut</section_header_level_1>
<text><loc_61><loc_134><loc_268><loc_182>The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.</text>
<text><loc_61><loc_191><loc_267><loc_239>The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.</text>
<text><loc_61><loc_248><loc_268><loc_311>The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.</text>
<text><loc_61><loc_320><loc_264><loc_336>Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is</text>
<picture><loc_59><loc_343><loc_261><loc_449><caption><loc_61><loc_454><loc_155><loc_461>Figure 7-26. Self-locking nuts.</caption></picture>
<text><loc_270><loc_28><loc_473><loc_76>the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.</text>
<text><loc_270><loc_77><loc_274><loc_84>.</text>
<section_header_level_1><loc_270><loc_85><loc_380><loc_92>Stainless Steel Self-Locking Nut</section_header_level_1>
<text><loc_270><loc_94><loc_478><loc_231>The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.</text>
<section_header_level_1><loc_270><loc_240><loc_327><loc_247>Elastic Stop Nut</section_header_level_1>
<text><loc_270><loc_249><loc_465><loc_264>The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This</text>
<picture><loc_270><loc_272><loc_470><loc_447><caption><loc_270><loc_452><loc_405><loc_459>Figure 7-27. Stainless steel self-locking nut.</caption></picture>
<page_footer><loc_453><loc_470><loc_472><loc_478>7-45</page_footer>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/amt_handbook_sample.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "amt_handbook_sample", "origin": {"mimetype": "application/pdf", "binary_hash": 10189692113572347872, "filename": "amt_handbook_sample.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/texts/2"}, {"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/texts/5"}, {"cref": "#/texts/6"}, {"cref": "#/pictures/0"}, {"cref": "#/texts/13"}, {"cref": "#/texts/14"}, {"cref": "#/texts/15"}, {"cref": "#/texts/16"}, {"cref": "#/texts/17"}, {"cref": "#/texts/18"}, {"cref": "#/pictures/1"}, {"cref": "#/texts/26"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992126, "t": 730.31635, "r": 314.11212, "b": 681.34637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 244]}], "orig": "pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.", "text": "pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws."}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992302, "t": 667.81635, "r": 313.1546, "b": 593.84637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 376]}], "orig": "The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.", "text": "The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type."}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 71.992302, "t": 580.1864, "r": 167.27231, "b": 568.84637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 22]}], "orig": "Boots Self-Locking Nut", "text": "Boots Self-Locking Nut", "level": 1}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992294, "t": 565.81635, "r": 318.49225, "b": 491.84637000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 319]}], "orig": "The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.", "text": "The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut."}, {"self_ref": "#/texts/4", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992294, "t": 478.31638000000004, "r": 316.65729, "b": 404.34637000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 332]}], "orig": "The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.", "text": "The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly."}, {"self_ref": "#/texts/5", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992294, "t": 390.81638000000004, "r": 318.81229, "b": 291.84637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 477]}], "orig": "The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.", "text": "The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency."}, {"self_ref": "#/texts/6", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 71.992294, "t": 278.31638, "r": 313.91229, "b": 254.34636999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 122]}], "orig": "Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is", "text": "Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is"}, {"self_ref": "#/texts/7", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 72.0, "t": 71.80239900000004, "r": 184.14828, "b": 60.99040200000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 31]}], "orig": "Figure 7-26. Self-locking nuts.", "text": "Figure 7-26. Self-locking nuts."}, {"self_ref": "#/texts/8", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 102.4155, "t": 186.23509, "r": 161.3187, "b": 176.61909000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 18]}], "orig": "Boots aircraft nut", "text": "Boots aircraft nut"}, {"self_ref": "#/texts/9", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 91.685997, "t": 94.690201, "r": 129.77399, "b": 85.07420300000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "Flexloc nut", "text": "Flexloc nut"}, {"self_ref": "#/texts/10", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 162.48109, "t": 94.690201, "r": 207.85629, "b": 85.07420300000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "Fiber locknut", "text": "Fiber locknut"}, {"self_ref": "#/texts/11", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 237.31379999999996, "t": 94.690201, "r": 289.561, "b": 85.07420300000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "Elastic stop nut", "text": "Elastic stop nut"}, {"self_ref": "#/texts/12", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 216.9326, "t": 186.23509, "r": 277.7966, "b": 176.61909000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 18]}], "orig": "Elastic anchor nut", "text": "Elastic anchor nut"}, {"self_ref": "#/texts/13", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 320.99231, "t": 730.31635, "r": 561.80835, "b": 656.34637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 368]}], "orig": "the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.", "text": "the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only."}, {"self_ref": "#/texts/14", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 320.99542, "t": 655.31635, "r": 325.99542, "b": 643.84637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": ".", "text": "."}, {"self_ref": "#/texts/15", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 320.99542, "t": 642.6864, "r": 450.99542, "b": 631.34637, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}], "orig": "Stainless Steel Self-Locking Nut", "text": "Stainless Steel Self-Locking Nut", "level": 1}, {"self_ref": "#/texts/16", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 320.99542, "t": 628.31635, "r": 568.00439, "b": 416.84637000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1015]}], "orig": "The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.", "text": "The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened."}, {"self_ref": "#/texts/17", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 320.99542, "t": 403.18636999999995, "r": 388.50543, "b": 391.84637000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "Elastic Stop Nut", "text": "Elastic Stop Nut", "level": 1}, {"self_ref": "#/texts/18", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 320.99542, "t": 388.81638000000004, "r": 552.35132, "b": 364.84637000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 108]}], "orig": "The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This", "text": "The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This"}, {"self_ref": "#/texts/19", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 321.0, "t": 73.82240300000001, "r": 481.64931999999993, "b": 63.010403, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 46]}], "orig": "Figure 7-27. Stainless steel self-locking nut.", "text": "Figure 7-27. Stainless steel self-locking nut."}, {"self_ref": "#/texts/20", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 479.1354999999999, "t": 101.2654, "r": 531.16748, "b": 91.35340099999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "Tightened nut", "text": "Tightened nut"}, {"self_ref": "#/texts/21", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 474.3699, "t": 242.1082, "r": 535.23389, "b": 232.1962000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "Untightened nut", "text": "Untightened nut"}, {"self_ref": "#/texts/22", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 456.7558900000001, "t": 342.00259, "r": 487.08388999999994, "b": 332.3866, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Nut case", "text": "Nut case"}, {"self_ref": "#/texts/23", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 434.62299, "t": 196.17650000000003, "r": 497.47183000000007, "b": 186.56050000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 17]}], "orig": "Threaded nut core", "text": "Threaded nut core"}, {"self_ref": "#/texts/24", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 448.55081, "t": 220.6794, "r": 507.686, "b": 211.0634, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "Locking shoulder", "text": "Locking shoulder"}, {"self_ref": "#/texts/25", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 424.78421, "t": 109.88840000000005, "r": 452.10339000000005, "b": 100.27240000000006, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "Keyway", "text": "Keyway"}, {"self_ref": "#/texts/26", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 1, "bbox": {"l": 537.98541, "t": 46.019698999999946, "r": 560.77539, "b": 33.70970199999999, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 4]}], "orig": "7-45", "text": "7-45"}], "pictures": [{"self_ref": "#/pictures/0", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/7"}, {"cref": "#/texts/8"}, {"cref": "#/texts/9"}, {"cref": "#/texts/10"}, {"cref": "#/texts/11"}, {"cref": "#/texts/12"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 1, "bbox": {"l": 70.59269714355469, "t": 242.77777099609375, "r": 309.863037109375, "b": 79.6090087890625, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"cref": "#/texts/7"}], "references": [], "footnotes": [], "image": null, "annotations": []}, {"self_ref": "#/pictures/1", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/19"}, {"cref": "#/texts/20"}, {"cref": "#/texts/21"}, {"cref": "#/texts/22"}, {"cref": "#/texts/23"}, {"cref": "#/texts/24"}, {"cref": "#/texts/25"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 1, "bbox": {"l": 320.4467468261719, "t": 352.359375, "r": 558.8576049804688, "b": 81.689208984375, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"cref": "#/texts/19"}], "references": [], "footnotes": [], "image": null, "annotations": []}], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 594.0, "height": 774.0}, "image": null, "page_no": 1}}}

================================================
File: tests/data/groundtruth/docling_v2/amt_handbook_sample.md
================================================
pulleys, provided the inner race of the bearing is clamped to the supporting structure by the nut and bolt. Plates must be attached to the structure in a positive manner to eliminate rotation or misalignment when tightening the bolts or screws.

The two general types of self-locking nuts currently in use are the all-metal type and the fiber lock type. For the sake of simplicity, only three typical kinds of self-locking nuts are considered in this handbook: the Boots self-locking and the stainless steel self-locking nuts, representing the all-metal types; and the elastic stop nut, representing the fiber insert type.

## Boots Self-Locking Nut

The Boots self-locking nut is of one piece, all-metal construction designed to hold tight despite severe vibration. Note in Figure 7-26 that it has two sections and is essentially two nuts in one: a locking nut and a load-carrying nut. The two sections are connected with a spring, which is an integral part of the nut.

The spring keeps the locking and load-carrying sections such a distance apart that the two sets of threads are out of phase or spaced so that a bolt, which has been screwed through the load-carrying section, must push the locking section outward against the force of the spring to engage the threads of the locking section properly.

The spring, through the medium of the locking section, exerts a constant locking force on the bolt in the same direction as a force that would tighten the nut. In this nut, the load-carrying section has the thread strength of a standard nut of comparable size, while the locking section presses against the threads of the bolt and locks the nut firmly in position. Only a wrench applied to the nut loosens it. The nut can be removed and reused without impairing its efficiency.

Boots self-locking nuts are made with three different spring styles and in various shapes and sizes. The wing type that is

Figure 7-26. Self-locking nuts.

<!-- image -->

the most common ranges in size for No. 6 up to 1 / 4 inch, the Rol-top ranges from 1 / 4 inch to 1 / 6 inch, and the bellows type ranges in size from No. 8 up to 3 / 8 inch. Wing-type nuts are made of anodized aluminum alloy, cadmium-plated carbon steel, or stainless steel. The Rol-top nut is cadmium-plated steel, and the bellows type is made of aluminum alloy only.

.

## Stainless Steel Self-Locking Nut

The stainless steel self-locking nut may be spun on and off by hand as its locking action takes places only when the nut is seated against a solid surface and tightened. The nut consists of two parts: a case with a beveled locking shoulder and key and a thread insert with a locking shoulder and slotted keyway. Until the nut is tightened, it spins on the bolt easily, because the threaded insert is the proper size for the bolt. However, when the nut is seated against a solid surface and tightened, the locking shoulder of the insert is pulled downward and wedged against the locking shoulder of the case. This action compresses the threaded insert and causes it to clench the bolt tightly. The cross-sectional view in Figure 7-27 shows how the key of the case fits into the slotted keyway of the insert so that when the case is turned, the threaded insert is turned with it. Note that the slot is wider than the key. This permits the slot to be narrowed and the insert to be compressed when the nut is tightened.

## Elastic Stop Nut

The elastic stop nut is a standard nut with the height increased to accommodate a fiber locking collar. This

Figure 7-27. Stainless steel self-locking nut.

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v2/blocks.md.md
================================================
Unordered list:

- foo

Empty unordered list:

Ordered list:

- bar

Empty ordered list:

Heading:

# my heading

Empty heading:

Indented code block:

```
print("Hi!")
```

Empty indented code block:

Fenced code block:

```
print("Hello world!")
```

Empty fenced code block:


================================================
File: tests/data/groundtruth/docling_v2/bmj_sample.xml.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Evolving general practice consul ...  Britain: issues of length and context
    item-2 at level 2: paragraph: George K Freeman, John P Horder, ... on P Hill, Nayan C Shah, Andrew Wilson
    item-3 at level 2: paragraph: Centre for Primary Care and Soci ... ersity of Leicester, Leicester LE5 4PW
    item-4 at level 2: text: In 1999 Shah1 and others said th ...  per consultation in general practice?
    item-5 at level 2: text: We report on the outcome of exte ...  review identified 14 relevant papers.
    item-6 at level 2: section_header: Summary points
      item-7 at level 3: list: group list
        item-8 at level 4: list_item: Longer consultations are associa ... ith a range of better patient outcomes
        item-9 at level 4: list_item: Modern consultations in general  ... th more serious and chronic conditions
        item-10 at level 4: list_item: Increasing patient participation ...  interaction, which demands extra time
        item-11 at level 4: list_item: Difficulties with access and wit ... e and lead to further pressure on time
        item-12 at level 4: list_item: Longer consultations should be a ... t to maximise interpersonal continuity
        item-13 at level 4: list_item: Research on implementation is needed
    item-14 at level 2: section_header: Longer consultations: benefits for patients
      item-15 at level 3: text: The systematic review consistent ... ther some doctors insist on more time.
      item-16 at level 3: text: A national survey in 1998 report ... s the effects of their own experience.
    item-17 at level 2: section_header: Context of modern consultations
      item-18 at level 3: text: Shorter consultations were more  ...  potential length of the consultation.
    item-19 at level 2: section_header: Participatory consultation style
      item-20 at level 3: text: The most effective consultations ... style usually lengthens consultations.
    item-21 at level 2: section_header: Extended professional agenda
      item-22 at level 3: text: The traditional consultation in  ... agerial expectations of good practice.
      item-23 at level 3: text: Adequate time is essential. It m ...  inevitably leads to pressure on time.
    item-24 at level 2: section_header: Access problems
      item-25 at level 3: text: In a service free at the point o ... ort notice squeeze consultation times.
      item-26 at level 3: text: While appointment systems can an ...  for the inadequate access to doctors.
      item-27 at level 3: text: In response to perception of del ... ntation is currently being negotiated.
      item-28 at level 3: text: Virtually all patients think tha ... e that is free at the point of access.
      item-29 at level 3: text: A further government initiative  ... ealth advice and first line treatment.
    item-30 at level 2: section_header: Loss of interpersonal continuity
      item-31 at level 3: text: If a patient has to consult seve ... unning and professional frustration.18
      item-32 at level 3: text: Mechanic described how loss of l ... patient and professional satisfaction.
    item-33 at level 2: section_header: Health service reforms
      item-34 at level 3: text: Finally, for the past 15 years t ... ents and staff) and what is delivered.
    item-35 at level 2: section_header: The future
      item-36 at level 3: text: We think that the way ahead must ... p further the care of chronic disease.
      item-37 at level 3: text: The challenge posed to general p ... ermedicalisation need to be exploited.
      item-38 at level 3: text: We must ensure better communicat ... between planned and ad hoc consulting.
    item-39 at level 2: section_header: Next steps
      item-40 at level 3: text: General practitioners do not beh ... ailable time in complex consultations.
      item-41 at level 3: text: Devising appropriate incentives  ... and interpersonal knowledge and trust.
    item-42 at level 2: section_header: Acknowledgments
      item-43 at level 3: text: We thank the other members of th ... Practitioners for administrative help.
    item-44 at level 2: section_header: References
      item-45 at level 3: list: group list
        item-46 at level 4: list_item: Shah NC. Viewpoint: Consultation ... y men!”. Br J Gen Pract 49:497 (1999).
        item-47 at level 4: list_item: Mechanic D. How should hamsters  ... BMJ 323:266–268 (2001). PMID: 11485957
        item-48 at level 4: list_item: Howie JGR, Porter AMD, Heaney DJ ... n Pract 41:48–54 (1991). PMID: 2031735
        item-49 at level 4: list_item: Howie JGR, Heaney DJ, Maxwell M, ... BMJ 319:738–743 (1999). PMID: 10487999
        item-50 at level 4: list_item: Kaplan SH, Greenfield S, Ware JE ... c disease. Med Care 27:110–125 (1989).
        item-51 at level 4: list_item: Airey C, Erens B. National surve ... e, 1998. London: NHS Executive (1999).
        item-52 at level 4: list_item: Hart JT. Expectations of health  ... h Expect 1:3–13 (1998). PMID: 11281857
        item-53 at level 4: list_item: Tuckett D, Boulton M, Olson C, W ... London: Tavistock Publications (1985).
        item-54 at level 4: list_item: General Medical Council. Draft r ... ctors/index.htm (accessed 2 Jan 2002).
        item-55 at level 4: list_item: Balint M. The doctor, his patien ... the illness. London: Tavistock (1957).
        item-56 at level 4: list_item: Stott NCH, Davies RH. The except ...  J R Coll Gen Pract 29:210–205 (1979).
        item-57 at level 4: list_item: Hill AP, Hill AP. Challenges for ... nium. London: King's Fund75–86 (2000).
        item-58 at level 4: list_item: National service framework for c ... . London: Department of Health (2000).
        item-59 at level 4: list_item: Hart JT. A new kind of doctor: t ... ommunity. London: Merlin Press (1988).
        item-60 at level 4: list_item: Morrison I, Smith R. Hamster hea ... J 321:1541–1542 (2000). PMID: 11124164
        item-61 at level 4: list_item: Arber S, Sawyer L. Do appointmen ...  BMJ 284:478–480 (1982). PMID: 6800503
        item-62 at level 4: list_item: Hjortdahl P, Borchgrevink CF. Co ... MJ 303:1181–1184 (1991). PMID: 1747619
        item-63 at level 4: list_item: Howie JGR, Hopton JL, Heaney DJ, ... Pract 42:181–185 (1992). PMID: 1389427
        item-64 at level 4: list_item: Freeman G, Shepperd S, Robinson  ... ), Summer 2000. London: NCCSDO (2001).
        item-65 at level 4: list_item: Wilson A, McDonald P, Hayes L, C ... Pract 41:184–187 (1991). PMID: 1878267
        item-66 at level 4: list_item: De Maeseneer J, Hjortdahl P, Sta ... J 320:1616–1617 (2000). PMID: 10856043
        item-67 at level 4: list_item: Freeman G, Hjortdahl P. What fut ... MJ 314:1870–1873 (1997). PMID: 9224130
        item-68 at level 4: list_item: Kibbe DC, Bentz E, McLaughlin CP ... Pract 36:304–308 (1993). PMID: 8454977
        item-69 at level 4: list_item: Williams M, Neal RD. Time for a  ... ct 48:1783–1786 (1998). PMID: 10198490

================================================
File: tests/data/groundtruth/docling_v2/bmj_sample.xml.md
================================================
# Evolving general practice consultation in Britain: issues of length and context

George K Freeman, John P Horder, John G R Howie, A Pali Hungin, Alison P Hill, Nayan C Shah, Andrew Wilson

Centre for Primary Care and Social Medicine, Imperial College of Science, Technology and Medicine, London W6 8RP; Royal College of General Practitioners, London SW7 1PU; Department of General Practice, University of Edinburgh, Edinburgh EH8 9DX; Centre for Health Studies, University of Durham, Durham DH1 3HN; Kilburn Park Medical Centre, London NW6; Department of General Practice and Primary Health Care, University of Leicester, Leicester LE5 4PW

In 1999 Shah1 and others said that the Royal College of General Practitioners should advocate longer consultations in general practice as a matter of policy. The college set up a working group chaired by A P Hungin, and a systematic review of literature on consultation length in general practice was commissioned. The working group agreed that the available evidence would be hard to interpret without discussion of the changing context within which consultations now take place. For many years general practitioners and those who have surveyed patients' opinions in the United Kingdom have complained about short consultation time, despite a steady increase in actual mean length. Recently Mechanic pointed out that this is also true in the United States.2 Is there any justification for a further increase in mean time allocated per consultation in general practice?

We report on the outcome of extensive debate among a group of general practitioners with an interest in the process of care, with reference to the interim findings of the commissioned systematic review and our personal databases. The review identified 14 relevant papers.

## Summary points

- Longer consultations are associated with a range of better patient outcomes
- Modern consultations in general practice deal with patients with more serious and chronic conditions
- Increasing patient participation means more complex interaction, which demands extra time
- Difficulties with access and with loss of continuity add to perceived stress and poor performance and lead to further pressure on time
- Longer consultations should be a professional priority, combined with increased use of technology and more flexible practice management to maximise interpersonal continuity
- Research on implementation is needed

## Longer consultations: benefits for patients

The systematic review consistently showed that doctors with longer consultation times prescribe less and offer more advice on lifestyle and other health promoting activities. Longer consultations have been significantly associated with better recognition and handling of psychosocial problems3 and with better patient enablement.4 Also clinical care for some chronic illnesses is better in practices with longer booked intervals between one appointment and the next.5 It is not clear whether time is itself the main influence or whether some doctors insist on more time.

A national survey in 1998 reported that most (87%) patients were satisfied with the length of their most recent consultation.6 Satisfaction with any service will be high if expectations are met or exceeded. But expectations are modified by previous experience.7 The result is that primary care patients are likely to be satisfied with what they are used to unless the context modifies the effects of their own experience.

## Context of modern consultations

Shorter consultations were more appropriate when the population was younger, when even a brief absence from employment due to sickness required a doctor's note, and when many simple remedies were available only on prescription. Recently at least five important influences have increased the content and hence the potential length of the consultation.

## Participatory consultation style

The most effective consultations are those in which doctors most directly acknowledge and perhaps respond to patients' problems and concerns. In addition, for patients to be committed to taking advantage of medical advice they must agree with both the goals and methods proposed. A landmark publication in the United Kingdom was Meetings Between Experts, which argued that while doctors are the experts about medical problems in general patients are the experts on how they themselves experience these problems.8 New emphasis on teaching consulting skills in general practice advocated specific attention to the patient's agenda, beliefs, understanding, and agreement. Currently the General Medical Council, aware that communication difficulties underlie many complaints about doctors, has further emphasised the importance of involving patients in consultations in its revised guidance to medical schools.9 More patient involvement should give a better outcome, but this participatory style usually lengthens consultations.

## Extended professional agenda

The traditional consultation in general practice was brief.2 The patient presented symptoms and the doctor prescribed treatment. In 1957 Balint gave new insights into the meaning of symptoms.10 By 1979 an enhanced model of consultation was presented, in which the doctors dealt with ongoing as well as presenting problems and added health promotion and education about future appropriate use of services.11 Now, with an ageing population and more community care of chronic illness, there are more issues to be considered at each consultation. Ideas of what constitutes good general practice are more complex.12 Good practice now includes both extended care of chronic medical problems—for example, coronary heart disease13—and a public health role. At first this model was restricted to those who lead change (“early adopters”) and enthusiasts14 but now it is embedded in professional and managerial expectations of good practice.

Adequate time is essential. It may be difficult for an elderly patient with several active problems to undress, be examined, and get adequate professional consideration in under 15 minutes. Here the doctor is faced with the choice of curtailing the consultation or of reducing the time available for the next patient. Having to cope with these situations often contributes to professional dissatisfaction.15 This combination of more care, more options, and more genuine discussion of those options with informed patient choice inevitably leads to pressure on time.

## Access problems

In a service free at the point of access, rising demand will tend to increase rationing by delay. But attempts to improve access by offering more consultations at short notice squeeze consultation times.

While appointment systems can and should reduce queuing time for consultations, they have long tended to be used as a brake on total demand.16 This may seriously erode patients' confidence in being able to see their doctor or nurse when they need to. Patients are offered appointments further ahead but may keep these even if their symptoms have remitted “just in case.” Availability of consultations is thus blocked. Receptionists are then inappropriately blamed for the inadequate access to doctors.

In response to perception of delay, the government has set targets in the NHS plan of “guaranteed access to a primary care professional within 24 hours and to a primary care doctor within 48 hours.” Implementation is currently being negotiated.

Virtually all patients think that they would not consult unless it was absolutely necessary. They do not think they are wasting NHS time and do not like being made to feel so. But underlying general practitioners' willingness to make patients wait several days is their perception that few of the problems are urgent. Patients and general practitioners evidently do not agree about the urgency of so called minor problems. To some extent general practice in the United Kingdom may have scored an “own goal” by setting up perceived access barriers (appointment systems and out of hours cooperatives) in the attempt to increase professional standards and control demand in a service that is free at the point of access.

A further government initiative has been to bypass general practice with new services—notably, walk-in centres (primary care clinics in which no appointment is needed) and NHS Direct (a professional telephone helpline giving advice on simple remedies and access to services). Introduced widely and rapidly, these services each potentially provide significant features of primary care—namely, quick access to skilled health advice and first line treatment.

## Loss of interpersonal continuity

If a patient has to consult several different professionals, particularly over a short period of time, there is inevitable duplication of stories, risk of naive diagnoses, potential for conflicting advice, and perhaps loss of trust. Trust is essential if patients are to accept the “wait and see” management policy which is, or should be, an important part of the management of self limiting conditions, which are often on the boundary between illness and non-illness.17 Such duplication again increases pressure for more extra (unscheduled) consultations resulting in late running and professional frustration.18

Mechanic described how loss of longitudinal (and perhaps personal and relational19) continuity influences the perception and use of time through an inability to build on previous consultations.2 Knowing the doctor well, particularly in smaller practices, is associated with enhanced patient enablement in shorter time.4 Though Mechanic pointed out that three quarters of UK patients have been registered with their general practitioner five years or more, this may be misleading. Practices are growing, with larger teams and more registered patients. Being registered with a doctor in a larger practice is usually no guarantee that the patient will be able to see the same doctor or the doctor of his or her choice, who may be different. Thus the system does not encourage adequate personal continuity. This adds to pressure on time and reduces both patient and professional satisfaction.

## Health service reforms

Finally, for the past 15 years the NHS has experienced unprecedented change with a succession of major administrative reforms. Recent reforms have focused on an NHS led by primary care, including the aim of shifting care from the secondary specialist sector to primary care. One consequence is increased demand for primary care of patients with more serious and less stable problems. With the limited piloting of reforms we do not know whether such major redirection can be achieved without greatly altering the delicate balance between expectations (of both patients and staff) and what is delivered.

## The future

We think that the way ahead must embrace both longer mean consultation times and more flexibility. More time is needed for high quality consultations with patients with major and complex problems of all kinds. But patients also need access to simpler services and advice. This should be more appropriate (and cost less) when it is given by professionals who know the patient and his or her medical history and social circumstances. For doctors, the higher quality associated with longer consultations may lead to greater professional satisfaction and, if these longer consultations are combined with more realistic scheduling, to reduced levels of stress.20 They will also find it easier to develop further the care of chronic disease.

The challenge posed to general practice by walk-in centres and NHS Direct is considerable, and the diversion of funding from primary care is large. The risk of waste and duplication increases as more layers of complexity are added to a primary care service that started out as something familiar, simple, and local and which is still envied in other developed countries.21 Access needs to be simple, and the advantages of personal knowledge and trust in minimising duplication and overmedicalisation need to be exploited.

We must ensure better communication and access so that patients can more easily deal with minor issues and queries with someone they know and trust and avoid the formality and inconvenience of a full face to face consultation. Too often this has to be with a different professional, unfamiliar with the nuances of the case. There should be far more managerial emphasis on helping patients to interact with their chosen practitioner22; such a programme has been described.23 Modern information systems make it much easier to record which doctor(s) a patient prefers to see and to monitor how often this is achieved. The telephone is hardly modern but is underused. Email avoids the problems inherent in arranging simultaneous availability necessary for telephone consultations but at the cost of reducing the communication of emotions. There is a place for both.2 Access without prior appointment is a valued feature of primary care, and we need to know more about the right balance between planned and ad hoc consulting.

## Next steps

General practitioners do not behave in a uniform way. They can be categorised as slow, medium, and fast and react in different ways to changes in consulting speed.18 They are likely to have differing views about a widespread move to lengthen consultation time. We do not need further confirmation that longer consultations are desirable and necessary, but research could show us the best way to learn how to introduce them with minimal disruption to the way in which patients and practices like primary care to be provided.24 We also need to learn how to make the most of available time in complex consultations.

Devising appropriate incentives and helping practices move beyond just reacting to demand in the traditional way by working harder and faster is perhaps our greatest challenge in the United Kingdom. The new primary are trusts need to work together with the growing primary care research networks to carry out the necessary development work. In particular, research is needed on how a primary care team can best provide the right balance of quick access and interpersonal knowledge and trust.

## Acknowledgments

We thank the other members of the working group: Susan Childs, Paul Freeling, Iona Heath, Marshall Marinker, and Bonnie Sibbald. We also thank Fenny Green of the Royal College of General Practitioners for administrative help.

## References

- Shah NC. Viewpoint: Consultation time—time for a change? Still the “perfunctory work of perfunctory men!”. Br J Gen Pract 49:497 (1999).
- Mechanic D. How should hamsters run? Some observations about sufficient patient time in primary care. BMJ 323:266–268 (2001). PMID: 11485957
- Howie JGR, Porter AMD, Heaney DJ, Hopton JL. Long to short consultation ratio: a proxy measure of quality of care for general practice. Br J Gen Pract 41:48–54 (1991). PMID: 2031735
- Howie JGR, Heaney DJ, Maxwell M, Walker JJ, Freeman GK, Rai H. Quality at general practice consultations: cross-sectional survey. BMJ 319:738–743 (1999). PMID: 10487999
- Kaplan SH, Greenfield S, Ware JE. Assessing the effects of physician-patient interactions on the outcome of chronic disease. Med Care 27:110–125 (1989).
- Airey C, Erens B. National surveys of NHS patients: general practice, 1998. London: NHS Executive (1999).
- Hart JT. Expectations of health care: promoted, managed or shared?. Health Expect 1:3–13 (1998). PMID: 11281857
- Tuckett D, Boulton M, Olson C, Williams A. Meetings between experts: an approach to sharing ideas in medical consultations. London: Tavistock Publications (1985).
- General Medical Council. Draft recommendations on undergraduate medical education. July 2001. www.gmc-uk.org/med\_ed/tomorrowsdoctors/index.htm (accessed 2 Jan 2002).
- Balint M. The doctor, his patient and the illness. London: Tavistock (1957).
- Stott NCH, Davies RH. The exceptional potential in each primary care consultation. J R Coll Gen Pract 29:210–205 (1979).
- Hill AP, Hill AP. Challenges for primary care. What's gone wrong with health care? Challenges for the new millennium. London: King's Fund75–86 (2000).
- National service framework for coronary heart disease. London: Department of Health (2000).
- Hart JT. A new kind of doctor: the general practitioner's part in the health of the community. London: Merlin Press (1988).
- Morrison I, Smith R. Hamster health care. BMJ 321:1541–1542 (2000). PMID: 11124164
- Arber S, Sawyer L. Do appointment systems work?. BMJ 284:478–480 (1982). PMID: 6800503
- Hjortdahl P, Borchgrevink CF. Continuity of care: influence of general practitioners' knowledge about their patients on use of resources in consultations. BMJ 303:1181–1184 (1991). PMID: 1747619
- Howie JGR, Hopton JL, Heaney DJ, Porter AMD. Attitudes to medical care, the organization of work, and stress among general practitioners. Br J Gen Pract 42:181–185 (1992). PMID: 1389427
- Freeman G, Shepperd S, Robinson I, Ehrich K, Richards SC, Pitman P. Continuity of care: report of a scoping exercise for the national co-ordinating centre for NHS Service Delivery and Organisation R&amp;D (NCCSDO), Summer 2000. London: NCCSDO (2001).
- Wilson A, McDonald P, Hayes L, Cooney J. Longer booking intervals in general practice: effects on doctors' stress and arousal. Br J Gen Pract 41:184–187 (1991). PMID: 1878267
- De Maeseneer J, Hjortdahl P, Starfield B. Fix what's wrong, not what's right, with general practice in Britain. BMJ 320:1616–1617 (2000). PMID: 10856043
- Freeman G, Hjortdahl P. What future for continuity of care in general practice?. BMJ 314:1870–1873 (1997). PMID: 9224130
- Kibbe DC, Bentz E, McLaughlin CP. Continuous quality improvement for continuity of care. J Fam Pract 36:304–308 (1993). PMID: 8454977
- Williams M, Neal RD. Time for a change? The process of lengthening booking intervals in general practice. Br J Gen Pract 48:1783–1786 (1998). PMID: 10198490

================================================
File: tests/data/groundtruth/docling_v2/code_and_formula.doctags.txt
================================================
<doctag><section_header_level_1><loc_109><loc_79><loc_258><loc_87>JavaScript Code Example</section_header_level_1>
<text><loc_109><loc_94><loc_390><loc_183>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<text><loc_109><loc_185><loc_390><loc_213>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,</text>
<code<loc_110><loc_231><loc_215><loc_257><_unknown_>function add(a, b) { return a + b; } console.log(add(3, 5));</code
<caption><loc_182><loc_221><loc_317><loc_226>Listing 1: Simple JavaScript Program</caption>
<text><loc_109><loc_265><loc_390><loc_353>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<text><loc_109><loc_355><loc_390><loc_383>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,</text>
<page_footer><loc_248><loc_439><loc_252><loc_445>1</page_footer>
<page_break>
<section_header_level_1><loc_112><loc_74><loc_161><loc_82>Formula</section_header_level_1>
<text><loc_112><loc_89><loc_401><loc_172>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<text><loc_112><loc_174><loc_401><loc_208>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.</text>
<formula><loc_236><loc_215><loc_278><loc_222></formula>
<text><loc_112><loc_227><loc_401><loc_311>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<text><loc_112><loc_313><loc_401><loc_353>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.</text>
<text><loc_112><loc_355><loc_401><loc_396>Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.</text>
<page_footer><loc_255><loc_413><loc_259><loc_418>1</page_footer>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/code_and_formula.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "code_and_formula", "origin": {"mimetype": "application/pdf", "binary_hash": 8967166443255744998, "filename": "code_and_formula.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/texts/2"}, {"cref": "#/texts/3"}, {"cref": "#/texts/5"}, {"cref": "#/texts/6"}, {"cref": "#/texts/7"}, {"cref": "#/texts/8"}, {"cref": "#/texts/9"}, {"cref": "#/texts/10"}, {"cref": "#/texts/11"}, {"cref": "#/texts/12"}, {"cref": "#/texts/13"}, {"cref": "#/texts/14"}, {"cref": "#/texts/15"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 667.19122, "r": 315.91595, "b": 654.45184, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 23]}], "orig": "JavaScript Code Example", "text": "JavaScript Code Example", "level": 1}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 642.32806, "r": 477.48276, "b": 501.97412, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 498.86591, "r": 477.47876, "b": 454.15417, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 298]}], "orig": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,", "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,"}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/4"}], "content_layer": "body", "label": "code", "prov": [{"page_no": 1, "bbox": {"l": 134.239, "t": 425.6004899999999, "r": 263.22409, "b": 385.25446, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 60]}], "orig": "function add(a, b) { return a + b; } console.log(add(3, 5));", "text": "function add(a, b) { return a + b; } console.log(add(3, 5));", "captions": [{"cref": "#/texts/4"}], "references": [], "footnotes": [], "image": null, "code_language": "unknown"}, {"self_ref": "#/texts/4", "parent": {"cref": "#/texts/3"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 223.15500000000003, "t": 442.07895, "r": 388.09375, "b": 433.23218, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 36]}], "orig": "Listing 1: Simple JavaScript Program", "text": "Listing 1: Simple JavaScript Program"}, {"self_ref": "#/texts/5", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 372.93902999999995, "r": 477.48172000000005, "b": 232.58536000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/6", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 229.47713999999996, "r": 477.47876, "b": 184.76436, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 298]}], "orig": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,", "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,"}, {"self_ref": "#/texts/7", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 1, "bbox": {"l": 303.133, "t": 96.27914399999997, "r": 308.11429, "b": 87.43235000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "1", "text": "1"}, {"self_ref": "#/texts/8", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 2, "bbox": {"l": 133.76801021944917, "t": 717.0812439593145, "r": 191.5272403142044, "b": 704.341863888975, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 7]}], "orig": "Formula", "text": "Formula", "level": 1}, {"self_ref": "#/texts/9", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76801021944917, "t": 692.2180838220343, "r": 477.48276078332026, "b": 551.8641430470798, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/10", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76801021944917, "t": 548.7559230299179, "r": 477.48163078331845, "b": 492.0881027170305, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 369]}], "orig": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.", "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt."}, {"self_ref": "#/texts/11", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "formula", "prov": [{"page_no": 2, "bbox": {"l": 280.5540204602546, "t": 479.06467264512247, "r": 330.6965605425145, "b": 468.178102585013, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 12]}], "orig": "a 2 + 8 = 12", "text": ""}, {"self_ref": "#/texts/12", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76799021944913, "t": 459.091862534844, "r": 477.4816907833186, "b": 318.7382217598911, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/13", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76799021944913, "t": 315.6300017427293, "r": 477.48370078332186, "b": 247.0072913638337, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 415]}], "orig": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.", "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat."}, {"self_ref": "#/texts/14", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76799021944913, "t": 243.8990813466719, "r": 477.48370078332186, "b": 175.27629096777594, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 415]}], "orig": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.", "text": "Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat."}, {"self_ref": "#/texts/15", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 2, "bbox": {"l": 303.13300049729594, "t": 146.16808080705698, "r": 308.1142905054678, "b": 137.32129075821, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "1", "text": "1"}], "pictures": [], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 612.0, "height": 792.0}, "image": null, "page_no": 1}, "2": {"size": {"width": 595.2760009765625, "height": 841.8900146484375}, "image": null, "page_no": 2}}}

================================================
File: tests/data/groundtruth/docling_v2/code_and_formula.md
================================================
## JavaScript Code Example

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,

```
function add(a, b) { return a + b; } console.log(add(3, 5));
```

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet,

## Formula

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt.

<!-- formula-not-decoded -->

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.

Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.

================================================
File: tests/data/groundtruth/docling_v2/csv-comma-in-cell.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [5x4]

================================================
File: tests/data/groundtruth/docling_v2/csv-comma-in-cell.csv.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "csv-comma-in-cell",
  "origin": {
    "mimetype": "text/csv",
    "binary_hash": 17599039665518552414,
    "filename": "csv-comma-in-cell.csv"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/tables/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": ",",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 4,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": ",",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/csv-comma-in-cell.csv.md
================================================
| 1   | 2   | 3   | 4   |
|-----|-----|-----|-----|
| a   | b   | c   | d   |
| a   | ,   | c   | d   |
| a   | b   | c   | d   |
| a   | b   | c   | d   |

================================================
File: tests/data/groundtruth/docling_v2/csv-comma.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [6x12]

================================================
File: tests/data/groundtruth/docling_v2/csv-comma.csv.md
================================================
|   Index | Customer Id     | First Name   | Last Name   | Company                         | City              | Country                    | Phone 1                | Phone 2               | Email                       | Subscription Date   | Website                     |
|---------|-----------------|--------------|-------------|---------------------------------|-------------------|----------------------------|------------------------|-----------------------|-----------------------------|---------------------|-----------------------------|
|       1 | DD37Cf93aecA6Dc | Sheryl       | Baxter      | Rasmussen Group                 | East Leonard      | Chile                      | 229.077.5154           | 397.884.0519x718      | zunigavanessa@smith.info    | 2020-08-24          | http://www.stephenson.com/  |
|       2 | 1Ef7b82A4CAAD10 | Preston      | Lozano, Dr  | Vega-Gentry                     | East Jimmychester | Djibouti                   | 5153435776             | 686-620-1820x944      | vmata@colon.com             | 2021-04-23          | http://www.hobbs.com/       |
|       3 | 6F94879bDAfE5a6 | Roy          | Berry       | Murillo-Perry                   | Isabelborough     | Antigua and Barbuda        | +1-539-402-0259        | (496)978-3969x58947   | beckycarr@hogan.com         | 2020-03-25          | http://www.lawrence.com/    |
|       4 | 5Cef8BFA16c5e3c | Linda        | Olsen       | Dominguez, Mcmillan and Donovan | Bensonview        | Dominican Republic         | 001-808-617-6467x12895 | +1-813-324-8756       | stanleyblackwell@benson.org | 2020-06-02          | http://www.good-lyons.com/  |
|       5 | 053d585Ab6b3159 | Joanna       | Bender      | Martin, Lang and Andrade        | West Priscilla    | Slovakia (Slovak Republic) | 001-234-203-0635x76146 | 001-199-446-3860x3486 | colinalvarado@miles.net     | 2021-04-17          | https://goodwin-ingram.com/ |

================================================
File: tests/data/groundtruth/docling_v2/csv-inconsistent-header.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [5x4]

================================================
File: tests/data/groundtruth/docling_v2/csv-inconsistent-header.csv.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "csv-inconsistent-header",
  "origin": {
    "mimetype": "text/csv",
    "binary_hash": 5480400768780756370,
    "filename": "csv-inconsistent-header.csv"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/tables/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 4,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/csv-inconsistent-header.csv.md
================================================
| 1   | 2   | 3   |    |
|-----|-----|-----|----|
| a   | b   | c   | d  |
| a   | b   | c   | d  |
| a   | b   | c   | d  |
| a   | b   | c   | d  |

================================================
File: tests/data/groundtruth/docling_v2/csv-pipe.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [6x12]

================================================
File: tests/data/groundtruth/docling_v2/csv-pipe.csv.md
================================================
|   Index | Customer Id     | First Name   | Last Name   | Company                        | City              | Country                    | Phone 1                | Phone 2               | Email                       | Subscription Date   | Website                     |
|---------|-----------------|--------------|-------------|--------------------------------|-------------------|----------------------------|------------------------|-----------------------|-----------------------------|---------------------|-----------------------------|
|       1 | DD37Cf93aecA6Dc | Sheryl       | Baxter      | Rasmussen Group                | East Leonard      | Chile                      | 229.077.5154           | 397.884.0519x718      | zunigavanessa@smith.info    | 2020-08-24          | http://www.stephenson.com/  |
|       2 | 1Ef7b82A4CAAD10 | Preston      | Lozano      | Vega-Gentry                    | East Jimmychester | Djibouti                   | 5153435776             | 686-620-1820x944      | vmata@colon.com             | 2021-04-23          | http://www.hobbs.com/       |
|       3 | 6F94879bDAfE5a6 | Roy          | Berry       | Murillo-Perry                  | Isabelborough     | Antigua and Barbuda        | +1-539-402-0259        | (496)978-3969x58947   | beckycarr@hogan.com         | 2020-03-25          | http://www.lawrence.com/    |
|       4 | 5Cef8BFA16c5e3c | Linda        | Olsen       | Dominguez|Mcmillan and Donovan | Bensonview        | Dominican Republic         | 001-808-617-6467x12895 | +1-813-324-8756       | stanleyblackwell@benson.org | 2020-06-02          | http://www.good-lyons.com/  |
|       5 | 053d585Ab6b3159 | Joanna       | Bender      | Martin|Lang and Andrade        | West Priscilla    | Slovakia (Slovak Republic) | 001-234-203-0635x76146 | 001-199-446-3860x3486 | colinalvarado@miles.net     | 2021-04-17          | https://goodwin-ingram.com/ |

================================================
File: tests/data/groundtruth/docling_v2/csv-semicolon.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [6x12]

================================================
File: tests/data/groundtruth/docling_v2/csv-semicolon.csv.md
================================================
|   Index | Customer Id     | First Name   | Last Name   | Company                        | City              | Country                    | Phone 1                | Phone 2               | Email                       | Subscription Date   | Website                     |
|---------|-----------------|--------------|-------------|--------------------------------|-------------------|----------------------------|------------------------|-----------------------|-----------------------------|---------------------|-----------------------------|
|       1 | DD37Cf93aecA6Dc | Sheryl       | Baxter      | Rasmussen Group                | East Leonard      | Chile                      | 229.077.5154           | 397.884.0519x718      | zunigavanessa@smith.info    | 2020-08-24          | http://www.stephenson.com/  |
|       2 | 1Ef7b82A4CAAD10 | Preston      | Lozano      | Vega-Gentry                    | East Jimmychester | Djibouti                   | 5153435776             | 686-620-1820x944      | vmata@colon.com             | 2021-04-23          | http://www.hobbs.com/       |
|       3 | 6F94879bDAfE5a6 | Roy          | Berry       | Murillo-Perry                  | Isabelborough     | Antigua and Barbuda        | +1-539-402-0259        | (496)978-3969x58947   | beckycarr@hogan.com         | 2020-03-25          | http://www.lawrence.com/    |
|       4 | 5Cef8BFA16c5e3c | Linda        | Olsen       | Dominguez;Mcmillan and Donovan | Bensonview        | Dominican Republic         | 001-808-617-6467x12895 | +1-813-324-8756       | stanleyblackwell@benson.org | 2020-06-02          | http://www.good-lyons.com/  |
|       5 | 053d585Ab6b3159 | Joanna       | Bender      | Martin;Lang and Andrade        | West Priscilla    | Slovakia (Slovak Republic) | 001-234-203-0635x76146 | 001-199-446-3860x3486 | colinalvarado@miles.net     | 2021-04-17          | https://goodwin-ingram.com/ |

================================================
File: tests/data/groundtruth/docling_v2/csv-tab.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [6x12]

================================================
File: tests/data/groundtruth/docling_v2/csv-tab.csv.md
================================================
|   Index | Customer Id     | First Name   | Last Name   | Company         | City              | Country                    | Phone 1                | Phone 2               | Email                       | Subscription Date   | Website                     |
|---------|-----------------|--------------|-------------|-----------------|-------------------|----------------------------|------------------------|-----------------------|-----------------------------|---------------------|-----------------------------|
|       1 | DD37Cf93aecA6Dc | Sheryl       | Baxter      | Rasmussen Group | East Leonard      | Chile                      | 229.077.5154           | 397.884.0519x718      | zunigavanessa@smith.info    | 2020-08-24          | http://www.stephenson.com/  |
|       2 | 1Ef7b82A4CAAD10 | Preston      | Lozano      | Vega-Gentry     | East Jimmychester | Djibouti                   | 5153435776             | 686-620-1820x944      | vmata@colon.com             | 2021-04-23          | http://www.hobbs.com/       |
|       3 | 6F94879bDAfE5a6 | Roy          | Berry       | Murillo-Perry   | Isabelborough     | Antigua and Barbuda        | +1-539-402-0259        | (496)978-3969x58947   | beckycarr@hogan.com         | 2020-03-25          | http://www.lawrence.com/    |
|       4 | 5Cef8BFA16c5e3c | Linda        | Olsen       | Dominguez	Mcmillan and Donovan                 | Bensonview        | Dominican Republic         | 001-808-617-6467x12895 | +1-813-324-8756       | stanleyblackwell@benson.org | 2020-06-02          | http://www.good-lyons.com/  |
|       5 | 053d585Ab6b3159 | Joanna       | Bender      | Martin	Lang and Andrade                 | West Priscilla    | Slovakia (Slovak Republic) | 001-234-203-0635x76146 | 001-199-446-3860x3486 | colinalvarado@miles.net     | 2021-04-17          | https://goodwin-ingram.com/ |

================================================
File: tests/data/groundtruth/docling_v2/csv-too-few-columns.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [5x4]

================================================
File: tests/data/groundtruth/docling_v2/csv-too-few-columns.csv.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "csv-too-few-columns",
  "origin": {
    "mimetype": "text/csv",
    "binary_hash": 6079936590967298763,
    "filename": "csv-too-few-columns.csv"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/tables/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "'b'",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 4,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "'b'",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/csv-too-few-columns.csv.md
================================================
| 1   | 2   | 3   | 4   |
|-----|-----|-----|-----|
| a   | 'b' | c   | d   |
| a   | b   | c   |     |
| a   | b   | c   | d   |
| a   | b   | c   | d   |

================================================
File: tests/data/groundtruth/docling_v2/csv-too-many-columns.csv.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: table with [5x5]

================================================
File: tests/data/groundtruth/docling_v2/csv-too-many-columns.csv.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "csv-too-many-columns",
  "origin": {
    "mimetype": "text/csv",
    "binary_hash": 10142252432152444595,
    "filename": "csv-too-many-columns.csv"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/tables/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "e",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "a",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "b",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "c",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "d",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 5,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "e",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "a",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "b",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "c",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "d",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/csv-too-many-columns.csv.md
================================================
| 1   | 2   | 3   | 4   |    |
|-----|-----|-----|-----|----|
| a   | b   | c   | d   |    |
| a   | b   | c   | d   | e  |
| a   | b   | c   | d   |    |
| a   | b   | c   | d   |    |

================================================
File: tests/data/groundtruth/docling_v2/duck.md.md
================================================
Summer activities

# Swimming in the lake

Duck

Figure 1: This is a cute duckling

## Let’s swim!

To get started with swimming, first lay down in a water and try not to drown:

- You can relax and look around
- Paddle about
- Enjoy summer warmth

Also, don’t forget:

- Wear sunglasses
- Don’t forget to drink water
- Use sun cream

Hmm, what else…

## Let’s eat

After we had a good day of swimming in the lake, it’s important to eat something nice

I like to eat leaves

Here are some interesting things a respectful duck could eat:

|         | Food                             |   Calories per portion |
|---------|----------------------------------|------------------------|
| Leaves  | Ash, Elm, Maple                  |                     50 |
| Berries | Blueberry, Strawberry, Cranberry |                    150 |
| Grain   | Corn, Buckwheat, Barley          |                    200 |

And let’s add another list in the end:

- Leaves
- Berries
- Grain

And here my listing in code:

```
Leaves

Berries
Grain
```


================================================
File: tests/data/groundtruth/docling_v2/elife-56337.xml.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: KRAB-zinc finger protein gene ex ... retrotransposons in the murine lineage
    item-2 at level 2: paragraph: Gernot Wolf, Alberto de Iaco, Mi ...  Ralls, Didier Trono, Todd S Macfarlan
    item-3 at level 2: paragraph: The Eunice Kennedy Shriver Natio ... Lausanne (EPFL), Lausanne, Switzerland
    item-4 at level 2: section_header: Abstract
      item-5 at level 3: text: The Krüppel-associated box zinc  ... edundant role restricting TE activity.
    item-6 at level 2: section_header: Introduction
      item-7 at level 3: text: Nearly half of the human and mou ... s are active beyond early development.
      item-8 at level 3: text: TEs, especially long terminal re ... f evolutionarily young KRAB-ZFP genes.
    item-9 at level 2: section_header: Results
      item-10 at level 3: section_header: Mouse KRAB-ZFPs target retrotransposons
        item-11 at level 4: text: We analyzed the RNA expression p ... duplications (Kauzlaric et al., 2017).
        item-12 at level 4: text: To determine the binding sites o ... ctive in the early embryo (Figure 1A).
        item-13 at level 4: picture
          item-13 at level 5: caption: Figure 1. Genome-wide binding patterns of mouse KRAB-ZFPs. (A) Probability heatmap of KRAB-ZFP binding to TEs. Blue color intensity (main field) corresponds to -log10 (adjusted p-value) enrichment of ChIP-seq peak overlap with TE groups (Fisher’s exact test). The green/red color intensity (top panel) represents mean KAP1 (GEO accession: GSM1406445) and H3K9me3 (GEO accession: GSM1327148) enrichment (respectively) at peaks overlapping significantly targeted TEs (adjusted p-value<1e-5) in WT ES cells. (B) Summarized ChIP-seq signal for indicated KRAB-ZFPs and previously published KAP1 and H3K9me3 in WT ES cells across 127 intact ETn elements. (C) Heatmaps of KRAB-ZFP ChIP-seq signal at ChIP-seq peaks. For better comparison, peaks for all three KRAB-ZFPs were called with the same parameters (p<1e-10, peak enrichment >20). The top panel shows a schematic of the arrangement of the contact amino acid composition of each zinc finger. Zinc fingers are grouped and colored according to similarity, with amino acid differences relative to the five consensus fingers highlighted in white.
        item-14 at level 4: table with [9x5]
          item-14 at level 5: caption: Table 1. KRAB-ZFP genes clusters in the mouse genome that were investigated in this study. * Number of protein-coding KRAB-ZFP genes identified in a previously published screen (Imbeault et al., 2017) and the ChIP-seq data column indicates the number of KRAB-ZFPs for which ChIP-seq was performed in this study.
        item-15 at level 4: text: We generally observed that KRAB- ... responsible for this silencing effect.
        item-16 at level 4: text: To further test the hypothesis t ... t easily evade repression by mutation.
        item-17 at level 4: text: Our KRAB-ZFP ChIP-seq dataset al ... ntirely shift the mode of DNA binding.
      item-18 at level 3: section_header: Genetic deletion of KRAB-ZFP gen ...  leads to retrotransposon reactivation
        item-19 at level 4: text: The majority of KRAB-ZFP genes a ... ung et al., 2014; Deniz et al., 2018).
        item-20 at level 4: picture
          item-20 at level 5: caption: Figure 2. Retrotransposon reactivation in KRAB-ZFP cluster KO ES cells. (A) RNA-seq analysis of TE expression in five KRAB-ZFP cluster KO ES cells. Green and grey squares on top of the panel represent KRAB-ZFPs with or without ChIP-seq data, respectively, within each deleted gene cluster. Reactivated TEs that are bound by one or several KRAB-ZFPs are indicated by green squares in the panel. Significantly up- and downregulated elements (adjusted p-value<0.05) are highlighted in red and green, respectively. (B) Differential KAP1 binding and H3K9me3 enrichment at TE groups (summarized across all insertions) in Chr2-cl and Chr4-cl KO ES cells. TE groups targeted by one or several KRAB-ZFPs encoded within the deleted clusters are highlighted in blue (differential enrichment over the entire TE sequences) and red (differential enrichment at TE regions that overlap with KRAB-ZFP ChIP-seq peaks). (C) DNA methylation status of CpG sites at indicated TE groups in WT and Chr4-cl KO ES cells grown in serum containing media or in hypomethylation-inducing media (2i + Vitamin C). P-values were calculated using paired t-test.
      item-21 at level 3: section_header: KRAB-ZFP cluster deletions license TE-borne enhancers
        item-22 at level 4: text: We next used our RNA-seq dataset ... vating effects of TEs on nearby genes.
        item-23 at level 4: picture
          item-23 at level 5: caption: Figure 3. TE-dependent gene activation in KRAB-ZFP cluster KO ES cells. (A) Differential gene expression in Chr2-cl and Chr4-cl KO ES cells. Significantly up- and downregulated genes (adjusted p-value<0.05) are highlighted in red and green, respectively, KRAB-ZFP genes within the deleted clusters are shown in blue. (B) Correlation of TEs and gene deregulation. Plots show enrichment of TE groups within 100 kb of up- and downregulated genes relative to all genes. Significantly overrepresented LTR and LINE groups (adjusted p-value<0.1) are highlighted in blue and red, respectively. (C) Schematic view of the downstream region of Chst1 where a 5’ truncated ETn insertion is located. ChIP-seq (Input subtracted from ChIP) data for overexpressed epitope-tagged Gm13051 (a Chr4-cl KRAB-ZFP) in F9 EC cells, and re-mapped KAP1 (GEO accession: GSM1406445) and H3K9me3 (GEO accession: GSM1327148) in WT ES cells are shown together with RNA-seq data from Chr4-cl WT and KO ES cells (mapped using Bowtie (-a -m 1 --strata -v 2) to exclude reads that cannot be uniquely mapped). (D) RT-qPCR analysis of Chst1 mRNA expression in Chr4-cl WT and KO ES cells with or without the CRISPR/Cas9 deleted ETn insertion near Chst1. Values represent mean expression (normalized to Gapdh) from three biological replicates per sample (each performed in three technical replicates) in arbitrary units. Error bars represent standard deviation and asterisks indicate significance (p<0.01, Student’s t-test). n.s.: not significant. (E) Mean coverage of ChIP-seq data (Input subtracted from ChIP) in Chr4-cl WT and KO ES cells over 127 full-length ETn insertions. The binding sites of the Chr4-cl KRAB-ZFPs Rex2 and Gm13051 are indicated by dashed lines.
        item-24 at level 4: text: While we generally observed that ... he internal region and not on the LTR.
      item-25 at level 3: section_header: ETn retrotransposition in Chr4-cl KO and WT mice
        item-26 at level 4: text: IAP, ETn/ETnERV and MuLV/RLTR4 r ... s may contribute to reduced viability.
        item-27 at level 4: text: We reasoned that retrotransposon ... Tn insertions at a high recovery rate.
        item-28 at level 4: text: Using this dataset, we first con ... nsertions in our pedigree (Figure 4A).
        item-29 at level 4: picture
          item-29 at level 5: caption: Figure 4. ETn retrotransposition in Chr4-cl KO mice. (A) Pedigree of mice used for transposon insertion screening by capture-seq in mice of different strain backgrounds. The number of novel ETn insertions (only present in one animal) are indicated. For animals whose direct ancestors have not been screened, the ETn insertions are shown in parentheses since parental inheritance cannot be excluded in that case. Germ line insertions are indicated by asterisks. All DNA samples were prepared from tail tissues unless noted (-S: spleen, -E: ear, -B:Blood) (B) Statistical analysis of ETn insertion frequency in tail tissue from 30 Chr4-cl KO, KO/WT and WT mice that were derived from one Chr4-c KO x KO/WT and two Chr4-cl KO/WT x KO/WT matings. Only DNA samples that were collected from juvenile tails were considered for this analysis. P-values were calculated using one-sided Wilcoxon Rank Sum Test. In the last panel, KO, WT and KO/WT mice derived from all matings were combined for the statistical analysis.
        item-30 at level 4: text: To validate some of the novel ET ... ess might have truncated this element.
        item-31 at level 4: text: Besides novel ETn insertions tha ... tions (Figure 4—figure supplement 3D).
        item-32 at level 4: text: Finally, we asked whether there  ... s clearly also play an important role.
    item-33 at level 2: section_header: Discussion
      item-34 at level 3: text: C2H2 zinc finger proteins, about ... ) depending upon their insertion site.
      item-35 at level 3: text: Despite a lack of widespread ETn ... ion of the majority of KRAB-ZFP genes.
    item-36 at level 2: section_header: Materials and methods
      item-37 at level 3: table with [31x5]
        item-37 at level 4: caption: Key resources table
      item-38 at level 3: section_header: Cell lines and transgenic mice
        item-39 at level 4: text: Mouse ES cells and F9 EC cells w ... KO/KO and KO/WT (B6/129 F2) offspring.
      item-40 at level 3: section_header: Generation of KRAB-ZFP expressing cell lines
        item-41 at level 4: text: KRAB-ZFP ORFs were PCR-amplified ... led and further expanded for ChIP-seq.
      item-42 at level 3: section_header: CRISPR/Cas9 mediated deletion of KRAB-ZFP clusters and an MMETn insertion
        item-43 at level 4: text: All gRNAs were expressed from th ... PCR genotyping (Supplementary file 3).
      item-44 at level 3: section_header: ChIP-seq analysis
        item-45 at level 4: text: For ChIP-seq analysis of KRAB-ZF ... 010 or Khil et al., 2012 respectively.
        item-46 at level 4: text: ChIP-seq libraries were construc ...  were re-mapped using Bowtie (--best).
      item-47 at level 3: section_header: Luciferase reporter assays
        item-48 at level 4: text: For KRAB-ZFP repression assays,  ... after transfection as described above.
      item-49 at level 3: section_header: RNA-seq analysis
        item-50 at level 4: text: Whole RNA was purified using RNe ... lemented in the R function p.adjust().
      item-51 at level 3: section_header: Reduced representation bisulfite sequencing (RRBS-seq)
        item-52 at level 4: text: For RRBS-seq analysis, Chr4-cl W ... h sample were considered for analysis.
      item-53 at level 3: section_header: Retrotransposition assay
        item-54 at level 4: text: The retrotransposition vectors p ... were stained with Amido Black (Sigma).
      item-55 at level 3: section_header: Capture-seq screen
        item-56 at level 4: text: To identify novel retrotransposo ... assembly using the Unicycler software.
    item-57 at level 2: section_header: Funding Information
      item-58 at level 3: text: This paper was supported by the following grants:
      item-59 at level 3: list: group list
        item-60 at level 4: list_item: http://dx.doi.org/10.13039/10000 ... ment 1ZIAHD008933 to Todd S Macfarlan.
        item-61 at level 4: list_item: http://dx.doi.org/10.13039/50110 ... ndation 310030_152879 to Didier Trono.
        item-62 at level 4: list_item: http://dx.doi.org/10.13039/50110 ... dation 310030B_173337 to Didier Trono.
        item-63 at level 4: list_item: http://dx.doi.org/10.13039/50110 ... ch Council No. 268721 to Didier Trono.
        item-64 at level 4: list_item: http://dx.doi.org/10.13039/50110 ... rch Council No 694658 to Didier Trono.
    item-65 at level 2: section_header: Acknowledgements
      item-66 at level 3: text: We thank Alex Grinberg, Jeanne Y ...  268721; Transpos-X, No. 694658) (DT).
    item-67 at level 2: section_header: Additional information
    item-68 at level 2: section_header: Additional files
    item-69 at level 2: section_header: Data availability
      item-70 at level 3: text: All NGS data has been deposited  ... GenBank database (MH449667- MH449669).
      item-71 at level 3: text: The following datasets were generated:
      item-72 at level 3: text: Wolf G. Retrotransposon reactiva ... ession Omnibus (2019). NCBI: GSE115291
      item-73 at level 3: text: Wolf G. Mus musculus musculus st ... e. NCBI GenBank (2019). NCBI: MH449667
      item-74 at level 3: text: Wolf G. Mus musculus musculus st ... e. NCBI GenBank (2019). NCBI: MH449668
      item-75 at level 3: text: Wolf G. Mus musculus musculus st ... e. NCBI GenBank (2019). NCBI: MH449669
      item-76 at level 3: text: The following previously published datasets were used:
      item-77 at level 3: text: Castro-Diaz N, Ecco G, Coluccio  ... ssion Omnibus (2014). NCBI: GSM1406445
      item-78 at level 3: text: Andrew ZX. H3K9me3_ChIPSeq (Ctrl ... ssion Omnibus (2014). NCBI: GSM1327148
    item-79 at level 2: section_header: References
      item-80 at level 3: list: group list
        item-81 at level 4: list_item: Bailey TL, Boden M, Buske FA, Fr ... OI: 10.1093/nar/gkp335, PMID: 19458158
        item-82 at level 4: list_item: Baust C, Gagnier L, Baillie GJ,  ... 77.21.11448-11458.2003, PMID: 14557630
        item-83 at level 4: list_item: Blaschke K, Ebata KT, Karimi MM, ... I: 10.1038/nature12362, PMID: 23812591
        item-84 at level 4: list_item: Brodziak A, Ziółko E, Muc-Wierzg ... I: 10.12659/msm.882892, PMID: 22648263
        item-85 at level 4: list_item: Castro-Diaz N, Ecco G, Coluccio  ... 10.1101/gad.241661.114, PMID: 24939876
        item-86 at level 4: list_item: Chuong EB, Elde NC, Feschotte C. ... 0.1126/science.aad5497, PMID: 26941318
        item-87 at level 4: list_item: Dan J, Liu Y, Liu N, Chiourea M, ... 6/j.devcel.2014.03.004, PMID: 24735877
        item-88 at level 4: list_item: De Iaco A, Planet E, Coluccio A, ... . DOI: 10.1038/ng.3858, PMID: 28459456
        item-89 at level 4: list_item: Deniz Ö, de la Rica L, Cheng KCL ... 1186/s13059-017-1376-y, PMID: 29351814
        item-90 at level 4: list_item: Dewannieux M, Heidmann T. Endoge ... 6/j.coviro.2013.08.005, PMID: 24004725
        item-91 at level 4: list_item: Ecco G, Cassano M, Kauzlaric A,  ... 6/j.devcel.2016.02.024, PMID: 27003935
        item-92 at level 4: list_item: Ecco G, Imbeault M, Trono D. KRA ... OI: 10.1242/dev.132605, PMID: 28765213
        item-93 at level 4: list_item: Frank JA, Feschotte C. Co-option ... 6/j.coviro.2017.07.021, PMID: 28818736
        item-94 at level 4: list_item: Gagnier L, Belancio VP, Mager DL ... 1186/s13100-019-0157-4, PMID: 31011371
        item-95 at level 4: list_item: Groner AC, Meylan S, Ciuffi A, Z ... 1/journal.pgen.1000869, PMID: 20221260
        item-96 at level 4: list_item: Hancks DC, Kazazian HH. Roles fo ... 1186/s13100-016-0065-9, PMID: 27158268
        item-97 at level 4: list_item: Imbeault M, Helleboid PY, Trono  ... I: 10.1038/nature21683, PMID: 28273063
        item-98 at level 4: list_item: Jacobs FM, Greenberg D, Nguyen N ... I: 10.1038/nature13760, PMID: 25274305
        item-99 at level 4: list_item: Kano H, Kurahashi H, Toda T. Gen ... 0.1073/pnas.0705483104, PMID: 17984064
        item-100 at level 4: list_item: Karimi MM, Goyal P, Maksakova IA ... 016/j.stem.2011.04.004, PMID: 21624812
        item-101 at level 4: list_item: Kauzlaric A, Ecco G, Cassano M,  ... 1/journal.pone.0173746, PMID: 28334004
        item-102 at level 4: list_item: Khil PP, Smagulova F, Brick KM,  ...  10.1101/gr.130583.111, PMID: 22367190
        item-103 at level 4: list_item: Krueger F, Andrews SR. Bismark:  ... /bioinformatics/btr167, PMID: 21493656
        item-104 at level 4: list_item: Langmead B, Salzberg SL. Fast ga ... OI: 10.1038/nmeth.1923, PMID: 22388286
        item-105 at level 4: list_item: Legiewicz M, Zolotukhin AS, Pilk ... 0.1074/jbc.M110.182840, PMID: 20978285
        item-106 at level 4: list_item: Lehoczky JA, Thomas PE, Patrie K ... 1/journal.pgen.1003967, PMID: 24339789
        item-107 at level 4: list_item: Leung D, Du T, Wagner U, Xie W,  ... 0.1073/pnas.1322273111, PMID: 24757056
        item-108 at level 4: list_item: Lilue J, Doran AG, Fiddes IT, Ab ... 1038/s41588-018-0223-8, PMID: 30275530
        item-109 at level 4: list_item: Liu S, Brind'Amour J, Karimi MM, ... 10.1101/gad.244848.114, PMID: 25228647
        item-110 at level 4: list_item: Love MI, Huber W, Anders S. Mode ... 1186/s13059-014-0550-8, PMID: 25516281
        item-111 at level 4: list_item: Lugani F, Arora R, Papeta N, Pat ... 1/journal.pgen.1003206, PMID: 23437001
        item-112 at level 4: list_item: Macfarlan TS, Gifford WD, Drisco ... I: 10.1038/nature11244, PMID: 22722858
        item-113 at level 4: list_item: Maksakova IA, Romanish MT, Gagni ... 1/journal.pgen.0020002, PMID: 16440055
        item-114 at level 4: list_item: Matsui T, Leung D, Miyashita H,  ... I: 10.1038/nature08858, PMID: 20164836
        item-115 at level 4: list_item: Najafabadi HS, Mnaimneh S, Schmi ...  DOI: 10.1038/nbt.3128, PMID: 25690854
        item-116 at level 4: list_item: Nellåker C, Keane TM, Yalcin B,  ... .1186/gb-2012-13-6-r45, PMID: 22703977
        item-117 at level 4: list_item: O'Geen H, Frietze S, Farnham PJ. ... 7/978-1-60761-753-2_27, PMID: 20680851
        item-118 at level 4: list_item: Patel A, Yang P, Tinkham M, Prad ... 016/j.cell.2018.02.058, PMID: 29551271
        item-119 at level 4: list_item: Ribet D, Dewannieux M, Heidmann  ... OI: 10.1101/gr.2924904, PMID: 15479948
        item-120 at level 4: list_item: Richardson SR, Gerdes P, Gerhard ...  10.1101/gr.219022.116, PMID: 28483779
        item-121 at level 4: list_item: Rowe HM, Jakobsson J, Mesnard D, ... I: 10.1038/nature08674, PMID: 20075919
        item-122 at level 4: list_item: Rowe HM, Kapopoulou A, Corsinott ...  10.1101/gr.147678.112, PMID: 23233547
        item-123 at level 4: list_item: Schauer SN, Carreira PE, Shukla  ...  10.1101/gr.226993.117, PMID: 29643204
        item-124 at level 4: list_item: Schultz DC, Ayyanathan K, Negore ... OI: 10.1101/gad.973302, PMID: 11959841
        item-125 at level 4: list_item: Semba K, Araki K, Matsumoto K, S ... 1/journal.pgen.1003204, PMID: 23436999
        item-126 at level 4: list_item: Sripathy SP, Stevens J, Schultz  ... : 10.1128/MCB.00487-06, PMID: 16954381
        item-127 at level 4: list_item: Thomas JH, Schneider S. Coevolut ...  10.1101/gr.121749.111, PMID: 21784874
        item-128 at level 4: list_item: Thompson PJ, Macfarlan TS, Lorin ... 6/j.molcel.2016.03.029, PMID: 27259207
        item-129 at level 4: list_item: Treger RS, Pope SD, Kong Y, Toku ... 6/j.immuni.2018.12.022, PMID: 30709743
        item-130 at level 4: list_item: Vlangos CN, Siuniak AN, Robinson ... 1/journal.pgen.1003205, PMID: 23437000
        item-131 at level 4: list_item: Wang J, Xie G, Singh M, Ghanbari ... I: 10.1038/nature13804, PMID: 25317556
        item-132 at level 4: list_item: Wolf D, Hug K, Goff SP. TRIM28 m ... 0.1073/pnas.0805540105, PMID: 18713861
        item-133 at level 4: list_item: Wolf G, Greenberg D, Macfarlan T ... 1186/s13100-015-0050-8, PMID: 26435754
        item-134 at level 4: list_item: Wolf G, Yang P, Füchtbauer AC, F ... 10.1101/gad.252767.114, PMID: 25737282
        item-135 at level 4: list_item: Yamauchi M, Freitag B, Khan C, B ... JVI.69.2.1142-1149.1995, PMID: 7529329
        item-136 at level 4: list_item: Zhang Y, Liu T, Meyer CA, Eeckho ... .1186/gb-2008-9-9-r137, PMID: 18798982
  item-137 at level 1: caption: Figure 1. Genome-wide binding pa ... onsensus fingers highlighted in white.
  item-138 at level 1: caption: Table 1. KRAB-ZFP genes clusters ...  ChIP-seq was performed in this study.
  item-139 at level 1: caption: Figure 2. Retrotransposon reacti ... s were calculated using paired t-test.
  item-140 at level 1: caption: Figure 3. TE-dependent gene acti ... Gm13051 are indicated by dashed lines.
  item-141 at level 1: caption: Figure 4. ETn retrotransposition ... combined for the statistical analysis.
  item-142 at level 1: caption: Key resources table

================================================
File: tests/data/groundtruth/docling_v2/ending_with_table.md.md
================================================
| Character      | Name in German   | Name in French   | Name in Italian   |
|----------------|------------------|------------------|-------------------|
| Scrooge McDuck | Dagobert Duck    | Balthazar Picsou | Paperone          |
| Huey           | Tick             | Riri             | Qui               |
| Dewey          | Trick            | Fifi             | Quo               |
| Louie          | Track            | Loulou           | Qua               |


================================================
File: tests/data/groundtruth/docling_v2/example_01.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Introduction
    item-2 at level 2: paragraph: This is the first paragraph of the introduction.
    item-3 at level 2: section_header: Background
      item-4 at level 3: paragraph: Some background information here.
      item-5 at level 3: picture
      item-6 at level 3: list: group list
        item-7 at level 4: list_item: First item in unordered list
        item-8 at level 4: list_item: Second item in unordered list
      item-9 at level 3: ordered_list: group ordered list
        item-10 at level 4: list_item: First item in ordered list
        item-11 at level 4: list_item: Second item in ordered list

================================================
File: tests/data/groundtruth/docling_v2/example_01.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_01",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 13782069548509991617,
    "filename": "example_01.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/texts/5"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/texts/7"
        }
      ],
      "content_layer": "body",
      "name": "ordered list",
      "label": "ordered_list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/2"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Introduction",
      "text": "Introduction"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is the first paragraph of the introduction.",
      "text": "This is the first paragraph of the introduction."
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/pictures/0"
        },
        {
          "$ref": "#/groups/0"
        },
        {
          "$ref": "#/groups/1"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Background",
      "text": "Background",
      "level": 2
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Some background information here.",
      "text": "Some background information here."
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in unordered list",
      "text": "First item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in unordered list",
      "text": "Second item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in ordered list",
      "text": "First item in ordered list",
      "enumerated": true,
      "marker": "1."
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in ordered list",
      "text": "Second item in ordered list",
      "enumerated": true,
      "marker": "2."
    }
  ],
  "pictures": [
    {
      "self_ref": "#/pictures/0",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "annotations": []
    }
  ],
  "tables": [],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_01.html.md
================================================
# Introduction

This is the first paragraph of the introduction.

## Background

Some background information here.

<!-- image -->

- First item in unordered list
- Second item in unordered list

1. First item in ordered list
2. Second item in ordered list

================================================
File: tests/data/groundtruth/docling_v2/example_02.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Introduction
    item-2 at level 2: paragraph: This is the first paragraph of the introduction.
    item-3 at level 2: section_header: Background
      item-4 at level 3: paragraph: Some background information here.
      item-5 at level 3: list: group list
        item-6 at level 4: list_item: First item in unordered list
        item-7 at level 4: list_item: Second item in unordered list
      item-8 at level 3: ordered_list: group ordered list
        item-9 at level 4: list_item: First item in ordered list
        item-10 at level 4: list_item: Second item in ordered list

================================================
File: tests/data/groundtruth/docling_v2/example_02.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_02",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 17361433184833793580,
    "filename": "example_02.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/texts/5"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/texts/7"
        }
      ],
      "content_layer": "body",
      "name": "ordered list",
      "label": "ordered_list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/2"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Introduction",
      "text": "Introduction"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is the first paragraph of the introduction.",
      "text": "This is the first paragraph of the introduction."
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/groups/0"
        },
        {
          "$ref": "#/groups/1"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Background",
      "text": "Background",
      "level": 2
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Some background information here.",
      "text": "Some background information here."
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in unordered list",
      "text": "First item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in unordered list",
      "text": "Second item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in ordered list",
      "text": "First item in ordered list",
      "enumerated": true,
      "marker": "1."
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in ordered list",
      "text": "Second item in ordered list",
      "enumerated": true,
      "marker": "2."
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_02.html.md
================================================
# Introduction

This is the first paragraph of the introduction.

## Background

Some background information here.

- First item in unordered list
- Second item in unordered list

1. First item in ordered list
2. Second item in ordered list

================================================
File: tests/data/groundtruth/docling_v2/example_03.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Example Document
    item-2 at level 2: section_header: Introduction
      item-3 at level 3: paragraph: This is the first paragraph of the introduction.
    item-4 at level 2: section_header: Background
      item-5 at level 3: paragraph: Some background information here.
      item-6 at level 3: list: group list
        item-7 at level 4: list_item: First item in unordered list
          item-8 at level 5: list: group list
            item-9 at level 6: list_item: Nested item 1
            item-10 at level 6: list_item: Nested item 2
        item-11 at level 4: list_item: Second item in unordered list
      item-12 at level 3: ordered_list: group ordered list
        item-13 at level 4: list_item: First item in ordered list
          item-14 at level 5: ordered_list: group ordered list
            item-15 at level 6: list_item: Nested ordered item 1
            item-16 at level 6: list_item: Nested ordered item 2
        item-17 at level 4: list_item: Second item in ordered list
    item-18 at level 2: section_header: Data Table
      item-19 at level 3: table with [4x3]

================================================
File: tests/data/groundtruth/docling_v2/example_03.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_03",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 17768514429310008971,
    "filename": "example_03.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [
        {
          "$ref": "#/texts/5"
        },
        {
          "$ref": "#/texts/8"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/texts/5"
      },
      "children": [
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/texts/7"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/2",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [
        {
          "$ref": "#/texts/9"
        },
        {
          "$ref": "#/texts/12"
        }
      ],
      "content_layer": "body",
      "name": "ordered list",
      "label": "ordered_list"
    },
    {
      "self_ref": "#/groups/3",
      "parent": {
        "$ref": "#/texts/9"
      },
      "children": [
        {
          "$ref": "#/texts/10"
        },
        {
          "$ref": "#/texts/11"
        }
      ],
      "content_layer": "body",
      "name": "ordered list",
      "label": "ordered_list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/texts/13"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Example Document",
      "text": "Example Document"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/2"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Introduction",
      "text": "Introduction",
      "level": 2
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is the first paragraph of the introduction.",
      "text": "This is the first paragraph of the introduction."
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/groups/0"
        },
        {
          "$ref": "#/groups/2"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Background",
      "text": "Background",
      "level": 2
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Some background information here.",
      "text": "Some background information here."
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [
        {
          "$ref": "#/groups/1"
        }
      ],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in unordered list",
      "text": "First item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Nested item 1",
      "text": "Nested item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Nested item 2",
      "text": "Nested item 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in unordered list",
      "text": "Second item in unordered list",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [
        {
          "$ref": "#/groups/3"
        }
      ],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "First item in ordered list",
      "text": "First item in ordered list",
      "enumerated": true,
      "marker": "1"
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Nested ordered item 1",
      "text": "Nested ordered item 1",
      "enumerated": true,
      "marker": "1."
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Nested ordered item 2",
      "text": "Nested ordered item 2",
      "enumerated": true,
      "marker": "2."
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Second item in ordered list",
      "text": "Second item in ordered list",
      "enumerated": true,
      "marker": "2."
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/tables/0"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Data Table",
      "text": "Data Table",
      "level": 2
    }
  ],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/texts/13"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Header 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Header 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Header 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 1, Col 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 1, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 1, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 2, Col 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 2, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 2, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 3, Col 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 3, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 3, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 4,
        "num_cols": 3,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Header 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Header 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Header 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 1, Col 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 1, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 1, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 2, Col 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 2, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 2, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 3, Col 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 3, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 3, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_03.html.md
================================================
# Example Document

## Introduction

This is the first paragraph of the introduction.

## Background

Some background information here.

- First item in unordered list
    - Nested item 1
    - Nested item 2
- Second item in unordered list

1 First item in ordered list
    1. Nested ordered item 1
    2. Nested ordered item 2
2. Second item in ordered list

## Data Table

| Header 1     | Header 2     | Header 3     |
|--------------|--------------|--------------|
| Row 1, Col 1 | Row 1, Col 2 | Row 1, Col 3 |
| Row 2, Col 1 | Row 2, Col 2 | Row 2, Col 3 |
| Row 3, Col 1 | Row 3, Col 2 | Row 3, Col 3 |

================================================
File: tests/data/groundtruth/docling_v2/example_04.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Data Table with Rowspan and Colspan
    item-2 at level 2: table with [4x3]

================================================
File: tests/data/groundtruth/docling_v2/example_04.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_04",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 2846345769602286603,
    "filename": "example_04.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/tables/0"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Data Table with Rowspan and Colspan",
      "text": "Data Table with Rowspan and Colspan"
    }
  ],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Header 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 2,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 3,
            "text": "Header 2 & 3 (colspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 2,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 1 & 2, Col 1 (rowspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 1, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 1, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 2,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 3,
            "text": "Row 2, Col 2 & 3 (colspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 3, Col 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 3, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 3, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 4,
        "num_cols": 3,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Header 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Header 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Header 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 2,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 1 & 2, Col 1 (rowspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 1, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 1, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 2,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 1 & 2, Col 1 (rowspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Row 2, Col 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Row 2, Col 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 3, Col 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 3, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 3, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_04.html.md
================================================
# Data Table with Rowspan and Colspan

| Header 1                   | Header 2 &amp; 3 (colspan)     | Header 2 &amp; 3 (colspan)     |
|----------------------------|----------------------------|----------------------------|
| Row 1 &amp; 2, Col 1 (rowspan) | Row 1, Col 2               | Row 1, Col 3               |
| Row 1 &amp; 2, Col 1 (rowspan) | Row 2, Col 2 &amp; 3 (colspan) | Row 2, Col 2 &amp; 3 (colspan) |
| Row 3, Col 1               | Row 3, Col 2               | Row 3, Col 3               |

================================================
File: tests/data/groundtruth/docling_v2/example_05.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Omitted html and body tags
    item-2 at level 2: table with [4x3]

================================================
File: tests/data/groundtruth/docling_v2/example_05.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_05",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 1499806583410518209,
    "filename": "example_05.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/tables/0"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Omitted html and body tags",
      "text": "Omitted html and body tags"
    }
  ],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Header 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 2,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 3,
            "text": "Header 2 & 3 (colspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 2,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 1 & 2, Col 1 (rowspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 1, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 1, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 2,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 3,
            "text": "Row 2, Col 2 & 3 (colspan)",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Row 3, Col 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Row 3, Col 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Row 3, Col 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 4,
        "num_cols": 3,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Header 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Header 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Header 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 2,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 1 & 2, Col 1 (rowspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 1, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 1, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 2,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 1 & 2, Col 1 (rowspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Row 2, Col 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 2,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 3,
              "text": "Row 2, Col 2 & 3 (colspan)",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Row 3, Col 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Row 3, Col 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Row 3, Col 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_05.html.md
================================================
# Omitted html and body tags

| Header 1                   | Header 2 &amp; 3 (colspan)     | Header 2 &amp; 3 (colspan)     |
|----------------------------|----------------------------|----------------------------|
| Row 1 &amp; 2, Col 1 (rowspan) | Row 1, Col 2               | Row 1, Col 3               |
| Row 1 &amp; 2, Col 1 (rowspan) | Row 2, Col 2 &amp; 3 (colspan) | Row 2, Col 2 &amp; 3 (colspan) |
| Row 3, Col 1               | Row 3, Col 2               | Row 3, Col 3               |

================================================
File: tests/data/groundtruth/docling_v2/example_06.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: paragraph: This is a div with text.
  item-2 at level 1: paragraph: This is another div with text.
  item-3 at level 1: paragraph: This is a regular paragraph.
  item-4 at level 1: paragraph: This is a third div
with a new line.
  item-5 at level 1: paragraph: This is a fourth div with a bold paragraph.

================================================
File: tests/data/groundtruth/docling_v2/example_06.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "example_06",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 14574683870626799530,
    "filename": "example_06.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      },
      {
        "$ref": "#/texts/1"
      },
      {
        "$ref": "#/texts/2"
      },
      {
        "$ref": "#/texts/3"
      },
      {
        "$ref": "#/texts/4"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is a div with text.",
      "text": "This is a div with text."
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is another div with text.",
      "text": "This is another div with text."
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is a regular paragraph.",
      "text": "This is a regular paragraph."
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is a third div\nwith a new line.",
      "text": "This is a third div\nwith a new line."
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "This is a fourth div with a bold paragraph.",
      "text": "This is a fourth div with a bold paragraph."
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/example_06.html.md
================================================
This is a div with text.

This is another div with text.

This is a regular paragraph.

This is a third div
with a new line.

This is a fourth div with a bold paragraph.

================================================
File: tests/data/groundtruth/docling_v2/ipa20200022300.md
================================================
# SYSTEM FOR CONTROLLING THE OPERATION OF AN ACTUATOR MOUNTED ON A SEED PLANTING IMPLEMENT

## ABSTRACT

In one aspect, a system for controlling an operation of an actuator mounted on a seed planting implement may include an actuator configured to adjust a position of a row unit of the seed planting implement relative to a toolbar of the seed planting implement. The system may also include a flow restrictor fluidly coupled to a fluid chamber of the actuator, with the flow restrictor being configured to reduce a rate at which fluid is permitted to exit the fluid chamber in a manner that provides damping to the row unit. Furthermore, the system may include a valve fluidly coupled to the flow restrictor in a parallel relationship such that the valve is configured to permit the fluid exiting the fluid chamber to flow through the flow restrictor and the fluid entering the fluid chamber to bypass the flow restrictor.

## FIELD

The present disclosure generally relates to seed planting implements and, more particularly, to systems for controlling the operation of an actuator mounted on a seed planting implement in a manner that provides damping to one or more components of the seed planting implement.

## BACKGROUND

Modern farming practices strive to increase yields of agricultural fields. In this respect, seed planting implements are towed behind a tractor or other work vehicle to deposit seeds in a field. For example, seed planting implements typically include one or more ground engaging tools or openers that form a furrow or trench in the soil. One or more dispensing devices of the seed planting implement may, in turn, deposit seeds into the furrow(s). After deposition of the seeds, a packer wheel may pack the soil on top of the deposited seeds.

In certain instances, the packer wheel may also control the penetration depth of the furrow. In this regard, the position of the packer wheel may be moved vertically relative to the associated opener(s) to adjust the depth of the furrow. Additionally, the seed planting implement includes an actuator configured to exert a downward force on the opener(s) to ensure that the opener(s) is able to penetrate the soil to the depth set by the packer wheel. However, the seed planting implement may bounce or chatter when traveling at high speeds and/or when the opener(s) encounters hard or compacted soil. As such, operators generally operate the seed planting implement with the actuator exerting more downward force on the opener(s) than is necessary in order to prevent such bouncing or chatter. Operation of the seed planting implement with excessive down pressure applied to the opener(s), however, reduces the overall stability of the seed planting implement.

Accordingly, an improved system for controlling the operation of an actuator mounted on s seed planting implement to enhance the overall operation of the implement would be welcomed in the technology.

## BRIEF DESCRIPTION

Aspects and advantages of the technology will be set forth in part in the following description, or may be obvious from the description, or may be learned through practice of the technology.

In one aspect, the present subject matter is directed to a system for controlling an operation of an actuator mounted on a seed planting implement. The system may include a toolbar and a row unit adjustably mounted on the toolbar. The system may also include a fluid-driven actuator configured to adjust a position of the row unit relative to the toolbar, with the fluid-driven actuator defining first and second fluid chambers. Furthermore, the system may include a flow restrictor fluidly coupled to the first fluid chamber, with the flow restrictor being configured to reduce a rate at which fluid is permitted to exit the first fluid chamber in a manner that provides viscous damping to the row unit. Additionally, the system may include a valve fluidly coupled to the first fluid chamber. The valve may further be fluidly coupled to the flow restrictor in a parallel relationship such that the valve is configured to permit the fluid exiting the first fluid chamber to flow through the flow restrictor and the fluid entering the first fluid chamber to bypass the flow restrictor.

In another aspect, the present subject matter is directed to a seed planting implement including a toolbar and a plurality of row units adjustably coupled to the toolbar. Each row unit may include a ground engaging tool configured to form a furrow in the soil. The seed planting implement may also include plurality of fluid-driven actuators, with each fluid-driven actuator being coupled between the toolbar and a corresponding row unit of the plurality of row units. As such, each fluid-driven actuator may be configured to adjust a position of the corresponding row unit relative to the toolbar. Moreover, each fluid-driven actuator may define first and second fluid chambers. Furthermore, the seed planting implement may include a flow restrictor fluidly coupled to the first fluid chamber of a first fluid-driven actuator of the plurality of fluid-driven actuators. The flow restrictor may be configured to reduce a rate at which fluid is permitted to exit the first fluid chamber of the first fluid-driven actuator in a manner that provides viscous damping to the corresponding row unit. Additionally, the seed planting implement may include a valve fluidly coupled to the first fluid chamber of the first fluid-driven actuator. The valve further may be fluidly coupled to the flow restrictor in a parallel relationship such that the valve is configured to permit the fluid exiting the first fluid chamber to flow through the flow restrictor and the fluid entering the first fluid chamber to bypass the flow restrictor.

In a further aspect, the present subject matter is directed to a system for providing damping to a row unit of a seed planting implement. The system may include a toolbar, a row unit adjustably mounted on the toolbar, and a fluid-driven actuator configured to adjust a position of the row unit relative to the toolbar. As such, the fluid-driven actuator may define a fluid chamber. The system may also include a flow restrictor fluidly coupled to the fluid chamber. The flow restrictor may define an adjustable throat configured to reduce a rate at which fluid is permitted to exit the fluid chamber. In this regard, the throat may be adjustable between a first size configured to provide a first damping rate to the row unit and a second size configured to provide a second damping rate to the row unit, with the first and second damping rates being different.

These and other features, aspects and advantages of the present technology will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the technology and, together with the description, serve to explain the principles of the technology.

## BRIEF DESCRIPTION OF THE DRAWINGS

A full and enabling disclosure of the present technology, including the best mode thereof, directed to one of ordinary skill in the art, is set forth in the specification, which makes reference to the appended figures, in which:

FIG. 1 illustrates a perspective view of one embodiment of a seed planting implement in accordance with aspects of the present subject matter;

FIG. 2 illustrates a side view of one embodiment of a row unit suitable for use with a seed planting implement in accordance with aspects of the present subject matter;

FIG. 3 illustrates a schematic view of one embodiment of a system for controlling the operation of an actuator mounted on a seed planting implement in accordance with aspects of the present subject matter;

FIG. 4 illustrates a cross-sectional view of one embodiment of a flow restrictor suitable for use in the system shown in FIG. 3, particularly illustrating the flow restrictor defining a throat having a fixed size in accordance with aspects of the present subject matter;

FIG. 5 illustrates a cross-sectional view of another embodiment of a flow restrictor suitable for use in the system shown in FIG. 3, particularly illustrating the flow restrictor defining a throat having an adjustable size in accordance with aspects of the present subject matter;

FIG. 6 illustrates a simplified cross-sectional view of the flow restrictor shown in FIG. 5, particularly illustrating the throat having a first size configured to provide a first damping rate in accordance with aspects of the present subject matter;

FIG. 7 illustrates a simplified cross-sectional view of the flow restrictor shown in FIG. 5, particularly illustrating the throat having a second size configured to provide a second damping rate in accordance with aspects of the present subject matter;

FIG. 8 illustrates a cross-sectional view of another embodiment of a system for controlling the operation of an actuator mounted on a seed planting implement in accordance with aspects of the present subject matter, particularly illustrating the system including a fluidly actuated check valve; and

FIG. 9 illustrates a cross-sectional view of a further embodiment of a system for controlling the operation of an actuator mounted on a seed planting implement in accordance with aspects of the present subject matter, particularly illustrating the system including an electrically actuated check valve.

Repeat use of reference characters in the present specification and drawings is intended to represent the same or analogous features or elements of the present technology.

## DETAILED DESCRIPTION

Reference now will be made in detail to embodiments of the invention, one or more examples of which are illustrated in the drawings. Each example is provided by way of explanation of the invention, not limitation of the invention. In fact, it will be apparent to those skilled in the art that various modifications and variations can be made in the present invention without departing from the scope or spirit of the invention. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present invention covers such modifications and variations as come within the scope of the appended claims and their equivalents.

In general, the present subject matter is directed to systems for controlling the operation of an actuator mounted on a seed planting implement. Specifically, the disclosed systems may be configured to control the operation of the actuator in a manner that provides damping to one or more components of the seed planting implement. For example, in several embodiments, the seed planting implement may include a toolbar and one or more row units adjustably coupled to the toolbar. One or more fluid-driven actuators of the seed planting implement may be configured to control and/or adjust the position of the row unit(s) relative to the toolbar. Furthermore, a flow restrictor may be fluidly coupled to a fluid chamber of the actuator and configured to reduce the rate at which fluid is permitted to exit the fluid chamber so as to provide viscous damping to the row unit(s). In this regard, when the row unit(s) moves relative to the toolbar (e.g., when the row unit contacts a rock or other impediment in the soil), the flow restrictor may be configured to reduce the relative speed and/or displacement of such movement, thereby damping the movement of the row unit(s) relative to the toolbar.

In one embodiment, the flow restrictor may be configured to provide a variable damping rate to the component(s) of the seed planting implement. Specifically, in such embodiment, the flow restrictor may be configured as an adjustable valve having one or more components that may be adjusted to change the size of a fluid passage or throat defined by the valve. In this regard, changing the throat size of the valve varies the rate at which the fluid may exit the fluid chamber of the actuator, thereby adjusting the damping rate provided by the disclosed system. For example, adjusting the valve so as to increase the size of the throat may allow the fluid to exit the fluid chamber more quickly, thereby reducing the damping rate of the system. Conversely, adjusting the valve so as to decrease the size of the throat may allow the fluid to exit the fluid chamber more slowly, thereby increasing the damping rate of the system.

In accordance with aspects of the present subject matter, the system may further include a check valve fluidly coupled to the fluid chamber of the actuator. Specifically, in several embodiments, the check valve may also be fluidly coupled to the flow restrictor in a parallel relationship. As such, the check valve may be configured to direct the fluid exiting the fluid chamber of the actuator (e.g., when one of the row units hits a rock) to flow through the flow restrictor, thereby reducing the relative speed and/or displacement between the row unit(s) in the toolbar. Furthermore, the check valve may be configured to permit the fluid entering the fluid chamber to bypass the flow restrictor. For example, the fluid may return to the fluid chamber as the row unit(s) returns to its initial position following contact with the rock. In this regard, allowing the returning fluid to bypass the flow restrictor may increase the rate at which the fluid flows back into the fluid chamber, thereby further increasing the damping provided by the disclosed system.

Referring now to FIG. 1, a perspective view of one embodiment of a seed planting implement 10 is illustrated in accordance with aspects of the present subject matter. As shown in FIG. 1, the implement 10 may include a laterally extending toolbar or frame assembly 12 connected at its middle to a forwardly extending tow bar 14 to allow the implement 10 to be towed by a work vehicle (not shown), such as an agricultural tractor, in a direction of travel (e.g., as indicated by arrow 16). The toolbar 12 may generally be configured to support a plurality of tool frames 18. Each tool frame 18 may, in turn, be configured to support a plurality of row units 20. As will be described below, each row unit 20 may include one or more ground engaging tools configured to excavate a furrow or trench in the soil.

It should be appreciated that, for purposes of illustration, only a portion of the row units 20 of the implement 10 have been shown in FIG. 1. In general, the implement 10 may include any number of row units 20, such as six, eight, twelve, sixteen, twenty-four, thirty-two, or thirty-six row units. In addition, it should be appreciated that the lateral spacing between row units 20 may be selected based on the type of crop being planted. For example, the row units 20 may be spaced approximately thirty inches from one another for planting corn, and approximately fifteen inches from one another for planting soybeans.

It should also be appreciated that the configuration of the implement 10 described above and shown in FIG. 1 is provided only to place the present subject matter in an exemplary field of use. Thus, it should be appreciated that the present subject matter may be readily adaptable to any manner of implement configuration.

Referring now to FIG. 2, a side view of one embodiment of a row unit 20 is illustrated in accordance with aspects of the present subject matter. As shown, the row unit 20 is configured as a hoe opener row unit. However, it should be appreciated that, in alternative embodiments, the row unit 20 may be configured as a disc opener row unit or any other suitable type of seed planting unit. Furthermore, it should be appreciated that, although the row unit 20 will generally be described in the context of the implement 10 shown in FIG. 1, the row unit 20 may generally be configured to be installed on any suitable seed planting implement having any suitable implement configuration.

As shown, the row unit 20 may be adjustably coupled to one of the tool frames 18 of the implement 10 by a suitable linkage assembly 22. For example, in one embodiment, the linkage assembly 22 may include a mounting bracket 24 coupled to the tool frame 18. Furthermore, the linkage assembly 22 may include first and second linkage members 26, 28. One end of each linkage member 26, 28 may be pivotably coupled to the mounting bracket 24, while an opposed end of each linkage member 26, 28 may be pivotally coupled to a support member 30 of the row unit 20. In this regard, the linkage assembly 22 may form a four bar linkage with the support member 30 that permits relative pivotable movement between the row unit 20 and the associated tool frame 18. However, it should be appreciated that, in alternative embodiments, the row unit 20 may be adjustably coupled to the tool frame 18 or the toolbar 12 via any other suitable linkage assembly. Furthermore, it should be appreciated that, in further embodiments the linkage assembly 22 may couple the row unit 20 directly to the toolbar 12.

Furthermore, the support member 30 may be configured to support one or more components of the row unit 20. For example, in several embodiments, a ground engaging shank 32 may be mounted or otherwise supported on support member 22. As shown, the shank 32 may include an opener 34 configured to excavate a furrow or trench in the soil as the implement 10 moves in the direction of travel 12 to facilitate deposition of a flowable granular or particulate-type agricultural product, such as seed, fertilizer, and/or the like. Moreover, the row unit 20 may include a packer wheel 36 configured to roll along the soil and close the furrow after deposition of the agricultural product. In one embodiment, the packer wheel 36 may be coupled to the support member 30 by an arm 38. It should be appreciated that, in alternative embodiments, any other suitable component(s) may be supported on or otherwise coupled to the support member 30. For example, the row unit 20 may include a ground engaging disc opener (not shown) in lieu of the ground engaging shank 32.

Additionally, in several embodiments, a fluid-driven actuator 102 of the implement 10 may be configured to adjust the position of one or more components of the row unit 20 relative to the tool frame 18. For example, in one embodiment, a rod 104 of the actuator 102 may be coupled to the shank 32 (e.g., the end of the shank 32 opposed from the opener 34), while a cylinder 106 of the actuator 102 may be coupled to the mounting bracket 24. As such, the rod 104 may be configured to extend and/or retract relative to the cylinder 106 to adjust the position of the shank 32 relative to the tool frame 18, which, in turn, adjusts the force being applied to the shank 32. However, it should be appreciated that, in alternative embodiments, the rod 104 may be coupled to the mounting bracket 24, while the cylinder 106 may be coupled to the shank 32. Furthermore, it should be appreciated that, in further embodiments, the actuator 102 may be coupled to any other suitable component of the row unit 20 and/or directly to the toolbar 12.

Moreover, it should be appreciated that the configuration of the row unit 20 described above and shown in FIG. 2 is provided only to place the present subject matter in an exemplary field of use. Thus, it should be appreciated that the present subject matter may be readily adaptable to any manner of seed planting unit configuration.

Referring now to FIG. 3, a schematic view of one embodiment of a system 100 for controlling the operation of an actuator mounted on a seed planting implement is illustrated in accordance with aspects of the present subject matter. In general, the system 100 will be described herein with reference to the seed planting implement 10 and the row unit 20 described above with reference to FIGS. 1 and 2. However, it should be appreciated by those of ordinary skill in the art that the disclosed system 100 may generally be utilized with seed planting implements having any other suitable implement configuration and/or seed planting units having any other suitable unit configuration.

As shown in FIG. 3, the system 100 may include a fluid-driven actuator, such as the actuator 102 of the row unit 20 described above with reference to FIG. 2. As shown, the actuator 102 may correspond to a hydraulic actuator. Thus, in several embodiments, the actuator 102 may include a piston 108 housed within the cylinder 106. One end of the rod 104 may be coupled to the piston 108, while an opposed end of the rod 104 may extend outwardly from the cylinder 106. Additionally, the actuator 102 may include a cap-side chamber 110 and a rod-side chamber 112 defined within the cylinder 106. As is generally understood, by regulating the pressure of the fluid supplied to one or both of the cylinder chambers 110, 112, the actuation of the rod 104 may be controlled. However, it should be appreciated that, in alternative embodiments, the actuator 102 may be configured as any other suitable type of actuator, such as a pneumatic actuator. Furthermore, it should be appreciated that, in further embodiments, the system 100 may include any other suitable number of fluid-driven actuators, such as additional actuators 102 mounted on the implement 10.

Furthermore, the system 100 may include various components configured to provide fluid (e.g., hydraulic oil) to the cylinder chambers 110, 112 of the actuator 102. For example, in several embodiments, the system 100 may include a fluid reservoir 114 and first and second fluid conduits 116, 118. As shown, a first fluid conduit 116 may extend between and fluidly couple the reservoir 114 and the rod-side chamber 112 of the actuator 102. Similarly, a second fluid conduit 118 may extend between and fluidly couple the reservoir 114 and the cap-side chamber 110 of the actuator 102. Additionally, a pump 115 and a remote switch 117 or other valve(s) may be configured to control the flow of the fluid between the reservoir 114 and the cylinder chambers 110, 112 of the actuator 102. In one embodiment, the reservoir 114, the pump 115, and the remote switch 117 may be mounted on the work vehicle (not shown) configured to tow the implement 10. However, it should be appreciated that, in alternative embodiments, the reservoir 114, the pump 115, and/or the remote switch 117 may be mounted on the implement 10. Furthermore, it should be appreciated that the system 100 may include any other suit component(s) configured to control the flow of fluid between the reservoir and the actuator 102.

In several embodiments, the system 100 may also include a flow restrictor 120 that is fluidly coupled to the cap-side chamber 110. As such, the flow restrictor 120 may be provided in series with the second fluid conduit 118. As will be described below, the flow restrictor 120 may be configured to reduce the flow rate of the fluid exiting the cap-side chamber 110 in a manner that provides damping to one or more components of the implement 10. However, it should be appreciated that, in alternative embodiments, the flow restrictor 120 may be fluidly coupled to the rod-side chamber 120 such that the flow restrictor 120 is provided in series with the first fluid conduit 116.

Additionally, in several embodiments, the system 100 may include a check valve 122 that is fluidly coupled to the cap-side chamber 110 and provided in series with the second fluid conduit 118. As shown, the check valve 122 may be fluidly coupled to the flow restrictor 120 in parallel. In this regard, the check valve 122 may be provided in series with a first branch 124 of the second fluid conduit 118, while the flow restrictor 120 may be provided in series with a second branch 126 of the second fluid conduit 118. As such, the check valve 122 may be configured to allow the fluid to flow through the first branch 124 of the second fluid conduit 118 from the reservoir 114 to the cap-side chamber 110. However, the check valve 122 may be configured to occlude or prevent the fluid from flowing through the first branch 124 of the second fluid conduit 118 from the cap-side chamber 110 to the reservoir 114. In this regard, the check valve 122 directs all of the fluid exiting the cap-side chamber 110 into the flow restrictor 120. Conversely, the check valve 122 permits the fluid flowing to the cap-side chamber 110 to bypass the flow restrictor 120. As will be described below, such configuration facilitates damping of one or more components of the implement 10. However, it should be appreciated that, in alternative embodiments, the check valve 122 may be fluidly coupled to the rod-side chamber 112 in combination with the flow restrictor 120 such that the check valve 122 is provided in series with the first fluid conduit 116.

As indicated above, the system 100 may generally be configured to provide viscous damping to one or more components of the implement 10. For example, when a ground engaging tool of the implement 10, such as the shank 32, contacts a rock or other impediment in the soil, the corresponding row unit 20 may pivot relative to the corresponding tool frame 18 and/or the toolbar 12 against the down pressure load applied to the row unit 20 by the corresponding actuator 102. In several embodiments, such movement may cause the rod 104 of the actuator 102 to retract into the cylinder 106, thereby moving the piston 108 in a manner that decreases the volume of the cap-side chamber 110. In such instances, some of the fluid present within the cap-side chamber 110 may exit and flow into the second fluid conduit 118 toward the reservoir 114. The check valve 122 may prevent the fluid exiting the cap-side chamber 110 from flowing through the first branch 124 of the second fluid conduit 118. As such, all fluid exiting the cap-side chamber 110 may be directed into the second branch 126 and through the flow restrictor 120. As indicated above, the flow restrictor 120 reduces or limits the rate at which the fluid may flow through the second fluid conduit 118 so as to reduce the rate at which the fluid may exit the cap-side chamber 110. In this regard, the speed at which and/or the amount that the rod 104 retracts into the cylinder 106 when the shank 32 contacts a soil impediment may be reduced (e.g., because of the reduced rate at which the fluid is discharged from the cap-side chamber 110), thereby damping the movement of the row unit 20 relative to the corresponding tool frame 18 and/or the toolbar 12. Furthermore, after the initial retraction of the rod 104 into the cylinder 106, the piston 108 may then move in a manner that increases the volume of the cap-side chamber 110, thereby extending the rod 104 from the cylinder 106. In such instances, fluid present within the reservoir 114 and the second fluid conduit 118 may be drawn back into the cap-side chamber 110. As indicated above, the check valve 122 may permit the fluid within the second fluid conduit 118 to bypass the flow restrictor 120 and flow unobstructed through the first branch 124, thereby maximizing the rate at which the fluid returns to the cap-side chamber 110. Increasing the rate at which the fluid returns to the cap-side chamber 110 may decrease the time that the row unit 20 is displaced relative to the tool frame 18, thereby further damping of the row unit 20 relative to the corresponding tool frame 18 and/or the toolbar 12.

Referring now to FIG. 4, a cross-sectional view of one embodiment of the flow restrictor 120 is illustrated in accordance with aspects of the present subject matter. For example, in the illustrated embodiment, the flow restrictor 120 may include a restrictor body 128 coupled to the second branch 126 of the second fluid conduit 118, with the restrictor body 128, in turn, defining a fluid passage 130 extending therethrough. Furthermore, the flow restrictor 120 may include an orifice plate 132 extending inward from the restrictor body 128 into the fluid passage 130. As shown, the orifice plate 132 may define a central aperture or throat 134 extending therethrough. In general, the size (e.g., the area, diameter, etc.) of the throat 134 may be smaller than the size of the fluid passage 130 so as to reduce the flow rate of the fluid through the flow restrictor 120. It should be appreciated that, in the illustrated embodiment, the throat 134 has a fixed size such that the throat 134 provides a fixed or constant backpressure for a given fluid flow rate. In this regard, in such embodiment, a fixed or constant damping rate is provided by the system 100. However, it should be appreciated that, in alternative embodiments, the flow restrictor 120 may have any other suitable configuration that reduces the flow rate of the fluid flowing therethrough.

Referring now to FIG. 5, a cross-sectional view of another embodiment of the flow restrictor 120 is illustrated in accordance with aspects of the present subject matter. As shown, the flow restrictor 120 may generally be configured the same as or similar to that described above with reference to FIG. 4. For instance, the flow restrictor 120 may define the throat 134, which is configured to reduce the flow rate of the fluid through the flow restrictor 120. However, as shown in FIG. 5, unlike the above-describe embodiment, the size (e.g., the area, diameter, etc.) of the throat 134 is adjustable. For example, in such embodiment, the flow restrictor 120 may be configured as an adjustable valve 136. As shown, the valve 136 may include a valve body 138 coupled to the second branch 126 of the second fluid conduit 118, a shaft 140 rotatably coupled to the valve body 138, a disc 142 coupled to the shaft 140, and an actuator 144 (e.g., a suitable electric motor) coupled to the shaft 140. As such, the actuator 144 may be configured to rotate the shaft 140 and the disc 142 relative to the valve body 138 (e.g., as indicated by arrow 146 in FIG. 5) to change the size of the throat 134 defined between the disc 142 and the valve body 138. Although the valve 136 is configured as a butterfly valve in FIG. 5, it should be appreciated that, in alternative embodiments, the valve 136 may be configured as any other suitable type of valve or adjustable flow restrictor. For example, in one embodiment, the valve 136 may be configured as a suitable ball valve.

In accordance with aspects of the present disclosure, by adjusting the size of the throat 134, the system 100 may be able to provide variable damping rates. In general, the size of the throat 134 may be indicative of the amount of damping provided by the system 100. For example, in several embodiments, the disc 142 may be adjustable between a first position shown in FIG. 6 and a second position shown in FIG. 7. More specifically, when the disc 142 is at the first position, the throat 134 defines a first size (e.g., as indicated by arrow 148 in FIG. 6), thereby providing a first damping rate. Conversely, when the disc 142 is at the second position, the throat 134 defines a second size (e.g., as indicated by arrow 150 in FIG. 7), thereby providing a second damping rate. As shown in FIGS. 6 and 7, the first distance 148 is larger than the second distance 150. In such instance, the system 100 provides greater damping when the throat 134 is adjusted to the first size than when the throat 134 is adjusted to the second size. It should be appreciated that, in alternative embodiments, the disc 142 may be adjustable between any other suitable positions that provide any other suitable damping rates. For example, the disc 142 may be adjustable to a plurality of different positions defined between the fully opened and fully closed positions of the valve, thereby providing for a corresponding number of different damping rates. Furthermore, it should be appreciated that the disc 142 may be continuously adjustable or adjustable between various discrete positions.

Referring back to FIG. 5, a controller 152 of the system 100 may be configured to electronically control the operation of one or more components of the valve 138, such as the actuator 144. In general, the controller 152 may comprise any suitable processor-based device known in the art, such as a computing device or any suitable combination of computing devices. Thus, in several embodiments, the controller 152 may include one or more processor(s) 154 and associated memory device(s) 156 configured to perform a variety of computer-implemented functions. As used herein, the term “processor” refers not only to integrated circuits referred to in the art as being included in a computer, but also refers to a controller, a microcontroller, a microcomputer, a programmable logic controller (PLC), an application specific integrated circuit, and other programmable circuits. Additionally, the memory device(s) 156 of the controller 152 may generally comprise memory element(s) including, but not limited to, a computer readable medium (e.g., random access memory (RAM)), a computer readable non-volatile medium (e.g., a flash memory), a floppy disk, a compact disc-read only memory (CD-ROM), a magneto-optical disk (MOD), a digital versatile disc (DVD) and/or other suitable memory elements. Such memory device(s) 156 may generally be configured to store suitable computer-readable instructions that, when implemented by the processor(s) 154, configure the controller 152 to perform various computer-implemented functions. In addition, the controller 152 may also include various other suitable components, such as a communications circuit or module, one or more input/output channels, a data/control bus and/or the like.

It should be appreciated that the controller 152 may correspond to an existing controller of the implement 10 or associated work vehicle (not shown) or the controller 152 may correspond to a separate processing device. For instance, in one embodiment, the controller 152 may form all or part of a separate plug-in module that may be installed within the implement 10 or associated work vehicle to allow for the disclosed system and method to be implemented without requiring additional software to be uploaded onto existing control devices of the implement 10 or associated work vehicle.

Furthermore, in one embodiment, a user interface 158 of the system 100 may be communicatively coupled to the controller 152 via a wired or wireless connection to allow feedback signals (e.g., as indicated by dashed line 160 in FIG. 5) to be transmitted from the controller 152 to the user interface 158. More specifically, the user interface 158 may be configured to receive an input from an operator of the implement 10 or the associated work vehicle, such as an input associated with a desired damping characteristic(s) to be provided by the system 100. As such, the user interface 158 may include one or more input devices (not shown), such as touchscreens, keypads, touchpads, knobs, buttons, sliders, switches, mice, microphones, and/or the like. In addition, some embodiments of the user interface 158 may include one or more one or more feedback devices (not shown), such as display screens, speakers, warning lights, and/or the like, which are configured to communicate such feedback from the controller 152 to the operator of the implement 10. However, in alternative embodiments, the user interface 158 may have any suitable configuration.

Moreover, in one embodiment, one or more sensors 162 of the system 100 may be communicatively coupled to the controller 152 via a wired or wireless connection to allow sensor data (e.g., as indicated by dashed line 164 in FIG. 5) to be transmitted from the sensor(s) 162 to the controller 152. For example, in one embodiment, the sensor(s) 162 may include a location sensor, such as a GNSS-based sensor, that is configured to detect a parameter associated with the location of the implement 10 or associated work vehicle within the field. In another embodiment, the sensor(s) 162 may include a speed sensor, such as a Hall Effect sensor, that is configured to detect a parameter associated with the speed at which the implement 10 is moved across the field. However, it should be appreciated that, in alternative embodiments, the sensor(s) 162 may include any suitable sensing device(s) configured to detect any suitable operating parameter of the implement 10 and/or the associated work vehicle.

In several embodiments, the controller 152 may be configured to control the operation of the valve 136 based on the feedback signals 160 received from the user interface 158 and/or the sensor data 164 received from the sensor(s) 162. Specifically, as shown in FIG. 5, the controller 152 may be communicatively coupled to the actuator 144 of the valve 136 via a wired or wireless connection to allow control signals (e.g., indicated by dashed lines 166 in FIG. 5) to be transmitted from the controller 152 to the actuator 144. Such control signals 166 may be configured to regulate the operation of the actuator 144 to adjust the position of the disc 142 relative to the valve body 138, such as by moving the disc 142 along the direction 146 between the first position (FIG. 6) and the second position (FIG. 7). For example, the feedback signals 116 received by the controller 152 may be indicative that the operator desires to adjust the damping provided by the system 100. Furthermore, upon receipt of the sensor data 164 (e.g., data indicative of the location and/or speed of the implement 10), the controller 152 may be configured to determine that the damping rate of the system 100 should be adjusted. In either instance, the controller 152 may be configured to transmit the control signals 166 to the actuator 144, with such control signals 166 being configured to control the operation of the actuator 144 to adjust the position of the disc 142 to provide the desired damping rate. However, it should be appreciated that, in alternative embodiments, the controller 152 may be configured to control the operation of the valve 136 based on any other suitable input(s) and/or parameter(s).

Referring now to FIG. 8, a schematic view of another embodiment of the system 100 is illustrated in accordance with aspects of the present subject matter. As shown, the system 100 may generally be configured the same as or similar to that described above with reference to FIG. 3. For instance, the system 100 may include the flow restrictor 120 and the check valve 122 fluidly coupled to the cap-side chamber 110 of the actuator 102 via the second fluid conduit 118. Furthermore, the flow restrictor 120 and the check valve 122 may be fluidly coupled together in parallel. However, as shown in FIG. 8, unlike the above-describe embodiment, the check valve 122 may be configured as a pilot-operated or fluid actuated three-way valve that is fluidly coupled to the first fluid conduit 116 by a pilot conduit 168.

In general, when the row unit 20 is lifted from an operational position relative to the ground to a raised position relative to the ground, it may be desirable for fluid to exit the cap-side chamber 110 without its flow rate being limited by the flow restrictor 120. For example, permitting such fluid to bypass the flow restrictor 120 may reduce the time required to lift the row unit 20 from the operational position to the raised position. More specifically, when lifting the row unit 20 from the operational position to the raised position, a pump (not shown) may pump fluid through the first fluid conduit 116 from the reservoir 114 to the rod-side chamber 112 of the actuator 102, thereby retracting the rod 104 into the cylinder 106. This may, in turn, discharge fluid from the cap-side chamber 110 into the second fluid conduit 118. As described above, the check valve 122 may generally be configured to direct all fluid exiting the cap-side chamber 110 into the flow restrictor 120. However, in the configuration of the system 100 shown in FIG. 8, when lifting the row unit 20 to the raised position, the pilot conduit 168 supplies fluid flowing through the first fluid conduit 116 to the check valve 122. The fluid received from the pilot conduit 168 may, in turn, actuate suitable component(s) of the check valve 122 (e.g., a diaphragm(s), a spring(s), and/or the like) in a manner that causes the check valve 122 to open, thereby permitting the fluid exiting the cap-side chamber 110 to bypass the flow restrictor 120 and flow unobstructed through the check valve 122 toward the reservoir 114. Conversely, when the row unit 20 is at the operational position, the check valve 122 may be closed, thereby directing all fluid exiting the cap-side chamber 110 into the flow restrictor 120.

Referring now to FIG. 9, a schematic view of a further embodiment of the system 100 is illustrated in accordance with aspects of the present subject matter. As shown, the system 100 may generally be configured the same as or similar to that described above with reference to FIGS. 3 and 8. For instance, the system 100 may include the flow restrictor 120 and the check valve 122 fluidly coupled to the cap-side chamber 110 of the actuator 102 via the second fluid conduit 118. Furthermore, the flow restrictor 120 and the check valve 122 may be fluidly coupled together in parallel. However, as shown in FIG. 9, unlike the above-describe embodiments, the check valve 122 may be configured as an electrically actuated valve. Specifically, as shown, the controller 152 may be communicatively coupled to the check valve 122 via a wired or wireless connection to allow control signals (e.g., indicated by dashed lines 170 in FIG. 9) to be transmitted from the controller 152 to the check valve 122. In this regard, when the row unit 20 is lifted from the operational position to the raised position, the control signals 170 may be configured to instruct the check valve 122 to open in a manner that permits the fluid exiting the cap-side chamber 110 to bypass the flow restrictor 120 and flow unobstructed through the check valve 122 toward the reservoir 114. Conversely, when the row unit 20 is at the operational position, the control signals 170 may be configured to instruct the check valve 122 to close, thereby directing all fluid exiting the cap-side chamber 110 into the flow restrictor 120.

This written description uses examples to disclose the technology, including the best mode, and also to enable any person skilled in the art to practice the technology, including making and using any devices or systems and performing any incorporated methods. The patentable scope of the technology is defined by the claims, and may include other examples that occur to those skilled in the art. Such other examples are intended to be within the scope of the claims if they include structural elements that do not differ from the literal language of the claims, or if they include equivalent structural elements with insubstantial differences from the literal language of the claims.

## CLAIMS

1. A system for controlling an operation of an actuator mounted on a seed planting implement, the system comprising: a toolbar; a row unit adjustably mounted on the toolbar; a fluid-driven actuator configured to adjust a position of the row unit relative to the toolbar, the fluid-driven actuator defining first and second fluid chambers; a flow restrictor fluidly coupled to the first fluid chamber, the flow restrictor being configured to reduce a rate at which fluid is permitted to exit the first fluid chamber in a manner that provides damping to the row unit; and a valve fluidly coupled to the first fluid chamber, the valve further being fluidly coupled to the flow restrictor in a parallel relationship such that the valve is configured to permit the fluid exiting the first fluid chamber to flow through the flow restrictor and the fluid entering the first fluid chamber to bypass the flow restrictor.

2. The system of claim 1, wherein, when fluid is supplied to the second fluid chamber, the valve is configured to permit fluid exiting the first fluid chamber to bypass the flow restrictor.

3. The system of claim 1, wherein the valve is fluidly actuated.

4. The system of claim 3, further comprising: a fluid line configured to supply the fluid to the second fluid chamber, the fluid line being fluidly coupled to the valve such that, when the fluid flows through the fluid line to the second fluid chamber, the valve opens in a manner that permits the fluid exiting first fluid chamber to bypass the flow restrictor.

5. The system of claim 1, wherein the valve is electrically actuated.

6. The system of claim 1, wherein the flow restrictor defines a throat having a fixed size.

7. The system of claim 1, wherein the flow restrictor defines a throat having an adjustable size.

8. A seed planting implement, comprising: a toolbar; a plurality of row units adjustably coupled to the toolbar, each row unit including a ground engaging tool configured to form a furrow in the soil; a plurality of fluid-driven actuators, each fluid-driven actuator being coupled between the toolbar and a corresponding row unit of the plurality of row units, each fluid-driven actuator being configured to adjust a position of the corresponding row unit relative to the toolbar, each fluid-driven actuator defining first and second fluid chambers; a flow restrictor fluidly coupled to the first fluid chamber of a first fluid-driven actuator of the plurality of fluid-driven actuators, the flow restrictor being configured to reduce a rate at which fluid is permitted to exit the first fluid chamber of the first fluid-driven actuator in a manner that provides damping to the corresponding row unit; and a valve fluidly coupled to the first fluid chamber of the first fluid-driven actuator, the valve further being fluidly coupled to the flow restrictor in a parallel relationship such that the valve is configured to permit the fluid exiting the first fluid chamber to flow through the flow restrictor and the fluid entering the first fluid chamber to bypass the flow restrictor.

9. The seed planting implement of claim 8, wherein, when fluid is supplied to the second fluid chamber of the first fluid-driven actuator, the valve is configured to permit fluid exiting the first fluid chamber of the first fluid-driven actuator to bypass the flow restrictor.

10. The seed planting implement of claim 8, wherein the valve is fluidly actuated.

11. The seed planting implement of claim 10, further comprising: a fluid line configured to supply fluid to the second fluid chamber of the first fluid-driven actuator, the fluid line being fluidly coupled to the valve such that, when fluid flows through the fluid line to the second fluid chamber of the first fluid-driven actuator, the valve opens in a manner that permits the fluid exiting first fluid chamber of the first fluid-driven actuator to bypass the flow restrictor.

12. The seed planting implement of claim 8, wherein the valve is electrically actuated.

13. The seed planting implement of claim 8, wherein the flow restrictor defines a throat having a fixed size.

14. The seed planting implement of claim 8, wherein the flow restrictor defines a throat having an adjustable size.

15. A system for providing damping to a row unit of a seed planting implement, the system comprising: a toolbar; a row unit adjustably mounted on the toolbar; a fluid-driven actuator configured to adjust a position of the row unit relative to the toolbar, the fluid-driven actuator defining a fluid chamber; and a flow restrictor fluidly coupled to the fluid chamber, the flow restrictor defining an adjustable throat configured to reduce a rate at which fluid is permitted to exit the fluid chamber, the throat being adjustable between a first size configured to provide a first damping rate to the row unit and a second size configured to provide a second damping rate to the row unit, the first and second damping rates being different.

16. The system of claim 15, wherein the throat is adjustable between the first and second damping rates based on an operator input.

17. The system of claim 15, wherein the throat is adjustable between the first and second damping rates based on data received from one or more sensors on the seed planting implement.

18. The system of claim 15, further comprising: a valve fluidly coupled to the fluid chamber, the valve being configured to selectively occlude the flow of fluid such that fluid exiting the fluid chamber flows through the flow restrictor and fluid entering the fluid chamber bypasses the flow restrictor.

19. The system of claim 18, wherein the flow restrictor and the valve are fluidly coupled in a parallel relationship.

================================================
File: tests/data/groundtruth/docling_v2/lorem_ipsum.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: paragraph: Lorem ipsum dolor sit amet, cons ... quam non, sodales sem. Nulla facilisi.
  item-2 at level 1: paragraph: 
  item-3 at level 1: paragraph: Duis condimentum dui eget ullamc ... cus tempor, et tristique ante aliquet.
  item-4 at level 1: paragraph: 
  item-5 at level 1: paragraph: Maecenas id neque pharetra, elei ... ulla faucibus eu. Donec ut nisl metus.
  item-6 at level 1: paragraph: 
  item-7 at level 1: paragraph: Duis ac tellus sed turpis feugia ... pellentesque rhoncus, blandit eu nisl.
  item-8 at level 1: paragraph: 
  item-9 at level 1: paragraph: Nunc vehicula mattis erat ac con ... udin, vehicula turpis eu, tempus nibh.

================================================
File: tests/data/groundtruth/docling_v2/lorem_ipsum.docx.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "lorem_ipsum",
  "origin": {
    "mimetype": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "binary_hash": 14540608742338341240,
    "filename": "lorem_ipsum.docx"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      },
      {
        "$ref": "#/texts/1"
      },
      {
        "$ref": "#/texts/2"
      },
      {
        "$ref": "#/texts/3"
      },
      {
        "$ref": "#/texts/4"
      },
      {
        "$ref": "#/texts/5"
      },
      {
        "$ref": "#/texts/6"
      },
      {
        "$ref": "#/texts/7"
      },
      {
        "$ref": "#/texts/8"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin elit mi, fermentum vitae dolor facilisis, porttitor mollis quam. Cras quam massa, venenatis faucibus libero vel, euismod sollicitudin ipsum. Aliquam semper sapien leo, ac ultrices nibh mollis congue. Cras luctus ultrices est, ut scelerisque eros euismod ut. Curabitur ac tincidunt felis, non scelerisque lectus. Praesent sollicitudin vulputate est id consequat. Vestibulum pharetra ligula sit amet varius porttitor. Sed eros diam, gravida non varius at, scelerisque in libero. Ut auctor finibus mauris sit amet ornare. Sed facilisis leo at urna rhoncus, in facilisis arcu eleifend. Sed tincidunt lacinia fermentum. Cras non purus fringilla, semper quam non, sodales sem. Nulla facilisi.",
      "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin elit mi, fermentum vitae dolor facilisis, porttitor mollis quam. Cras quam massa, venenatis faucibus libero vel, euismod sollicitudin ipsum. Aliquam semper sapien leo, ac ultrices nibh mollis congue. Cras luctus ultrices est, ut scelerisque eros euismod ut. Curabitur ac tincidunt felis, non scelerisque lectus. Praesent sollicitudin vulputate est id consequat. Vestibulum pharetra ligula sit amet varius porttitor. Sed eros diam, gravida non varius at, scelerisque in libero. Ut auctor finibus mauris sit amet ornare. Sed facilisis leo at urna rhoncus, in facilisis arcu eleifend. Sed tincidunt lacinia fermentum. Cras non purus fringilla, semper quam non, sodales sem. Nulla facilisi."
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Duis condimentum dui eget ullamcorper maximus. Nulla tortor lectus, hendrerit at diam fermentum, euismod ornare orci. Integer ac mauris sed augue ultricies pellentesque. Etiam condimentum turpis a risus dictum, sed tempor arcu vestibulum. Quisque at venenatis tellus. Morbi id lobortis elit. In gravida metus at ornare suscipit. Donec euismod nibh sit amet commodo porttitor. Integer commodo sit amet nisi vel accumsan. Donec lacinia posuere porta. Pellentesque vulputate porta risus, vel consectetur nisl gravida sit amet. Nam scelerisque enim sodales lacus tempor, et tristique ante aliquet.",
      "text": "Duis condimentum dui eget ullamcorper maximus. Nulla tortor lectus, hendrerit at diam fermentum, euismod ornare orci. Integer ac mauris sed augue ultricies pellentesque. Etiam condimentum turpis a risus dictum, sed tempor arcu vestibulum. Quisque at venenatis tellus. Morbi id lobortis elit. In gravida metus at ornare suscipit. Donec euismod nibh sit amet commodo porttitor. Integer commodo sit amet nisi vel accumsan. Donec lacinia posuere porta. Pellentesque vulputate porta risus, vel consectetur nisl gravida sit amet. Nam scelerisque enim sodales lacus tempor, et tristique ante aliquet."
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Maecenas id neque pharetra, eleifend lectus a, vehicula sapien. Aliquam erat volutpat. Ut arcu erat, blandit id elementum at, aliquet pretium mauris. Nulla at semper orci. Nunc sed maximus metus. Duis eget tristique arcu. Phasellus fringilla augue est, ut bibendum est bibendum vitae. Nam et urna interdum, egestas velit a, consectetur metus. Pellentesque facilisis vehicula orci, eu posuere justo imperdiet non. Vestibulum tincidunt orci ac lorem consequat semper. Fusce semper sollicitudin orci, id lacinia nulla faucibus eu. Donec ut nisl metus.",
      "text": "Maecenas id neque pharetra, eleifend lectus a, vehicula sapien. Aliquam erat volutpat. Ut arcu erat, blandit id elementum at, aliquet pretium mauris. Nulla at semper orci. Nunc sed maximus metus. Duis eget tristique arcu. Phasellus fringilla augue est, ut bibendum est bibendum vitae. Nam et urna interdum, egestas velit a, consectetur metus. Pellentesque facilisis vehicula orci, eu posuere justo imperdiet non. Vestibulum tincidunt orci ac lorem consequat semper. Fusce semper sollicitudin orci, id lacinia nulla faucibus eu. Donec ut nisl metus."
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Duis ac tellus sed turpis feugiat aliquam sed vel justo. Fusce sit amet volutpat massa. Duis tristique finibus metus quis tincidunt. Etiam dapibus fringilla diam at pharetra. Vivamus dolor est, hendrerit ac ligula nec, pharetra lacinia sapien. Phasellus at malesuada orci. Maecenas est justo, mollis non ultrices ut, sagittis commodo odio. Integer viverra mauris pellentesque bibendum vestibulum. Sed eu felis mattis, efficitur justo non, finibus lorem. Phasellus viverra diam et sapien imperdiet interdum. Cras a convallis libero. Integer maximus dui vel lorem hendrerit, sit amet convallis ligula lobortis. Duis eu lacus elementum, scelerisque nunc eget, dignissim libero. Suspendisse mi quam, vehicula sit amet pellentesque rhoncus, blandit eu nisl.",
      "text": "Duis ac tellus sed turpis feugiat aliquam sed vel justo. Fusce sit amet volutpat massa. Duis tristique finibus metus quis tincidunt. Etiam dapibus fringilla diam at pharetra. Vivamus dolor est, hendrerit ac ligula nec, pharetra lacinia sapien. Phasellus at malesuada orci. Maecenas est justo, mollis non ultrices ut, sagittis commodo odio. Integer viverra mauris pellentesque bibendum vestibulum. Sed eu felis mattis, efficitur justo non, finibus lorem. Phasellus viverra diam et sapien imperdiet interdum. Cras a convallis libero. Integer maximus dui vel lorem hendrerit, sit amet convallis ligula lobortis. Duis eu lacus elementum, scelerisque nunc eget, dignissim libero. Suspendisse mi quam, vehicula sit amet pellentesque rhoncus, blandit eu nisl."
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Nunc vehicula mattis erat ac consectetur. Etiam pharetra mauris ut tempor pellentesque. Sed vel libero vitae ante tempus sagittis vel sit amet dolor. Etiam faucibus viverra sodales. Pellentesque ullamcorper magna libero, non malesuada dui bibendum quis. Donec sed dolor non sem luctus volutpat. Morbi vel diam ut urna euismod gravida a id lectus. Vestibulum vel mauris eu tellus hendrerit dapibus. Etiam scelerisque lacus vel ante ultricies vulputate. In ullamcorper malesuada justo, vel scelerisque nisl lacinia at. Donec sodales interdum ipsum, ac bibendum ipsum pharetra interdum. Vivamus condimentum ac ante vel aliquam. Ut consectetur eu nibh nec gravida. Vestibulum accumsan, purus at mollis rutrum, sapien tortor accumsan purus, vitae fermentum urna mauris ut lacus. Fusce vitae leo sollicitudin, vehicula turpis eu, tempus nibh.",
      "text": "Nunc vehicula mattis erat ac consectetur. Etiam pharetra mauris ut tempor pellentesque. Sed vel libero vitae ante tempus sagittis vel sit amet dolor. Etiam faucibus viverra sodales. Pellentesque ullamcorper magna libero, non malesuada dui bibendum quis. Donec sed dolor non sem luctus volutpat. Morbi vel diam ut urna euismod gravida a id lectus. Vestibulum vel mauris eu tellus hendrerit dapibus. Etiam scelerisque lacus vel ante ultricies vulputate. In ullamcorper malesuada justo, vel scelerisque nisl lacinia at. Donec sodales interdum ipsum, ac bibendum ipsum pharetra interdum. Vivamus condimentum ac ante vel aliquam. Ut consectetur eu nibh nec gravida. Vestibulum accumsan, purus at mollis rutrum, sapien tortor accumsan purus, vitae fermentum urna mauris ut lacus. Fusce vitae leo sollicitudin, vehicula turpis eu, tempus nibh."
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/lorem_ipsum.docx.md
================================================
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin elit mi, fermentum vitae dolor facilisis, porttitor mollis quam. Cras quam massa, venenatis faucibus libero vel, euismod sollicitudin ipsum. Aliquam semper sapien leo, ac ultrices nibh mollis congue. Cras luctus ultrices est, ut scelerisque eros euismod ut. Curabitur ac tincidunt felis, non scelerisque lectus. Praesent sollicitudin vulputate est id consequat. Vestibulum pharetra ligula sit amet varius porttitor. Sed eros diam, gravida non varius at, scelerisque in libero. Ut auctor finibus mauris sit amet ornare. Sed facilisis leo at urna rhoncus, in facilisis arcu eleifend. Sed tincidunt lacinia fermentum. Cras non purus fringilla, semper quam non, sodales sem. Nulla facilisi.

Duis condimentum dui eget ullamcorper maximus. Nulla tortor lectus, hendrerit at diam fermentum, euismod ornare orci. Integer ac mauris sed augue ultricies pellentesque. Etiam condimentum turpis a risus dictum, sed tempor arcu vestibulum. Quisque at venenatis tellus. Morbi id lobortis elit. In gravida metus at ornare suscipit. Donec euismod nibh sit amet commodo porttitor. Integer commodo sit amet nisi vel accumsan. Donec lacinia posuere porta. Pellentesque vulputate porta risus, vel consectetur nisl gravida sit amet. Nam scelerisque enim sodales lacus tempor, et tristique ante aliquet.

Maecenas id neque pharetra, eleifend lectus a, vehicula sapien. Aliquam erat volutpat. Ut arcu erat, blandit id elementum at, aliquet pretium mauris. Nulla at semper orci. Nunc sed maximus metus. Duis eget tristique arcu. Phasellus fringilla augue est, ut bibendum est bibendum vitae. Nam et urna interdum, egestas velit a, consectetur metus. Pellentesque facilisis vehicula orci, eu posuere justo imperdiet non. Vestibulum tincidunt orci ac lorem consequat semper. Fusce semper sollicitudin orci, id lacinia nulla faucibus eu. Donec ut nisl metus.

Duis ac tellus sed turpis feugiat aliquam sed vel justo. Fusce sit amet volutpat massa. Duis tristique finibus metus quis tincidunt. Etiam dapibus fringilla diam at pharetra. Vivamus dolor est, hendrerit ac ligula nec, pharetra lacinia sapien. Phasellus at malesuada orci. Maecenas est justo, mollis non ultrices ut, sagittis commodo odio. Integer viverra mauris pellentesque bibendum vestibulum. Sed eu felis mattis, efficitur justo non, finibus lorem. Phasellus viverra diam et sapien imperdiet interdum. Cras a convallis libero. Integer maximus dui vel lorem hendrerit, sit amet convallis ligula lobortis. Duis eu lacus elementum, scelerisque nunc eget, dignissim libero. Suspendisse mi quam, vehicula sit amet pellentesque rhoncus, blandit eu nisl.

Nunc vehicula mattis erat ac consectetur. Etiam pharetra mauris ut tempor pellentesque. Sed vel libero vitae ante tempus sagittis vel sit amet dolor. Etiam faucibus viverra sodales. Pellentesque ullamcorper magna libero, non malesuada dui bibendum quis. Donec sed dolor non sem luctus volutpat. Morbi vel diam ut urna euismod gravida a id lectus. Vestibulum vel mauris eu tellus hendrerit dapibus. Etiam scelerisque lacus vel ante ultricies vulputate. In ullamcorper malesuada justo, vel scelerisque nisl lacinia at. Donec sodales interdum ipsum, ac bibendum ipsum pharetra interdum. Vivamus condimentum ac ante vel aliquam. Ut consectetur eu nibh nec gravida. Vestibulum accumsan, purus at mollis rutrum, sapien tortor accumsan purus, vitae fermentum urna mauris ut lacus. Fusce vitae leo sollicitudin, vehicula turpis eu, tempus nibh.

================================================
File: tests/data/groundtruth/docling_v2/mixed.md.md
================================================
# Title

Some text

## Famous ducks

Here is a table:

| Character      | Name in German   | Name in French   | Name in Italian   |
|----------------|------------------|------------------|-------------------|
| Scrooge McDuck | Dagobert Duck    | Balthazar Picsou | Paperone          |
| Huey           | Tick             | Riri             | Qui               |
| Dewey          | Trick            | Fifi             | Quo               |
| Louie          | Track            | Loulou           | Qua               |

And here is more HTML:

Some paragraph.

Now a div — almost there...

- foo
- bar

The end!


================================================
File: tests/data/groundtruth/docling_v2/nested.md.md
================================================
# Nesting

A list featuring nesting:

- abc
    - abc123
        - abc1234
            - abc12345
                - a.
                - b.
        - abcd1234：
            - abcd12345：
                - a.
                - b.
- def：
    - def1234：
        - def12345。
- after one empty line
    - foo
- afer two empty lines
    - bar

- changing symbol

A nested HTML list:

- First item
- Second item with subitems:
    - Subitem 1
    - Subitem 2
- Last list item


================================================
File: tests/data/groundtruth/docling_v2/pa20010031492.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Assay reagent
    item-2 at level 2: section_header: ABSTRACT
      item-3 at level 3: paragraph: A cell-derived assay reagent prepared from cells which have been killed by treatment with an antibiotic selected from the bleomycin-phleomycin family of antibiotics but which retain a signal-generating metabolic activity such as bioluminescence. 
    item-4 at level 2: paragraph: This application is a continuation of PCT/GB99/01730, filed Jun. 1, 1999 designating the United States (the disclosure of which is incorporated herein by reference) and claiming priority from British application serial no. 9811845.8, filed Jun. 2, 1998.
    item-5 at level 2: paragraph: The invention relates to a cell-derived assay reagent, in particular to an assay reagent prepared from cells which have been killed but which retain a signal-generating metabolic activity such as bioluminescence and also to assay methods using the cell-derived reagent such as, for example, toxicity testing methods.
    item-6 at level 2: paragraph: The use of bacteria with a signal-generating metabolic activity as indicators of toxicity is well established. UK patent number GB 2005018 describes a method of assaying a liquid sample for toxic substances which involves contacting a suspension of bioluminescent microorganisms with a sample suspected of containing a toxic substance and observing the change in the light output of the bioluminescent organisms as a result of contact with the suspected toxic substance. Furthermore, a toxicity monitoring system embodying the same assay principle, which is manufactured and sold under the Trade Mark Microtox®, is in routine use in both environmental laboratories and for a variety of industrial applications. An improved toxicity assay method using bioluminescent bacteria, which can be used in a wider range of test conditions than the method of GB 2005018, is described in International patent application number WO 95/10767.
    item-7 at level 2: paragraph: The assay methods known in the prior art may utilize naturally occurring bioluminescent organisms, including Photobacterium phosphoreum and Vibrio fischeri. However, recent interest has focused on the use of genetically modified microorganisms which have been engineered to express bioluminescence. These genetically modified bioluminescent microorganisms usually express lux genes, encoding the enzyme luciferase, which have been cloned from a naturally occurring bioluminescent microorganism (E. A. Meighen (1994) Genetics of Bacterial Bioluminescence. Ann. Rev. Genet. 28: 117-139; Stewart, G. S. A. B. Jassin, S. A. A. and Denyer, S. P. (1993), Engineering Microbial bioluminescence and biosensor applications. In Molecular Diagnosis. Eds R. Rapley and M. R. Walker Blackwell Scientific Pubs/Oxford). A process for producing genetically modified bioluminescent microorganisms expressing lux genes cloned from Vibrio harveyi is described in U.S. Pat. No. 4,581,335.
    item-8 at level 2: paragraph: The use of genetically modified bioluminescent microorganisms in toxicity testing applications has several advantages over the use of naturally occurring microorganisms. For example, it is possible to engineer microorganisms with different sensitivities to a range of different toxic substances or to a single toxic substance. However, genetically modified microorganisms are subject to marketing restrictions as a result of government legislation and there is major concern relating to the deliberate release of genetically modified microorganisms into the environment as components of commercial products. This is particularly relevant with regard to toxicity testing which is often performed in the field rather than within the laboratory. The potential risk from release of potentially pathogenic genetically modified microorganisms into the environment where they may continue to grow in an uncontrollable manner has led to the introduction of legal restrictions on the use of genetically modified organisms in the field in many countries.
    item-9 at level 2: paragraph: It has been suggested, to avoid the problems discussed above, to use genetically modified bioluminescent microorganisms which have been treated so that they retain the metabolic function of bioluminescence but an no longer reproduce. The use of radiation (gamma-radiation), X-rays or an electron beam) to kill bioluminescent cells whilst retaining the metabolic function of bioluminescence is demonstrated in International patent application number WO 95/07346. It is an object of the present invention to provide an alternative method of killing bioluminescent cells whilst retaining the metabolic function of bioluminescence which does not require the use of radiation and, as such, can be easily carried out without the need for specialized radiation equipment and containment facilities and without the risk to laboratory personnel associated with the use of radiation.
    item-10 at level 2: paragraph: Accordingly, in a first aspect the invention provides a method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of cells with signal-generating metabolic activity with a member of the bleomycin/phleomycin family of antibiotics.
    item-11 at level 2: paragraph: Bleomycin and phleomycin are closely related glycopeptide antibiotics that are isolated in the form of copper chelates from cultures of Streptomyces verticillus. They represent a group of proteins with molecular weights ranging from 1000 to 1000 kda that are potent antibiotics and anti-tumour agents. So far more than 200 members of the bleomycin/phleomycin family have been isolated and characterised as complex basic glycopeptides. Family members resemble each other with respect to their physicochemical properties and their structure, indicating that functionally they all behave in the same manner. Furthermore, the chemical structure of the active moiety is conserved between family members and consists of 5 amino acids, L-glucose, 3-O-carbamoyl-D-mannose and a terminal cation. The various different bleomycin/phleomycin family members differ from each other in the nature of the terminal cation moiety, which is usually an amine. A preferred bleomycin/phleomycin antibiotic for use in the method of the invention is phleomycin D1, sold under the trade name Zeocin™.
    item-12 at level 2: paragraph: Bleomycin and phleomycin are strong, selective inhibitors of DNA synthesis in intact bacteria and in mammalian cells. Bleomycin can be observed to attack purified DNA in vitro when incubated under appropriate conditions and analysis of the bleomycin damaged DNA shows that both single-stranded and double-stranded cleavages occur, the latter being the result of staggered single strand breaks formed approximately two base pairs apart in the complementary strands.
    item-13 at level 2: paragraph: In in vivo systems, after being taken up by the cell, bleomycin enters the cell nucleus, binds to DNA (by virtue of the interaction between its positively charged terminal amine moiety and a negatively charged phosphate group of the DNA backbone) and causes strand scission. Bleomycin causes strand scission of DNA in viruses, bacteria and eukaryotic cell systems.
    item-14 at level 2: paragraph: The present inventors have surprisingly found that treatment of a culture of cells with signal-generating metabolic activity with a bleomycin/phleomycin antibiotic renders the culture non-viable whilst retaining a level of signal-generating metabolic activity suitable for use in toxicity testing applications. In the context of this application the term non-viable is taken to mean that the cells are unable to reproduce. The process of rendering cells non-viable whilst retaining signal-generating metabolic activity may hereinafter be referred to as ‘inactivation’ and cells which have been rendered non-viable according to the method of the invention may be referred to as ‘inactivated’.
    item-15 at level 2: paragraph: Because of the broad spectrum of action of the bleomycin/phleomycin family of antibiotics the method of the invention is equally applicable to bacterial cells and to eukaryotic cells with signal generating metabolic activity. Preferably the signal-generating metabolic activity is bioluminescence but other signal-generating metabolic activities which are reporters of toxic damage could be used with equivalent effect.
    item-16 at level 2: paragraph: The method of the invention is preferred for use with bacteria or eukaryotic cells that have been genetically modified to express a signal-generating metabolic activity. The examples given below relate to E. coil which have been engineered to express bioluminescence by transformation with a plasmid carrying lux genes. The eukaryotic equivalent would be cells transfected with a vector containing nucleic acid encoding a eukaryotic luciferase enzyme (abbreviated luc) such as, for example, luciferase from the firefly Photinus pyralis. A suitable plasmid vector containing cDNA encoding firefly luciferase under the control of an SV40 viral promoter is available from Promega Corporation, Madison Wis., USA. However, in connection with the present invention it is advantageous to use recombinant cells containing the entire eukaryotic luc operon so as to avoid the need to add an exogenous substrate ( e.g. luciferin) in order to generate light output.
    item-17 at level 2: paragraph: The optimum concentration of bleomycin/phleomycin antibiotic and contact time required to render a culture of cells non-viable whilst retaining a useful level of signal-generating metabolic activity may vary according to the cell type but can be readily determined by routine experiment. In general, the lower the concentration of antibiotic used the longer the contact time required for cell inactivation. In connection with the production of assay reagents for use in toxicity testing applications, it is generally advantageous to keep the concentration of antibiotic low (e.g. around 1-1.5 mg/ml) and increase the contact time for inactivation. As will be shown in Example 1, treatment with Zeocin™ at a concentration of 1.5 mg/ml for 3 to 5 hours is sufficient to completely inactivate a culture of recombinant E. coli.
    item-18 at level 2: paragraph: In the case of bacteria, the contact time required to inactivate a culture of bacterial cells is found to vary according to the stage of growth of the bacterial culture at the time the antibiotic is administered. Although the method of the invention can be used on bacteria at all stages of growth it is generally preferable to perform the method on bacterial cells in an exponential growth phase because the optimum antibiotic contact time has been observed to be shortest when the antibiotic is administered to bacterial cells in an exponential growth phase.
    item-19 at level 2: paragraph: Following treatment with bleomycin/phleomycin antibiotic the non-viable preparation of cells is preferably stabilised for ease of storage or shipment. The cells can be stabilised using known techniques such as, for example, freeze drying (lyophilization) or other cell preservation techniques known in the art. Stabilization by freeze drying has the added advantage that the freeze drying procedure itself can render cells non-viable. Thus, any cells in the preparation which remain viable after treatment of the culture with bleomycin/phleomycin antibiotic will be rendered non-viable by freeze drying. It is thought that freeze drying inactivates any remaining viable cells by enhancing the effect of antibiotic, such that sub-lethally injured cells in the culture are more sensitive to the stresses applied during freeze drying.
    item-20 at level 2: paragraph: Prior to use the stabilised cell preparation is reconstituted using a reconstitution buffer to form an assay reagent. This reconstituted assay reagent may then be used directly in assays for analytes, for example in toxicity testing applications. It is preferable that the stabilised (i.e. freeze dried) assay reagent be reconstituted immediately prior to use, but after reconstitution it is generally necessary to allow sufficient time prior to use for the reconstituted reagent to reach a stable, high level of signal-generating activity. Suitable reconstitution buffers preferably contain an osmotically potent non-salt compound such as sucrose, dextran or polyethylene glycol, although salt based stabilisers may also be used.
    item-21 at level 2: paragraph: Whilst the assay reagent of the invention is particularly suitable for use in toxicity testing applications it is to be understood that the invention is not limited to assay reagents for use in toxicity testing. The cell inactivation method of the invention can be used to inactivate any recombinant cells (prokaryotic or eukaryotic) with a signal generating metabolic activity that is not dependent upon cell viability.
    item-22 at level 2: paragraph: In a further aspect the invention provides a method of assaying a potentially toxic analyte comprising the steps of,
    item-23 at level 2: paragraph: (a) contacting a sample to be assayed for the analyte with a sample of assay reagent comprising a non-viable preparation of cells with a signal-generating metabolic activity;
    item-24 at level 2: paragraph: (b) measuring the level of signal generated; and
    item-25 at level 2: paragraph: (c) using the measurement obtained as an indicator of the toxicity of the analyte.
    item-26 at level 2: paragraph: In a still further aspect, the invention provides a kit for performing the above-stated assay comprising an assay reagent with signal generating metabolic activity and means for contacting the assay reagent with a sample to be assayed for an analyte.
    item-27 at level 2: paragraph: The analytes tested using the assay of the invention are usually toxic substances, but it is to be understood that the precise nature of the analyte to be tested is not material to the invention.
    item-28 at level 2: paragraph: Toxicity is a general term used to describe an adverse effect on biological system and the term ‘toxic substances’ includes both toxicants (synthetic chemicals that are toxic) and toxins (natural poisons). Toxicity is usually expressed as an effective concentration (EC) or inhibitory concentration (IC) value. The EC/IC value is usually denoted as a percentage response e.g. EC₅₀, EC₁₀ which denotes the concentration (dose) of a particular substance which affects the designated criteria for assessing toxicity (i.e. a behavioural trait or death) in the indicated proportion of the population tested. For example, an EC₅₀ of 10 ppm indicates that 50% of the population will be affected by a concentration of 10 ppm. In the case of a toxicity assay based on the use of a bioluminescent assay reagent, the EC₅₀ value is usually the concentration of sample substance causing a 50% change in light output.
    item-29 at level 2: paragraph: The present invention will be further understood by way of the following Examples with reference to the accompanying Figures in which:
    item-30 at level 2: paragraph: FIG. 1 is a graph to show the effect of Zeocin™ treatment on viable count and light output of recombinant bioluminescent E. coil cells.
    item-31 at level 2: paragraph: FIG. 2 is a graph to show the light output from five separate vials of reconstituted assay reagent. The assay reagent was prepared from recombinant bioluminescent E. coil exposed to 1.5 mg/ml Zeocin™ for 300 minutes. Five vials were used to reduce discrepancies resulting from vial to vial variation.
    item-32 at level 2: paragraph: FIGS. 3 to 8 are graphs to show the effect of Zeocin™ treatment on the sensitivity of bioluminescent assay reagent to toxicant (ZnSO₄):
    item-33 at level 2: paragraph: FIG. 3: Control cells, lag phase.
    item-34 at level 2: paragraph: FIG. 4: Zeocin™ treated cells, lag phase.
    item-35 at level 2: paragraph: FIG. 5: Control cells, mid-exponential growth.
    item-36 at level 2: paragraph: FIG. 6: Zeocin™ treated cells, mid-exponential growth.
    item-37 at level 2: paragraph: FIG. 7: Control cells, stationary phase.
    item-38 at level 2: paragraph: FIG. 8: Zeocin™ treated cells, stationary phase.
    item-39 at level 2: section_header: EXAMPLE 1
    item-40 at level 2: section_header: (A) Inactivation of Bioluminescent E. coil Method
      item-41 at level 3: paragraph: 1. Bioluminescent genetically modified E. coil strain HB101 (E. coli HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of Vibrio fischeri constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) were grown from a frozen stock in 5 ml of low salt medium (LB (5 g/ml NaCl)+glycerol+MgSO₄) for 24 hours.
      item-42 at level 3: paragraph: 2. 1 ml of the 5 ml culture was then used to inoculate 200 ml of low salt medium in a shaker flask and the resultant culture grown to an OD₆₃₀ of 0.407 (exponential growth phase).
      item-43 at level 3: paragraph: 3. 50 ml of this culture was removed to a fresh sterile shaker flask (control cells).
      item-44 at level 3: paragraph: 4. Zeocin™ was added to the 150 ml of culture in the original shaker flash, to a final concentration of 1.5 mg/ml. At the same time, an equivalent volume of water was added to the 50 ml culture removed from the original flask (control cells).
      item-45 at level 3: paragraph: 5. The time course of cell inactivation was monitored by removing samples from the culture at 5, 60, 120, 180, 240 and 300 minutes after the addition of Zeocin™ and taking measurements of both light output (measured using a Deltatox luminometer) and viable count (per ml, determined using the method given in Example 3 below) for each of the samples. Samples of the control cells were removed at 5 and 300 minutes after the addition of water and measurements of light output and viable count taken as for the Zeocin™ treated cells.
      item-46 at level 3: paragraph: FIG. 1 shows the effect of Zeocin™ treatment on the light output and viable count (per ml) of recombinant bioluminescent E. coil. Zeocin™ was added to a final concentration of 1.5 mg/ml at time zero. The number of viable cells in the culture was observed to decrease with increasing contact cells with Zeocin™, the culture being completely inactivated after 3 hours. The light output from the culture was observed to decrease gradually with increasing Zeocin™ contact time.
    item-47 at level 2: section_header: (B) Production of Assay Reagent
      item-48 at level 3: paragraph: Five hours after the addition of Zeocin™ or water the remaining bacterial cells in the Zeocin™ treated and control cultures were harvested by the centrifugation, washed (to remove traces of Zeocin™ from the Zeocin™ treated culture), re-centrifuged and resuspended in cryoprotectant to an OD₆₃₀ of 0.25. 200 μl aliquots of the cells in cryoprotectant were dispensed into single shot vials, and freeze dried. Freeze dried samples of the Zeocin™ treated cells and control cells were reconstituted in 0.2M sucrose to form assay reagents and the light output of the assay reagents measured at various times after reconstitution.
      item-49 at level 3: paragraph: The light output from assay reagent prepared from cells exposed to 1.5 mg/ml Zeocin™ for 5 hours was not significantly different to the light output from assay reagent prepared from control (Zeocin™ untreated) cells, indicating that Zeocin™ treatment does not affect the light output of the reconstituted freeze dried assay reagent. Both Zeocin™ treated and Zeocin™ untreated assay reagents produced stable light output 15 minutes after reconstitution.
      item-50 at level 3: paragraph: FIG. 2 shows the light output from five separate vials of reconstituted Zeocin™ treated assay reagent inactivated according to the method of Example 1(A) and processed into assay reagent as described in Example 1(B). Reconstitution solution was added at time zero and thereafter light output was observed to increase steadily before stabilising out at around 15 minutes after reconstitution. All five vials were observed to give similar light profiles after reconstitution.
    item-51 at level 2: section_header: EXAMPLE 2
    item-52 at level 2: section_header: Sensitivity of Zeocin™ Treated Assay Reagent to Toxicant Method
      item-53 at level 3: paragraph: 1. Bioluminescent genetically modified E. coil strain HB101 (E. coli HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of vibrio fischeri constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) was grown in fermenter as a batch culture in low salt medium (LB(5 g/ml NaCl)+glycerol+MgSO₄).
      item-54 at level 3: paragraph: 2. Two aliquots of the culture were removed from the fermenter into separate sterile shaker flasks at each of three different stages of growth i.e. at OD₆₃₀ values of 0.038 (lag phase growth), 1.31 (mid-exponential phase growth) and 2.468 (stationary phase growth).
      item-55 at level 3: paragraph: 3. One aliquot of culture for each of the three growth stages was inactivated by contact with Zeocin™ (1 mg Zeocin™ added per 2.5×10⁶ cells, i.e. the concentration of Zeocin™ per cell is kept constant) for 300 minutes and then processed into assay reagent by freeze drying and reconstitution, as described in part (B) of Example 1.
      item-56 at level 3: paragraph: 4. An equal volume of water was added to the second aliquot of culture for each of the three growth stages and the cultures processed into assay reagent as described above.
      item-57 at level 3: paragraph: 5. Samples of each of the three Zeocin™ treated and three control assay reagents were then evaluated for sensitivity to toxicant (ZnSO₄) according to the following assay protocol:
      item-58 at level 3: paragraph: ZnSO₄ Sensitivity Assay
      item-59 at level 3: paragraph: 1. ZnSO₄ solutions were prepared in pure water at 30, 10, 3, 1, 0.3 and 0.1 ppm. Pure water was also used as a control.
      item-60 at level 3: paragraph: 2. Seven vials of each of the three Zeocin™ treated and each of the three control assay reagents (i.e. one for each of the six ZnSO₄ solutions and one for the pure water control) were reconstituted using 0.5 ml of reconstitution solution (eg 0.2M sucrose) and then left to stand at room temperature for 15 minutes to allow the light output to stabilize. Base line (time zero) readings of light output were then measured for each of the reconstituted reagents.
      item-61 at level 3: paragraph: 3. 0.5 ml aliquots of each of the six ZnSO₄ solutions and the pure water control were added to separate vials of reconstituted assay reagent. This was repeated for each of the different Zeocin™ treated and control assay reagents.
      item-62 at level 3: paragraph: 4. The vials were incubated at room temperature and light output readings were taken 5, 10, 15, 20, 25 and 30 minutes after addition of ZnSO₄ solution.
      item-63 at level 3: paragraph: 5. The % toxic effect for each sample was calculated as follows:
      item-64 at level 3: paragraph: where: Cₒ=light in control at time zero
      item-65 at level 3: paragraph: Ct=light in control at reading time
      item-66 at level 3: paragraph: Sₒ=light in sample at time zero
      item-67 at level 3: paragraph: St=light in sample at reading time
      item-68 at level 3: paragraph: The results of toxicity assays for sensitivity to ZnSO₄ for all the Zeocin™ treated and control assay reagents are shown in FIGS. 3 to 8:
      item-69 at level 3: paragraph: FIG. 3: Control cells, lag phase.
      item-70 at level 3: paragraph: FIG. 4: Zeocin™ treated cells, lag phase.
      item-71 at level 3: paragraph: FIG. 5: Control cells, mid-exponential growth.
      item-72 at level 3: paragraph: FIG. 6: Zeocin™ treated cells, mid-exponential growth.
      item-73 at level 3: paragraph: FIG. 7: Control cells, stationary phase.
      item-74 at level 3: paragraph: FIG. 8: Zeocin™ treated cells, stationary phase.
      item-75 at level 3: table with [6x3]
      item-76 at level 3: paragraph: In each case, separate graphs of % toxic effect against log₁₀ concentration of ZnSO₄ were plotted on the same axes for each value of time (minutes) after addition of Zeocin™ or water. The sensitivities of the various reagents, expressed as an EC₅₀ value for 15 minutes exposed to ZnSO₄, are summarised in Table 1 below.
      item-77 at level 3: paragraph: Table 1: Sensitivity of the different assay reagents to ZnSo₄ expressed as EC₅₀ values for 15 minutes exposure to ZNSO₄.
      item-78 at level 3: paragraph: The results of the toxicity assays indicate that Zeocin™ treatment does not significantly affect the sensitivity of a recombinant bioluminescent E. coli derived assay reagent to ZnSO₄. Similar results could be expected with other toxic substances which have an effect on signal-generating metabolic activities.
    item-79 at level 2: section_header: EXAMPLE 3
    item-80 at level 2: section_header: Method to Determine Viable Count
      item-81 at level 3: paragraph: 1. Samples of bacterial culture to be assayed for viable count were centrifuged at 10,000 rpm for 5 minutes to pellet the bacterial cells.
      item-82 at level 3: paragraph: 2. Bacterial cells were washed by resuspending in 1 ml of M9 medium, re-centrifuged at 10,000 rpm for 5 minutes and finally re-suspended in 1 ml of M9 medium.
      item-83 at level 3: paragraph: 3. Serial dilutions of the bacterial cell suspension from 10⁻¹ to 10⁻⁷ were prepared in M9 medium.
      item-84 at level 3: paragraph: 4. Three separate 10 μl aliquots of each of the serial dilutions were plated out on standard agar plates and the plates incubated at 37° C.
      item-85 at level 3: paragraph: 5. The number of bacterial colonies present for each of the three aliquots at each of the serial dilutions were counted and the values averaged. Viable count was calculated per ml of bacterial culture.
    item-86 at level 2: section_header: CLAIMS
      item-87 at level 3: paragraph: 1. A method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of said cells having signal-generating metabolic activity with an antibiotic selected from the bleomycin/phleomycin family of antibiotics.
      item-88 at level 3: paragraph: 2. The method as claimed in claim 1 wherein following contact with antibiotic, said cells are subjected to a stabilization step.
      item-89 at level 3: paragraph: 3. The method as claimed in claim 2 wherein said stabilization step comprises freeze drying.
      item-90 at level 3: paragraph: 4. The method as claimed in claim 1 wherein said antibiotic is phleomycin D1.
      item-91 at level 3: paragraph: 5. The method as claimed in claim 5 wherein said signal-generating metabolic activity is bioluminescence.
      item-92 at level 3: paragraph: 6. The method as claimed in claim 5 wherein said cells are bacteria.
      item-93 at level 3: paragraph: 7. The method as claimed in claim 6 wherein said bacteria are in an exponential growth phase when contacted with said antibiotic.
      item-94 at level 3: paragraph: 8. The method as claimed in claim 6 wherein said bacteria are genetically modified.
      item-95 at level 3: paragraph: 9. The method as claimed in claim 8 wherein said genetically modified bacteria contain nucleic acid encoding luciferase.
      item-96 at level 3: paragraph: 10. The method as claimed in claim 9 wherein said bacteria are E. coli.
      item-97 at level 3: paragraph: 11. The method as claimed in claim 5 wherein said cells are eukaryotic cells.
      item-98 at level 3: paragraph: 12. The method as claimed in claim 11 wherein said eukaryotic cells are genetically modified.
      item-99 at level 3: paragraph: 13. The method as claimed in claim 12 wherein said genetically modified eukaryotic cells contain nucleic acid encoding luciferase.
      item-100 at level 3: paragraph: 14. A method of making a non-viable preparation of prokaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of a genetically modified E. coli strain made bioluminescent by transformation with a plasmid carrying the lux operon of Vibrio fischeri with an antibiotic selected from the bleomycin/phleomycin family of antibiotics.
      item-101 at level 3: paragraph: 15. The method as claimed in claim 14 wherein said cells are contacted with phleomycin D1 at a concentration of at least about 1.5 mg/ml.
      item-102 at level 3: paragraph: 16. The method as claimed in claim 15 wherein said contact is maintained for at least about 3 hours.
      item-103 at level 3: paragraph: 17. The method as claimed in claim 16 wherein said antibiotic-treated cells are harvested, washed and freeze-dried.
  item-104 at level 1: section_header: Drawings

================================================
File: tests/data/groundtruth/docling_v2/pa20010031492.md
================================================
# Assay reagent

## ABSTRACT

A cell-derived assay reagent prepared from cells which have been killed by treatment with an antibiotic selected from the bleomycin-phleomycin family of antibiotics but which retain a signal-generating metabolic activity such as bioluminescence. 

This application is a continuation of PCT/GB99/01730, filed Jun. 1, 1999 designating the United States (the disclosure of which is incorporated herein by reference) and claiming priority from British application serial no. 9811845.8, filed Jun. 2, 1998.

The invention relates to a cell-derived assay reagent, in particular to an assay reagent prepared from cells which have been killed but which retain a signal-generating metabolic activity such as bioluminescence and also to assay methods using the cell-derived reagent such as, for example, toxicity testing methods.

The use of bacteria with a signal-generating metabolic activity as indicators of toxicity is well established. UK patent number GB 2005018 describes a method of assaying a liquid sample for toxic substances which involves contacting a suspension of bioluminescent microorganisms with a sample suspected of containing a toxic substance and observing the change in the light output of the bioluminescent organisms as a result of contact with the suspected toxic substance. Furthermore, a toxicity monitoring system embodying the same assay principle, which is manufactured and sold under the Trade Mark Microtox®, is in routine use in both environmental laboratories and for a variety of industrial applications. An improved toxicity assay method using bioluminescent bacteria, which can be used in a wider range of test conditions than the method of GB 2005018, is described in International patent application number WO 95/10767.

The assay methods known in the prior art may utilize naturally occurring bioluminescent organisms, including Photobacterium phosphoreum and Vibrio fischeri. However, recent interest has focused on the use of genetically modified microorganisms which have been engineered to express bioluminescence. These genetically modified bioluminescent microorganisms usually express lux genes, encoding the enzyme luciferase, which have been cloned from a naturally occurring bioluminescent microorganism (E. A. Meighen (1994) Genetics of Bacterial Bioluminescence. Ann. Rev. Genet. 28: 117-139; Stewart, G. S. A. B. Jassin, S. A. A. and Denyer, S. P. (1993), Engineering Microbial bioluminescence and biosensor applications. In Molecular Diagnosis. Eds R. Rapley and M. R. Walker Blackwell Scientific Pubs/Oxford). A process for producing genetically modified bioluminescent microorganisms expressing lux genes cloned from Vibrio harveyi is described in U.S. Pat. No. 4,581,335.

The use of genetically modified bioluminescent microorganisms in toxicity testing applications has several advantages over the use of naturally occurring microorganisms. For example, it is possible to engineer microorganisms with different sensitivities to a range of different toxic substances or to a single toxic substance. However, genetically modified microorganisms are subject to marketing restrictions as a result of government legislation and there is major concern relating to the deliberate release of genetically modified microorganisms into the environment as components of commercial products. This is particularly relevant with regard to toxicity testing which is often performed in the field rather than within the laboratory. The potential risk from release of potentially pathogenic genetically modified microorganisms into the environment where they may continue to grow in an uncontrollable manner has led to the introduction of legal restrictions on the use of genetically modified organisms in the field in many countries.

It has been suggested, to avoid the problems discussed above, to use genetically modified bioluminescent microorganisms which have been treated so that they retain the metabolic function of bioluminescence but an no longer reproduce. The use of radiation (gamma-radiation), X-rays or an electron beam) to kill bioluminescent cells whilst retaining the metabolic function of bioluminescence is demonstrated in International patent application number WO 95/07346. It is an object of the present invention to provide an alternative method of killing bioluminescent cells whilst retaining the metabolic function of bioluminescence which does not require the use of radiation and, as such, can be easily carried out without the need for specialized radiation equipment and containment facilities and without the risk to laboratory personnel associated with the use of radiation.

Accordingly, in a first aspect the invention provides a method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of cells with signal-generating metabolic activity with a member of the bleomycin/phleomycin family of antibiotics.

Bleomycin and phleomycin are closely related glycopeptide antibiotics that are isolated in the form of copper chelates from cultures of Streptomyces verticillus. They represent a group of proteins with molecular weights ranging from 1000 to 1000 kda that are potent antibiotics and anti-tumour agents. So far more than 200 members of the bleomycin/phleomycin family have been isolated and characterised as complex basic glycopeptides. Family members resemble each other with respect to their physicochemical properties and their structure, indicating that functionally they all behave in the same manner. Furthermore, the chemical structure of the active moiety is conserved between family members and consists of 5 amino acids, L-glucose, 3-O-carbamoyl-D-mannose and a terminal cation. The various different bleomycin/phleomycin family members differ from each other in the nature of the terminal cation moiety, which is usually an amine. A preferred bleomycin/phleomycin antibiotic for use in the method of the invention is phleomycin D1, sold under the trade name Zeocin™.

Bleomycin and phleomycin are strong, selective inhibitors of DNA synthesis in intact bacteria and in mammalian cells. Bleomycin can be observed to attack purified DNA in vitro when incubated under appropriate conditions and analysis of the bleomycin damaged DNA shows that both single-stranded and double-stranded cleavages occur, the latter being the result of staggered single strand breaks formed approximately two base pairs apart in the complementary strands.

In in vivo systems, after being taken up by the cell, bleomycin enters the cell nucleus, binds to DNA (by virtue of the interaction between its positively charged terminal amine moiety and a negatively charged phosphate group of the DNA backbone) and causes strand scission. Bleomycin causes strand scission of DNA in viruses, bacteria and eukaryotic cell systems.

The present inventors have surprisingly found that treatment of a culture of cells with signal-generating metabolic activity with a bleomycin/phleomycin antibiotic renders the culture non-viable whilst retaining a level of signal-generating metabolic activity suitable for use in toxicity testing applications. In the context of this application the term non-viable is taken to mean that the cells are unable to reproduce. The process of rendering cells non-viable whilst retaining signal-generating metabolic activity may hereinafter be referred to as ‘inactivation’ and cells which have been rendered non-viable according to the method of the invention may be referred to as ‘inactivated’.

Because of the broad spectrum of action of the bleomycin/phleomycin family of antibiotics the method of the invention is equally applicable to bacterial cells and to eukaryotic cells with signal generating metabolic activity. Preferably the signal-generating metabolic activity is bioluminescence but other signal-generating metabolic activities which are reporters of toxic damage could be used with equivalent effect.

The method of the invention is preferred for use with bacteria or eukaryotic cells that have been genetically modified to express a signal-generating metabolic activity. The examples given below relate to E. coil which have been engineered to express bioluminescence by transformation with a plasmid carrying lux genes. The eukaryotic equivalent would be cells transfected with a vector containing nucleic acid encoding a eukaryotic luciferase enzyme (abbreviated luc) such as, for example, luciferase from the firefly Photinus pyralis. A suitable plasmid vector containing cDNA encoding firefly luciferase under the control of an SV40 viral promoter is available from Promega Corporation, Madison Wis., USA. However, in connection with the present invention it is advantageous to use recombinant cells containing the entire eukaryotic luc operon so as to avoid the need to add an exogenous substrate ( e.g. luciferin) in order to generate light output.

The optimum concentration of bleomycin/phleomycin antibiotic and contact time required to render a culture of cells non-viable whilst retaining a useful level of signal-generating metabolic activity may vary according to the cell type but can be readily determined by routine experiment. In general, the lower the concentration of antibiotic used the longer the contact time required for cell inactivation. In connection with the production of assay reagents for use in toxicity testing applications, it is generally advantageous to keep the concentration of antibiotic low (e.g. around 1-1.5 mg/ml) and increase the contact time for inactivation. As will be shown in Example 1, treatment with Zeocin™ at a concentration of 1.5 mg/ml for 3 to 5 hours is sufficient to completely inactivate a culture of recombinant E. coli.

In the case of bacteria, the contact time required to inactivate a culture of bacterial cells is found to vary according to the stage of growth of the bacterial culture at the time the antibiotic is administered. Although the method of the invention can be used on bacteria at all stages of growth it is generally preferable to perform the method on bacterial cells in an exponential growth phase because the optimum antibiotic contact time has been observed to be shortest when the antibiotic is administered to bacterial cells in an exponential growth phase.

Following treatment with bleomycin/phleomycin antibiotic the non-viable preparation of cells is preferably stabilised for ease of storage or shipment. The cells can be stabilised using known techniques such as, for example, freeze drying (lyophilization) or other cell preservation techniques known in the art. Stabilization by freeze drying has the added advantage that the freeze drying procedure itself can render cells non-viable. Thus, any cells in the preparation which remain viable after treatment of the culture with bleomycin/phleomycin antibiotic will be rendered non-viable by freeze drying. It is thought that freeze drying inactivates any remaining viable cells by enhancing the effect of antibiotic, such that sub-lethally injured cells in the culture are more sensitive to the stresses applied during freeze drying.

Prior to use the stabilised cell preparation is reconstituted using a reconstitution buffer to form an assay reagent. This reconstituted assay reagent may then be used directly in assays for analytes, for example in toxicity testing applications. It is preferable that the stabilised (i.e. freeze dried) assay reagent be reconstituted immediately prior to use, but after reconstitution it is generally necessary to allow sufficient time prior to use for the reconstituted reagent to reach a stable, high level of signal-generating activity. Suitable reconstitution buffers preferably contain an osmotically potent non-salt compound such as sucrose, dextran or polyethylene glycol, although salt based stabilisers may also be used.

Whilst the assay reagent of the invention is particularly suitable for use in toxicity testing applications it is to be understood that the invention is not limited to assay reagents for use in toxicity testing. The cell inactivation method of the invention can be used to inactivate any recombinant cells (prokaryotic or eukaryotic) with a signal generating metabolic activity that is not dependent upon cell viability.

In a further aspect the invention provides a method of assaying a potentially toxic analyte comprising the steps of,

(a) contacting a sample to be assayed for the analyte with a sample of assay reagent comprising a non-viable preparation of cells with a signal-generating metabolic activity;

(b) measuring the level of signal generated; and

(c) using the measurement obtained as an indicator of the toxicity of the analyte.

In a still further aspect, the invention provides a kit for performing the above-stated assay comprising an assay reagent with signal generating metabolic activity and means for contacting the assay reagent with a sample to be assayed for an analyte.

The analytes tested using the assay of the invention are usually toxic substances, but it is to be understood that the precise nature of the analyte to be tested is not material to the invention.

Toxicity is a general term used to describe an adverse effect on biological system and the term ‘toxic substances’ includes both toxicants (synthetic chemicals that are toxic) and toxins (natural poisons). Toxicity is usually expressed as an effective concentration (EC) or inhibitory concentration (IC) value. The EC/IC value is usually denoted as a percentage response e.g. EC₅₀, EC₁₀ which denotes the concentration (dose) of a particular substance which affects the designated criteria for assessing toxicity (i.e. a behavioural trait or death) in the indicated proportion of the population tested. For example, an EC₅₀ of 10 ppm indicates that 50% of the population will be affected by a concentration of 10 ppm. In the case of a toxicity assay based on the use of a bioluminescent assay reagent, the EC₅₀ value is usually the concentration of sample substance causing a 50% change in light output.

The present invention will be further understood by way of the following Examples with reference to the accompanying Figures in which:

FIG. 1 is a graph to show the effect of Zeocin™ treatment on viable count and light output of recombinant bioluminescent E. coil cells.

FIG. 2 is a graph to show the light output from five separate vials of reconstituted assay reagent. The assay reagent was prepared from recombinant bioluminescent E. coil exposed to 1.5 mg/ml Zeocin™ for 300 minutes. Five vials were used to reduce discrepancies resulting from vial to vial variation.

FIGS. 3 to 8 are graphs to show the effect of Zeocin™ treatment on the sensitivity of bioluminescent assay reagent to toxicant (ZnSO₄):

FIG. 3: Control cells, lag phase.

FIG. 4: Zeocin™ treated cells, lag phase.

FIG. 5: Control cells, mid-exponential growth.

FIG. 6: Zeocin™ treated cells, mid-exponential growth.

FIG. 7: Control cells, stationary phase.

FIG. 8: Zeocin™ treated cells, stationary phase.

## EXAMPLE 1

## (A) Inactivation of Bioluminescent E. coil Method

1. Bioluminescent genetically modified E. coil strain HB101 (E. coli HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of Vibrio fischeri constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) were grown from a frozen stock in 5 ml of low salt medium (LB (5 g/ml NaCl)+glycerol+MgSO₄) for 24 hours.

2. 1 ml of the 5 ml culture was then used to inoculate 200 ml of low salt medium in a shaker flask and the resultant culture grown to an OD₆₃₀ of 0.407 (exponential growth phase).

3. 50 ml of this culture was removed to a fresh sterile shaker flask (control cells).

4. Zeocin™ was added to the 150 ml of culture in the original shaker flash, to a final concentration of 1.5 mg/ml. At the same time, an equivalent volume of water was added to the 50 ml culture removed from the original flask (control cells).

5. The time course of cell inactivation was monitored by removing samples from the culture at 5, 60, 120, 180, 240 and 300 minutes after the addition of Zeocin™ and taking measurements of both light output (measured using a Deltatox luminometer) and viable count (per ml, determined using the method given in Example 3 below) for each of the samples. Samples of the control cells were removed at 5 and 300 minutes after the addition of water and measurements of light output and viable count taken as for the Zeocin™ treated cells.

FIG. 1 shows the effect of Zeocin™ treatment on the light output and viable count (per ml) of recombinant bioluminescent E. coil. Zeocin™ was added to a final concentration of 1.5 mg/ml at time zero. The number of viable cells in the culture was observed to decrease with increasing contact cells with Zeocin™, the culture being completely inactivated after 3 hours. The light output from the culture was observed to decrease gradually with increasing Zeocin™ contact time.

## (B) Production of Assay Reagent

Five hours after the addition of Zeocin™ or water the remaining bacterial cells in the Zeocin™ treated and control cultures were harvested by the centrifugation, washed (to remove traces of Zeocin™ from the Zeocin™ treated culture), re-centrifuged and resuspended in cryoprotectant to an OD₆₃₀ of 0.25. 200 μl aliquots of the cells in cryoprotectant were dispensed into single shot vials, and freeze dried. Freeze dried samples of the Zeocin™ treated cells and control cells were reconstituted in 0.2M sucrose to form assay reagents and the light output of the assay reagents measured at various times after reconstitution.

The light output from assay reagent prepared from cells exposed to 1.5 mg/ml Zeocin™ for 5 hours was not significantly different to the light output from assay reagent prepared from control (Zeocin™ untreated) cells, indicating that Zeocin™ treatment does not affect the light output of the reconstituted freeze dried assay reagent. Both Zeocin™ treated and Zeocin™ untreated assay reagents produced stable light output 15 minutes after reconstitution.

FIG. 2 shows the light output from five separate vials of reconstituted Zeocin™ treated assay reagent inactivated according to the method of Example 1(A) and processed into assay reagent as described in Example 1(B). Reconstitution solution was added at time zero and thereafter light output was observed to increase steadily before stabilising out at around 15 minutes after reconstitution. All five vials were observed to give similar light profiles after reconstitution.

## EXAMPLE 2

## Sensitivity of Zeocin™ Treated Assay Reagent to Toxicant Method

1. Bioluminescent genetically modified E. coil strain HB101 (E. coli HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of vibrio fischeri constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) was grown in fermenter as a batch culture in low salt medium (LB(5 g/ml NaCl)+glycerol+MgSO₄).

2. Two aliquots of the culture were removed from the fermenter into separate sterile shaker flasks at each of three different stages of growth i.e. at OD₆₃₀ values of 0.038 (lag phase growth), 1.31 (mid-exponential phase growth) and 2.468 (stationary phase growth).

3. One aliquot of culture for each of the three growth stages was inactivated by contact with Zeocin™ (1 mg Zeocin™ added per 2.5×10⁶ cells, i.e. the concentration of Zeocin™ per cell is kept constant) for 300 minutes and then processed into assay reagent by freeze drying and reconstitution, as described in part (B) of Example 1.

4. An equal volume of water was added to the second aliquot of culture for each of the three growth stages and the cultures processed into assay reagent as described above.

5. Samples of each of the three Zeocin™ treated and three control assay reagents were then evaluated for sensitivity to toxicant (ZnSO₄) according to the following assay protocol:

ZnSO₄ Sensitivity Assay

1. ZnSO₄ solutions were prepared in pure water at 30, 10, 3, 1, 0.3 and 0.1 ppm. Pure water was also used as a control.

2. Seven vials of each of the three Zeocin™ treated and each of the three control assay reagents (i.e. one for each of the six ZnSO₄ solutions and one for the pure water control) were reconstituted using 0.5 ml of reconstitution solution (eg 0.2M sucrose) and then left to stand at room temperature for 15 minutes to allow the light output to stabilize. Base line (time zero) readings of light output were then measured for each of the reconstituted reagents.

3. 0.5 ml aliquots of each of the six ZnSO₄ solutions and the pure water control were added to separate vials of reconstituted assay reagent. This was repeated for each of the different Zeocin™ treated and control assay reagents.

4. The vials were incubated at room temperature and light output readings were taken 5, 10, 15, 20, 25 and 30 minutes after addition of ZnSO₄ solution.

5. The % toxic effect for each sample was calculated as follows:

where: Cₒ=light in control at time zero

Ct=light in control at reading time

Sₒ=light in sample at time zero

St=light in sample at reading time

The results of toxicity assays for sensitivity to ZnSO₄ for all the Zeocin™ treated and control assay reagents are shown in FIGS. 3 to 8:

FIG. 3: Control cells, lag phase.

FIG. 4: Zeocin™ treated cells, lag phase.

FIG. 5: Control cells, mid-exponential growth.

FIG. 6: Zeocin™ treated cells, mid-exponential growth.

FIG. 7: Control cells, stationary phase.

FIG. 8: Zeocin™ treated cells, stationary phase.

|                   | SENSITIVITY-EC50 VALUES   | SENSITIVITY-EC50 VALUES   |
|-------------------|---------------------------|---------------------------|
| GROWTH STAGE OF   | ZEOCIN                    | CONTROL                   |
| ASSAY REAGENT     | TREATED                   | CELLS                     |
| Lag Phase         | 1.445 ppm ZnSO4           | 1.580 ppm ZnSO4           |
| Expotential phase | 0.446 ppm ZnSO4           | 0.446 ZnSO4               |
| Stationary phase  | 0.426 ppm ZnSO4           | 0.457 ppm ZnSO4           |

In each case, separate graphs of % toxic effect against log₁₀ concentration of ZnSO₄ were plotted on the same axes for each value of time (minutes) after addition of Zeocin™ or water. The sensitivities of the various reagents, expressed as an EC₅₀ value for 15 minutes exposed to ZnSO₄, are summarised in Table 1 below.

Table 1: Sensitivity of the different assay reagents to ZnSo₄ expressed as EC₅₀ values for 15 minutes exposure to ZNSO₄.

The results of the toxicity assays indicate that Zeocin™ treatment does not significantly affect the sensitivity of a recombinant bioluminescent E. coli derived assay reagent to ZnSO₄. Similar results could be expected with other toxic substances which have an effect on signal-generating metabolic activities.

## EXAMPLE 3

## Method to Determine Viable Count

1. Samples of bacterial culture to be assayed for viable count were centrifuged at 10,000 rpm for 5 minutes to pellet the bacterial cells.

2. Bacterial cells were washed by resuspending in 1 ml of M9 medium, re-centrifuged at 10,000 rpm for 5 minutes and finally re-suspended in 1 ml of M9 medium.

3. Serial dilutions of the bacterial cell suspension from 10⁻¹ to 10⁻⁷ were prepared in M9 medium.

4. Three separate 10 μl aliquots of each of the serial dilutions were plated out on standard agar plates and the plates incubated at 37° C.

5. The number of bacterial colonies present for each of the three aliquots at each of the serial dilutions were counted and the values averaged. Viable count was calculated per ml of bacterial culture.

## CLAIMS

1. A method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of said cells having signal-generating metabolic activity with an antibiotic selected from the bleomycin/phleomycin family of antibiotics.

2. The method as claimed in claim 1 wherein following contact with antibiotic, said cells are subjected to a stabilization step.

3. The method as claimed in claim 2 wherein said stabilization step comprises freeze drying.

4. The method as claimed in claim 1 wherein said antibiotic is phleomycin D1.

5. The method as claimed in claim 5 wherein said signal-generating metabolic activity is bioluminescence.

6. The method as claimed in claim 5 wherein said cells are bacteria.

7. The method as claimed in claim 6 wherein said bacteria are in an exponential growth phase when contacted with said antibiotic.

8. The method as claimed in claim 6 wherein said bacteria are genetically modified.

9. The method as claimed in claim 8 wherein said genetically modified bacteria contain nucleic acid encoding luciferase.

10. The method as claimed in claim 9 wherein said bacteria are E. coli.

11. The method as claimed in claim 5 wherein said cells are eukaryotic cells.

12. The method as claimed in claim 11 wherein said eukaryotic cells are genetically modified.

13. The method as claimed in claim 12 wherein said genetically modified eukaryotic cells contain nucleic acid encoding luciferase.

14. A method of making a non-viable preparation of prokaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of a genetically modified E. coli strain made bioluminescent by transformation with a plasmid carrying the lux operon of Vibrio fischeri with an antibiotic selected from the bleomycin/phleomycin family of antibiotics.

15. The method as claimed in claim 14 wherein said cells are contacted with phleomycin D1 at a concentration of at least about 1.5 mg/ml.

16. The method as claimed in claim 15 wherein said contact is maintained for at least about 3 hours.

17. The method as claimed in claim 16 wherein said antibiotic-treated cells are harvested, washed and freeze-dried.

## Drawings

================================================
File: tests/data/groundtruth/docling_v2/pftaps057006474.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Carbocation containing cyanine-type dye
    item-2 at level 2: section_header: ABSTRACT
      item-3 at level 3: paragraph: To provide a reagent with excellent stability under storage, which can detect a subject compound to be measured with higher specificity and sensitibity. Complexes of a compound represented by the general formula (IV):
    item-4 at level 2: section_header: BACKGROUND OF THE INVENTION
      item-5 at level 3: paragraph: 1. Field of the Invention
      item-6 at level 3: paragraph: The present invention relates to a labeled complex for microassay using near-infrared radiation. More specifically, the present invention relates to a labeled complex capable of specifically detecting a certain particular component in a complex mixture with a higher sensitivity.
      item-7 at level 3: paragraph: 2. Related Background Art
      item-8 at level 3: paragraph: On irradiating a laser beam on a trace substance labeled with dyes and the like, information due to the substance is generated such as scattered light, absorption light, fluorescent light and furthermore light acoustics. It is widely known in the field of analysis using lasers, to detect such information so as to practice microassays rapidly with a higher precision.
      item-9 at level 3: paragraph: A gas laser represented by an argon laser and a helium laser has conventionally been used exclusively as a laser source. In recent years, however, a semi-conductor laser has been developed, and based on the characteristic features thereof such as inexpensive cost, small scale and easy output control, it is now desired to use the semiconductor laser as a light source.
      item-10 at level 3: paragraph: If diagnostically useful substances from living organisms are assayed by means of the wave-length in ultraviolet and visible regions as has conventionally been used, the background (blank) via the intrinsic fluorescence of naturally occurring products, such as flavin, pyridine coenzyme and serum proteins, which are generally contained in samples, is likely to increase. Only if a light source in a near-infrared region can be used, such background from naturally occurring products can be eliminated so that the sensitivity to substances to be measured might be enhanced, consequently.
      item-11 at level 3: paragraph: However, the oscillation wavelength of a semiconductor laser is generally in red and near-infrared regions (670 to 830 nm), where not too many dyes generate fluorescence via absorption or excitation. A representative example of such dyes is polymethine-type dye having a longer conjugated chain. Examples of labeling substances from living organisms with a polymethine-type dye and using the labeled substances for microanalysis are reported by K. Sauda, T. Imasaka, et al. in the report in Anal. Chem., 58, 2649-2653 (1986), such that plasma protein is labeled with a cyanine dye having a sulfonate group (for example, Indocyanine Green) for the analysis by high-performance liquid chromatography.
      item-12 at level 3: paragraph: Japanese Patent Application Laid-open No. 2-191674 discloses that various cyanine dyes having sulfonic acid groups or sulfonate groups are used for labeling substances from living organisms and for detecting the fluorescence.
      item-13 at level 3: paragraph: However, these known cyanine dyes emitting fluorescence via absorption or excitation in the near-infrared region are generally not particularly stable under light or heat.
      item-14 at level 3: paragraph: If the dyes are used as labeling agents and bonded to substances from living organisms such as antibodies for preparing complexes, the complexes are likely to be oxidized easily by environmental factors such as light, heat, moisture, atmospheric oxygen and the like or to be subjected to modification such as generating cross-links. Particularly in water, a modification such as hydrolysis is further accelerated, disadvantageously. Therefore, the practical use of these complexes as detecting reagents in carrying out the microassay of the components of living organisms has encountered difficulties because of their poor stability under storage.
    item-15 at level 2: section_header: SUMMARY OF THE INVENTION
      item-16 at level 3: paragraph: The present inventors have made various investigations so as to solve the above problems, and have found that a dye of a particular structure, more specifically a particular polymethine dye, and among others, a dye having an azulene skelton, are extremely stable even after the immobilization thereof as a labeling agent onto substances from living organisms. Thus, the inventors have achieved the present invention. It is an object of the present invention to provide a labeled complex with excellent storage stability which can overcome the above problems.
      item-17 at level 3: paragraph: According to an aspect of the present invention, there is provided a labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance and is bonded to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (I), (II) or (III): wherein R.sub.1 through R.sub.7 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, sulfonate group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.1 through R.sub.7 may be bonded to each other to form a substituted or an unsubstituted condensed ring; R.sub.1 represents a divalent organic residue; and X.sub.1.sup..crclbar. represents an anion; wherein R.sub.8 through R14 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, sulfonate group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.8 through R14 may be bonded to each other to form a substituted or an unsubstituted condensed ring; and R.sub.A represents a divalent organic residue; wherein R.sub.15 through R.sub.21 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, a substituted or an unsubstituted aralkyl group, a substituted or an unsubstituted amino group, a substituted or an unsubstituted styryl group, nitro group, sulfonate group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.15 through R.sub.21 may or may not be bonded to each other to form a substituted or an unsubstituted condensed ring; R.sub.B represents a divalent organic residue; and X.sub.1.sup..crclbar. represents an anion.
      item-18 at level 3: paragraph: According to another aspect of the present invention, there is provided a labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance and is bonded to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; 1 is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.
      item-19 at level 3: paragraph: According to another aspect of the present invention, there is provided a method of detecting a subject compound to be analyzed by means of optical means which method comprises using a labeled complex comprised of a substance from a living organism and a labeling agent fixed onto the substance and bonding the complex to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (I), (II) or (III).
      item-20 at level 3: paragraph: According to still another aspect of the present invention, there is provided a method of detecting a subject compound to be analyzed by means of optical means which method comprises using a labeled complex comprised of a substance from a living organism and a labeling agent fixed onto the substance and bonding the complex to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (iv).
    item-21 at level 2: section_header: BRIEF DESCRIPTION OF THE DRAWINGS
      item-22 at level 3: paragraph: FIG. 1 depicts one example of fluorescence emitting wave form of a labeling agent.
    item-23 at level 2: section_header: DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
      item-24 at level 3: paragraph: The present invention will now be explained in detail hereinbelow.
      item-25 at level 3: paragraph: In accordance with the present invention, the compound of the general formula (I), (II) or (III) is employed as a labeling agent, wherein R.sub.1 to R.sub.21 individually represent hydrogen atom, halogen atom (chlorine atom, bromine atom, and iodine atom) or a monovalent organic residue, and other such functional groups described above. The monovalent organic residue can be selected from a wide variety of such residues.
      item-26 at level 3: paragraph: The alkyl group is preferably in straight chain or branched chain, having a carbon number of 1 to 12, such as for example methyl group, ethyl group, n-propyl group, iso-propyl group, n-butyl group, sec-butyl group, iso-butyl group, t-butyl group, n-amyl group, t-amyl group, n-hexyl group, n-octyl group, t-octyl group and the like.
      item-27 at level 3: paragraph: The aryl group preferably has a carbon number of 6 to 20, such as for example phenyl group, naphthyl group, methoxyphenyl group, diethylaminophenyl group, dimethylaminophenyl group and the like.
      item-28 at level 3: paragraph: The substituted aralkyl group preferably has a carbon number of 7 to 19, such as for example carboxybenzyl group, sulfobenzyl group, hydroxybenzyl group and the like.
      item-29 at level 3: paragraph: The unsubstituted aralkyl group preferably has a carbon number of 7 to 19, such as for example benzyl group, phenethyl group, .alpha.-naphthylmethyl group, .beta.-naphthylmethyl group and the like.
      item-30 at level 3: paragraph: The substituted or unsubstituted amino group preferably has a carbon number of 10 or less, such as for example amino group, dimethylamino group, diethylamino group, dipropylamino group, acetylamino group, benzoylamino group and the like.
      item-31 at level 3: paragraph: The substituted or unsubstituted styryl group preferably has a carbon number of 8 to 14, such as for example styryl group, dimethylaminostyryl group, diethylaminostyryl group, dipropylaminostyryl group, methoxystyryl group, ethoxystyryl group, methylstyryl group and the like.
      item-32 at level 3: paragraph: The aryl azo group preferably has a carbon number of 6 to 14, such as for example phenylazo group, .alpha.-naphthylazo group, .beta.-naphthylazo group, dimethylaminophenylazo group, chlorophenylazo group, nitrophenylazo group, methoxyphenylazo group and the like.
      item-33 at level 3: paragraph: Of the combinations of R.sub.1 and R.sub.2, R.sub.2 and R.sub.3, R.sub.3 and R.sub.4, R.sub.4 and R.sub.5, R.sub.5 and R.sub.6, and R.sub.6 and R.sub.7 of the general formula (I), at least one combination may form a substituted or an unsubstituted condensed ring. The condensed ring may be five, six or seven membered, including aromatic ring (benzene, naphthalene, chlorobenzene, bromobenzene, methyl benzene, ethyl benzene, methoxybenzene, ethoxybenzene and the like); heterocyclic ring (furan ring, benzofuran ring, pyrrole ring, thiophene ring, pyridine ring, quinoline ring, thiazole ring and the like); and aliphatic ring (dimethylene, trimethylene, tetramethylene and the like). This is the case with the general formulas (II) and (III).
      item-34 at level 3: paragraph: For the general formula (II), at least one combination among the combinations of R.sub.8 and R.sub.9, R.sub.9 and R.sub.10, R.sub.10 and R.sub.11, R.sub.11 and R.sub.12, R.sub.12 and R.sub.13, and R.sub.13 and R.sub.14, may form a substituted or an unsubstituted condensed ring.
      item-35 at level 3: paragraph: Also for the general formula (III), at least one combination of the combinations of R.sub.15 and R.sub.16, R.sub.16 and R.sub.17, R.sub.17 and R.sub.18, R.sub.18 and R.sub.19, R.sub.19 and R.sub.20, and R.sub.20 and R.sub.21, may form a substituted or an unsubstituted condensed ring.
      item-36 at level 3: paragraph: In the general formulas (I) to (IV) described above, the general formula (I) is specifically preferable; preference is also given individually to hydrogen atom, alkyl group and sulfonate group in the case of R.sub.1 to R.sub.7 ; hydrogen atom, alkyl group and sulfonate group in the case of R.sub.8 to R.sub.14 ; hydrogen atom, alkyl group and sulfonate group in the case of R.sub.15 to R.sub.21 ; alkyl group and aryl group in the case of A, B, D and E; hydrogen atom and alkyl group in the case Of r.sub.1 ' to r.sub.2 '.
      item-37 at level 3: paragraph: In the general formula (I), R represents a divalent organic residue bonded via a double bond. Specific examples of a compound containing such R to be used in the present invention, include those represented by the following general formulas (1) to (12), wherein Q.sup..sym. represents the following azulenium salt nucleus and the right side excluding Q.sup..sym. represents R. wherein the relation between the azulenium salt nucleus represented by Q.sup..crclbar.  and the azulene salt nucleus on the right side in the formula (3) may be symmetric or asymmetric. In the above formulas (1) to (12) as in the case of R.sub.1 to R.sub.7, R.sub.1 ' to R.sub.7 ' and R.sub.1 " to R.sub.7 " independently represent hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group or aryl azo group, while R.sub.1 ' to R.sub.7 ' and R.sub.1 " to R.sub.7 " independently may form a substituted or an unsubstituted condensed ring; n is 0, 1 or 2; r is an integer of 1 to 8; S represents 0 or 1; and t represents 1 or 2.
      item-38 at level 3: paragraph: M.sub.2 represents a non-metallic atom group required for the completion of a nitrogen-containing heterocyclic ring.
      item-39 at level 3: paragraph: Specific examples of M.sub.2 are atom groups required for the completion of a nitrogen-containing heterocyclic ring, including pyridine, thiazole, benzothiazole, naphthothiazole, oxazole, benzoxazole, naphthoxazole, imidazole, benzimidazole, naphthoimidazole, 2-quinoline, 4-quinoline, isoquinoline or indole, and may be substituted by halogen atom (chlorine atom, bromine atom, iodine atom and the like), alkyl group (methyl, ethyl, propyl, butyl and the like), aryl group (phenyl, tolyl, xylyl and the like), and aralkyl (benzene, p-trimethyl, and the like).
      item-40 at level 3: paragraph: R.sub.22 represents hydrogen atom, nitro group, sulfonate group, cyano group, alkyl group (methyl, ethyl, propyl, butyl and the like), or aryl group (phenyl, tolyl, xylyl and the like). R.sub.23 represents alkyl group (methyl, ethyl, propyl, butyl and the like), a substituted alkyl group (2-hydroxyethyl, 2-methoxyethyl, 2-ethoxyethyl, 3-hydroxypropyl, 3-methoxypropyl, 3-ethoxypropyl, 3-chloropropyl, 3-bromopropyl, 3-carboxylpropyl and the like ), a cyclic alkyl group (cyclohexyl, cyclopropyl), aryl aralkyl group (benzene, 2-phenylethyl, 3-phenylpropyl, 3-phenylbutyl, 4-phenylbutyl, .alpha.-naphthylmethyl, .beta.-naphthylmethyl), a substituted aralkyl group (methylbenzyl, ethylbenzyl, dimethylbenzyl, trimethylbenzyl, chlorobenzyl, bromobenzyl and the like), aryl group (phenyl, tolyl, xylyl, .alpha.-naphtyl, .beta.-naphthyl) or a substituted aryl group (chlorophenyl, dichlorophenyl, trichlorophenyl, ethylphenyl, methoxydiphenyl, dimethoxyphenyl, aminophenyl, sulfonate phenyl, nitrophenyl, hydroxyphenyl and the like).
      item-41 at level 3: paragraph: R.sub.24 represents a substituted or an unsubstituted aryl group or the cation group thereof, specifically including a substituted or an unsubstituted aryl group (phenyl, tolyl, xylyl, biphenyl, aminophenyl, .alpha.-naphthyl, .beta.-napthyl, anthranyl, pyrenyl, methoxyphenyl, dimethoxyphenyl, trimethoxyphenyl, ethoxyphenyl, diethoxyphenyl, chlorophenyl, dichlorophenyl, trichlorophenyl, bromophenyl, dibromophenyl, tribromophenyl, ethylphenyl, diethylphenyl, nitrophenyl, aminophenyl, dimethylaminophenyl, diethylaminophenyl, dibenzylaminophenyl, dipropylaminophenyl, morpholinophenyl, piperidinylphenyl, piperidinophenyl, diphenylaminophenyl, acetylaminophenyl, benzoylaminophenyl, acetylphenyl, benzoylphenyl, cyanophenyl, sulfonate phenyl, carboxylate phenyl and the like).
      item-42 at level 3: paragraph: R.sub.25 represents a heterocyclic ring or the cation group thereof, specifically including a monovalent heterocyclic ring derived from cyclic rings, such as furan, thiophene, benzofuran, thionaphthene, dibenzofuran, carbazole, phenothiazine phenoxazine, pyridine and the like.
      item-43 at level 3: paragraph: R.sub.26 represents hydrogen atom, alkyl group (methyl, ethyl, propyl, butyl and the like), or a substituted or an unsubstituted aryl group (phenyl, tolyl, xylyl, biphenyl, ethylphenyl, chlorophenyl, methoxyphenyl, ethoxyphenyl, nitrophenyl, aminophenyl, dimethylaminophenyl, diethylaminophenyl, acetylaminophenyl, .alpha.-naphthyl, .beta.-naphthyl, anthraryl, pyrenyl, sulfonate phenyl, carboxylate phenyl and the like. In the formula, Z.sub.7 represents an atom group required for the completion of pyran, thiapyran, selenapyran, telluropyran, benzopyran, benzothiapyran, benzoselenapyran, benzotelluropyran, naphthopyran, naphthothiapyran, or naphthoselenapyran, or naphthotelluropyran.
      item-44 at level 3: paragraph: L.sub.7 represents sulfur atom, oxygen atom or selenium atom or tellurium atom.
      item-45 at level 3: paragraph: R.sub.27 and R.sub.28 individually represent hydrogen atom, alkoxy group, a substituted or an unsubstituted aryl group, alkenyl group and a heterocyclic group,
      item-46 at level 3: paragraph: More specifically, R.sub.27 and R.sub.28 individually represent hydrogen atom, alkyl group (methyl, ethyl, propyl, butyl and the like), alkyl sulfonate group, alkoxyl group (methoxy, ethoxy, propoxy, ethoxyethyl, methoxyethyl and the like), aryl group (phenyl, tolyl, xylyl, sulfonate phenyl, chlorophenyl, biphenyl, methoxyphenyl and the like), a substituted or an unsubstituted styryl group (styryl, p-methylstyryl, o-chlorostyryl, p-methoxystyryl and the like), a substituted or an unsubstituted 4-phenyl, 1,3-butadienyl group (r-phenyl, 1,3-butadienyl, 4-(p-methylphenyl), 1,3-butadienyl and the like), or a substituted or an unsubstituted heterocyclic group (quinolyl, pyridyl, carbazoyl, furyl and the like).
      item-47 at level 3: paragraph: As in the case of R, the same is true with R.sub.A and R.sub.B of the general formulas (II) and (III), respectively.
      item-48 at level 3: paragraph: Then, in R, the symbols R.sub.8 ' to R.sub.14 ' individually correspond to R.sub.1 ' to R.sub.7 '; R.sub.8 " to R.sub.14 " individually correspond to R.sub.1 " to R.sub.7 "; in R.sub.B, R.sub.14 ' to R.sub.21 " individually correspond to R.sub.1 ' to R.sub.7 '; R.sub.14 " to R.sub.21 " individually correspond to R.sub.1 " to R.sub.7 ".
      item-49 at level 3: paragraph: In the azulenium nucleus of the (1) to (12), described above, those represented by the formulas (3), (9) and (10) are more preferably used; and particularly, the formula (3) is preferable.
      item-50 at level 3: paragraph: R.sub.1 to R.sub.28, R.sub.1 ' to R.sub.21 ' and R.sub.1 " to R.sub.21 " preferably contain one or more well-known polar groups in order to impart water solubility to a compound (labeling agent) represented by the general formula (I), (II) or (III). The polar groups include, for example, hydroxyl group, alkylhydroxyl group, sulfonate group, alkylsulfonate group, carboxylate group, alkylcarboxylate group, tetra-ammonium base and the like. R.sub.1 to R.sub.28, R.sub.1 ' to R.sub.21 ', and R.sub.1 " to R.sub.21 " preferably contain one or more well-known reactive groups in order that the compound of the general formula (I) can form a covalent bond with a substance from a living organism.
      item-51 at level 3: paragraph: The reactive groups include the reactive sites of isocyanate, isothiocyanate, succinimide ester, sulfosuccinimide ester, imide ester, hydrazine, nitroaryl halide, piperidine disulfide, maleimide, thiophthalimide, acid halide, sulfonyl halide, aziridine, azide nitrophenyl, azide amino, 3-(2-pyridyldithio) propionamide and the like. In these reactive sites, the following spacer groups (n=0, 1 to 6) may be interposed in order to prevent steric hindrance during on the bonding of a labeling agent and a substance from a living organism.
      item-52 at level 3: paragraph: Preferable such reactive groups include isothiocyanate, sulfosuccinimide ester, succinimide ester maleimide and the like X.sub.1.sup..sym. represents an anion, including chloride ion, bromide ion, iodide ion, perchlorate ion, benzenesulfonate ion, p-toluene sulfonate ion, methylsulfate ion, ethylsulfate ion, propylsulfate ion, tetrafluoroborate ion, tetraphenylborate ion, hexafluorophosphate ion, benzenesulfinic acid salt ion, acetate ion, trifluoroacetate ion, propionate ion, benzoate ion, oxalate ion, succinate ion, malonate ion, oleate ion, stearate ion, citrate ion, monohydrogen diphosphate ion, dihydrogen monophosphate ion, pentachlorostannate ion, chlorosulfonate ion, fluorosulfonate ion, trifluoromethane sulfonate ion, hexafluoroantimonate ion, molybdate ion, tungstate ion, titanate ion, zirconate ion and the like.
      item-53 at level 3: paragraph: Specific examples of these labeling agents are illustrated in Tables 1, 2 and 3, but are not limited thereto.
      item-54 at level 3: paragraph: The synthetic method of these azulene dyes is described in U.S. Pat. No. 4,738,908.
    item-55 at level 2: section_header: CLAIMS
      item-56 at level 3: paragraph: 1. A labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance, the substance capable of specifically binding to the subject compound, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group, and at least one of A and B is a substituted or unsubstituted aryl group, and at least one of D and E is a substituted or unsubstituted aryl group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.
      item-57 at level 3: paragraph: 2. The labeled complex according to claim 1, wherein the substance from a living organism is an antibody or an antigen.
      item-58 at level 3: paragraph: 3. The labeled complex according to claim 1, wherein the substance from a living organism is a nucleic acid.
      item-59 at level 3: paragraph: 4. The labeled complex according to claim 1, wherein the substituted aryl group constituting at least one of A and B is phenyl group substituted by dialkylamino group.
      item-60 at level 3: paragraph: 5. The labeled complex according to claim 1, wherein the substituted aryl group constituting at least one of D and E is phenyl group substituted by dialkylamino group.
      item-61 at level 3: paragraph: 6. The labeled complex according to claim 4 or 5, wherein the dialkylamino group is a diethylamino group.
      item-62 at level 3: paragraph: 7. The labeled complex according to claim 1, wherein each of A, B and D is dimethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.
      item-63 at level 3: paragraph: 8. The labeled complex according to claim 1, wherein each of A, B and D is diethylaminophenyl group, E is phenyl group substituted by carboxyl group, k is 0 and l is 1.
      item-64 at level 3: paragraph: 9. The labeled complex according to claim 1, wherein each of A, B, D and E is diethylaminophenyl group, k is 1 and l is 0.
      item-65 at level 3: paragraph: 10. The labeled complex according to claim 1, wherein each of A, B, and D is diethylaminophenyl group, E is aminophenyl group, K is 0 and l is 1.
      item-66 at level 3: paragraph: 11. The labeled complex according to claim 1, wherein A is dimethylaminophenyl group, each of B and E is ethoxyphenyl group, k is 0, 1 is l and D is represented by the following formula:
      item-67 at level 3: paragraph: 12. A method of detecting a subject compound to be analyzed in a sample comprising the steps of: providing a labeled complex comprising a substance from a living organisms and a labeling agent fixed onto the substance, the substance being capable of specifically binding to the subject compound; binding the labeled complex to the subject compound; and detecting the labeled complex to which the subject compound is bonded by means of optical means, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group, and at least one of A and B is a substituted or unsubstituted aryl group, and at least one of D and E is a substituted or unsubstituted aryl group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.
      item-68 at level 3: paragraph: 13. The method according to claim 12, wherein the substance from a living organism is an antibody or an antigen.
      item-69 at level 3: paragraph: 14. The method according to claim 12, wherein the substance from a living organism is a nucleic acid.
      item-70 at level 3: paragraph: 15. The analyzing method according to any one of claims 12, 13 and 14, wherein the optical means is an optical means using near-infrared ray.
      item-71 at level 3: paragraph: 16. The method according to claim 12, wherein each of A, B and D is dimethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.
      item-72 at level 3: paragraph: 17. The method according to claim 12, wherein each of A, B and D is diethylaminophenyl group, E is phenyl group substituted by carboxyl group, k is 0 and l is 1.
      item-73 at level 3: paragraph: 18. The method according to claim 12, wherein each of A, B, D and E is diethylaminophenyl group, k is 1 and l is 0.
      item-74 at level 3: paragraph: 19. The method according to claim 12, wherein each of A, B and D is diethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.
      item-75 at level 3: paragraph: 20. The method according to claim 12, wherein A is dimethylaminophenyl group, each of B and E is ethoxyphenyl group, k is 0, l is 1 and D is represented by the following formula: ##STR102##                                                              

================================================
File: tests/data/groundtruth/docling_v2/pftaps057006474.md
================================================
# Carbocation containing cyanine-type dye

## ABSTRACT

To provide a reagent with excellent stability under storage, which can detect a subject compound to be measured with higher specificity and sensitibity. Complexes of a compound represented by the general formula (IV):

## BACKGROUND OF THE INVENTION

1. Field of the Invention

The present invention relates to a labeled complex for microassay using near-infrared radiation. More specifically, the present invention relates to a labeled complex capable of specifically detecting a certain particular component in a complex mixture with a higher sensitivity.

2. Related Background Art

On irradiating a laser beam on a trace substance labeled with dyes and the like, information due to the substance is generated such as scattered light, absorption light, fluorescent light and furthermore light acoustics. It is widely known in the field of analysis using lasers, to detect such information so as to practice microassays rapidly with a higher precision.

A gas laser represented by an argon laser and a helium laser has conventionally been used exclusively as a laser source. In recent years, however, a semi-conductor laser has been developed, and based on the characteristic features thereof such as inexpensive cost, small scale and easy output control, it is now desired to use the semiconductor laser as a light source.

If diagnostically useful substances from living organisms are assayed by means of the wave-length in ultraviolet and visible regions as has conventionally been used, the background (blank) via the intrinsic fluorescence of naturally occurring products, such as flavin, pyridine coenzyme and serum proteins, which are generally contained in samples, is likely to increase. Only if a light source in a near-infrared region can be used, such background from naturally occurring products can be eliminated so that the sensitivity to substances to be measured might be enhanced, consequently.

However, the oscillation wavelength of a semiconductor laser is generally in red and near-infrared regions (670 to 830 nm), where not too many dyes generate fluorescence via absorption or excitation. A representative example of such dyes is polymethine-type dye having a longer conjugated chain. Examples of labeling substances from living organisms with a polymethine-type dye and using the labeled substances for microanalysis are reported by K. Sauda, T. Imasaka, et al. in the report in Anal. Chem., 58, 2649-2653 (1986), such that plasma protein is labeled with a cyanine dye having a sulfonate group (for example, Indocyanine Green) for the analysis by high-performance liquid chromatography.

Japanese Patent Application Laid-open No. 2-191674 discloses that various cyanine dyes having sulfonic acid groups or sulfonate groups are used for labeling substances from living organisms and for detecting the fluorescence.

However, these known cyanine dyes emitting fluorescence via absorption or excitation in the near-infrared region are generally not particularly stable under light or heat.

If the dyes are used as labeling agents and bonded to substances from living organisms such as antibodies for preparing complexes, the complexes are likely to be oxidized easily by environmental factors such as light, heat, moisture, atmospheric oxygen and the like or to be subjected to modification such as generating cross-links. Particularly in water, a modification such as hydrolysis is further accelerated, disadvantageously. Therefore, the practical use of these complexes as detecting reagents in carrying out the microassay of the components of living organisms has encountered difficulties because of their poor stability under storage.

## SUMMARY OF THE INVENTION

The present inventors have made various investigations so as to solve the above problems, and have found that a dye of a particular structure, more specifically a particular polymethine dye, and among others, a dye having an azulene skelton, are extremely stable even after the immobilization thereof as a labeling agent onto substances from living organisms. Thus, the inventors have achieved the present invention. It is an object of the present invention to provide a labeled complex with excellent storage stability which can overcome the above problems.

According to an aspect of the present invention, there is provided a labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance and is bonded to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (I), (II) or (III): wherein R.sub.1 through R.sub.7 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, sulfonate group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.1 through R.sub.7 may be bonded to each other to form a substituted or an unsubstituted condensed ring; R.sub.1 represents a divalent organic residue; and X.sub.1.sup..crclbar. represents an anion; wherein R.sub.8 through R14 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, sulfonate group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.8 through R14 may be bonded to each other to form a substituted or an unsubstituted condensed ring; and R.sub.A represents a divalent organic residue; wherein R.sub.15 through R.sub.21 are independently selected from the group consisting of hydrogen atom, halogen atom, alkyl group, aryl group, a substituted or an unsubstituted aralkyl group, a substituted or an unsubstituted amino group, a substituted or an unsubstituted styryl group, nitro group, sulfonate group, hydroxyl group, carboxyl group, cyano group, or arylazo group; R.sub.15 through R.sub.21 may or may not be bonded to each other to form a substituted or an unsubstituted condensed ring; R.sub.B represents a divalent organic residue; and X.sub.1.sup..crclbar. represents an anion.

According to another aspect of the present invention, there is provided a labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance and is bonded to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; 1 is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.

According to another aspect of the present invention, there is provided a method of detecting a subject compound to be analyzed by means of optical means which method comprises using a labeled complex comprised of a substance from a living organism and a labeling agent fixed onto the substance and bonding the complex to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (I), (II) or (III).

According to still another aspect of the present invention, there is provided a method of detecting a subject compound to be analyzed by means of optical means which method comprises using a labeled complex comprised of a substance from a living organism and a labeling agent fixed onto the substance and bonding the complex to the subject compound to be analyzed, wherein the labeling agent comprises a compound represented by the general formula (iv).

## BRIEF DESCRIPTION OF THE DRAWINGS

FIG. 1 depicts one example of fluorescence emitting wave form of a labeling agent.

## DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS

The present invention will now be explained in detail hereinbelow.

In accordance with the present invention, the compound of the general formula (I), (II) or (III) is employed as a labeling agent, wherein R.sub.1 to R.sub.21 individually represent hydrogen atom, halogen atom (chlorine atom, bromine atom, and iodine atom) or a monovalent organic residue, and other such functional groups described above. The monovalent organic residue can be selected from a wide variety of such residues.

The alkyl group is preferably in straight chain or branched chain, having a carbon number of 1 to 12, such as for example methyl group, ethyl group, n-propyl group, iso-propyl group, n-butyl group, sec-butyl group, iso-butyl group, t-butyl group, n-amyl group, t-amyl group, n-hexyl group, n-octyl group, t-octyl group and the like.

The aryl group preferably has a carbon number of 6 to 20, such as for example phenyl group, naphthyl group, methoxyphenyl group, diethylaminophenyl group, dimethylaminophenyl group and the like.

The substituted aralkyl group preferably has a carbon number of 7 to 19, such as for example carboxybenzyl group, sulfobenzyl group, hydroxybenzyl group and the like.

The unsubstituted aralkyl group preferably has a carbon number of 7 to 19, such as for example benzyl group, phenethyl group, .alpha.-naphthylmethyl group, .beta.-naphthylmethyl group and the like.

The substituted or unsubstituted amino group preferably has a carbon number of 10 or less, such as for example amino group, dimethylamino group, diethylamino group, dipropylamino group, acetylamino group, benzoylamino group and the like.

The substituted or unsubstituted styryl group preferably has a carbon number of 8 to 14, such as for example styryl group, dimethylaminostyryl group, diethylaminostyryl group, dipropylaminostyryl group, methoxystyryl group, ethoxystyryl group, methylstyryl group and the like.

The aryl azo group preferably has a carbon number of 6 to 14, such as for example phenylazo group, .alpha.-naphthylazo group, .beta.-naphthylazo group, dimethylaminophenylazo group, chlorophenylazo group, nitrophenylazo group, methoxyphenylazo group and the like.

Of the combinations of R.sub.1 and R.sub.2, R.sub.2 and R.sub.3, R.sub.3 and R.sub.4, R.sub.4 and R.sub.5, R.sub.5 and R.sub.6, and R.sub.6 and R.sub.7 of the general formula (I), at least one combination may form a substituted or an unsubstituted condensed ring. The condensed ring may be five, six or seven membered, including aromatic ring (benzene, naphthalene, chlorobenzene, bromobenzene, methyl benzene, ethyl benzene, methoxybenzene, ethoxybenzene and the like); heterocyclic ring (furan ring, benzofuran ring, pyrrole ring, thiophene ring, pyridine ring, quinoline ring, thiazole ring and the like); and aliphatic ring (dimethylene, trimethylene, tetramethylene and the like). This is the case with the general formulas (II) and (III).

For the general formula (II), at least one combination among the combinations of R.sub.8 and R.sub.9, R.sub.9 and R.sub.10, R.sub.10 and R.sub.11, R.sub.11 and R.sub.12, R.sub.12 and R.sub.13, and R.sub.13 and R.sub.14, may form a substituted or an unsubstituted condensed ring.

Also for the general formula (III), at least one combination of the combinations of R.sub.15 and R.sub.16, R.sub.16 and R.sub.17, R.sub.17 and R.sub.18, R.sub.18 and R.sub.19, R.sub.19 and R.sub.20, and R.sub.20 and R.sub.21, may form a substituted or an unsubstituted condensed ring.

In the general formulas (I) to (IV) described above, the general formula (I) is specifically preferable; preference is also given individually to hydrogen atom, alkyl group and sulfonate group in the case of R.sub.1 to R.sub.7 ; hydrogen atom, alkyl group and sulfonate group in the case of R.sub.8 to R.sub.14 ; hydrogen atom, alkyl group and sulfonate group in the case of R.sub.15 to R.sub.21 ; alkyl group and aryl group in the case of A, B, D and E; hydrogen atom and alkyl group in the case Of r.sub.1 ' to r.sub.2 '.

In the general formula (I), R represents a divalent organic residue bonded via a double bond. Specific examples of a compound containing such R to be used in the present invention, include those represented by the following general formulas (1) to (12), wherein Q.sup..sym. represents the following azulenium salt nucleus and the right side excluding Q.sup..sym. represents R. wherein the relation between the azulenium salt nucleus represented by Q.sup..crclbar.  and the azulene salt nucleus on the right side in the formula (3) may be symmetric or asymmetric. In the above formulas (1) to (12) as in the case of R.sub.1 to R.sub.7, R.sub.1 ' to R.sub.7 ' and R.sub.1 " to R.sub.7 " independently represent hydrogen atom, halogen atom, alkyl group, aryl group, aralkyl group, amino group, styryl group, nitro group, hydroxyl group, carboxyl group, cyano group or aryl azo group, while R.sub.1 ' to R.sub.7 ' and R.sub.1 " to R.sub.7 " independently may form a substituted or an unsubstituted condensed ring; n is 0, 1 or 2; r is an integer of 1 to 8; S represents 0 or 1; and t represents 1 or 2.

M.sub.2 represents a non-metallic atom group required for the completion of a nitrogen-containing heterocyclic ring.

Specific examples of M.sub.2 are atom groups required for the completion of a nitrogen-containing heterocyclic ring, including pyridine, thiazole, benzothiazole, naphthothiazole, oxazole, benzoxazole, naphthoxazole, imidazole, benzimidazole, naphthoimidazole, 2-quinoline, 4-quinoline, isoquinoline or indole, and may be substituted by halogen atom (chlorine atom, bromine atom, iodine atom and the like), alkyl group (methyl, ethyl, propyl, butyl and the like), aryl group (phenyl, tolyl, xylyl and the like), and aralkyl (benzene, p-trimethyl, and the like).

R.sub.22 represents hydrogen atom, nitro group, sulfonate group, cyano group, alkyl group (methyl, ethyl, propyl, butyl and the like), or aryl group (phenyl, tolyl, xylyl and the like). R.sub.23 represents alkyl group (methyl, ethyl, propyl, butyl and the like), a substituted alkyl group (2-hydroxyethyl, 2-methoxyethyl, 2-ethoxyethyl, 3-hydroxypropyl, 3-methoxypropyl, 3-ethoxypropyl, 3-chloropropyl, 3-bromopropyl, 3-carboxylpropyl and the like ), a cyclic alkyl group (cyclohexyl, cyclopropyl), aryl aralkyl group (benzene, 2-phenylethyl, 3-phenylpropyl, 3-phenylbutyl, 4-phenylbutyl, .alpha.-naphthylmethyl, .beta.-naphthylmethyl), a substituted aralkyl group (methylbenzyl, ethylbenzyl, dimethylbenzyl, trimethylbenzyl, chlorobenzyl, bromobenzyl and the like), aryl group (phenyl, tolyl, xylyl, .alpha.-naphtyl, .beta.-naphthyl) or a substituted aryl group (chlorophenyl, dichlorophenyl, trichlorophenyl, ethylphenyl, methoxydiphenyl, dimethoxyphenyl, aminophenyl, sulfonate phenyl, nitrophenyl, hydroxyphenyl and the like).

R.sub.24 represents a substituted or an unsubstituted aryl group or the cation group thereof, specifically including a substituted or an unsubstituted aryl group (phenyl, tolyl, xylyl, biphenyl, aminophenyl, .alpha.-naphthyl, .beta.-napthyl, anthranyl, pyrenyl, methoxyphenyl, dimethoxyphenyl, trimethoxyphenyl, ethoxyphenyl, diethoxyphenyl, chlorophenyl, dichlorophenyl, trichlorophenyl, bromophenyl, dibromophenyl, tribromophenyl, ethylphenyl, diethylphenyl, nitrophenyl, aminophenyl, dimethylaminophenyl, diethylaminophenyl, dibenzylaminophenyl, dipropylaminophenyl, morpholinophenyl, piperidinylphenyl, piperidinophenyl, diphenylaminophenyl, acetylaminophenyl, benzoylaminophenyl, acetylphenyl, benzoylphenyl, cyanophenyl, sulfonate phenyl, carboxylate phenyl and the like).

R.sub.25 represents a heterocyclic ring or the cation group thereof, specifically including a monovalent heterocyclic ring derived from cyclic rings, such as furan, thiophene, benzofuran, thionaphthene, dibenzofuran, carbazole, phenothiazine phenoxazine, pyridine and the like.

R.sub.26 represents hydrogen atom, alkyl group (methyl, ethyl, propyl, butyl and the like), or a substituted or an unsubstituted aryl group (phenyl, tolyl, xylyl, biphenyl, ethylphenyl, chlorophenyl, methoxyphenyl, ethoxyphenyl, nitrophenyl, aminophenyl, dimethylaminophenyl, diethylaminophenyl, acetylaminophenyl, .alpha.-naphthyl, .beta.-naphthyl, anthraryl, pyrenyl, sulfonate phenyl, carboxylate phenyl and the like. In the formula, Z.sub.7 represents an atom group required for the completion of pyran, thiapyran, selenapyran, telluropyran, benzopyran, benzothiapyran, benzoselenapyran, benzotelluropyran, naphthopyran, naphthothiapyran, or naphthoselenapyran, or naphthotelluropyran.

L.sub.7 represents sulfur atom, oxygen atom or selenium atom or tellurium atom.

R.sub.27 and R.sub.28 individually represent hydrogen atom, alkoxy group, a substituted or an unsubstituted aryl group, alkenyl group and a heterocyclic group,

More specifically, R.sub.27 and R.sub.28 individually represent hydrogen atom, alkyl group (methyl, ethyl, propyl, butyl and the like), alkyl sulfonate group, alkoxyl group (methoxy, ethoxy, propoxy, ethoxyethyl, methoxyethyl and the like), aryl group (phenyl, tolyl, xylyl, sulfonate phenyl, chlorophenyl, biphenyl, methoxyphenyl and the like), a substituted or an unsubstituted styryl group (styryl, p-methylstyryl, o-chlorostyryl, p-methoxystyryl and the like), a substituted or an unsubstituted 4-phenyl, 1,3-butadienyl group (r-phenyl, 1,3-butadienyl, 4-(p-methylphenyl), 1,3-butadienyl and the like), or a substituted or an unsubstituted heterocyclic group (quinolyl, pyridyl, carbazoyl, furyl and the like).

As in the case of R, the same is true with R.sub.A and R.sub.B of the general formulas (II) and (III), respectively.

Then, in R, the symbols R.sub.8 ' to R.sub.14 ' individually correspond to R.sub.1 ' to R.sub.7 '; R.sub.8 " to R.sub.14 " individually correspond to R.sub.1 " to R.sub.7 "; in R.sub.B, R.sub.14 ' to R.sub.21 " individually correspond to R.sub.1 ' to R.sub.7 '; R.sub.14 " to R.sub.21 " individually correspond to R.sub.1 " to R.sub.7 ".

In the azulenium nucleus of the (1) to (12), described above, those represented by the formulas (3), (9) and (10) are more preferably used; and particularly, the formula (3) is preferable.

R.sub.1 to R.sub.28, R.sub.1 ' to R.sub.21 ' and R.sub.1 " to R.sub.21 " preferably contain one or more well-known polar groups in order to impart water solubility to a compound (labeling agent) represented by the general formula (I), (II) or (III). The polar groups include, for example, hydroxyl group, alkylhydroxyl group, sulfonate group, alkylsulfonate group, carboxylate group, alkylcarboxylate group, tetra-ammonium base and the like. R.sub.1 to R.sub.28, R.sub.1 ' to R.sub.21 ', and R.sub.1 " to R.sub.21 " preferably contain one or more well-known reactive groups in order that the compound of the general formula (I) can form a covalent bond with a substance from a living organism.

The reactive groups include the reactive sites of isocyanate, isothiocyanate, succinimide ester, sulfosuccinimide ester, imide ester, hydrazine, nitroaryl halide, piperidine disulfide, maleimide, thiophthalimide, acid halide, sulfonyl halide, aziridine, azide nitrophenyl, azide amino, 3-(2-pyridyldithio) propionamide and the like. In these reactive sites, the following spacer groups (n=0, 1 to 6) may be interposed in order to prevent steric hindrance during on the bonding of a labeling agent and a substance from a living organism.

Preferable such reactive groups include isothiocyanate, sulfosuccinimide ester, succinimide ester maleimide and the like X.sub.1.sup..sym. represents an anion, including chloride ion, bromide ion, iodide ion, perchlorate ion, benzenesulfonate ion, p-toluene sulfonate ion, methylsulfate ion, ethylsulfate ion, propylsulfate ion, tetrafluoroborate ion, tetraphenylborate ion, hexafluorophosphate ion, benzenesulfinic acid salt ion, acetate ion, trifluoroacetate ion, propionate ion, benzoate ion, oxalate ion, succinate ion, malonate ion, oleate ion, stearate ion, citrate ion, monohydrogen diphosphate ion, dihydrogen monophosphate ion, pentachlorostannate ion, chlorosulfonate ion, fluorosulfonate ion, trifluoromethane sulfonate ion, hexafluoroantimonate ion, molybdate ion, tungstate ion, titanate ion, zirconate ion and the like.

Specific examples of these labeling agents are illustrated in Tables 1, 2 and 3, but are not limited thereto.

The synthetic method of these azulene dyes is described in U.S. Pat. No. 4,738,908.

## CLAIMS

1. A labeled complex for detecting a subject compound to be analyzed by means of optical means using near-infrared radiation which complex comprises a substance from a living organism and a labeling agent fixed onto the substance, the substance capable of specifically binding to the subject compound, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group, and at least one of A and B is a substituted or unsubstituted aryl group, and at least one of D and E is a substituted or unsubstituted aryl group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.

2. The labeled complex according to claim 1, wherein the substance from a living organism is an antibody or an antigen.

3. The labeled complex according to claim 1, wherein the substance from a living organism is a nucleic acid.

4. The labeled complex according to claim 1, wherein the substituted aryl group constituting at least one of A and B is phenyl group substituted by dialkylamino group.

5. The labeled complex according to claim 1, wherein the substituted aryl group constituting at least one of D and E is phenyl group substituted by dialkylamino group.

6. The labeled complex according to claim 4 or 5, wherein the dialkylamino group is a diethylamino group.

7. The labeled complex according to claim 1, wherein each of A, B and D is dimethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.

8. The labeled complex according to claim 1, wherein each of A, B and D is diethylaminophenyl group, E is phenyl group substituted by carboxyl group, k is 0 and l is 1.

9. The labeled complex according to claim 1, wherein each of A, B, D and E is diethylaminophenyl group, k is 1 and l is 0.

10. The labeled complex according to claim 1, wherein each of A, B, and D is diethylaminophenyl group, E is aminophenyl group, K is 0 and l is 1.

11. The labeled complex according to claim 1, wherein A is dimethylaminophenyl group, each of B and E is ethoxyphenyl group, k is 0, 1 is l and D is represented by the following formula:

12. A method of detecting a subject compound to be analyzed in a sample comprising the steps of: providing a labeled complex comprising a substance from a living organisms and a labeling agent fixed onto the substance, the substance being capable of specifically binding to the subject compound; binding the labeled complex to the subject compound; and detecting the labeled complex to which the subject compound is bonded by means of optical means, wherein the labeling agent comprises a compound represented by the general formula (IV): wherein A, B, D and E are independently selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group having two or more carbon atoms, alkenyl group, aralkyl group, aryl group, styryl group and heterocyclic group, and at least one of A and B is a substituted or unsubstituted aryl group, and at least one of D and E is a substituted or unsubstituted aryl group; r.sub.1 ' and r.sub.2 ' are individually selected from the group consisting of hydrogen atom, a substituted or an unsubstituted alkyl group, cyclic alkyl group, alkenyl group, aralkyl group and aryl group; k is 0 or 1; is 0, 1 or 2; and X.sub.2.sup..crclbar.  represents an anion.

13. The method according to claim 12, wherein the substance from a living organism is an antibody or an antigen.

14. The method according to claim 12, wherein the substance from a living organism is a nucleic acid.

15. The analyzing method according to any one of claims 12, 13 and 14, wherein the optical means is an optical means using near-infrared ray.

16. The method according to claim 12, wherein each of A, B and D is dimethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.

17. The method according to claim 12, wherein each of A, B and D is diethylaminophenyl group, E is phenyl group substituted by carboxyl group, k is 0 and l is 1.

18. The method according to claim 12, wherein each of A, B, D and E is diethylaminophenyl group, k is 1 and l is 0.

19. The method according to claim 12, wherein each of A, B and D is diethylaminophenyl group, E is aminophenyl group, k is 0 and l is 1.

20. The method according to claim 12, wherein A is dimethylaminophenyl group, each of B and E is ethoxyphenyl group, k is 0, l is 1 and D is represented by the following formula: ##STR102##

================================================
File: tests/data/groundtruth/docling_v2/pg06442728.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Methods and apparatus for turbo code
    item-2 at level 2: section_header: ABSTRACT
      item-3 at level 3: paragraph: An interleaver receives incoming data frames of size N. The interleaver indexes the elements of the frame with an N₁×N₂ index array. The interleaver then effectively rearranges (permutes) the data by permuting the rows of the index array. The interleaver employs the equation I(j,k)=I(j,αjk+βj)modP) to permute the columns (indexed by k) of each row (indexed by j). P is at least equal to N₂, βj is a constant which may be different for each row, and each αj is a relative prime number relative to P. After permuting, the interleaver outputs the data in a different order than received (e.g., receives sequentially row by row, outputs sequentially each column by column).
    item-4 at level 2: section_header: CROSS-REFERENCE TO RELATED APPLICATIONS
      item-5 at level 3: paragraph: This application claims the benefit of U.S. Provisional Application No. 60/115,394 filed Jan. 11, 1999.
    item-6 at level 2: section_header: FIELD OF THE INVENTION
      item-7 at level 3: paragraph: This invention relates generally to communication systems and, more particularly, to interleavers for performing code modulation.
    item-8 at level 2: section_header: BACKGROUND OF THE INVENTION
      item-9 at level 3: paragraph: Techniques for encoding communication channels, known as coded modulation, have been found to improve the bit error rate (BER) of electronic communication systems such as modem and wireless communication systems. Turbo coded modulation has proven to be a practical, power-efficient, and bandwidth-efficient modulation method for “random-error” channels characterized by additive white Gaussian noise (AWGN) or fading. These random-error channels can be found, for example, in the code division multiple access (CDMA) environment. Since the capacity of a CDMA environment is dependent upon the operating signal to noise ratio, improved performance translates into higher capacity.
      item-10 at level 3: paragraph: An aspect of turbo coders which makes them so effective is an interleaver which permutes the original received or transmitted data frame before it is input to a second encoder. The permuting is accomplished by randomizing portions of the signal based upon one or more randomizing algorithms. Combining the permuted data frames with the original data frames has been shown to achieve low BERs in AWGN and fading channels. The interleaving process increases the diversity in the data such that if the modulated symbol is distorted in transmission the error may be recoverable with the use of error correcting algorithms in the decoder.
      item-11 at level 3: paragraph: A conventional interleaver collects, or frames, the signal points to be transmitted into an array, where the array is sequentially filled up row by row. After a predefined number of signal points have been framed, the interleaver is emptied by sequentially reading out the columns of the array for transmission. As a result, signal points in the same row of the array that were near each other in the original signal point flow are separated by a number of signal points equal to the number of rows in the array. Ideally, the number of columns and rows would be picked such that interdependent signal points, after transmission, would be separated by more than the expected length of an error burst for the channel.
      item-12 at level 3: paragraph: Non-uniform interleaving achieves “maximum scattering” of data and “maximum disorder” of the output sequence. Thus the redundancy introduced by the two convolutional encoders is more equally spread in the output sequence of the turbo encoder. The minimum distance is increased to much higher values than for uniform interleaving. A persistent problem for non-uniform interleaving is how to practically implement the interleaving while achieving sufficient “non-uniformity,” and minimizing delay compensations which limit the use for applications with real-time requirements.
      item-13 at level 3: paragraph: Finding an effective interleaver is a current topic in the third generation CDMA standard activities. It has been determined and generally agreed that, as the frame size approaches infinity, the most effective interleaver is the random interleaver. However, for finite frame sizes, the decision as to the most effective interleaver is still open for discussion.
      item-14 at level 3: paragraph: Accordingly there exists a need for systems and methods of interleaving codes that improve non-uniformity for finite frame sizes.
      item-15 at level 3: paragraph: There also exists a need for such systems and methods of interleaving codes which are relatively simple to implement.
      item-16 at level 3: paragraph: It is thus an object of the present invention to provide systems and methods of interleaving codes that improve non-uniformity for finite frame sizes.
      item-17 at level 3: paragraph: It is also an object of the present invention to provide systems and methods of interleaving codes which are relatively simple to implement.
      item-18 at level 3: paragraph: These and other objects of the invention will become apparent to those skilled in the art from the following description thereof.
    item-19 at level 2: section_header: SUMMARY OF THE INVENTION
      item-20 at level 3: paragraph: The foregoing objects, and others, may be accomplished by the present invention, which interleaves a data frame, where the data frame has a predetermined size and is made up of portions. An embodiment of the invention includes an interleaver for interleaving these data frames. The interleaver includes an input memory configured to store a received data frame as an array organized into rows and columns, a processor connected to the input memory and configured to permute the received data frame in accordance with the equation D(j,k)=D (j, (αjk+βj)modP), and a working memory in electrical communication with the processor and configured to store a permuted version of the data frame. The elements of the equation are as follows: D is the data frame, j and k are indexes to the rows and columns, respectively, in the data frame, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. (“Relative prime numbers” connotes a set of numbers that have no common divisor other than 1. Members of a set of relative prime numbers, considered by themselves, need not be prime numbers.)
      item-21 at level 3: paragraph: Another embodiment of the invention includes a method of storing a data frame and indexing it by an N₁×N₂ index array I, where the product of N₁ and N₂ is at least equal to N. The elements of the index array indicate positions of the elements of the data frame. The data frame elements may be stored in any convenient manner and need not be organized as an array. The method further includes permuting the index array according to I(j,k)=I(j,(αjk+βj)modP), wherein I is the index array, and as above j and k are indexes to the rows and columns, respectively, in the index array, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. The data frame, as indexed by the permuted index array I, is effectively permuted.
      item-22 at level 3: paragraph: Still another embodiment of the invention includes an interleaver which includes a storage device for storing a data frame and for storing an N₁×N₂ index array I, where the product of N₁ and N₂ is at least equal to N. The elements of the index array indicate positions of the elements of the data frame. The data frame elements may be stored in any convenient manner and need not be organized as an array. The interleaver further includes a permuting device for permuting the index array according to I(j,k)=I(j,(αjk+βj)modP), wherein I is the index array, and as above j and k are indexes to the rows and columns, respectively, in the index array, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. The data frame, as indexed by the permuted index array I, is effectively permuted.
      item-23 at level 3: paragraph: The invention will next be described in connection with certain illustrated embodiments and practices. However, it will be clear to those skilled in the art that various modifications, additions and subtractions can be made without departing from the spirit or scope of the claims.
    item-24 at level 2: section_header: BRIEF DESCRIPTION OF THE DRAWINGS
      item-25 at level 3: paragraph: The invention will be more clearly understood by reference to the following detailed description of an exemplary embodiment in conjunction with the accompanying drawings, in which:
      item-26 at level 3: paragraph: FIG. 1 depicts a diagram of a conventional turbo encoder.
      item-27 at level 3: paragraph: FIG. 2 depicts a block diagram of the interleaver illustrated in FIG. 1;
      item-28 at level 3: paragraph: FIG. 3 depicts an array containing a data frame, and permutation of that array;
      item-29 at level 3: paragraph: FIG. 4 depicts a data frame stored in consecutive storage locations;
      item-30 at level 3: paragraph: FIG. 5 depicts an index array for indexing the data frame shown in FIG. 4, and permutation of the index array.
    item-31 at level 2: section_header: DETAILED DESCRIPTION OF THE INVENTION
      item-32 at level 3: paragraph: FIG. 1 illustrates a conventional turbo encoder. As illustrated, conventional turbo encoders include two encoders 20 and an interleaver 100. An interleaver 100 in accordance with the present invention receives incoming data frames 110 of size N, where N is the number of bits, number of bytes, or the number of some other portion the frame may be separated into, which are regarded as frame elements. The interleaver 100 separates the N frame elements into sets of data, such as rows. The interleaver then rearranges (permutes) the data in each set (row) in a pseudo-random fashion. The interleaver 100 may employ different methods for rearranging the data of the different sets. However, those skilled in the art will recognize that one or more of the methods could be reused on one or more of the sets without departing from the scope of the invention. After permuting the data in each of the sets, the interleaver outputs the data in a different order than received.
      item-33 at level 3: paragraph: The interleaver 100 may store the data frame 110 in an array of size N₁×N₂ such that N₁*N₂=N. An example depicted in FIG. 3 shows an array 350 having 3 rows (N₁=3) of 6 columns (N₂=6)for storing a data frame 110 having 18 elements, denoted Frame Element 00 (FE00) through FE17 (N=18). While this is the preferred method, the array may also be designed such that N₁*N₂ is a fraction of N such that one or more of the smaller arrays is/are operated on in accordance with the present invention and the results from each of the smaller arrays are later combined.
      item-34 at level 3: paragraph: To permute array 350 according to the present invention, each row j of array 350 is individually operated on, to permute the columns k of each row according to the equation:
      item-35 at level 3: paragraph: D₁(j,k)=D(j,(αk+β)modP)
      item-36 at level 3: paragraph: where:
      item-37 at level 3: paragraph: j and k are row and column indices, respectively, in array 350;
      item-38 at level 3: paragraph: P is a number greater than or equal to N₂;
      item-39 at level 3: paragraph: αj and P arc relative prime numbers (one or both can be non-prime numbers, but the only divisor that they have in common is 1);
      item-40 at level 3: paragraph: βj is a constant, one value associated with each row.
      item-41 at level 3: paragraph: Once the data for all of the rows are permuted, the new array is read out column by column. Also, once the rows have been permuted, it is possible (but not required) to permute the data grouped by column before outputting the data. In the event that both the rows and columns are permuted, the rows, the columns or both may be permuted in accordance with the present invention. It is also possible to transpose rows of array, for example by transposing bits in the binary representation of the row index j. (In a four-row array, for example, the second and third rows would be transposed under this scheme.) It is also possible that either the rows or the columns, but not both may be permuted in accordance with a different method of permuting. Those skilled in the art will recognize that the system could be rearranged to store the data column by column, permute each set of data in a column and read out the results row by row without departing from the scope of the invention.
      item-42 at level 3: paragraph: These methods of interleaving are based on number theory and may be implemented in software and/or hardware (i.e. application specific integrated circuits (ASIC), programmable logic arrays (PLA), or any other suitable logic devices). Further, a single pseudo random sequence generator (i.e. m-sequence, M-sequence, Gold sequence, Kasami sequence . . . ) can be employed as the interleaver.
      item-43 at level 3: paragraph: In the example depicted in FIG. 3, the value selected for P is 6, the values of α are 5 for all three rows, and the values of β are 1, 2, and 3 respectively for the three rows. (These are merely exemplary. Other numbers may be chosen to achieve different permutation results.) The values of α (5) are each relative prime numbers relative to the value of P (6), as stipulated above.
      item-44 at level 3: paragraph: Calculating the specified equation with the specified values for permuting row 0 of array D 350 into row 0 of array D₁ 360 proceeds as:
      item-45 at level 3: paragraph: and the permuted data frame is contained in array D₁ 360 shown in FIG. 3. Outputting the array column by column outputs the frame elements in the order:
      item-46 at level 3: paragraph: 1,8,15,0,7,14,5,6,13,4,11,12,3,10,17,2,9,16.
      item-47 at level 3: paragraph: In an alternative practice of the invention, data frame 110 is stored in consecutive storage locations, not as an array or matrix, and a separate index array is stored to index the elements of the data frame, the index array is permuted according to the equations of the present invention, and the data frame is output as indexed by the permuted index array.
      item-48 at level 3: paragraph: FIG. 4 depicts a block 400 of storage 32 elements in length (thus having offsets of 0 through 31 from a starting storage location). A data frame 110, taken in this example to be 22 elements long and thus to consist of elements FE00 through FE21, occupies offset locations 00 through 21 within block 400. Offset locations 22 through 31 of block 400 contain unknown contents. A frame length of 22 elements is merely exemplary, and other lengths could be chosen. Also, storage of the frame elements in consecutive locations is exemplary, and non-consecutive locations could be employed.
      item-49 at level 3: paragraph: FIG. 5 depicts index array I 550 for indexing storage block 400. It is organized as 4 rows of 8 columns each (N₁=4, N₂=8, N=N₁*N₂=32). Initial contents are filled in to array I 550 as shown in FIG. 5 sequentially. This sequential initialization yields the same effect as a row-by-row read-in of data frame 110.
      item-50 at level 3: paragraph: The index array is permuted according to
      item-51 at level 3: paragraph: I₁(j,k)=I(j,(αj*k+βj)modP)
      item-52 at level 3: paragraph: where
      item-53 at level 3: paragraph: α=1, 3, 5, 7
      item-54 at level 3: paragraph: β=0, 0, 0, 0
      item-55 at level 3: paragraph: P=8
      item-56 at level 3: paragraph: These numbers are exemplary and other numbers could be chosen, as long as the stipulations are observed that P is at least equal to N₂ and that each value of α is a relative prime number relative to the chosen value of P.
      item-57 at level 3: paragraph: If the equation is applied to the columns of row 2, for example, it yields:
      item-58 at level 3: paragraph: Applying the equation comparably to rows 0, 1, and 3 produces the permuted index array I₁ 560 shown in FIG. 5.
      item-59 at level 3: paragraph: The data frame 110 is read out of storage block 400 and output in the order specified in the permuted index array I₁ 560 taken column by column. This would output storage locations in offset order:
      item-60 at level 3: paragraph: 0,8,16,24,1,11,21,31,2,14,18,30,3,9,23,29,4,12,20,28,5,15,17,27,6,10,22,26,7,13,19,25.
      item-61 at level 3: paragraph: However, the example assumed a frame length of 22 elements, with offset locations 22-31 in block 400 not being part of the data frame. Accordingly, when outputting the data frame it would be punctured or pruned to a length of 22; i.e., offset locations greater than 21 are ignored. The data frame is thus output with an element order of 0,8,16,1,11,21,2,14,18,3,9,4,12,20,5,15,17,6,10,7,13,19.
      item-62 at level 3: paragraph: In one aspect of the invention, rows of the array may be transposed prior to outputting, for example by reversing the bits in the binary representations of row index j.
      item-63 at level 3: paragraph: There are a number of different ways to implement the interleavers 100 of the present invention. FIG. 2 illustrates an embodiment of the invention wherein the interleaver 100 includes an input memory 300 for receiving and storing the data frame 110. This memory 300 may include shift registers, RAM or the like. The interleaver 100 may also include a working memory 310 which may also include RAM, shift registers or the like. The interleaver includes a processor 320 (e.g., a microprocessor, ASIC, etc.) which may be configured to process I(j,k) in real time according to the above-identified equation or to access a table which includes the results of I(j,k) already stored therein. Those skilled in the art will recognize that memory 300 and memory 310 may be the same memory or they may be separate memories.
      item-64 at level 3: paragraph: For real-time determinations of I(j,k), the first row of the index array is permuted and the bytes corresponding to the permuted index are stored in the working memory. Then the next row is permuted and stored, etc. until all rows have been permuted and stored. The permutation of rows may be done sequentially or in parallel.
      item-65 at level 3: paragraph: Whether the permuted I(j,k) is determined in real time or by lookup, the data may be stored in the working memory in a number of different ways. It can be stored by selecting the data from the input memory in the same order as the I(j,k)s in the permuted index array (i.e., indexing the input memory with the permuting function) and placing them in the working memory in sequential available memory locations. It may also be stored by selecting the bytes in the sequence they were stored in the input memory (i.e., FIFO) and storing them in the working memory directly into the location determined by the permuted I(j,k)s (i.e., indexing the working memory with the permuting function). Once this is done, the data may be read out of the working memory column by column based upon the permuted index array. As stated above, the data could be subjected to another round of permuting after it is stored in the working memory based upon columns rather than on rows to achieve different results.
      item-66 at level 3: paragraph: If the system is sufficiently fast, one of the memories could be eliminated and as a data element is received it could be placed into the working memory, in real time or by table lookup, in the order corresponding to the permuted index array.
      item-67 at level 3: paragraph: The disclosed interleavers are compatible with existing turbo code structures. These interleavers offer superior performance without increasing system complexity.
      item-68 at level 3: paragraph: In addition, those skilled in the art will realize that de-interleavers can be used to decode the interleaved data frames. The construction of de-interleavers used in decoding turbo codes is well known in the art. As such they are not further discussed herein. However, a de-interleaver corresponding to the embodiments can be constructed using the permuted sequences discussed above.
      item-69 at level 3: paragraph: Although the embodiment described above is a turbo encoder such as is found in a CDMA system, those skilled in the art realize that the practice of the invention is not limited thereto and that the invention may be practiced for any type of interleaving and de-interleaving in any communication system.
      item-70 at level 3: paragraph: It will thus be seen that the invention efficiently attains the objects set forth above, among those made apparent from the preceding description. In particular, the invention provides improved apparatus and methods of interleaving codes of finite length while minimizing the complexity of the implementation.
      item-71 at level 3: paragraph: It will be understood that changes may be made in the above construction and in the foregoing sequences of operation without departing from the scope of the invention. It is accordingly intended that all matter contained in the above description or shown in the accompanying drawings be interpreted as illustrative rather than in a limiting sense.
      item-72 at level 3: paragraph: It is also to be understood that the following claims are intended to cover all of the generic and specific features of the invention as described herein, and all statements of the scope of the invention which, as a matter of language, might be said to fall therebetween.
    item-73 at level 2: section_header: CLAIMS
      item-74 at level 3: paragraph: 1. A method of interleaving elements of frames of signal data communication channel, the method comprising; storing a frame of signal data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1; and permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P.
      item-75 at level 3: paragraph: 2. The method according to claim 1 wherein said elements of array D are stored in accordance with a first order and wherein said elements of array D₁ are output in accordance with a second order.
      item-76 at level 3: paragraph: 3. The method according to claim 2 wherein elements of array D are stored row by row and elements of array D₁ are output column by column.
      item-77 at level 3: paragraph: 4. The method according to claim 1 further including outputting of array D₁ and wherein the product of N₁ and N₂ is greater than the number of elements in the frame and the frame is punctured during outputting to the number of elements in the frame.
      item-78 at level 3: paragraph: 5. A method of interleaving elements of frames of signal data communication channel, the method comprising; creating and storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, storing elements of a frame of signal data in each of a plurality of storage locations; storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and permuting array I into array I₁ according to I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, whereby the frame of signal data as indexed by array I₁ is effectively permuted.
      item-79 at level 3: paragraph: 6. The method according to claim 5 further including permuting said stored elements according to said permuted index array I₁.
      item-80 at level 3: paragraph: 7. The method according to claim 5 wherein said elements of the frame of data are output as indexed by entries of array I₁ taken other than row by row.
      item-81 at level 3: paragraph: 8. The method according to claim 7 wherein elements of the frame of data are output as indexed by entries of array I₁ taken column by column.
      item-82 at level 3: paragraph: 9. The method according to claim 5 including the step of transposing rows of array I prior to the step of permuting array I.
      item-83 at level 3: paragraph: 10. The method according to claim 5 wherein N₁ is equal to 4, N₂ is equal to 8, P is equal to 8, and the values of αj are different for each row and are chosen from a group consisting of 1, 3, 5, and 7.
      item-84 at level 3: paragraph: 11. The method according to claim 10 wherein the values of αj are 1, 3, 5, and 7 for j=0, 1, 2, and 3 respectively.
      item-85 at level 3: paragraph: 12. The method according to claim 11 wherein all values of β are zero.
      item-86 at level 3: paragraph: 13. The method according to claim 10 wherein the values of αj are 1, 5, 3, and 7 for j=0, 1, 2, and 3 respectively.
      item-87 at level 3: paragraph: 14. The method according to claim 13 wherein all values of β are zero.
      item-88 at level 3: paragraph: 15. The method according to claim 5 wherein all values of β are zero.
      item-89 at level 3: paragraph: 16. The method according to claim 5 wherein at least two values of β are the same.
      item-90 at level 3: paragraph: 17. The method according to claim 5 further including outputting of the frame of data and wherein the product of N₁ and N₂ is greater than the number of elements in the frame of data and the frame of data is punctured during outputting to the number of elements in the frame of data.
      item-91 at level 3: paragraph: 18. An interleaver for interleaving elements of frames of data, the interleaver comprising; storage means for storing a frame of data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₂−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and permuting means for permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P.
      item-92 at level 3: paragraph: 19. The interleaver according to claim 18 including means for storing said elements of array D in accordance with a first order and means for outputting said elements of array D₁ in accordance with a second order.
      item-93 at level 3: paragraph: 20. The interleaver according to claim 19 wherein said means for storing said elements of array D stores row by row and said means for outputting elements of array D₁ outputs column by column.
      item-94 at level 3: paragraph: 21. The interleaver according to claim 18 including means for outputting said array D₁ and for puncturing said array D₁ to the number of elements in the frame when the product of N₁ and N₂ is greater than the number of elements in the frame.
      item-95 at level 3: paragraph: 22. An interleaver for interleaving elements of frames of data, the interleaver comprising; means for storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and means for receiving a frame of data and storing elements of the frame of data in each of a plurality of storage locations; means for storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and means for permuting array I into array I₁ according to: I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, whereby the frame of data as indexed by array I₁ is effectively permuted.
      item-96 at level 3: paragraph: 23. The interleaver according to claim 22 further including means for permuting said stored elements according to said permuted index array I₁.
      item-97 at level 3: paragraph: 24. The interleaver according to claim 22 including means for outputting frame elements as indexed by entries of array I₁ taken other than row by row.
      item-98 at level 3: paragraph: 25. The interleaver according to claim 24 including means for outputting frame elements as indexed by entries of array I₁ taken column by column.
      item-99 at level 3: paragraph: 26. The interleaver according to claim 22 wherein the product of N₁ and N₂ is greater than the number of elements in the frame and the frame is punctured by the means for outputting to the number of elements in the frame.
      item-100 at level 3: paragraph: 27. An interleaver for interleaving elements of frames of data, the interleaver comprising; an input memory for storing a received frame of data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1; a processor coupled to said input memory for permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, and a working memory coupled to said processor and configured to store the permuted array D₁.
      item-101 at level 3: paragraph: 28. The interlcavcr according to claim 27 wherein said input memory stores said elements of array D in accordance with a first order and said working memory outputs said elements of array D₁ in accordance with a second order.
      item-102 at level 3: paragraph: 29. The interleaver according to claim 28 wherein said input memory stores elements of array D row by row and said working memory outputs elements of array D₁ column by column.
      item-103 at level 3: paragraph: 30. The interleaver according to claim 27 said working memory punctures said array D₁ to the number of elements in the frame when the product of N₁ and N₂ is greater than the number of elements in the frame.
      item-104 at level 3: paragraph: 31. An interleaver for interleaving elements of frames of data, the interleaver comprising; a memory for storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and said memory also for storing elements of a received frame of data in each of a plurality of storage locations; a processor coupled to said memory for storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and said processor also for permuting array I into array I₁ stored in said memory according to: I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, and whereby the frame of data as indexed by array I₁ is effectively permuted.
      item-105 at level 3: paragraph: 32. The interleaver according to claim 31 wherein said processor permutes said stored elements according to said permuted index array I₁.
      item-106 at level 3: paragraph: 33. The interleaver according to claim 31 wherein said memory outputs frame elements as indexed by entries of array I₁ taken other than row by row.
      item-107 at level 3: paragraph: 34. The interleaver according to claim 33 wherein said memory outputs frame elements as indexed by entries of array I₁ taken column by column.
      item-108 at level 3: paragraph: 35. The interleaver according to claim 31 wherein said memory punctures the frame of data to the number of elements in the frame of data when the product of N₁ and N₂ is greater than the number of elements in the frame of data.

================================================
File: tests/data/groundtruth/docling_v2/pg06442728.md
================================================
# Methods and apparatus for turbo code

## ABSTRACT

An interleaver receives incoming data frames of size N. The interleaver indexes the elements of the frame with an N₁×N₂ index array. The interleaver then effectively rearranges (permutes) the data by permuting the rows of the index array. The interleaver employs the equation I(j,k)=I(j,αjk+βj)modP) to permute the columns (indexed by k) of each row (indexed by j). P is at least equal to N₂, βj is a constant which may be different for each row, and each αj is a relative prime number relative to P. After permuting, the interleaver outputs the data in a different order than received (e.g., receives sequentially row by row, outputs sequentially each column by column).

## CROSS-REFERENCE TO RELATED APPLICATIONS

This application claims the benefit of U.S. Provisional Application No. 60/115,394 filed Jan. 11, 1999.

## FIELD OF THE INVENTION

This invention relates generally to communication systems and, more particularly, to interleavers for performing code modulation.

## BACKGROUND OF THE INVENTION

Techniques for encoding communication channels, known as coded modulation, have been found to improve the bit error rate (BER) of electronic communication systems such as modem and wireless communication systems. Turbo coded modulation has proven to be a practical, power-efficient, and bandwidth-efficient modulation method for “random-error” channels characterized by additive white Gaussian noise (AWGN) or fading. These random-error channels can be found, for example, in the code division multiple access (CDMA) environment. Since the capacity of a CDMA environment is dependent upon the operating signal to noise ratio, improved performance translates into higher capacity.

An aspect of turbo coders which makes them so effective is an interleaver which permutes the original received or transmitted data frame before it is input to a second encoder. The permuting is accomplished by randomizing portions of the signal based upon one or more randomizing algorithms. Combining the permuted data frames with the original data frames has been shown to achieve low BERs in AWGN and fading channels. The interleaving process increases the diversity in the data such that if the modulated symbol is distorted in transmission the error may be recoverable with the use of error correcting algorithms in the decoder.

A conventional interleaver collects, or frames, the signal points to be transmitted into an array, where the array is sequentially filled up row by row. After a predefined number of signal points have been framed, the interleaver is emptied by sequentially reading out the columns of the array for transmission. As a result, signal points in the same row of the array that were near each other in the original signal point flow are separated by a number of signal points equal to the number of rows in the array. Ideally, the number of columns and rows would be picked such that interdependent signal points, after transmission, would be separated by more than the expected length of an error burst for the channel.

Non-uniform interleaving achieves “maximum scattering” of data and “maximum disorder” of the output sequence. Thus the redundancy introduced by the two convolutional encoders is more equally spread in the output sequence of the turbo encoder. The minimum distance is increased to much higher values than for uniform interleaving. A persistent problem for non-uniform interleaving is how to practically implement the interleaving while achieving sufficient “non-uniformity,” and minimizing delay compensations which limit the use for applications with real-time requirements.

Finding an effective interleaver is a current topic in the third generation CDMA standard activities. It has been determined and generally agreed that, as the frame size approaches infinity, the most effective interleaver is the random interleaver. However, for finite frame sizes, the decision as to the most effective interleaver is still open for discussion.

Accordingly there exists a need for systems and methods of interleaving codes that improve non-uniformity for finite frame sizes.

There also exists a need for such systems and methods of interleaving codes which are relatively simple to implement.

It is thus an object of the present invention to provide systems and methods of interleaving codes that improve non-uniformity for finite frame sizes.

It is also an object of the present invention to provide systems and methods of interleaving codes which are relatively simple to implement.

These and other objects of the invention will become apparent to those skilled in the art from the following description thereof.

## SUMMARY OF THE INVENTION

The foregoing objects, and others, may be accomplished by the present invention, which interleaves a data frame, where the data frame has a predetermined size and is made up of portions. An embodiment of the invention includes an interleaver for interleaving these data frames. The interleaver includes an input memory configured to store a received data frame as an array organized into rows and columns, a processor connected to the input memory and configured to permute the received data frame in accordance with the equation D(j,k)=D (j, (αjk+βj)modP), and a working memory in electrical communication with the processor and configured to store a permuted version of the data frame. The elements of the equation are as follows: D is the data frame, j and k are indexes to the rows and columns, respectively, in the data frame, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. (“Relative prime numbers” connotes a set of numbers that have no common divisor other than 1. Members of a set of relative prime numbers, considered by themselves, need not be prime numbers.)

Another embodiment of the invention includes a method of storing a data frame and indexing it by an N₁×N₂ index array I, where the product of N₁ and N₂ is at least equal to N. The elements of the index array indicate positions of the elements of the data frame. The data frame elements may be stored in any convenient manner and need not be organized as an array. The method further includes permuting the index array according to I(j,k)=I(j,(αjk+βj)modP), wherein I is the index array, and as above j and k are indexes to the rows and columns, respectively, in the index array, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. The data frame, as indexed by the permuted index array I, is effectively permuted.

Still another embodiment of the invention includes an interleaver which includes a storage device for storing a data frame and for storing an N₁×N₂ index array I, where the product of N₁ and N₂ is at least equal to N. The elements of the index array indicate positions of the elements of the data frame. The data frame elements may be stored in any convenient manner and need not be organized as an array. The interleaver further includes a permuting device for permuting the index array according to I(j,k)=I(j,(αjk+βj)modP), wherein I is the index array, and as above j and k are indexes to the rows and columns, respectively, in the index array, α and β are sets of constants selected according to the current row, and P and each αj are relative prime numbers. The data frame, as indexed by the permuted index array I, is effectively permuted.

The invention will next be described in connection with certain illustrated embodiments and practices. However, it will be clear to those skilled in the art that various modifications, additions and subtractions can be made without departing from the spirit or scope of the claims.

## BRIEF DESCRIPTION OF THE DRAWINGS

The invention will be more clearly understood by reference to the following detailed description of an exemplary embodiment in conjunction with the accompanying drawings, in which:

FIG. 1 depicts a diagram of a conventional turbo encoder.

FIG. 2 depicts a block diagram of the interleaver illustrated in FIG. 1;

FIG. 3 depicts an array containing a data frame, and permutation of that array;

FIG. 4 depicts a data frame stored in consecutive storage locations;

FIG. 5 depicts an index array for indexing the data frame shown in FIG. 4, and permutation of the index array.

## DETAILED DESCRIPTION OF THE INVENTION

FIG. 1 illustrates a conventional turbo encoder. As illustrated, conventional turbo encoders include two encoders 20 and an interleaver 100. An interleaver 100 in accordance with the present invention receives incoming data frames 110 of size N, where N is the number of bits, number of bytes, or the number of some other portion the frame may be separated into, which are regarded as frame elements. The interleaver 100 separates the N frame elements into sets of data, such as rows. The interleaver then rearranges (permutes) the data in each set (row) in a pseudo-random fashion. The interleaver 100 may employ different methods for rearranging the data of the different sets. However, those skilled in the art will recognize that one or more of the methods could be reused on one or more of the sets without departing from the scope of the invention. After permuting the data in each of the sets, the interleaver outputs the data in a different order than received.

The interleaver 100 may store the data frame 110 in an array of size N₁×N₂ such that N₁*N₂=N. An example depicted in FIG. 3 shows an array 350 having 3 rows (N₁=3) of 6 columns (N₂=6)for storing a data frame 110 having 18 elements, denoted Frame Element 00 (FE00) through FE17 (N=18). While this is the preferred method, the array may also be designed such that N₁*N₂ is a fraction of N such that one or more of the smaller arrays is/are operated on in accordance with the present invention and the results from each of the smaller arrays are later combined.

To permute array 350 according to the present invention, each row j of array 350 is individually operated on, to permute the columns k of each row according to the equation:

D₁(j,k)=D(j,(αk+β)modP)

where:

j and k are row and column indices, respectively, in array 350;

P is a number greater than or equal to N₂;

αj and P arc relative prime numbers (one or both can be non-prime numbers, but the only divisor that they have in common is 1);

βj is a constant, one value associated with each row.

Once the data for all of the rows are permuted, the new array is read out column by column. Also, once the rows have been permuted, it is possible (but not required) to permute the data grouped by column before outputting the data. In the event that both the rows and columns are permuted, the rows, the columns or both may be permuted in accordance with the present invention. It is also possible to transpose rows of array, for example by transposing bits in the binary representation of the row index j. (In a four-row array, for example, the second and third rows would be transposed under this scheme.) It is also possible that either the rows or the columns, but not both may be permuted in accordance with a different method of permuting. Those skilled in the art will recognize that the system could be rearranged to store the data column by column, permute each set of data in a column and read out the results row by row without departing from the scope of the invention.

These methods of interleaving are based on number theory and may be implemented in software and/or hardware (i.e. application specific integrated circuits (ASIC), programmable logic arrays (PLA), or any other suitable logic devices). Further, a single pseudo random sequence generator (i.e. m-sequence, M-sequence, Gold sequence, Kasami sequence . . . ) can be employed as the interleaver.

In the example depicted in FIG. 3, the value selected for P is 6, the values of α are 5 for all three rows, and the values of β are 1, 2, and 3 respectively for the three rows. (These are merely exemplary. Other numbers may be chosen to achieve different permutation results.) The values of α (5) are each relative prime numbers relative to the value of P (6), as stipulated above.

Calculating the specified equation with the specified values for permuting row 0 of array D 350 into row 0 of array D₁ 360 proceeds as:

and the permuted data frame is contained in array D₁ 360 shown in FIG. 3. Outputting the array column by column outputs the frame elements in the order:

1,8,15,0,7,14,5,6,13,4,11,12,3,10,17,2,9,16.

In an alternative practice of the invention, data frame 110 is stored in consecutive storage locations, not as an array or matrix, and a separate index array is stored to index the elements of the data frame, the index array is permuted according to the equations of the present invention, and the data frame is output as indexed by the permuted index array.

FIG. 4 depicts a block 400 of storage 32 elements in length (thus having offsets of 0 through 31 from a starting storage location). A data frame 110, taken in this example to be 22 elements long and thus to consist of elements FE00 through FE21, occupies offset locations 00 through 21 within block 400. Offset locations 22 through 31 of block 400 contain unknown contents. A frame length of 22 elements is merely exemplary, and other lengths could be chosen. Also, storage of the frame elements in consecutive locations is exemplary, and non-consecutive locations could be employed.

FIG. 5 depicts index array I 550 for indexing storage block 400. It is organized as 4 rows of 8 columns each (N₁=4, N₂=8, N=N₁*N₂=32). Initial contents are filled in to array I 550 as shown in FIG. 5 sequentially. This sequential initialization yields the same effect as a row-by-row read-in of data frame 110.

The index array is permuted according to

I₁(j,k)=I(j,(αj*k+βj)modP)

where

α=1, 3, 5, 7

β=0, 0, 0, 0

P=8

These numbers are exemplary and other numbers could be chosen, as long as the stipulations are observed that P is at least equal to N₂ and that each value of α is a relative prime number relative to the chosen value of P.

If the equation is applied to the columns of row 2, for example, it yields:

Applying the equation comparably to rows 0, 1, and 3 produces the permuted index array I₁ 560 shown in FIG. 5.

The data frame 110 is read out of storage block 400 and output in the order specified in the permuted index array I₁ 560 taken column by column. This would output storage locations in offset order:

0,8,16,24,1,11,21,31,2,14,18,30,3,9,23,29,4,12,20,28,5,15,17,27,6,10,22,26,7,13,19,25.

However, the example assumed a frame length of 22 elements, with offset locations 22-31 in block 400 not being part of the data frame. Accordingly, when outputting the data frame it would be punctured or pruned to a length of 22; i.e., offset locations greater than 21 are ignored. The data frame is thus output with an element order of 0,8,16,1,11,21,2,14,18,3,9,4,12,20,5,15,17,6,10,7,13,19.

In one aspect of the invention, rows of the array may be transposed prior to outputting, for example by reversing the bits in the binary representations of row index j.

There are a number of different ways to implement the interleavers 100 of the present invention. FIG. 2 illustrates an embodiment of the invention wherein the interleaver 100 includes an input memory 300 for receiving and storing the data frame 110. This memory 300 may include shift registers, RAM or the like. The interleaver 100 may also include a working memory 310 which may also include RAM, shift registers or the like. The interleaver includes a processor 320 (e.g., a microprocessor, ASIC, etc.) which may be configured to process I(j,k) in real time according to the above-identified equation or to access a table which includes the results of I(j,k) already stored therein. Those skilled in the art will recognize that memory 300 and memory 310 may be the same memory or they may be separate memories.

For real-time determinations of I(j,k), the first row of the index array is permuted and the bytes corresponding to the permuted index are stored in the working memory. Then the next row is permuted and stored, etc. until all rows have been permuted and stored. The permutation of rows may be done sequentially or in parallel.

Whether the permuted I(j,k) is determined in real time or by lookup, the data may be stored in the working memory in a number of different ways. It can be stored by selecting the data from the input memory in the same order as the I(j,k)s in the permuted index array (i.e., indexing the input memory with the permuting function) and placing them in the working memory in sequential available memory locations. It may also be stored by selecting the bytes in the sequence they were stored in the input memory (i.e., FIFO) and storing them in the working memory directly into the location determined by the permuted I(j,k)s (i.e., indexing the working memory with the permuting function). Once this is done, the data may be read out of the working memory column by column based upon the permuted index array. As stated above, the data could be subjected to another round of permuting after it is stored in the working memory based upon columns rather than on rows to achieve different results.

If the system is sufficiently fast, one of the memories could be eliminated and as a data element is received it could be placed into the working memory, in real time or by table lookup, in the order corresponding to the permuted index array.

The disclosed interleavers are compatible with existing turbo code structures. These interleavers offer superior performance without increasing system complexity.

In addition, those skilled in the art will realize that de-interleavers can be used to decode the interleaved data frames. The construction of de-interleavers used in decoding turbo codes is well known in the art. As such they are not further discussed herein. However, a de-interleaver corresponding to the embodiments can be constructed using the permuted sequences discussed above.

Although the embodiment described above is a turbo encoder such as is found in a CDMA system, those skilled in the art realize that the practice of the invention is not limited thereto and that the invention may be practiced for any type of interleaving and de-interleaving in any communication system.

It will thus be seen that the invention efficiently attains the objects set forth above, among those made apparent from the preceding description. In particular, the invention provides improved apparatus and methods of interleaving codes of finite length while minimizing the complexity of the implementation.

It will be understood that changes may be made in the above construction and in the foregoing sequences of operation without departing from the scope of the invention. It is accordingly intended that all matter contained in the above description or shown in the accompanying drawings be interpreted as illustrative rather than in a limiting sense.

It is also to be understood that the following claims are intended to cover all of the generic and specific features of the invention as described herein, and all statements of the scope of the invention which, as a matter of language, might be said to fall therebetween.

## CLAIMS

1. A method of interleaving elements of frames of signal data communication channel, the method comprising; storing a frame of signal data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1; and permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P.

2. The method according to claim 1 wherein said elements of array D are stored in accordance with a first order and wherein said elements of array D₁ are output in accordance with a second order.

3. The method according to claim 2 wherein elements of array D are stored row by row and elements of array D₁ are output column by column.

4. The method according to claim 1 further including outputting of array D₁ and wherein the product of N₁ and N₂ is greater than the number of elements in the frame and the frame is punctured during outputting to the number of elements in the frame.

5. A method of interleaving elements of frames of signal data communication channel, the method comprising; creating and storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, storing elements of a frame of signal data in each of a plurality of storage locations; storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and permuting array I into array I₁ according to I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, whereby the frame of signal data as indexed by array I₁ is effectively permuted.

6. The method according to claim 5 further including permuting said stored elements according to said permuted index array I₁.

7. The method according to claim 5 wherein said elements of the frame of data are output as indexed by entries of array I₁ taken other than row by row.

8. The method according to claim 7 wherein elements of the frame of data are output as indexed by entries of array I₁ taken column by column.

9. The method according to claim 5 including the step of transposing rows of array I prior to the step of permuting array I.

10. The method according to claim 5 wherein N₁ is equal to 4, N₂ is equal to 8, P is equal to 8, and the values of αj are different for each row and are chosen from a group consisting of 1, 3, 5, and 7.

11. The method according to claim 10 wherein the values of αj are 1, 3, 5, and 7 for j=0, 1, 2, and 3 respectively.

12. The method according to claim 11 wherein all values of β are zero.

13. The method according to claim 10 wherein the values of αj are 1, 5, 3, and 7 for j=0, 1, 2, and 3 respectively.

14. The method according to claim 13 wherein all values of β are zero.

15. The method according to claim 5 wherein all values of β are zero.

16. The method according to claim 5 wherein at least two values of β are the same.

17. The method according to claim 5 further including outputting of the frame of data and wherein the product of N₁ and N₂ is greater than the number of elements in the frame of data and the frame of data is punctured during outputting to the number of elements in the frame of data.

18. An interleaver for interleaving elements of frames of data, the interleaver comprising; storage means for storing a frame of data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₂−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and permuting means for permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P.

19. The interleaver according to claim 18 including means for storing said elements of array D in accordance with a first order and means for outputting said elements of array D₁ in accordance with a second order.

20. The interleaver according to claim 19 wherein said means for storing said elements of array D stores row by row and said means for outputting elements of array D₁ outputs column by column.

21. The interleaver according to claim 18 including means for outputting said array D₁ and for puncturing said array D₁ to the number of elements in the frame when the product of N₁ and N₂ is greater than the number of elements in the frame.

22. An interleaver for interleaving elements of frames of data, the interleaver comprising; means for storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and means for receiving a frame of data and storing elements of the frame of data in each of a plurality of storage locations; means for storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and means for permuting array I into array I₁ according to: I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, whereby the frame of data as indexed by array I₁ is effectively permuted.

23. The interleaver according to claim 22 further including means for permuting said stored elements according to said permuted index array I₁.

24. The interleaver according to claim 22 including means for outputting frame elements as indexed by entries of array I₁ taken other than row by row.

25. The interleaver according to claim 24 including means for outputting frame elements as indexed by entries of array I₁ taken column by column.

26. The interleaver according to claim 22 wherein the product of N₁ and N₂ is greater than the number of elements in the frame and the frame is punctured by the means for outputting to the number of elements in the frame.

27. An interleaver for interleaving elements of frames of data, the interleaver comprising; an input memory for storing a received frame of data comprising a plurality of elements as an array D having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1; a processor coupled to said input memory for permuting array D into array D₁ according to D₁(𝑗,𝑘)=D(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays D and D₁; k is an index through the columns of arrays D and D₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, and a working memory coupled to said processor and configured to store the permuted array D₁.

28. The interlcavcr according to claim 27 wherein said input memory stores said elements of array D in accordance with a first order and said working memory outputs said elements of array D₁ in accordance with a second order.

29. The interleaver according to claim 28 wherein said input memory stores elements of array D row by row and said working memory outputs elements of array D₁ column by column.

30. The interleaver according to claim 27 said working memory punctures said array D₁ to the number of elements in the frame when the product of N₁ and N₂ is greater than the number of elements in the frame.

31. An interleaver for interleaving elements of frames of data, the interleaver comprising; a memory for storing an index array I having N₁ rows enumerated as 0, 1, . . . N₁−1; and N₂ columns enumerated as 0, 1, . . . N₂−1, wherein N₁ and N₂ are positive integers greater than 1, and said memory also for storing elements of a received frame of data in each of a plurality of storage locations; a processor coupled to said memory for storing in row-by-row sequential positions in array I values indicative of corresponding locations of frame elements; and said processor also for permuting array I into array I₁ stored in said memory according to: I₁(𝑗,𝑘)=I(𝑗,(αj𝑘+βj)𝑚𝑜𝑑𝑃)  wherein j is an index through the rows of arrays I and I₁; k is an index through the columns of arrays I and I₁; αj and βj are integers predetermined for each row j; P is an integer at least equal to N₂; and each αj is a relative prime number relative to P, and whereby the frame of data as indexed by array I₁ is effectively permuted.

32. The interleaver according to claim 31 wherein said processor permutes said stored elements according to said permuted index array I₁.

33. The interleaver according to claim 31 wherein said memory outputs frame elements as indexed by entries of array I₁ taken other than row by row.

34. The interleaver according to claim 33 wherein said memory outputs frame elements as indexed by entries of array I₁ taken column by column.

35. The interleaver according to claim 31 wherein said memory punctures the frame of data to the number of elements in the frame of data when the product of N₁ and N₂ is greater than the number of elements in the frame of data.

================================================
File: tests/data/groundtruth/docling_v2/picture_classification.doctags.txt
================================================
<doctag><section_header_level_1><loc_109><loc_79><loc_206><loc_87>Figures Example</section_header_level_1>
<text><loc_109><loc_94><loc_390><loc_183>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<picture><loc_110><loc_192><loc_389><loc_322><caption><loc_185><loc_334><loc_314><loc_340>Figure 1: This is an example image.</caption></picture>
<text><loc_109><loc_349><loc_390><loc_423>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.</text>
<page_footer><loc_248><loc_439><loc_252><loc_445>1</page_footer>
<page_break>
<text><loc_109><loc_81><loc_390><loc_169>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</text>
<picture><loc_179><loc_176><loc_320><loc_321><caption><loc_185><loc_330><loc_314><loc_336>Figure 2: This is an example image.</caption></picture>
<text><loc_109><loc_345><loc_390><loc_426>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.</text>
<page_footer><loc_248><loc_439><loc_252><loc_445>2</page_footer>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/picture_classification.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "picture_classification", "origin": {"mimetype": "application/pdf", "binary_hash": 6445357065749877499, "filename": "picture_classification.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/pictures/0"}, {"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/texts/5"}, {"cref": "#/pictures/1"}, {"cref": "#/texts/7"}, {"cref": "#/texts/8"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 667.19122, "r": 252.35513, "b": 654.45184, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "Figures Example", "text": "Figures Example", "level": 1}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 642.32806, "r": 477.48276, "b": 501.97412, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/2", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 1, "bbox": {"l": 226.89101, "t": 262.86505, "r": 384.3548, "b": 254.01826000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 35]}], "orig": "Figure 1: This is an example image.", "text": "Figure 1: This is an example image."}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 133.76801, "t": 238.95505000000003, "r": 477.48172000000005, "b": 122.51225, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 747]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."}, {"self_ref": "#/texts/4", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 1, "bbox": {"l": 303.133, "t": 96.27903700000002, "r": 308.11429, "b": 87.43224299999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "1", "text": "1"}, {"self_ref": "#/texts/5", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76801, "t": 664.1490499999999, "r": 477.48172000000005, "b": 523.7951, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 887]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."}, {"self_ref": "#/texts/6", "parent": {"cref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 2, "bbox": {"l": 226.89101, "t": 268.78903, "r": 384.3548, "b": 259.94226000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 35]}], "orig": "Figure 2: This is an example image.", "text": "Figure 2: This is an example image."}, {"self_ref": "#/texts/7", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 133.76801, "t": 245.71804999999995, "r": 477.48172000000005, "b": 117.32024000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 804]}], "orig": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.", "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum."}, {"self_ref": "#/texts/8", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 2, "bbox": {"l": 303.133, "t": 96.27903700000002, "r": 308.11429, "b": 87.43224299999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "2", "text": "2"}], "pictures": [{"self_ref": "#/pictures/0", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/2"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 1, "bbox": {"l": 134.9200439453125, "t": 487.109375, "r": 475.6635437011719, "b": 281.78173828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"cref": "#/texts/2"}], "references": [], "footnotes": [], "image": null, "annotations": []}, {"self_ref": "#/pictures/1", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/6"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 2, "bbox": {"l": 218.8155517578125, "t": 513.9846496582031, "r": 391.96246337890625, "b": 283.10589599609375, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"cref": "#/texts/6"}], "references": [], "footnotes": [], "image": null, "annotations": []}], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 612.0, "height": 792.0}, "image": null, "page_no": 1}, "2": {"size": {"width": 612.0, "height": 792.0}, "image": null, "page_no": 2}}}

================================================
File: tests/data/groundtruth/docling_v2/picture_classification.md
================================================
## Figures Example

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Figure 1: This is an example image.

<!-- image -->

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Figure 2: This is an example image.

<!-- image -->

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.

================================================
File: tests/data/groundtruth/docling_v2/pnas_sample.xml.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: The coreceptor mutation CCR5Δ32  ... V epidemics and is selected for by HIV
    item-2 at level 2: paragraph: Amy D. Sullivan, Janis Wigginton, Denise Kirschner
    item-3 at level 2: paragraph: Department of Microbiology and I ... dical School, Ann Arbor, MI 48109-0620
    item-4 at level 2: section_header: Abstract
      item-5 at level 3: text: We explore the impact of a host  ... creasing the frequency of this allele.
    item-6 at level 2: text: Nineteen million people have die ...  factors such as host genetics (4, 5).
    item-7 at level 2: text: To exemplify the contribution of ...  follow the CCR5Δ32 allelic frequency.
    item-8 at level 2: text: We hypothesize that CCR5Δ32 limi ... g the frequency of this mutant allele.
    item-9 at level 2: text: CCR5 is a host-cell chemokine re ... iral strain (such as X4 or R5X4) (30).
    item-10 at level 2: section_header: The Model
      item-11 at level 3: text: Because we are most concerned wi ... t both economic and social conditions.
      item-12 at level 3: picture
        item-12 at level 4: caption: Figure 1 A schematic representation of the basic compartmental HIV epidemic model. The criss-cross lines indicate the sexual mixing between different compartments. Each of these interactions has a positive probability of taking place; they also incorporate individual rates of transmission indicated as λ, but in full notation is λ î,,→i,j, where i,j,k is the phenotype of the infected partner and î, is the phenotype of the susceptible partner. Also shown are the different rates of disease progression, γ i,j,k , that vary according to genotype, gender, and stage. Thus, the interactions between different genotypes, genders, and stages are associated with a unique probability of HIV infection. M, male; F, female.
      item-13 at level 3: table with [6x5]
        item-13 at level 4: caption: Table 1 Children's genotype
      item-14 at level 3: section_header: Parameter Estimates for the Model.
        item-15 at level 4: text: Estimates for rates that govern  ... d in Fig. 1 are summarized as follows:
        item-16 at level 4: formula:  \frac{dS_{i,j}(t)}{dt}={\chi}_{ ... ,\hat {k}{\rightarrow}i,j}S_{i,j}(t), 
        item-17 at level 4: formula:  \hspace{1em}\hspace{1em}\hspace ... j,A}(t)-{\gamma}_{i,j,A}I_{i,j,A}(t), 
        item-18 at level 4: formula:  \frac{dI_{i,j,B}(t)}{dt}={\gamm ... j,B}(t)-{\gamma}_{i,j,B}I_{i,j,B}(t), 
        item-19 at level 4: formula:  \frac{dA(t)}{dt}={\gamma}_{i,j, ...  \right) -{\mu}_{A}A(t)-{\delta}A(t), 
        item-20 at level 4: text: where, in addition to previously ... on of the infected partner, and j ≠ .
        item-21 at level 4: table with [14x5]
          item-21 at level 5: caption: Table 2 Transmission probabilities
        item-22 at level 4: table with [8x3]
          item-22 at level 5: caption: Table 3 Progression rates
        item-23 at level 4: table with [20x3]
          item-23 at level 5: caption: Table 4 Parameter values
        item-24 at level 4: text: The effects of the CCR5 W/Δ32 an ... nting this probability of infection is
        item-25 at level 4: formula:  {\lambda}_{\hat {i},\hat {j},\h ... \hat {i},\hat {j},\hat {k}} \right] , 
        item-26 at level 4: text: where j ≠  is either male or fe ... e those with AIDS in the simulations).
        item-27 at level 4: text: The average rate of partner acqu ... owing the male rates to vary (36, 37).
        item-28 at level 4: section_header: Transmission probabilities.
          item-29 at level 5: text: The effect of a genetic factor i ... reported; ref. 42) (ref. 43, Table 2).
          item-30 at level 5: text: Given the assumption of no treat ... ases during the end stage of disease).
        item-31 at level 4: section_header: Disease progression.
          item-32 at level 5: text: We assume three stages of HIV in ... ssion rates are summarized in Table 3.
      item-33 at level 3: section_header: Demographic Setting.
        item-34 at level 4: text: Demographic parameters are based ... [suppressing (t) notation]: χ1,j 1,j =
        item-35 at level 4: formula:  B_{r}\hspace{.167em}{ \,\substa ... }+I_{2,M,k})}{N_{M}} \right] + \right 
        item-36 at level 4: formula:  p_{v} \left \left( \frac{(I_{1, ... ght] \right) \right] ,\hspace{.167em} 
        item-37 at level 4: text: where the probability of HIV ver ... heir values are summarized in Table 4.
    item-38 at level 2: section_header: Prevalence of HIV
      item-39 at level 3: section_header: Demographics and Model Validation.
        item-40 at level 4: text: The model was validated by using ... 5% to capture early epidemic behavior.
        item-41 at level 4: text: In deciding on our initial value ... n within given subpopulations (2, 49).
        item-42 at level 4: text: In the absence of HIV infection, ... those predicted by our model (Fig. 2).
        item-43 at level 4: picture
          item-43 at level 5: caption: Figure 2 Model simulation of HIV infection in a population lacking the protective CCR5Δ32 allele compared with national data from Kenya (healthy adults) and Mozambique (blood donors, ref. 17). The simulated population incorporates parameter estimates from sub-Saharan African demographics. Note the two outlier points from the Mozambique data were likely caused by underreporting in the early stages of the epidemic.
      item-44 at level 3: section_header: Effects of the Allele on Prevalence.
        item-45 at level 4: text: After validating the model in th ... among adults for total HIV/AIDS cases.
        item-46 at level 4: text: Although CCR5Δ32/Δ32 homozygosit ... frequency of the mutation as 0.105573.
        item-47 at level 4: text: Fig. 3 shows the prevalence of H ... mic, reaching 18% before leveling off.
        item-48 at level 4: picture
          item-48 at level 5: caption: Figure 3 Prevalence of HIV/AIDS in the adult population as predicted by the model. The top curve (○) indicates prevalence in a population lacking the protective allele. We compare that to a population with 19% heterozygous and 1% homozygous for the allele (implying an allelic frequency of 0.105573. Confidence interval bands (light gray) are shown around the median simulation () providing a range of uncertainty in evaluating parameters for the effect of the mutation on the infectivity and the duration of asymptomatic HIV for heterozygotes.
        item-49 at level 4: text: In contrast, when a proportion o ... gins to decline slowly after 70 years.
        item-50 at level 4: text: In the above simulations we assu ...  in the presence of the CCR5 mutation.
        item-51 at level 4: text: Because some parameters (e.g., r ... s a major influence on disease spread.
    item-52 at level 2: section_header: HIV Induces Selective Pressure on Genotype Frequency
      item-53 at level 3: text: To observe changes in the freque ...  for ≈1,600 years before leveling off.
      item-54 at level 3: picture
        item-54 at level 4: caption: Figure 4 Effects of HIV-1 on selection of the CCR5Δ32 allele. The Hardy-Weinberg equilibrium level is represented in the no-infection simulation (solid lines) for each population. Divergence from the original Hardy-Weinberg equilibrium is shown to occur in the simulations that include HIV infection (dashed lines). Fraction of the total subpopulations are presented: (A) wild types (W/W), (B) heterozygotes (W/Δ32), and (C) homozygotes (Δ32/Δ32). Note that we initiate this simulation with a much lower allelic frequency (0.00105) than used in the rest of the study to better exemplify the actual selective effect over a 1,000-year time scale. (D) The allelic selection effect over a 2,000-year time scale.
    item-55 at level 2: section_header: Discussion
      item-56 at level 3: text: This study illustrates how popul ... pulations where the allele is present.
      item-57 at level 3: text: We also observed that HIV can pr ... is) have been present for much longer.
      item-58 at level 3: text: Two mathematical models have con ... ce of the pathogen constant over time.
      item-59 at level 3: text: Even within our focus on host pr ... f a protective allele such as CCR5Δ32.
      item-60 at level 3: text: Although our models demonstrate  ... f the population to epidemic HIV (16).
      item-61 at level 3: text: In assessing the HIV/AIDS epidem ... for education and prevention programs.
    item-62 at level 2: section_header: Acknowledgments
      item-63 at level 3: text: We thank Mark Krosky, Katia Koel ... ers for extremely insightful comments.
    item-64 at level 2: section_header: References
      item-65 at level 3: list: group list
        item-66 at level 4: list_item: Weiss HA, Hawkes S. Leprosy Rev 72:92–98 (2001). PMID: 11355525
        item-67 at level 4: list_item: Taha TE, Dallabetta GA, Hoover D ...  AIDS 12:197–203 (1998). PMID: 9468369
        item-68 at level 4: list_item: AIDS Epidemic Update. Geneva: World Health Organization1–17 (1998).
        item-69 at level 4: list_item: D'Souza MP, Harden VA. Nat Med 2:1293–1300 (1996). PMID: 8946819
        item-70 at level 4: list_item: Martinson JJ, Chapman NH, Rees D ... Genet 16:100–103 (1997). PMID: 9140404
        item-71 at level 4: list_item: Roos MTL, Lange JMA, deGoede REY ...  Dis 165:427–432 (1992). PMID: 1347054
        item-72 at level 4: list_item: Garred P, Eugen-Olsen J, Iversen ...  Lancet 349:1884 (1997). PMID: 9217763
        item-73 at level 4: list_item: Katzenstein TL, Eugen-Olsen J, H ... rovirol 16:10–14 (1997). PMID: 9377119
        item-74 at level 4: list_item: deRoda H, Meyer K, Katzenstain W ... ce 273:1856–1862 (1996). PMID: 8791590
        item-75 at level 4: list_item: Meyer L, Magierowska M, Hubert J ...  AIDS 11:F73–F78 (1997). PMID: 9302436
        item-76 at level 4: list_item: Smith MW, Dean M, Carrington M,  ... ence 277:959–965 (1997). PMID: 9252328
        item-77 at level 4: list_item: Samson M, Libert F, Doranz BJ, R ... don) 382:722–725 (1996). PMID: 8751444
        item-78 at level 4: list_item: McNicholl JM, Smith DK, Qari SH, ... ct Dis 3:261–271 (1997). PMID: 9284370
        item-79 at level 4: list_item: Michael NL, Chang G, Louie LG, M ... at Med 3:338–340 (1997). PMID: 9055864
        item-80 at level 4: list_item: Mayaud P, Mosha F, Todd J, Balir ... IDS 11:1873–1880 (1997). PMID: 9412707
        item-81 at level 4: list_item: Hoffman IF, Jere CS, Taylor TE,  ... li P, Dyer JR. AIDS 13:487–494 (1998).
        item-82 at level 4: list_item: HIV/AIDS Surveillance Database.  ...  International Programs Center (1999).
        item-83 at level 4: list_item: Anderson RM, May RM, McLean AR.  ... don) 332:228–234 (1988). PMID: 3279320
        item-84 at level 4: list_item: Berger EA, Doms RW, Fenyo EM, Ko ... (London) 391:240 (1998). PMID: 9440686
        item-85 at level 4: list_item: Alkhatib G, Broder CC, Berger EA ... rol 70:5487–5494 (1996). PMID: 8764060
        item-86 at level 4: list_item: Choe H, Farzan M, Sun Y, Sulliva ... ell 85:1135–1148 (1996). PMID: 8674119
        item-87 at level 4: list_item: Deng H, Liu R, Ellmeier W, Choe  ... don) 381:661–666 (1996). PMID: 8649511
        item-88 at level 4: list_item: Doranz BJ, Rucker J, Yi Y, Smyth ... ell 85:1149–1158 (1996). PMID: 8674120
        item-89 at level 4: list_item: Dragic T, Litwin V, Allaway GP,  ... don) 381:667–673 (1996). PMID: 8649512
        item-90 at level 4: list_item: Zhu T, Mo H, Wang N, Nam DS, Cao ... ce 261:1179–1181 (1993). PMID: 8356453
        item-91 at level 4: list_item: Bjorndal A, Deng H, Jansson M, F ... rol 71:7478–7487 (1997). PMID: 9311827
        item-92 at level 4: list_item: Conner RI, Sheridan KE, Ceradini ...  Med 185:621–628 (1997). PMID: 9034141
        item-93 at level 4: list_item: Liu R, Paxton WA, Choe S, Ceradi ...  Cell 86:367–377 (1996). PMID: 8756719
        item-94 at level 4: list_item: Mussico M, Lazzarin A, Nicolosi  ... w) 154:1971–1976 (1994). PMID: 8074601
        item-95 at level 4: list_item: Michael NL, Nelson JA, KewalRama ... rol 72:6040–6047 (1998). PMID: 9621067
        item-96 at level 4: list_item: Hethcote HW, Yorke JA. Gonorrhea ...  and Control. Berlin: Springer (1984).
        item-97 at level 4: list_item: Anderson RM, May RM. Nature (London) 333:514–522 (1988). PMID: 3374601
        item-98 at level 4: list_item: Asiimwe-Okiror G, Opio AA, Musin ... IDS 11:1757–1763 (1997). PMID: 9386811
        item-99 at level 4: list_item: Carael M, Cleland J, Deheneffe J ... AIDS 9:1171–1175 (1995). PMID: 8519454
        item-100 at level 4: list_item: Blower SM, Boe C. J AIDS 6:1347–1352 (1993). PMID: 8254474
        item-101 at level 4: list_item: Kirschner D. J Appl Math 56:143–166 (1996).
        item-102 at level 4: list_item: Le Pont F, Blower S. J AIDS 4:987–999 (1991). PMID: 1890608
        item-103 at level 4: list_item: Kim MY, Lagakos SW. Ann Epidemiol 1:117–128 (1990). PMID: 1669741
        item-104 at level 4: list_item: Anderson RM, May RM. Infectious  ... ol. Oxford: Oxford Univ. Press (1992).
        item-105 at level 4: list_item: Ragni MV, Faruki H, Kingsley LA. ... ed Immune Defic Syndr 17:42–45 (1998).
        item-106 at level 4: list_item: Kaplan JE, Khabbaz RF, Murphy EL ... virol 12:193–201 (1996). PMID: 8680892
        item-107 at level 4: list_item: Padian NS, Shiboski SC, Glass SO ... nghoff E. Am J Edu 146:350–357 (1997).
        item-108 at level 4: list_item: Leynaert B, Downs AM, de Vincenzi I. Am J Edu 148:88–96 (1998).
        item-109 at level 4: list_item: Garnett GP, Anderson RM. J Acquired Immune Defic Syndr 9:500–513 (1995).
        item-110 at level 4: list_item: Stigum H, Magnus P, Harris JR, S ... eteig LS. Am J Edu 145:636–643 (1997).
        item-111 at level 4: list_item: Ho DD, Neumann AU, Perelson AS,  ... don) 373:123–126 (1995). PMID: 7816094
        item-112 at level 4: list_item: World Resources (1998–1999). Oxford: Oxford Univ. Press (1999).
        item-113 at level 4: list_item: Kostrikis LG, Neumann AU, Thomso ...  73:10264–10271 (1999). PMID: 10559343
        item-114 at level 4: list_item: Low-Beer D, Stoneburner RL, Muku ... at Med 3:553–557 (1997). PMID: 9142126
        item-115 at level 4: list_item: Grosskurth H, Mosha F, Todd J, S ... . AIDS 9:927–934 (1995). PMID: 7576329
        item-116 at level 4: list_item: Melo J, Beby-Defaux A, Faria C,  ... AIDS 23:203–204 (2000). PMID: 10737436
        item-117 at level 4: list_item: Iman RL, Helton JC, Campbell JE. J Quality Technol 13:174–183 (1981).
        item-118 at level 4: list_item: Iman RL, Helton JC, Campbell JE. J Quality Technol 13:232–240 (1981).
        item-119 at level 4: list_item: Blower SM, Dowlatabadi H. Int Stat Rev 62:229–243 (1994).
        item-120 at level 4: list_item: Porco TC, Blower SM. Theor Popul Biol 54:117–132 (1998). PMID: 9733654
        item-121 at level 4: list_item: Blower SM, Porco TC, Darby G. Nat Med 4:673–678 (1998). PMID: 9623975
        item-122 at level 4: list_item: Libert F, Cochaux P, Beckman G,  ...  Genet 7:399–406 (1998). PMID: 9466996
        item-123 at level 4: list_item: Lalani AS, Masters J, Zeng W, Ba ... e 286:1968–1971 (1999). PMID: 10583963
        item-124 at level 4: list_item: Kermack WO, McKendrick AG. Proc R Soc London 261:700–721 (1927).
        item-125 at level 4: list_item: Gupta S, Hill AVS. Proc R Soc London Ser B 260:271–277 (1995).
        item-126 at level 4: list_item: Ruwende C, Khoo SC, Snow RW, Yat ... don) 376:246–249 (1995). PMID: 7617034
        item-127 at level 4: list_item: McDermott DH, Zimmerman PA, Guig ... ncet 352:866–870 (1998). PMID: 9742978
        item-128 at level 4: list_item: Kostrikis LG, Huang Y, Moore JP, ... at Med 4:350–353 (1998). PMID: 9500612
        item-129 at level 4: list_item: Winkler C, Modi W, Smith MW, Nel ... ence 279:389–393 (1998). PMID: 9430590
        item-130 at level 4: list_item: Martinson JJ, Hong L, Karanicola ... AIDS 14:483–489 (2000). PMID: 10780710
        item-131 at level 4: list_item: Vernazza PL, Eron JJ, Fiscus SA, ... AIDS 13:155–166 (1999). PMID: 10202821
  item-132 at level 1: caption: Figure 1 A schematic representat ...  of HIV infection. M, male; F, female.
  item-133 at level 1: caption: Table 1 Children's genotype
  item-134 at level 1: caption: Table 2 Transmission probabilities
  item-135 at level 1: caption: Table 3 Progression rates
  item-136 at level 1: caption: Table 4 Parameter values
  item-137 at level 1: caption: Figure 2 Model simulation of HIV ... g in the early stages of the epidemic.
  item-138 at level 1: caption: Figure 3 Prevalence of HIV/AIDS  ... of asymptomatic HIV for heterozygotes.
  item-139 at level 1: caption: Figure 4 Effects of HIV-1 on sel ... n effect over a 2,000-year time scale.

================================================
File: tests/data/groundtruth/docling_v2/pnas_sample.xml.md
================================================
# The coreceptor mutation CCR5Δ32 influences the dynamics of HIV epidemics and is selected for by HIV

Amy D. Sullivan, Janis Wigginton, Denise Kirschner

Department of Microbiology and Immunology, University  of Michigan Medical School, Ann Arbor, MI 48109-0620

## Abstract

We explore the impact of a host genetic factor on heterosexual HIV epidemics by using a deterministic mathematical model. A protective allele unequally distributed across populations is exemplified in our models by the 32-bp deletion in the host-cell chemokine receptor CCR5, CCR5Δ32. Individuals homozygous for CCR5Δ32 are protected against HIV infection whereas those heterozygous for CCR5Δ32 have lower pre-AIDS viral loads and delayed progression to AIDS. CCR5Δ32 may limit HIV spread by decreasing the probability of both risk of infection and infectiousness. In this work, we characterize epidemic HIV within three dynamic subpopulations: CCR5/CCR5 (homozygous, wild type), CCR5/CCR5Δ32 (heterozygous), and CCR5Δ32/CCR5Δ32 (homozygous, mutant). Our results indicate that prevalence of HIV/AIDS is greater in populations lacking the CCR5Δ32 alleles (homozygous wild types only) as compared with populations that include people heterozygous or homozygous for CCR5Δ32. Also, we show that HIV can provide selective pressure for CCR5Δ32, increasing the frequency of this allele.

Nineteen million people have died of AIDS since the discovery of HIV in the 1980s. In 1999 alone, 5.4 million people were newly infected with HIV (ref. 1 and http://www.unaids.org/epidemicupdate/report/Epireport.html). (For brevity, HIV-1 is referred to as HIV in this paper.) Sub-Saharan Africa has been hardest hit, with more than 20% of the general population HIV-positive in some countries (2, 3). In comparison, heterosexual epidemics in developed, market-economy countries have not reached such severe levels. Factors contributing to the severity of the epidemic in economically developing countries abound, including economic, health, and social differences such as high levels of sexually transmitted diseases and a lack of prevention programs. However, the staggering rate at which the epidemic has spread in sub-Saharan Africa has not been adequately explained. The rate and severity of this epidemic also could indicate a greater underlying susceptibility to HIV attributable not only to sexually transmitted disease, economics, etc., but also to other more ubiquitous factors such as host genetics (4, 5).

To exemplify the contribution of such a host genetic factor to HIV prevalence trends, we consider a well-characterized 32-bp deletion in the host-cell chemokine receptor CCR5, CCR5Δ32. When HIV binds to host cells, it uses the CD4 receptor on the surface of host immune cells together with a coreceptor, mainly the CCR5 and CXCR4 chemokine receptors (6). Homozygous mutations for this 32-bp deletion offer almost complete protection from HIV infection, and heterozygous mutations are associated with lower pre-AIDS viral loads and delayed progression to AIDS (7–14). CCR5Δ32 generally is found in populations of European descent, with allelic frequencies ranging from 0 to 0.29 (13). African and Asian populations studied outside the United States or Europe appear to lack the CCR5Δ32 allele, with an allelic frequency of almost zero (5, 13). Thus, to understand the effects of a protective allele, we use a mathematical model to track prevalence of HIV in populations with or without CCR5Δ32 heterozygous and homozygous people and also to follow the CCR5Δ32 allelic frequency.

We hypothesize that CCR5Δ32 limits epidemic HIV by decreasing infection rates, and we evaluate the relative contributions to this by the probability of infection and duration of infectivity. To capture HIV infection as a chronic infectious disease together with vertical transmission occurring in untreated mothers, we model a dynamic population (i.e., populations that vary in growth rates because of fluctuations in birth or death rates) based on realistic demographic characteristics (18). This scenario also allows tracking of the allelic frequencies over time. This work considers how a specific host genetic factor affecting HIV infectivity and viremia at the individual level might influence the epidemic in a dynamic population and how HIV exerts selective pressure, altering the frequency of this mutant allele.

CCR5 is a host-cell chemokine receptor, which is also used as a coreceptor by R5 strains of HIV that are generally acquired during sexual transmission (6, 19–25). As infection progresses to AIDS the virus expands its repertoire of potential coreceptors to include other CC-family and CXC-family receptors in roughly 50% of patients (19, 26, 27). CCR5Δ32 was identified in HIV-resistant people (28). Benefits to individuals from the mutation in this allele are as follows. Persons homozygous for the CCR5Δ32 mutation are almost nonexistent in HIV-infected populations (11, 12) (see ref. 13 for review). Persons heterozygous for the mutant allele (CCR5 W/Δ32) tend to have lower pre-AIDS viral loads. Aside from the beneficial effects that lower viral loads may have for individuals, there is also an altruistic effect, as transmission rates are reduced for individuals with low viral loads (as compared with, for example, AZT and other studies; ref. 29). Finally, individuals heterozygous for the mutant allele (CCR5 W/Δ32) also have a slower progression to AIDS than those homozygous for the wild-type allele (CCR5 W/W) (7–10), remaining in the population 2 years longer, on average. Interestingly, the dearth of information on HIV disease progression in people homozygous for the CCR5Δ32 allele (CCR5 Δ32/Δ32) stems from the rarity of HIV infection in this group (4, 12, 28). However, in case reports of HIV-infected CCR5 Δ32/Δ32 homozygotes, a rapid decline in CD4+ T cells and a high viremia are observed, likely because of initial infection with a more aggressive viral strain (such as X4 or R5X4) (30).

## The Model

Because we are most concerned with understanding the severity of the epidemic in developing countries where the majority of infection is heterosexual, we consider a purely heterosexual model. To model the effects of the allele in the population, we examine the rate of HIV spread by using an enhanced susceptible-infected-AIDS model of epidemic HIV (for review see ref. 31). Our model compares two population scenarios: a CCR5 wild-type population and one with CCR5Δ32 heterozygotes and homozygotes in addition to the wild type. To model the scenario where there are only wild-type individuals present in the population (i.e., CCR5 W/W), we track the sexually active susceptibles at time t [Si,j (t)], where i = 1 refers to genotype (CCR5 W/W only in this case) and j is either the male or female subpopulation. We also track those who are HIV-positive at time t not yet having AIDS in Ii,j,k (t) where k refers to stage of HIV infection [primary (A) or asymptomatic (B)]. The total number of individuals with AIDS at time t are tracked in A(t). The source population are children, χ i,j (t), who mature into the sexually active population at time t (Fig. 1, Table 1). We compare the model of a population lacking the CCR5Δ32 allele to a demographically similar population with a high frequency of the allele. When genetic heterogeneity is included, male and female subpopulations are each further divided into three distinct genotypic groups, yielding six susceptible subpopulations, [Si,j (t), where i ranges from 1 to 3, where 1 = CCR5W/W; 2 = CCR5 W/Δ32; 3 = CCR5 Δ32/Δ32]. The infected classes, Ii,j,k (t), also increase in number to account for these new genotype compartments. In both settings we assume there is no treatment available and no knowledge of HIV status by people in the early acute and middle asymptomatic stages (both conditions exist in much of sub-Saharan Africa). In addition, we assume that sexual mixing in the population occurs randomly with respect to genotype and HIV disease status, all HIV-infected people eventually progress to AIDS, and no barrier contraceptives are used. These last assumptions reflect both economic and social conditions.

Figure 1 A schematic representation of the basic compartmental HIV epidemic model. The criss-cross lines indicate the sexual mixing between different compartments. Each of these interactions has a positive probability of taking place; they also incorporate individual rates of transmission indicated as λ, but in full notation is λ î,,→i,j, where i,j,k is the phenotype of the infected partner and î, is the phenotype of the susceptible partner. Also shown are the different rates of disease progression, γ i,j,k , that vary according to genotype, gender, and stage. Thus, the interactions between different genotypes, genders, and stages are associated with a unique probability of HIV infection. M, male; F, female.

<!-- image -->

Table 1 Children's genotype

| Parents   | Mother   | Mother             | Mother                       | Mother             |
|-----------|----------|--------------------|------------------------------|--------------------|
|           |          |                    |                              |                    |
| Father    |          | W/W                | W/Δ32                        | Δ32/Δ32            |
|           | W/W      | χ1,j 1,j           | χ1,j 1,j, χ2,j 2,j           | χ2,j 2,j           |
|           | W/Δ32    | χ1,j 1,j, χ2,j 2,j | χ1,j 1,j, χ2,j 2,j, χ3,j 3,j | χ2,j 2,j, χ3,j 3,j |
|           | Δ32/Δ32  | χ2,j 2,j           | χ2,j 2,j, χ3,j 3,j           | χ3,j 3,j           |

### Parameter Estimates for the Model.

Estimates for rates that govern the interactions depicted in Fig. 1 were derived from the extensive literature on HIV. Our parameters and their estimates are summarized in Tables 2–4. The general form of the equations describing the rates of transition between population classes as depicted in Fig. 1 are summarized as follows:

$$ \frac{dS_{i,j}(t)}{dt}={\chi}_{i,j}(t)-{\mu}_{j}S_{i,j}(t)-{\lambda}_{\hat {\imath},\hat {},\hat {k}{\rightarrow}i,j}S_{i,j}(t), $$

$$ \hspace{1em}\hspace{1em}\hspace{.167em}\frac{dI_{i,j,A}(t)}{dt}={\lambda}_{\hat {\imath},\hat {},\hat {k}{\rightarrow}i,j}S_{i,j}(t)-{\mu}_{j}I_{i,j,A}(t)-{\gamma}_{i,j,A}I_{i,j,A}(t), $$

$$ \frac{dI_{i,j,B}(t)}{dt}={\gamma}_{i,j,A}I_{i,j,A}(t)-{\mu}_{j}I_{i,j,B}(t)-{\gamma}_{i,j,B}I_{i,j,B}(t), $$

$$ \frac{dA(t)}{dt}={\gamma}_{i,j,B} \left( { \,\substack{ ^{3} \\ {\sum} \\ _{i=1} }\, }I_{i,F,B}(t)+I_{i,M,B}(t) \right) -{\mu}_{A}A(t)-{\delta}A(t), $$

where, in addition to previously defined populations and rates (with i equals genotype, j equals gender, and k equals stage of infection, either A or B), μ j , represents the non-AIDS (natural) death rate for males and females respectively, and μA is estimated by the average (μF + μM/2). This approximation allows us to simplify the model (only one AIDS compartment) without compromising the results, as most people with AIDS die of AIDS (δAIDS) and very few of other causes (μA). These estimates include values that affect infectivity (λ î,,→i,j ), transmission (β î,,→i,j ), and disease progression (γ i  ,  j  ,  k ) where the î,, notation represents the genotype, gender, and stage of infection of the infected partner, and j ≠ .

Table 2 Transmission probabilities

| HIV-infected partner (îıı^^, ^^, k k^^)   | Susceptible partner (i, j)   | Susceptible partner (i, j)   | Susceptible partner (i, j)   | Susceptible partner (i, j)   |
|-----------------------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|
| HIV-infected partner (îıı^^, ^^, k k^^)   |                              |                              |                              |                              |
| HIV-infected partner (îıı^^, ^^, k k^^)   | (^^ to j)                 | W/W                          | W/Δ32                        | Δ32/Δ32                      |
|                                               |                              |                              |                              |                              |
| Acute/primary                                 |                              |                              |                              |                              |
| W/W or Δ32/Δ32                                | M to F                       | 0.040                        | 0.040                        | 0.00040                      |
|                                               | F to M                       | 0.020                        | 0.020                        | 0.00020                      |
| W/Δ32                                         | M to F                       | 0.030                        | 0.030                        | 0.00030                      |
|                                               | F to M                       | 0.015                        | 0.015                        | 0.00015                      |
| Asymptomatic                                  |                              |                              |                              |                              |
| W/W or Δ32/Δ32                                | M to F                       | 0.0010                       | 0.0010                       | 10 × 10−6                    |
|                                               | F to M                       | 0.0005                       | 0.0005                       | 5 × 10−6                     |
| W/Δ32                                         | M to F                       | 0.0005                       | 0.0005                       | 5 × 10−6                     |
|                                               | F to M                       | 0.00025                      | 0.00025                      | 2.5 × 10−6                   |

Table 3 Progression rates

| Genotype   | Disease stage   | Males/females    |
|------------|-----------------|------------------|
|            |                 |                  |
| W/W        | A               | 3.5              |
|            | B               | 0.16667          |
| W/Δ32      | A               | 3.5              |
|            | B               | 0.125            |
| Δ32/Δ32    | A               | 3.5              |
|            | B               | 0.16667          |

Table 4 Parameter values

| Parameter                               | Definition                                               | Value                   |
|-----------------------------------------|----------------------------------------------------------|-------------------------|
|                                         |                                                          |                         |
| μ F  F, μ M  M                          | All-cause mortality for adult females (males)            | 0.015 (0.016) per year  |
| μχχ                                     | All-cause childhood mortality (&lt;15 years of age)         | 0.01 per year           |
| B  r  r                                 | Birthrate                                                | 0.25 per woman per year |
| SA  F  F                                | Percent females acquiring new partners (sexual activity) | 10%                     |
| SA  M  M                                | Percent males acquiring new partners (sexual activity)   | 25%                     |
| m  F  F(ς$$ {\mathrm{_{{F}}^{{2}}}} $$) | Mean (variance) no. of new partners for females          | 1.8 (1.2) per year      |
| ς$$ {\mathrm{_{{M}}^{{2}}}} $$          | Variance in no. of new partners for males                | 5.5 per year            |
| 1 − p  v  v                             | Probability of vertical transmission                     | 0.30 per birth          |
| I  i,j,k  i,j,k(0)                      | Initial total population HIV-positive                    | 0.50%                   |
| χ i,j  i,j(0)                           | Initial total children in population (&lt;15 years of age)  | 45%                     |
| W/W (0)                                 | Initial total wild types (W/W) in population             | 80%                     |
| W/Δ32(0)                                | Initial total heterozygotes (W/Δ32) in population        | 19%                     |
| Δ32/Δ32(0)                              | Initial total homozygotes (Δ32/Δ32) in population        | 1%                      |
| r  M  M(r  F  F)                        | Initial percent males (females) in total population      | 49% (51%)               |
| ϕ F  F, ϕ M  M                          | Number of sexual contacts a female (male) has            | 30 (24) per partner     |
| ɛ i,j,k  i,j,k                          | % effect of mutation on transmission rates (see Table 2) | 0 &lt; ɛ i,j,k  i,j,k &lt; 1  |
| δ                                       | Death rate for AIDS population                           | 1.0 per year            |
| q                                       | Allelic frequency of Δ32 allele                          | 0.105573                |

The effects of the CCR5 W/Δ32 and CCR5 Δ32/Δ32 genotypes are included in our model through both the per-capita probabilities of infection, λ î,,→i,j , and the progression rates, γ i  ,  j  ,  k . The infectivity coefficients, λ î,,→i,j , are calculated for each population subgroup based on the following: likelihood of HIV transmission in a sexual encounter between a susceptible and an infected (βîıı^^,j,k k^^→i,j ) person; formation of new partnerships (c  j  j); number of contacts in a given partnership (ϕ j ); and probability of encountering an infected individual (I  î,, /N   ). The formula representing this probability of infection is

$$ {\lambda}_{\hat {i},\hat {j},\hat {k}{\rightarrow}i,j}=\frac{C_{j}{\cdot}{\phi}_{j}}{N_{\hat {j}}}\hspace{.167em} \left[ { \,\substack{ \\ {\sum} \\ _{\hat {i},\hat {k}} }\, }{\beta}_{\hat {i},\hat {j},\hat {k}{\rightarrow}i,j}{\cdot}I_{\hat {i},\hat {j},\hat {k}} \right] , $$

where j ≠  is either male or female. N    represents the total population of gender  (this does not include those with AIDS in the simulations).

The average rate of partner acquisition, cj , includes the mean plus the variance to mean ratio of the relevant distribution of partner-change rates to capture the small number of high-risk people: cj  = mj  + (ς/m j) where the mean (mj ) and variance (ς) are annual figures for new partnerships only (32). These means are estimated from Ugandan data for the number of heterosexual partners in the past year (33) and the number of nonregular heterosexual partners (i.e., spouses or long-term partners) in the past year (34). In these sexual activity surveys, men invariably have more new partnerships; thus, we assumed that they would have fewer average contacts per partnership than women (a higher rate of new partner acquisition means fewer sexual contacts with a given partner; ref. 35). To incorporate this assumption in our model, the male contacts/partnership, ϕ M , was reduced by 20%. In a given population, the numbers of heterosexual interactions must equate between males and females. The balancing equation applied here is SA F·m F·N F = SA M·m M·N M, where SAj  are the percent sexually active and Nj  are the total in the populations for gender j. To specify changes in partner acquisition, we apply a male flexibility mechanism, holding the female rate of acquisition constant and allowing the male rates to vary (36, 37).

#### Transmission probabilities.

The effect of a genetic factor in a model of HIV transmission can be included by reducing the transmission coefficient. The probabilities of transmission per contact with an infected partner, βîıı^^,^^,k k^^→i,j , have been estimated in the literature (see ref. 38 for estimates in minimally treated groups). We want to capture a decreased risk in transmission based on genotype (ref. 39, Table 2). No studies have directly evaluated differences in infectivity between HIV-infected CCR5 W/Δ32 heterozygotes and HIV-infected CCR5 wild types. Thus, we base estimates for reduced transmission on studies of groups with various HIV serum viral loads (40), HTLV-I/II viral loads (41), and a study of the effect of AZT treatment on transmission (29). We decrease transmission probabilities for infecting CCR5Δ32/Δ32 persons by 100-fold to reflect the rarity of infections in these persons. However, we assume that infected CCR5Δ32/Δ32 homozygotes can infect susceptibles at a rate similar to CCR5W/W homozygotes, as the former generally have high viremias (ref. 30, Table 2). We also assume that male-to-female transmission is twice as efficient as female-to-male transmission (up to a 9-fold difference has been reported; ref. 42) (ref. 43, Table 2).

Given the assumption of no treatment, the high burden of disease in people with AIDS is assumed to greatly limit their sexual activity. Our initial model excludes people with AIDS from the sexually active groups. Subsequently, we allow persons with AIDS to be sexually active, fixing their transmission rates (βAIDS) to be the same across all CCR5 genotypes, and lower than transmission rates for primary-stage infection (as the viral burden on average is not as high as during the acute phase), and larger than transmission rates for asymptomatic-stage infection (as the viral burden characteristically increases during the end stage of disease).

#### Disease progression.

We assume three stages of HIV infection: primary (acute, stage A), asymptomatic HIV (stage B), and AIDS. The rates of transition through the first two stages are denoted by γ i,j,k  i,j,k, where i represents genotype, j is male/female, and k represents either stage A or stage B. Transition rates through each of these stages are assumed to be inversely proportional to the duration of that stage; however, other distributions are possible (31, 44, 45). Although viral loads generally peak in the first 2 months of infection, steady-state viral loads are established several months beyond this (46). For group A, the primary HIV-infecteds, duration is assumed to be 3.5 months. Based on results from European cohort studies (7–10), the beneficial effects of the CCR5 W/Δ32 genotype are observed mainly in the asymptomatic years of HIV infection; ≈7 years after seroconversion survival rates appear to be quite similar between heterozygous and homozygous individuals. We also assume that CCR5Δ32/Δ32-infected individuals and wild-type individuals progress similarly, and that men and women progress through each disease stage at the same rate. Given these observations, and that survival after infection may be shorter in untreated populations, we choose the duration time in stage B to be 6 years for wild-type individuals and 8 years for heterozygous individuals. Transition through AIDS, δAIDS, is inversely proportional to the duration of AIDS. We estimate this value to be 1 year for the time from onset of AIDS to death. The progression rates are summarized in Table 3.

### Demographic Setting.

Demographic parameters are based on data from Malawi, Zimbabwe, and Botswana (3, 47). Estimated birth and child mortality rates are used to calculate the annual numbers of children (χ i,j  i,j) maturing into the potentially sexually active, susceptible group at the age of 15 years (3). For example, in the case where the mother is CCR5 wild type and the father is CCR5 wild type or heterozygous, the number of CCR5 W/W children is calculated as follows [suppressing (t) notation]: χ1,j 1,j =

$$ B_{r}\hspace{.167em}{ \,\substack{ \\ {\sum} \\ _{k} }\, } \left[ S_{1,F}\frac{(S_{1,M}+I_{1,M,k})}{N_{M}}+ \left[ (0.5)S_{1,F}\frac{(S_{2,M}+I_{2,M,k})}{N_{M}} \right] + \right $$

$$ p_{v} \left \left( \frac{(I_{1,F,k}(S_{1,M}+I_{1,M,k}))}{N_{M}}+ \left[ (0.5)I_{1,F,k}\frac{(S_{2,M}+I_{2,M,k})}{N_{M}} \right] \right) \right] ,\hspace{.167em} $$

where the probability of HIV vertical transmission, 1 − pv , and the birthrate, Br , are both included in the equations together with the Mendelian inheritance values as presented in Table 1. The generalized version of this equation (i.e., χ i,j  i,j) can account for six categories of children (including gender and genotype). We assume that all children of all genotypes are at risk, although we can relax this condition if data become available to support vertical protection (e.g., ref. 48). All infected children are assumed to die before age 15. Before entering the susceptible group at age 15, there is additional loss because of mortality from all non-AIDS causes occurring less than 15 years of age at a rate of μχχ × χ i,j  i,j (where μχ is the mortality under 15 years of age). Children then enter the population as susceptibles at an annual rate, ς j  j × χ i,j  i,j/15, where ς j  distributes the children 51% females and 49% males. All parameters and their values are summarized in Table 4.

## Prevalence of HIV

### Demographics and Model Validation.

The model was validated by using parameters estimated from available demographic data. Simulations were run in the absence of HIV infection to compare the model with known population growth rates. Infection was subsequently introduced with an initial low HIV prevalence of 0.5% to capture early epidemic behavior.

In deciding on our initial values for parameters during infection, we use Joint United Nations Programme on HIV/AIDS national prevalence data for Malawi, Zimbabwe, and Botswana. Nationwide seroprevalence of HIV in these countries varies from ≈11% to over 20% (3), although there may be considerable variation within given subpopulations (2, 49).

In the absence of HIV infection, the annual percent population growth rate in the model is ≈2.5%, predicting the present-day values for an average of sub-Saharan African cities (data not shown). To validate the model with HIV infection, we compare our simulation of the HIV epidemic to existing prevalence data for Kenya and Mozambique (http://www.who.int/emc-hiv/fact-sheets/pdfs/kenya.pdf and ref. 51). Prevalence data collected from these countries follow similar trajectories to those predicted by our model (Fig. 2).

Figure 2 Model simulation of HIV infection in a population lacking the protective CCR5Δ32 allele compared with national data from Kenya (healthy adults) and Mozambique (blood donors, ref. 17). The simulated population incorporates parameter estimates from sub-Saharan African demographics. Note the two outlier points from the Mozambique data were likely caused by underreporting in the early stages of the epidemic.

<!-- image -->

### Effects of the Allele on Prevalence.

After validating the model in the wild type-only population, both CCR5Δ32 heterozygous and homozygous people are included. Parameter values for HIV transmission, duration of illness, and numbers of contacts per partner are assumed to be the same within both settings. We then calculate HIV/AIDS prevalence among adults for total HIV/AIDS cases.

Although CCR5Δ32/Δ32 homozygosity is rarely seen in HIV-positive populations (prevalence ranges between 0 and 0.004%), 1–20% of people in HIV-negative populations of European descent are homozygous. Thus, to evaluate the potential impact of CCR5Δ32, we estimate there are 19% CCR5 W/Δ32 heterozygous and 1% CCR5 Δ32/Δ32 homozygous people in our population. These values are in Hardy-Weinberg equilibrium with an allelic frequency of the mutation as 0.105573.

Fig. 3 shows the prevalence of HIV in two populations: one lacking the mutant CCR5 allele and another carrying that allele. In the population lacking the protective mutation, prevalence increases logarithmically for the first 35 years of the epidemic, reaching 18% before leveling off.

Figure 3 Prevalence of HIV/AIDS in the adult population as predicted by the model. The top curve (○) indicates prevalence in a population lacking the protective allele. We compare that to a population with 19% heterozygous and 1% homozygous for the allele (implying an allelic frequency of 0.105573. Confidence interval bands (light gray) are shown around the median simulation () providing a range of uncertainty in evaluating parameters for the effect of the mutation on the infectivity and the duration of asymptomatic HIV for heterozygotes.

<!-- image -->

In contrast, when a proportion of the population carries the CCR5Δ32 allele, the epidemic increases more slowly, but still logarithmically, for the first 50 years, and HIV/AIDS prevalence reaches ≈12% (Fig. 3). Prevalence begins to decline slowly after 70 years.

In the above simulations we assume that people with AIDS are not sexually active. However, when these individuals are included in the sexually active population the severity of the epidemic increases considerably (data not shown). Consistent with our initial simulations, prevalences are still relatively lower in the presence of the CCR5 mutation.

Because some parameters (e.g., rate constants) are difficult to estimate based on available data, we implement an uncertainty analysis to assess the variability in the model outcomes caused by any inaccuracies in estimates of the parameter values with regard to the effect of the allelic mutation. For these analyses we use Latin hypercube sampling, as described in refs. 52–56, Our uncertainty and sensitivity analyses focus on infectivity vs. duration of infectiousness. To this end, we assess the effects on the dynamics of the epidemic for a range of values of the parameters governing transmission and progression rates: βîıı^^,^^,k k^^→i,j  and γ i,j,k  i,j,k. All other parameters are held constant. These results are presented as an interval band about the average simulation for the population carrying the CCR5Δ32 allele (Fig. 3). Although there is variability in the model outcomes, the analysis indicates that the overall model predictions are consistent for a wide range of transmission and progression rates. Further, most of the variation observed in the outcome is because of the transmission rates for both heterosexual males and females in the primary stage of infection (β2,M,A →  i  ,F, β2,F,A →  i  ,M). As mentioned above, we assume lower viral loads correlate with reduced infectivity; thus, the reduction in viral load in heterozygotes has a major influence on disease spread.

## HIV Induces Selective Pressure on Genotype Frequency

To observe changes in the frequency of the CCR5Δ32 allele in a setting with HIV infection as compared with the Hardy-Weinberg equilibrium in the absence of HIV, we follow changes in the total number of CCR5Δ32 heterozygotes and homozygotes over 1,000 years (Fig. 4). We initially perform simulations in the absence of HIV infection as a negative control to show there is not significant selection of the allele in the absence of infection. To determine how long it would take for the allelic frequency to reach present-day levels (e.g., q = 0.105573), we initiate this simulation for 1,000 years with a very small allelic frequency (q = 0.00105). In the absence of HIV, the allelic frequency is maintained in equilibrium as shown by the constant proportions of CCR5Δ32 heterozygotes and homozygotes (Fig. 4, solid lines). The selection for CCR5Δ32 in the presence of HIV is seen in comparison (Fig. 4, dashed lines). We expand the time frame of this simulation to 2,000 years to view the point at which the frequency reaches present levels (where q ∼0.105573 at year = 1200). Note that the allelic frequency increases for ≈1,600 years before leveling off.

Figure 4 Effects of HIV-1 on selection of the CCR5Δ32 allele. The Hardy-Weinberg equilibrium level is represented in the no-infection simulation (solid lines) for each population. Divergence from the original Hardy-Weinberg equilibrium is shown to occur in the simulations that include HIV infection (dashed lines). Fraction of the total subpopulations are presented: (A) wild types (W/W), (B) heterozygotes (W/Δ32), and (C) homozygotes (Δ32/Δ32). Note that we initiate this simulation with a much lower allelic frequency (0.00105) than used in the rest of the study to better exemplify the actual selective effect over a 1,000-year time scale. (D) The allelic selection effect over a 2,000-year time scale.

<!-- image -->

## Discussion

This study illustrates how populations can differ in susceptibility to epidemic HIV/AIDS depending on a ubiquitous attribute such as a prevailing genotype. We have examined heterosexual HIV epidemics by using mathematical models to assess HIV transmission in dynamic populations either with or without CCR5Δ32 heterozygous and homozygous persons. The most susceptible population lacks the protective mutation in CCR5. In less susceptible populations, the majority of persons carrying the CCR5Δ32 allele are heterozygotes. We explore the hypothesis that lower viral loads (CCR5Δ32 heterozygotes) or resistance to infection (CCR5Δ32 homozygotes) observed in persons with this coreceptor mutation ultimately can influence HIV epidemic trends. Two contrasting influences of the protective CCR5 allele are conceivable: it may limit the epidemic by decreasing the probability of infection because of lower viral loads in infected heterozygotes, or it may exacerbate the epidemic by extending the time that infectious individuals remain in the sexually active population. Our results strongly suggest the former. Thus, the absence of this allele in Africa could explain the severity of HIV disease as compared with populations where the allele is present.

We also observed that HIV can provide selective pressure for the CCR5Δ32 allele within a population, increasing the allelic frequency. Other influences may have additionally selected for this allele. Infectious diseases such as plague and small pox have been postulated to select for CCR5Δ32 (57, 58). For plague, relatively high levels of CCR5Δ32 are believed to have arisen within ≈4,000 years, accounting for the prevalence of the mutation only in populations of European descent. Smallpox virus uses the CC-coreceptor, indicating that direct selection for mutations in CCR5 may have offered resistance to smallpox. Given the differences in the epidemic rates of plague (59), smallpox, and HIV, it is difficult to directly compare our results to these findings. However, our model suggests that the CCR5Δ32 mutation could have reached its present allelic frequency in Northern Europe within this time frame if selected for by a disease with virulence patterns similar to HIV. Our results further support the idea that HIV has been only recently introduced as a pathogen into African populations, as the frequency of the protective allele is almost zero, and our model predicts that selection of the mutant allele in this population by HIV alone takes at least 1,000 years. This prediction is distinct from the frequency of the CCR5Δ32 allele in European populations, where pathogens that may have influenced its frequency (e.g., Yersinia pestis) have been present for much longer.

Two mathematical models have considered the role of parasite and host genetic heterogeneity with regard to susceptibility to another pathogen, namely malaria (60, 61). In each it was determined that heterogeneity of host resistance facilitates the maintenance of diversity in parasite virulence. Given our underlying interest in the coevolution of pathogen and host, we focus on changes in a host protective mutation, holding the virulence of the pathogen constant over time.

Even within our focus on host protective mutations, numerous genetic factors, beneficial or detrimental, could potentially influence epidemics. Other genetically determined host factors affecting HIV susceptibility and disease progression include a CCR5 A/A to G/G promoter polymorphism (62), a CCR2 point mutation (11, 63), and a mutation in the CXCR4 ligand (64). The CCR2b mutation, CCR264I, is found in linkage with at least one CCR5 promoter polymorphism (65) and is prevalent in populations where CCR5Δ32 is nonexistent, such as sub-Saharan Africa (63). However, as none of these mutations have been consistently shown to be as protective as the CCR5Δ32 allele, we simplified our model to incorporate only the effect of CCR5Δ32. Subsequent models could be constructed from our model to account for the complexity of multiple protective alleles. It is interesting to note that our model predicts that even if CCR264I is present at high frequencies in Africa, its protective effects may not augment the lack of a protective allele such as CCR5Δ32.

Although our models demonstrate that genetic factors can contribute to the high prevalence of HIV in sub-Saharan Africa, demographic factors are also clearly important in this region. Our models explicitly incorporated such factors, for example, lack of treatment availability. Additional factors were implicitly controlled for by varying only the presence of the CCR5Δ32 allele. More complex models eventually could include interactions with infectious diseases that serve as cofactors in HIV transmission. The role of high sexually transmitted disease prevalences in HIV infection has long been discussed, especially in relation to core populations (15, 50, 66). Malaria, too, might influence HIV transmission, as it is associated with transient increases in semen HIV viral loads and thus could increase the susceptibility of the population to epidemic HIV (16).

In assessing the HIV/AIDS epidemic, considerable attention has been paid to the influence of core groups in driving sexually transmitted disease epidemics. Our results also highlight how characteristics more uniformly distributed in a population can affect susceptibility. We observed that the genotypic profile of a population affects its susceptibility to epidemic HIV/AIDS. Additional studies are needed to better characterize the influence of these genetic determinants on HIV transmission, as they may be crucial in estimating the severity of the epidemic in some populations. This information can influence the design of treatment strategies as well as point to the urgency for education and prevention programs.

## Acknowledgments

We thank Mark Krosky, Katia Koelle, and Kevin Chung for programming and technical assistance. We also thank Drs. V. J. DiRita, P. Kazanjian, and S. M. Blower for helpful comments and discussions. We thank the reviewers for extremely insightful comments.

## References

- Weiss HA, Hawkes S. Leprosy Rev 72:92–98 (2001). PMID: 11355525
- Taha TE, Dallabetta GA, Hoover DR, Chiphangwi JD, Mtimavalye LAR. AIDS 12:197–203 (1998). PMID: 9468369
- AIDS Epidemic Update. Geneva: World Health Organization1–17 (1998).
- D'Souza MP, Harden VA. Nat Med 2:1293–1300 (1996). PMID: 8946819
- Martinson JJ, Chapman NH, Rees DC, Liu YT, Clegg JB. Nat Genet 16:100–103 (1997). PMID: 9140404
- Roos MTL, Lange JMA, deGoede REY, Miedema PT, Tersmette F, Coutinho M, Schellekens RA. J Infect Dis 165:427–432 (1992). PMID: 1347054
- Garred P, Eugen-Olsen J, Iversen AKN, Benfield TL, Svejgaard A, Hofmann B. Lancet 349:1884 (1997). PMID: 9217763
- Katzenstein TL, Eugen-Olsen J, Hofman B, Benfield T, Pedersen C, Iversen AK, Sorensen AM, Garred P, Koppelhus U, Svejgaard A, Gerstoft J. J Acquired Immune Defic Syndr Hum Retrovirol 16:10–14 (1997). PMID: 9377119
- deRoda H, Meyer K, Katzenstain W, Dean M. Science 273:1856–1862 (1996). PMID: 8791590
- Meyer L, Magierowska M, Hubert JB, Rouzioux C, Deveau C, Sanson F, Debre P, Delfraissy JF, Theodorou I. AIDS 11:F73–F78 (1997). PMID: 9302436
- Smith MW, Dean M, Carrington M, Winkler C, Huttley DA, Lomb GA, Goedert JJ, O'Brien TR, Jacobson LP, Kaslow R, et al. Science 277:959–965 (1997). PMID: 9252328
- Samson M, Libert F, Doranz BJ, Rucker J, Liesnard C, Farber CM, Saragosti S, Lapoumeroulie C, Cognaux J, Forceille C, et al. Nature (London) 382:722–725 (1996). PMID: 8751444
- McNicholl JM, Smith DK, Qari SH, Hodge T. Emerging Infect Dis 3:261–271 (1997). PMID: 9284370
- Michael NL, Chang G, Louie LG, Mascola JR, Dondero D, Birx DL, Sheppard HW. Nat Med 3:338–340 (1997). PMID: 9055864
- Mayaud P, Mosha F, Todd J, Balira R, Mgara J, West B, Rusizoka M, Mwijarubi E, Gabone R, Gavyole A, et al. AIDS 11:1873–1880 (1997). PMID: 9412707
- Hoffman IF, Jere CS, Taylor TE, Munthali P, Dyer JR. AIDS 13:487–494 (1998).
- HIV/AIDS Surveillance Database. Washington, DC: Population Division, International Programs Center (1999).
- Anderson RM, May RM, McLean AR. Nature (London) 332:228–234 (1988). PMID: 3279320
- Berger EA, Doms RW, Fenyo EM, Korber BT, Littman DR, Moore JP, Sattentau QJ, Schuitemaker H, Sodroski J, Weiss RA. Nature (London) 391:240 (1998). PMID: 9440686
- Alkhatib G, Broder CC, Berger EA. J Virol 70:5487–5494 (1996). PMID: 8764060
- Choe H, Farzan M, Sun Y, Sullivan N, Rollins B, Ponath PD, Wu L, Mackay CR, LaRosa G, Newman W, et al. Cell 85:1135–1148 (1996). PMID: 8674119
- Deng H, Liu R, Ellmeier W, Choe S, Unutmaz D, Burkhart M, Di Marzio P, Marmon S, Sutton RE, Hill CM, et al. Nature (London) 381:661–666 (1996). PMID: 8649511
- Doranz BJ, Rucker J, Yi Y, Smyth RJ, Samsom M, Peiper M, Parmentier SC, Collman RG, Doms RW. Cell 85:1149–1158 (1996). PMID: 8674120
- Dragic T, Litwin V, Allaway GP, Martin SR, Huang Y, Nagashima KA, Cayanan C, Maddon PJ, Koup RA, Moore JP, Paxton WA. Nature (London) 381:667–673 (1996). PMID: 8649512
- Zhu T, Mo H, Wang N, Nam DS, Cao Y, Koup RA, Ho DD. Science 261:1179–1181 (1993). PMID: 8356453
- Bjorndal A, Deng H, Jansson M, Fiore JR, Colognesi C, Karlsson A, Albert J, Scarlatti G, Littman DR, Fenyo EM. J Virol 71:7478–7487 (1997). PMID: 9311827
- Conner RI, Sheridan KE, Ceradinin D, Choe S, Landau NR. J Exp Med 185:621–628 (1997). PMID: 9034141
- Liu R, Paxton WA, Choe S, Ceradini D, Martin SR, Horuk R, MacDonald ME, Stuhlmann H, Koup RA, Landau NR. Cell 86:367–377 (1996). PMID: 8756719
- Mussico M, Lazzarin A, Nicolosi A, Gasparini M, Costigliola P, Arici C, Saracco A. Arch Intern Med (Moscow) 154:1971–1976 (1994). PMID: 8074601
- Michael NL, Nelson JA, KewalRamani VN, Chang G, O'Brien SJ, Mascola JR, Volsky B, Louder M, White GC, Littman DR, et al. J Virol 72:6040–6047 (1998). PMID: 9621067
- Hethcote HW, Yorke JA. Gonorrhea Transmission Dynamics and Control. Berlin: Springer (1984).
- Anderson RM, May RM. Nature (London) 333:514–522 (1988). PMID: 3374601
- Asiimwe-Okiror G, Opio AA, Musinguzi J, Madraa E, Tembo G, Carael M. AIDS 11:1757–1763 (1997). PMID: 9386811
- Carael M, Cleland J, Deheneffe JC, Ferry B, Ingham R. AIDS 9:1171–1175 (1995). PMID: 8519454
- Blower SM, Boe C. J AIDS 6:1347–1352 (1993). PMID: 8254474
- Kirschner D. J Appl Math 56:143–166 (1996).
- Le Pont F, Blower S. J AIDS 4:987–999 (1991). PMID: 1890608
- Kim MY, Lagakos SW. Ann Epidemiol 1:117–128 (1990). PMID: 1669741
- Anderson RM, May RM. Infectious Disease of Humans: Dynamics and Control. Oxford: Oxford Univ. Press (1992).
- Ragni MV, Faruki H, Kingsley LA. J Acquired Immune Defic Syndr 17:42–45 (1998).
- Kaplan JE, Khabbaz RF, Murphy EL, Hermansen S, Roberts C, Lal R, Heneine W, Wright D, Matijas L, Thomson R, et al. J Acquired Immune Defic Syndr Hum Retrovirol 12:193–201 (1996). PMID: 8680892
- Padian NS, Shiboski SC, Glass SO, Vittinghoff E. Am J Edu 146:350–357 (1997).
- Leynaert B, Downs AM, de Vincenzi I. Am J Edu 148:88–96 (1998).
- Garnett GP, Anderson RM. J Acquired Immune Defic Syndr 9:500–513 (1995).
- Stigum H, Magnus P, Harris JR, Samualson SO, Bakketeig LS. Am J Edu 145:636–643 (1997).
- Ho DD, Neumann AU, Perelson AS, Chen W, Leonard JM, Markowitz M. Nature (London) 373:123–126 (1995). PMID: 7816094
- World Resources (1998–1999). Oxford: Oxford Univ. Press (1999).
- Kostrikis LG, Neumann AU, Thomson B, Korber BT, McHardy P, Karanicolas R, Deutsch L, Huang Y, Lew JF, McIntosh K, et al. J Virol 73:10264–10271 (1999). PMID: 10559343
- Low-Beer D, Stoneburner RL, Mukulu A. Nat Med 3:553–557 (1997). PMID: 9142126
- Grosskurth H, Mosha F, Todd J, Senkoro K, Newell J, Klokke A, Changalucha J, West B, Mayaud P, Gavyole A. AIDS 9:927–934 (1995). PMID: 7576329
- Melo J, Beby-Defaux A, Faria C, Guiraud G, Folgosa E, Barreto A, Agius G. J AIDS 23:203–204 (2000). PMID: 10737436
- Iman RL, Helton JC, Campbell JE. J Quality Technol 13:174–183 (1981).
- Iman RL, Helton JC, Campbell JE. J Quality Technol 13:232–240 (1981).
- Blower SM, Dowlatabadi H. Int Stat Rev 62:229–243 (1994).
- Porco TC, Blower SM. Theor Popul Biol 54:117–132 (1998). PMID: 9733654
- Blower SM, Porco TC, Darby G. Nat Med 4:673–678 (1998). PMID: 9623975
- Libert F, Cochaux P, Beckman G, Samson M, Aksenova M, Cao A, Czeizel A, Claustres M, de la Rua C, Ferrari M, et al. Hum Mol Genet 7:399–406 (1998). PMID: 9466996
- Lalani AS, Masters J, Zeng W, Barrett J, Pannu R, Everett H, Arendt CW, McFadden G. Science 286:1968–1971 (1999). PMID: 10583963
- Kermack WO, McKendrick AG. Proc R Soc London 261:700–721 (1927).
- Gupta S, Hill AVS. Proc R Soc London Ser B 260:271–277 (1995).
- Ruwende C, Khoo SC, Snow RW, Yates SNR, Kwiatkowski D, Gupta S, Warn P, Allsopp CE, Gilbert SC, Peschu N. Nature (London) 376:246–249 (1995). PMID: 7617034
- McDermott DH, Zimmerman PA, Guignard F, Kleeberger CA, Leitman SF, Murphy PM. Lancet 352:866–870 (1998). PMID: 9742978
- Kostrikis LG, Huang Y, Moore JP, Wolinsky SM, Zhang L, Guo Y, Deutsch L, Phair J, Neumann AU, Ho DD. Nat Med 4:350–353 (1998). PMID: 9500612
- Winkler C, Modi W, Smith MW, Nelson GW, Wu X, Carrington M, Dean M, Honjo T, Tashiro K, Yabe D, et al. Science 279:389–393 (1998). PMID: 9430590
- Martinson JJ, Hong L, Karanicolas R, Moore JP, Kostrikis LG. AIDS 14:483–489 (2000). PMID: 10780710
- Vernazza PL, Eron JJ, Fiscus SA, Cohen MS. AIDS 13:155–166 (1999). PMID: 10202821

================================================
File: tests/data/groundtruth/docling_v2/pntd.0008301.xml.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Risk factors associated with fai ... s: Results of a multi-country analysis
    item-2 at level 2: paragraph: Clara R. Burgert-Brucker, Kathry ... garet Baker, John Kraemer, Molly Brady
    item-3 at level 2: paragraph: Global Health Division, RTI Inte ... shington, DC, United States of America
    item-4 at level 2: section_header: Abstract
      item-5 at level 3: text: Achieving elimination of lymphat ... as at highest risk of failing pre-TAS.
    item-6 at level 2: section_header: Author summary
      item-7 at level 3: text: Achieving elimination of lymphat ... ine prevalence and/or lower elevation.
    item-8 at level 2: section_header: Introduction
      item-9 at level 3: text: Lymphatic filariasis (LF), a dis ... 8 countries remain endemic for LF [3].
      item-10 at level 3: text: The road to elimination as a pub ... t elimination be officially validated.
      item-11 at level 3: text: Pre-TAS include at least one sen ... me of day that blood can be taken [5].
      item-12 at level 3: text: When a country fails to meet the ... o ensure rounds of MDA are not missed.
      item-13 at level 3: text: This study aims to understand wh ... e of limited LF elimination resources.
    item-14 at level 2: section_header: Methods
      item-15 at level 3: text: This is a secondary data analysi ... rch; no ethical approval was required.
      item-16 at level 3: text: Building on previous work, we de ... available global geospatial data sets.
      item-17 at level 3: table with [18x8]
        item-17 at level 4: caption: Table 1 Categorization of potential factors influencing pre-TAS results.
      item-18 at level 3: section_header: Data sources
        item-19 at level 4: text: Information on baseline prevalen ...  publicly available sources (Table 1).
      item-20 at level 3: section_header: Outcome and covariate variables
        item-21 at level 4: text: The outcome of interest for this ... r than or equal to 1% Mf or 2% Ag [4].
        item-22 at level 4: text: Potential covariates were derive ... is and the final categorizations used.
        item-23 at level 4: section_header: Baseline prevalence
          item-24 at level 5: text: Baseline prevalence can be assum ... (2) using the cut-off of <10% or ≥10%.
        item-25 at level 4: section_header: Agent
          item-26 at level 5: text: In terms of differences in trans ... dazole (DEC-ALB)] from the MDA domain.
        item-27 at level 4: section_header: Environment
          item-28 at level 5: text: LF transmission intensity is inf ... dicates a higher level of “greenness.”
          item-29 at level 5: text: We included the socio-economic v ...  proxy for socio-economic status [33].
          item-30 at level 5: text: Finally, all or parts of distric ... s were co-endemic with onchocerciasis.
        item-31 at level 4: section_header: MDA
          item-32 at level 5: text: Treatment effectiveness depends  ... esent a threat to elimination [41,42].
          item-33 at level 5: text: We considered three approaches w ... unds ever documented in that district.
        item-34 at level 4: section_header: Pre-TAS implementation
          item-35 at level 5: text: Pre-TAS results can be influence ... d throughout the time period of study.
      item-36 at level 3: section_header: Data inclusion criteria
        item-37 at level 4: text: The dataset, summarized at the d ... al analysis dataset had 554 districts.
      item-38 at level 3: section_header: Statistical analysis and modeling
        item-39 at level 4: text: Statistical analysis and modelin ... d the number of variables accordingly.
        item-40 at level 4: text: Sensitivity analysis was perform ... ot have been truly LF-endemic [43,44].
    item-41 at level 2: section_header: Results
      item-42 at level 3: text: The overall pre-TAS pass rate fo ... ts had baseline prevalences below 20%.
      item-43 at level 3: picture
        item-43 at level 4: caption: Fig 1 Number of pre-TAS by country.
      item-44 at level 3: picture
        item-44 at level 4: caption: Fig 2 District-level baseline prevalence by country.
      item-45 at level 3: text: Fig 3 shows the unadjusted analy ... overage, and sufficient rounds of MDA.
      item-46 at level 3: picture
        item-46 at level 4: caption: Fig 3 Percent pre-TAS failure by each characteristic (unadjusted).
      item-47 at level 3: text: The final log-binomial model inc ... igh baseline and diagnostic test used.
      item-48 at level 3: text: Fig 4 shows the risk ratio resul ... of failing pre-TAS (95% CI 1.95–4.83).
      item-49 at level 3: picture
        item-49 at level 4: caption: Fig 4 Adjusted risk ratios for pre-TAS failure with 95% Confidence Interval from log-binomial model.
      item-50 at level 3: text: Sensitivity analyses were conduc ... gnified by large confidence intervals.
      item-51 at level 3: table with [11x6]
        item-51 at level 4: caption: Table 2 Adjusted risk ratios for pre-TAS failure from log-binomial model sensitivity analysis.
      item-52 at level 3: text: Overall 74 districts in the data ... or 51% of all the failures (38 of 74).
      item-53 at level 3: picture
        item-53 at level 4: caption: Fig 5 Analysis of failures by model combinations.
    item-54 at level 2: section_header: Discussion
      item-55 at level 3: text: This paper reports for the first ... ctors associated with TAS failure [7].
      item-56 at level 3: text: Though diagnostic test used was  ...  FTS was more sensitive than ICT [45].
      item-57 at level 3: text: Elevation was the only environme ... ich impact vector chances of survival.
      item-58 at level 3: text: The small number of failures ove ... search has shown the opposite [15,16].
      item-59 at level 3: text: All other variables included in  ... are not necessary to lower prevalence.
      item-60 at level 3: text: Limitations to this study includ ...  reducing LF prevalence [41,48,51–53].
      item-61 at level 3: text: Fourteen districts were excluded ... ta to extreme outliners in a district.
      item-62 at level 3: text: As this analysis used data acros ... of individuals included in the survey.
      item-63 at level 3: text: This paper provides evidence fro ... th high baseline and/or low elevation.
    item-64 at level 2: section_header: Acknowledgments
      item-65 at level 3: text: The authors would like to thank  ... e surveys financially and technically.
    item-66 at level 2: section_header: References
      item-67 at level 3: list: group list
        item-68 at level 4: list_item: World Health Organization. Lymph ... trategic plan 2010–2020. Geneva; 2010.
        item-69 at level 4: list_item: World Health Organization. Valid ... a public health problem. Geneva; 2017.
        item-70 at level 4: list_item: World Health Organization. Globa ...  Wkly Epidemiol Rec. 2019;94: 457–472.
        item-71 at level 4: list_item: World Health Organization. Globa ... ass drug administration. Geneva; 2011.
        item-72 at level 4: list_item: World Health Organization. Stren ... Disease-specific Indicators. 2016; 42.
        item-73 at level 4: list_item: KyelemD, BiswasG, BockarieMJ, Br ... Trop Med Hyg. 2008;79: 480–4. 18840733
        item-74 at level 4: list_item: GoldbergEM, KingJD, MupfasoniD,  ... . 2019; 10.4269/ajtmh.18-0721 31115301
        item-75 at level 4: list_item: CanoJ, RebolloMP, GoldingN, Pull ... : 1–19. 10.1186/1756-3305-7-1 24411014
        item-76 at level 4: list_item: CGIAR-CSI. CGIAR-CSI SRTM 90m DE ...  Available: http://srtm.csi.cgiar.org/
        item-77 at level 4: list_item: USGS NASA. Vegetation indices 16 ... /lpdaac.usgs.gov/products/myd13a1v006/
        item-78 at level 4: list_item: FunkC, PetersonP, LandsfeldM, Pe ...  2015;2 10.1038/sdata.2015.66 26646728
        item-79 at level 4: list_item: LloydCT, SorichettaA, TatemAJ. H ... : 170001 10.1038/sdata.2017.1 28140386
        item-80 at level 4: list_item: ElvidgeCD, BaughKE, ZhizhinM, Hs ... Network; 2013;35: 62 10.7125/apan.35.7
        item-81 at level 4: list_item: JambulingamP, SubramanianS, De V ... 18. 10.1186/s13071-015-1291-6 26728523
        item-82 at level 4: list_item: MichaelE, Malecela-LazaroMN, Sim ... 10.1016/S1473-3099(04)00973-9 15050941
        item-83 at level 4: list_item: StolkWA, SwaminathanS, van Oortm ... ;188: 1371–81. 10.1086/378354 14593597
        item-84 at level 4: list_item: GradyCA, De RocharsMB, DirenyAN, ... 8–610. 10.3201/eid1304.061063 17553278
        item-85 at level 4: list_item: EvansD, McFarlandD, AdamaniW, Ei ... 0.1179/2047773211Y.0000000010 22325813
        item-86 at level 4: list_item: RichardsFO, EigegeA, MiriES, Kal ...  10.1371/journal.pntd.0001346 22022627
        item-87 at level 4: list_item: BiritwumNK, YikpoteyP, MarfoBK,  ... 90–695. 10.1093/trstmh/trx007 28938053
        item-88 at level 4: list_item: MoragaP, CanoJ, BaggaleyRF, Gyap ... 16. 10.1186/s13071-014-0608-1 25561160
        item-89 at level 4: list_item: IrvineMA, NjengaSM, GunawardenaS ... 18–124. 10.1093/trstmh/trv096 26822604
        item-90 at level 4: list_item: OttesenEA. Efficacy of diethylca ... iae in humans. Rev Infect Dis. 1985;7.
        item-91 at level 4: list_item: GambhirM, BockarieM, TischD, Kaz ... 2010;8 10.1186/1741-7007-8-22 20236528
        item-92 at level 4: list_item: World Health Organization. Globa ... cal entomology handbook. Geneva; 2013.
        item-93 at level 4: list_item: SlaterH, MichaelE. Predicting th ...  10.1371/journal.pone.0032202 22359670
        item-94 at level 4: list_item: SlaterH, MichaelE. Mapping, Baye ...  10.1371/journal.pone.0071574 23951194
        item-95 at level 4: list_item: SabesanS, RajuKHK, SubramanianS, ... 57–665. 10.1089/vbz.2012.1238 23808973
        item-96 at level 4: list_item: StantonMC, MolyneuxDH, KyelemD,  ... : 159–173. 10.4081/gh.2013.63 24258892
        item-97 at level 4: list_item: ManhenjeI, Teresa Galán-Puchades ... : 391–398. 10.4081/gh.2013.96 23733300
        item-98 at level 4: list_item: NgwiraBM, TambalaP, Perez aM, Bo ... ;6: 12 10.1186/1475-2883-6-12 18047646
        item-99 at level 4: list_item: SimonsenPE, MwakitaluME. Urban l ... 44. 10.1007/s00436-012-3226-x 23239094
        item-100 at level 4: list_item: ProvilleJ, Zavala-AraizaD, Wagne ...  10.1371/journal.pone.0174610 28346500
        item-101 at level 4: list_item: EndeshawT, TayeA, TadesseZ, Kata ... 10.1080/20477724.2015.1103501 26878935
        item-102 at level 4: list_item: RichardsFO, EigegeA, PamD, KalA, ... 4: 3–5. 10.1186/1475-2883-4-3 15916708
        item-103 at level 4: list_item: KyelemD, SanouS, BoatinB a., Med ... 8. 10.1179/000349803225002462 14754495
        item-104 at level 4: list_item: WeilGJ, LammiePJ, RichardsFO, Eb ... –816. 10.1093/infdis/164.4.814 1894943
        item-105 at level 4: list_item: KumarA, SachanP. Measuring impac ... rop Biomed. 2014;31: 225–229. 25134891
        item-106 at level 4: list_item: NjengaSM, MwandawiroCS, WamaeCN, ... 4: 1–9. 10.1186/1756-3305-4-1 21205315
        item-107 at level 4: list_item: BoydA, WonKY, McClintockSK, Dono ...  10.1371/journal.pntd.0000640 20351776
        item-108 at level 4: list_item: IrvineMA, ReimerLJ, NjengaSM, Gu ... 19. 10.1186/s13071-014-0608-1 25561160
        item-109 at level 4: list_item: IrvineMA, StolkWA, SmithME, Subr ... 10.1016/S1473-3099(16)30467-4 28012943
        item-110 at level 4: list_item: PionSD, MontavonC, ChesnaisCB, K ... 7–1423. 10.4269/ajtmh.16-0547 27729568
        item-111 at level 4: list_item: WanjiS, EsumME, NjouendouAJ, Mbe ...  10.1371/journal.pntd.0007192 30849120
        item-112 at level 4: list_item: ChesnaisCB, Awaca-UvonNP, BolayF ...  10.1371/journal.pntd.0005703 28892473
        item-113 at level 4: list_item: SilumbweA, ZuluJM, HalwindiH, Ja ... 15. 10.1186/s12889-017-4414-5 28532397
        item-114 at level 4: list_item: AdamsAM, VuckovicM, BirchE, Bran ... ;3 10.3390/tropicalmed3040122 30469342
        item-115 at level 4: list_item: RaoRU, SamarasekeraSD, Nagodavit ...  10.1371/journal.pntd.0006066 29084213
        item-116 at level 4: list_item: XuZ, GravesPM, LauCL, ClementsA, ...  10.1016/j.epidem.2018.12.003 30611745
        item-117 at level 4: list_item: IdCM, TetteviEJ, MechanF, IdunB, ... Ghana. PLoS Negl Trop Dis. 2019; 1–17.
        item-118 at level 4: list_item: EigegeA, KalA, MiriE, SallauA, U ...  10.1371/journal.pntd.0002508 24205421
        item-119 at level 4: list_item: Van den BergH, Kelly-HopeLA, Lin ... 10.1016/S1473-3099(12)70148-2 23084831
        item-120 at level 4: list_item: WebberR. Eradication of Wucherer ... ol. Trans R Soc Trop Med Hyg. 1979;73.
  item-121 at level 1: caption: Table 1 Categorization of potential factors influencing pre-TAS results.
  item-122 at level 1: caption: Fig 1 Number of pre-TAS by country.
  item-123 at level 1: caption: Fig 2 District-level baseline prevalence by country.
  item-124 at level 1: caption: Fig 3 Percent pre-TAS failure by each characteristic (unadjusted).
  item-125 at level 1: caption: Fig 4 Adjusted risk ratios for p ... ence Interval from log-binomial model.
  item-126 at level 1: caption: Table 2 Adjusted risk ratios for ... g-binomial model sensitivity analysis.
  item-127 at level 1: caption: Fig 5 Analysis of failures by model combinations.

================================================
File: tests/data/groundtruth/docling_v2/pone.0234687.xml.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Potential to reduce greenhouse g ...  cattle systems in subtropical regions
    item-2 at level 2: paragraph: Henrique M. N. Ribeiro-Filho, Maurício Civiero, Ermias Kebreab
    item-3 at level 2: paragraph: Department of Animal Science, Un ... atarina, Lages, Santa Catarina, Brazil
    item-4 at level 2: section_header: Abstract
      item-5 at level 3: text: Carbon (C) footprint of dairy pr ... uce the C footprint to a small extent.
    item-6 at level 2: section_header: Introduction
      item-7 at level 3: text: Greenhouse gas (GHG) emissions f ... suitable for food crop production [4].
      item-8 at level 3: text: Considering the key role of live ... anagement to mitigate the C footprint.
      item-9 at level 3: text: In subtropical climate zones, co ... t in tropical pastures (e.g. [17–19]).
      item-10 at level 3: text: It has been shown that dairy cow ... sions from crop and reduced DM intake.
      item-11 at level 3: text: The aim of this work was to quan ... uring lactation periods was evaluated.
    item-12 at level 2: section_header: Materials and methods
      item-13 at level 3: text: An LCA was developed according t ... 90816 - https://www.udesc.br/cav/ceua.
      item-14 at level 3: section_header: System boundary
        item-15 at level 4: text: The goal of the study was to ass ... n were outside of the system boundary.
        item-16 at level 4: picture
          item-16 at level 5: caption: Fig 1 Overview of the milk production system boundary considered in the study.
      item-17 at level 3: section_header: Functional unit
        item-18 at level 4: text: The functional unit was one kilo ... tein according to NRC [20] as follows:
        item-19 at level 4: text: ECM = Milk production × (0.0929  ...  characteristics described in Table 1.
        item-20 at level 4: table with [13x3]
          item-20 at level 5: caption: Table 1 Descriptive characteristics of the herd.
      item-21 at level 3: section_header: Data sources and livestock system description
        item-22 at level 4: text: The individual feed requirements ... ed to the ad libitum TMR intake group.
        item-23 at level 4: text: Using experimental data, three s ... med during an entire lactation period.
      item-24 at level 3: section_header: Impact assessment
        item-25 at level 4: text: The CO2e emissions were calculat ... 65 for CO2, CH4 and N2O, respectively.
      item-26 at level 3: section_header: Feed production
        item-27 at level 4: section_header: Diets composition
          item-28 at level 5: text: The DM intake of each ingredient ...  collected throughout the experiments.
          item-29 at level 5: table with [21x11]
            item-29 at level 6: caption: Table 2 Dairy cows’ diets in different scenariosa.
        item-30 at level 4: section_header: GHG emissions from crop and pasture production
          item-31 at level 5: text: GHG emission factors used for of ... onsume 70% of pastures during grazing.
          item-32 at level 5: table with [9x5]
            item-32 at level 6: caption: Table 3 GHG emission factors for Off- and On-farm feed production.
          item-33 at level 5: text: Emissions from on-farm feed prod ... factors described by Rotz et al. [42].
          item-34 at level 5: table with [28x5]
            item-34 at level 6: caption: Table 4 GHG emissions from On-farm feed production.
      item-35 at level 3: section_header: Animal husbandry
        item-36 at level 4: text: The CH4 emissions from enteric f ... 1) = 13.8 + 0.185 × NDF (% DM intake).
      item-37 at level 3: section_header: Manure from confined cows and urine and dung from grazing animals
        item-38 at level 4: text: The CH4 emission from manure (kg ... for dietary GE per kg of DM (MJ kg-1).
        item-39 at level 4: text: The OM digestibility was estimat ... h were 31%, 26% and 46%, respectively.
        item-40 at level 4: text: The N2O-N emissions from urine a ...  using the IPCC [38] emission factors.
      item-41 at level 3: section_header: Farm management
        item-42 at level 4: text: Emissions due to farm management ...  crop and pasture production’ section.
        item-43 at level 4: table with [12x4]
          item-43 at level 5: caption: Table 5 Factors for major resource inputs in farm management.
        item-44 at level 4: text: The amount of fuel use for manur ... me that animals stayed on confinement.
        item-45 at level 4: text: The emissions from fuel were est ...  × kg CO2e (kg machinery mass)-1 [42].
        item-46 at level 4: text: Emissions from electricity for m ... ws in naturally ventilated barns [47].
      item-47 at level 3: section_header: Co-product allocation
        item-48 at level 4: text: The C footprint for milk produce ...  directly assigned to milk production.
      item-49 at level 3: section_header: Sensitivity analysis
        item-50 at level 4: text: A sensitivity index was calculat ... ses a similar change in the footprint.
    item-51 at level 2: section_header: Results and discussion
      item-52 at level 3: text: The study has assessed the impac ... , feed production and electricity use.
      item-53 at level 3: section_header: Greenhouse gas emissions
        item-54 at level 4: text: Depending on emission factors us ... more than 5% of overall GHG emissions.
        item-55 at level 4: picture
          item-55 at level 5: caption: Fig 2 Overall greenhouse gas emissions in dairy cattle systems under various scenarios. TMR = ad libitum TMR intake, 75TMR = 75% of ad libitum TMR intake with access to pasture, 50TMR = 50% of ad libitum TMR intake with access to pasture. (a) N2O emission factors for urine and dung from IPCC [38], feed production emission factors from Table 3 without accounting for sequestered CO2-C from perennial pasture, production of electricity = 0.73 kg CO2e kWh-1 [41]. (b) N2O emission factors for urine and dung from IPCC [38], feed production emission factors from Table 3 without accounting for sequestered CO2-C from perennial pasture, production of electricity = 0.205 kg CO2e kWh-1 [46]; (c) N2O emission factors for urine and dung from local data [37], feed production EF from Table 4 without accounting for sequestered CO2-C from perennial pasture, production of electricity = 0.205 kg CO2e kWh-1 [46]. (d) N2O emission factors for urine and dung from local data [37], feed production emission factors from Table 4 accounting for sequestered CO2-C from perennial pasture, production of electricity = 0.205 kg CO2e kWh-1 [46].
        item-56 at level 4: text: Considering IPCC emission factor ...  the C footprint of the dairy systems.
        item-57 at level 4: text: The similarity of C footprint be ... of TMR was replaced by pasture access.
        item-58 at level 4: text: The lower C footprint in scenari ... r, averaging 0.004 kg N2O-N kg-1 [37].
      item-59 at level 3: section_header: Methane emissions
        item-60 at level 4: text: The enteric CH4 intensity was si ... ], which did not happen in this study.
        item-61 at level 4: picture
          item-61 at level 5: caption: Fig 3 Sensitivity of the C footprint. Sensitivity index = percentage change in C footprint for a 10% change in the given emission source divided by 10% of. (a) N2O emission factors for urine and dung from IPCC [38], feed production emission factors from Table 3, production of electricity = 0.73 kg CO2e kWh-1 [41]. (b) N2O emission factors for urine and dung from IPCC [38], feed production emission factors from Table 3, production of electricity = 0.205 kg CO2e kWh-1 [46]; (c) N2O emission factors for urine and dung from local data [37], feed production EF from Table 4 without accounting sequestered CO2-C from perennial pasture, production of electricity = 0.205 kg CO2e kWh-1 [46]. (d) N2O emission factors for urine and dung from local data [37], feed production emission factors from Table 4 accounting sequestered CO2-C from perennial pasture, production of electricity = 0.205 kg CO2e kWh-1 [46].
        item-62 at level 4: text: The lack of difference in enteri ...  same scenarios as in this study [26].
      item-63 at level 3: section_header: Emissions from excreta and feed production
        item-64 at level 4: text: Using IPCC emission factors for  ...  may not be captured by microbes [65].
        item-65 at level 4: picture
          item-65 at level 5: caption: Fig 4 Greenhouse gas emissions (GHG) from manure and feed production in dairy cattle systems. TMR = ad libitum TMR intake, 75TMR = 75% of ad libitum TMR intake with access to pasture, 50TMR = 50% of ad libitum TMR intake with access to pasture. (a) N2O emission factors for urine and dung from IPCC [38]. (b) Feed production emission factors from Table 3. (c) N2O emission factors for urine and dung from local data [37]. (d) Feed production emission factors from Table 4 accounting sequestered CO2-C from perennial pasture.
        item-66 at level 4: text: Using local emission factors for ... be revised for the subtropical region.
        item-67 at level 4: text: Emissions for feed production de ... act, particularly in confinements [9].
      item-68 at level 3: section_header: Farm management
        item-69 at level 4: text: The lower impact of emissions fr ...  greater than 5% of total C footprint.
        item-70 at level 4: text: Emissions from farm management d ...  gas and hard coal, respectively [46].
      item-71 at level 3: section_header: Assumptions and limitations
        item-72 at level 4: text: The milk production and composit ... ions as a function of soil management.
      item-73 at level 3: section_header: Further considerations
        item-74 at level 4: text: The potential for using pasture  ... g ECM)-1 in case of foot lesions [72].
        item-75 at level 4: text: Grazing lands may also improve b ... hange of CO2 would be negligible [76].
    item-76 at level 2: section_header: Conclusions
      item-77 at level 3: text: This study assessed the C footpr ... on with or without access to pastures.
    item-78 at level 2: section_header: Acknowledgments
      item-79 at level 3: text: Thanks to Anna Naranjo for helpf ...  of the herd considered in this study.
    item-80 at level 2: section_header: References
      item-81 at level 3: list: group list
        item-82 at level 4: list_item: IPCC. Climate Change and Land. Chapter 5: Food Security. 2019.
        item-83 at level 4: list_item: HerreroM, HendersonB, HavlíkP, T ...  2016;6: 452–461. 10.1038/nclimate2925
        item-84 at level 4: list_item: Rivera-FerreMG, López-i-GelatsF, ... hang. 2016;7: 869–892. 10.1002/wcc.421
        item-85 at level 4: list_item: van ZantenHHE, MollenhorstH, Klo ... 21: 747–758. 10.1007/s11367-015-0944-1
        item-86 at level 4: list_item: HristovAN, OhJ, FirkinsL, Dijkst ... 5–5069. 10.2527/jas.2013-6583 24045497
        item-87 at level 4: list_item: HristovAN, OttT, TricaricoJ, Rot ... 5–5113. 10.2527/jas.2013-6585 24045470
        item-88 at level 4: list_item: MontesF, MeinenR, DellC, RotzA,  ... 0–5094. 10.2527/jas.2013-6584 24045493
        item-89 at level 4: list_item: LedgardSF, WeiS, WangX, Falconer ... : 155–163. 10.1016/j.agwat.2018.10.009
        item-90 at level 4: list_item: O’BrienD, ShallooL, PattonJ, Buc ... 107: 33–46. 10.1016/j.agsy.2011.11.004
        item-91 at level 4: list_item: SalouT, Le MouëlC, van der WerfH ... od. 2017 10.1016/j.jclepro.2016.05.019
        item-92 at level 4: list_item: LizarraldeC, PicassoV, RotzCA, C ... gric Res. 2014;3: 1 10.5539/sar.v3n2p1
        item-93 at level 4: list_item: ClarkCEF, KaurR, MillapanLO, Gol ... –5465. 10.3168/jds.2017-13388 29550132
        item-94 at level 4: list_item: Food and Agriculture Organization. FAOSTAT. 2017.
        item-95 at level 4: list_item: VogelerI, MackayA, VibartR, Rend ... .1016/j.scitotenv.2016.05.006 27203517
        item-96 at level 4: list_item: WilkinsonJM, LeeMRF, RiveroMJ, C ... 0;75: 1–17. 10.1111/gfs.12458 32109974
        item-97 at level 4: list_item: WalesWJ, MarettLC, GreenwoodJS,  ... i. 2013;53: 1167–1178. 10.1071/AN13207
        item-98 at level 4: list_item: BargoF, MullerLD, DelahoyJE, Cas ... 168/jds.S0022-0302(02)74381-6 12487461
        item-99 at level 4: list_item: VibartRE, FellnerV, BurnsJC, Hun ... 80. 10.1017/S0022029908003361 18701000
        item-100 at level 4: list_item: MendozaA, CajarvilleC, RepettoJL ... –1944. 10.3168/jds.2015-10257 26778319
        item-101 at level 4: list_item: NRC. Nutrient Requirements of Da ... gton DC: National Academy Press; 2001.
        item-102 at level 4: list_item: INRA. INRA Feeding System for Ru ... shiers; 2018 10.3920/978-90-8686-872-8
        item-103 at level 4: list_item: LorenzH, ReinschT, HessS, TaubeF ... 161–170. 10.1016/j.jclepro.2018.11.113
        item-104 at level 4: list_item: ISO 14044. INTERNATIONAL STANDAR ... rements and guidelines. 2006;2006: 46.
        item-105 at level 4: list_item: ISO 14040. The International Sta ... ;2006: 1–28. 10.1136/bmj.332.7550.1107
        item-106 at level 4: list_item: FAO. Environmental Performance o ... nerships/leap/resources/guidelines/en/
        item-107 at level 4: list_item: CivieroM, Ribeiro-FilhoHMN, Scha ... nce,. Foz do Iguaçu; 2019 pp. 141–141.
        item-108 at level 4: list_item: IPCC—Intergovernmental Panel on  ... /2018/05/SYR_AR5_FINAL_full_wcover.pdf
        item-109 at level 4: list_item: INRA. Alimentation des bovins, o ... Inra 2007. 4th ed. INRA, editor. 2007.
        item-110 at level 4: list_item: DelagardeR, FaverdinP, BaratteC, ... 5–60. 10.1111/j.1365-2494.2010.00770.x
        item-111 at level 4: list_item: MaBL, LiangBC, BiswasDK, Morriso ... 2;94: 15–31. 10.1007/s10705-012-9522-0
        item-112 at level 4: list_item: RauccciGS, MoreiraCS, AlvesPS, M ... State. J Clean Prod. 2015;96: 418–425.
        item-113 at level 4: list_item: CamargoGGT, RyanMR, RichardTL. E ... 3;63: 263–273. 10.1525/bio.2013.63.4.6
        item-114 at level 4: list_item: da SilvaMSJ, JobimCC, PoppiEC, T ... 3–313. 10.1590/S1806-92902015000900001
        item-115 at level 4: list_item: Duchini PGPGGuzatti GCGC, Ribeir ... Sci. 2016;67: 574–581. 10.1071/CP15170
        item-116 at level 4: list_item: ScaravelliLFB, PereiraLET, Olivo ... teiras. Cienc Rural. 2007;37: 841–846.
        item-117 at level 4: list_item: SbrissiaAF, DuchiniPG, ZaniniGD, ... : 945–954. 10.2135/cropsci2017.07.0447
        item-118 at level 4: list_item: AlmeidaJGR, Dall-OrsolettaAC, Oz ... 12. 10.1017/S1751731119003057 31907089
        item-119 at level 4: list_item: Intergovernamental Panel on Clim ... Global Environmental Strategies; 2006.
        item-120 at level 4: list_item: RamalhoB, DieckowJ, BarthG, Simo ... il Sci. 2020; 1–14. 10.1111/ejss.12933
        item-121 at level 4: list_item: FernandesHC, da SilveiraJCM, Rin ... –1587. 10.1590/s1413-70542008000500034
        item-122 at level 4: list_item: Wang M Q. GREET 1.8a Spreadsheet ... transportation.anl.gov/software/GREET/
        item-123 at level 4: list_item: RotzCAA, MontesF, ChianeseDS, Ch ... 6–1282. 10.3168/jds.2009-2162 20172247
        item-124 at level 4: list_item: NiuM, KebreabE, HristovAN, OhJ,  ...  3368–3389. 10.1111/gcb.14094 29450980
        item-125 at level 4: list_item: EugèneM, SauvantD, NozièreP, Via ... 10.1016/j.jenvman.2018.10.086 30602259
        item-126 at level 4: list_item: ReedKF, MoraesLE, CasperDP, Kebr ... 5–3035. 10.3168/jds.2014-8397 25747829
        item-127 at level 4: list_item: BarrosMV, PiekarskiCM, De Franci ... . Energies. 2018;11 10.3390/en11061412
        item-128 at level 4: list_item: LudingtonD, JohnsonE. Dairy Farm ...  York State Energy Res Dev Auth. 2003.
        item-129 at level 4: list_item: ThomaG, JollietO, WangY. A bioph ...  2013;31 10.1016/j.idairyj.2012.08.012
        item-130 at level 4: list_item: NaranjoA, JohnsonA, RossowH. Gre ... . 2020 10.3168/jds.2019-16576 32037166
        item-131 at level 4: list_item: JayasundaraS, WordenD, WeersinkA ... 18–1028. 10.1016/j.jclepro.2019.04.013
        item-132 at level 4: list_item: WilliamsSRO, FisherPD, Berrisfor ... 4;19: 69–78. 10.1007/s11367-013-0619-8
        item-133 at level 4: list_item: GollnowS, LundieS, MooreAD, McLa ... : 31–38. 10.1016/j.idairyj.2014.02.005
        item-134 at level 4: list_item: O’BrienD, CapperJL, GarnsworthyP ... i. 2014 10.3168/jds.2013-7174 24440256
        item-135 at level 4: list_item: ChobtangJ, McLarenSJ, LedgardSF, ... 2017;21: 1139–1152. 10.1111/jiec.12484
        item-136 at level 4: list_item: GargMR, PhondbaBT, SherasiaPL, M ... Sci. 2016;56: 423–436. 10.1071/AN15464
        item-137 at level 4: list_item: de LéisCM, CherubiniE, RuviaroCF ... 5;20: 46–60. 10.1007/s11367-014-0813-3
        item-138 at level 4: list_item: O’BrienD, GeogheganA, McNamaraK, ... Sci. 2016;56: 495–500. 10.1071/AN15490
        item-139 at level 4: list_item: O’BrienD, BrennanP, HumphreysJ,  ... : 1469–1481. 10.1007/s11367-014-0755-9
        item-140 at level 4: list_item: BaekCY, LeeKM, ParkKH. Quantific ... : 50–60. 10.1016/j.jclepro.2014.02.010
        item-141 at level 4: list_item: Dall-OrsolettaAC, AlmeidaJGR, Ca ... –4383. 10.3168/jds.2015-10396 27016830
        item-142 at level 4: list_item: Dall-OrsolettaAC, OziemblowskiMM ... 5–73. 10.1016/j.anifeedsci.2019.05.009
        item-143 at level 4: list_item: NiuM, AppuhamyJADRN, LeytemAB, D ... Sci. 2016;56: 312–321. 10.1071/AN15498
        item-144 at level 4: list_item: WaghornGC, LawN, BryantM, Pachec ... i. 2019;59: 1261–1270. 10.1071/AN18018
        item-145 at level 4: list_item: DickhoeferU, GlowackiS, GómezCA, ...  109–118. 10.1016/j.livsci.2018.08.004
        item-146 at level 4: list_item: SchwabCG, BroderickGA. A 100-Yea ... 10112. 10.3168/jds.2017-13320 29153157
        item-147 at level 4: list_item: SordiA, DieckowJ, BayerC, Alburq ... 90: 94–103. 10.1016/j.agee.2013.09.004
        item-148 at level 4: list_item: SimonPL, DieckowJ, de KleinCAM,  ... 267: 74–82. 10.1016/j.agee.2018.08.013
        item-149 at level 4: list_item: WangX, LedgardS, LuoJ, GuoY, Zha ... .1016/j.scitotenv.2017.12.259 29291563
        item-150 at level 4: list_item: PirloG, LolliS. Environmental im ... 962–971. 10.1016/j.jclepro.2018.11.070
        item-151 at level 4: list_item: HerzogA, WincklerC, ZollitschW.  ... 7: 174–187. 10.1016/j.agee.2018.07.029
        item-152 at level 4: list_item: MostertPF, van MiddelaarCE, Bokk ... od. 2018 10.1016/j.jclepro.2017.10.019
        item-153 at level 4: list_item: MostertPF, van MiddelaarCE, de B ... 7: 206–212. 10.1016/j.agsy.2018.09.006
        item-154 at level 4: list_item: FoleyJA, RamankuttyN, BraumanKA, ...  337–342. 10.1038/nature10452 21993620
        item-155 at level 4: list_item: LalR. Soil Carbon Sequestration  ... 1627. 10.1126/science.1097396 15192216
        item-156 at level 4: list_item: BoddeyRM, JantaliaCP, ConceiçaoP ... –795. 10.1111/j.1365-2486.2009.02020.x
        item-157 at level 4: list_item: McConkeyB, AngersD, BenthamM, Bo ...  the LULUCF sector for NIR 2014. 2014.
  item-158 at level 1: caption: Fig 1 Overview of the milk produ ... stem boundary considered in the study.
  item-159 at level 1: caption: Table 1 Descriptive characteristics of the herd.
  item-160 at level 1: caption: Table 2 Dairy cows’ diets in different scenariosa.
  item-161 at level 1: caption: Table 3 GHG emission factors for Off- and On-farm feed production.
  item-162 at level 1: caption: Table 4 GHG emissions from On-farm feed production.
  item-163 at level 1: caption: Table 5 Factors for major resource inputs in farm management.
  item-164 at level 1: caption: Fig 2 Overall greenhouse gas emi ... lectricity = 0.205 kg CO2e kWh-1 [46].
  item-165 at level 1: caption: Fig 3 Sensitivity of the C footp ... lectricity = 0.205 kg CO2e kWh-1 [46].
  item-166 at level 1: caption: Fig 4 Greenhouse gas emissions ( ... uestered CO2-C from perennial pasture.

================================================
File: tests/data/groundtruth/docling_v2/powerpoint_sample.pptx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: chapter: group slide-0
    item-2 at level 2: title: Test Table Slide
    item-3 at level 2: paragraph: With footnote
    item-4 at level 2: table with [9x7]
  item-5 at level 1: chapter: group slide-1
    item-6 at level 2: title: Second slide title
    item-7 at level 2: paragraph: Let’s introduce a list
    item-8 at level 2: paragraph: With foo
    item-9 at level 2: paragraph: Bar
    item-10 at level 2: paragraph: And baz things
    item-11 at level 2: paragraph: A rectangle shape with this text inside.
  item-12 at level 1: chapter: group slide-2
    item-13 at level 2: ordered_list: group list
      item-14 at level 3: list_item: List item4
      item-15 at level 3: list_item: List item5
      item-16 at level 3: list_item: List item6
    item-17 at level 2: list: group list
      item-18 at level 3: list_item: I1
      item-19 at level 3: list_item: I2
      item-20 at level 3: list_item: I3
      item-21 at level 3: list_item: I4
    item-22 at level 2: paragraph: Some info:
    item-23 at level 2: list: group list
      item-24 at level 3: list_item: Item A
      item-25 at level 3: list_item: Item B
    item-26 at level 2: paragraph: Maybe a list?
    item-27 at level 2: ordered_list: group list
      item-28 at level 3: list_item: List1
      item-29 at level 3: list_item: List2
      item-30 at level 3: list_item: List3
    item-31 at level 2: list: group list
      item-32 at level 3: list_item: l1 
      item-33 at level 3: list_item: l2
      item-34 at level 3: list_item: l3

================================================
File: tests/data/groundtruth/docling_v2/powerpoint_sample.pptx.md
================================================
# Test Table Slide

With footnote

|    | Class1          | Class1          | Class1   | Class2   | Class2   | Class2   |
|----|-----------------|-----------------|----------|----------|----------|----------|
|    | A merged with B | A merged with B | C        | A        | B        | C        |
| R1 | True            | False           |          | False    | True     | True     |
| R2 |                 |                 | True     | False    |          |          |
| R3 | False           |                 |          |          | False    |          |
| R3 |                 | True            |          | True     |          |          |
| R4 |                 |                 | False    |          | False    |          |
| R4 |                 | True            |          | True     | False    | False    |
| R4 | True            | False           | True     | False    | True     | False    |

# Second slide title

Let’s introduce a list

With foo

Bar

And baz things

A rectangle shape with this text inside.

1. List item4
2. List item5
3. List item6

- I1
- I2
- I3
- I4

Some info:

- Item A
- Item B

Maybe a list?

1. List1
2. List2
3. List3

- l1 
- l2
- l3

================================================
File: tests/data/groundtruth/docling_v2/powerpoint_with_image.pptx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: chapter: group slide-0
    item-2 at level 2: title: Docling
    item-3 at level 2: paragraph: Image test
    item-4 at level 2: picture

================================================
File: tests/data/groundtruth/docling_v2/powerpoint_with_image.pptx.md
================================================
# Docling

Image test

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v2/redp5110_sampled.doctags.txt
================================================
<doctag><text><loc_235><loc_18><loc_342><loc_32>Front cover</text>
<picture><loc_419><loc_16><loc_479><loc_35></picture>
<section_header_level_1><loc_29><loc_53><loc_478><loc_105>Row and Column Access Control Support in IBM DB2 for i</section_header_level_1>
<picture><loc_27><loc_185><loc_478><loc_443></picture>
<text><loc_235><loc_18><loc_342><loc_32>Front cover</text>
<page_footer><loc_30><loc_474><loc_134><loc_483>ibm.com /redbooks</page_footer>
<page_break>
<section_header_level_1><loc_53><loc_47><loc_138><loc_61>Contents</section_header_level_1>
<otsl><loc_111><loc_83><loc_447><loc_452><fcel>Notices<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii<nl><fcel>Trademarks<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii<nl><fcel>DB2 for i Center of Excellence<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix<nl><fcel>Preface<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi<nl><fcel>Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi<ecel><nl><fcel>Now you can become a published author, too!<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii<nl><fcel>Comments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>xiii<nl><fcel>Stay connected to IBM Redbooks<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv<nl><fcel>Chapter 1. Securing and protecting IBM DB2 data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>1<nl><fcel>1.1 Security fundamentals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2<ecel><nl><fcel>1.2 Current state of IBM i security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>2<nl><fcel>1.3 DB2 for i security controls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3<ecel><nl><fcel>1.3.1 Existing row and column control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>4<nl><fcel>1.3.2 New controls: Row and Column Access Control. . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>5<nl><fcel>Chapter 2. Roles and separation of duties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>7<nl><fcel>2.1 Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>8<nl><fcel>2.1.1 DDM and DRDA application server access: QIBM_DB_DDMDRDA . . . . . . . . . . .<fcel>8<nl><fcel>2.1.2 Toolbox application server access: QIBM_DB_ZDA. . . . . . . . . . . . . . . . . . . . . . . .<fcel>8<nl><fcel>2.1.3 Database Administrator function: QIBM_DB_SQLADM . . . . . . . . . . . . . . . . . . . . .<fcel>9<nl><fcel>2.1.4 Database Information function: QIBM_DB_SYSMON<fcel>. . . . . . . . . . . . . . . . . . . . . . 9<nl><fcel>2.1.5 Security Administrator function: QIBM_DB_SECADM . . . . . . . . . . . . . . . . . . . . . .<fcel>9<nl><fcel>2.1.6 Change Function Usage CL command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>10<nl><fcel>2.1.7 Verifying function usage IDs for RCAC with the FUNCTION_USAGE view . . . . .<fcel>10<nl><fcel>2.2 Separation of duties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10<ecel><nl><fcel>Chapter 3. Row and Column Access Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>13<nl><fcel>3.1 Explanation of RCAC and the concept of access control . . . . . . . . . . . . . . . . . . . . . . .<fcel>14<nl><fcel>3.1.1 Row permission and column mask definitions<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . 14<nl><fcel>3.1.2 Enabling and activating RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>16<nl><fcel>3.2 Special registers and built-in global variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>18<nl><fcel>3.2.1 Special registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>18<nl><fcel>3.2.2 Built-in global variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>19<nl><fcel>3.3 VERIFY_GROUP_FOR_USER function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>20<nl><fcel>3.4 Establishing and controlling accessibility by using the RCAC rule text . . . . . . . . . . . . .<fcel>21<nl><fcel>. . . . . . . . . . . . . . . . . . . . . . . .<fcel>22<nl><fcel>3.5 SELECT, INSERT, and UPDATE behavior with RCAC 3.6 Human resources example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>22<nl><fcel>3.6.1 Assigning the QIBM_DB_SECADM function ID to the consultants. . . . . . . . . . . .<fcel>23<nl><fcel>3.6.2 Creating group profiles for the users and their roles . . . . . . . . . . . . . . . . . . . . . . .<fcel>23<nl><fcel>3.6.3 Demonstrating data access without RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>24<nl><fcel>3.6.4 Defining and creating row permissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>25<nl><fcel>3.6.5 Defining and creating column masks<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26<nl><fcel>3.6.6 Activating RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>28<nl><fcel>3.6.7 Demonstrating data access with RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>29<nl><fcel>3.6.8 Demonstrating data access with a view and RCAC . . . . . . . . . . . . . . . . . . . . . . .<fcel>32<nl></otsl>
<page_footer><loc_53><loc_477><loc_210><loc_482>' Copyright IBM Corp. 2014. All rights reserved.</page_footer>
<page_footer><loc_440><loc_477><loc_447><loc_482>iii</page_footer>
<page_break>
<text><loc_53><loc_47><loc_193><loc_54>DB2 for i Center of Excellence</text>
<text><loc_77><loc_87><loc_191><loc_98>Solution Brief IBM Systems Lab Services and Training</text>
<picture><loc_117><loc_171><loc_147><loc_180></picture>
<section_header_level_1><loc_118><loc_207><loc_154><loc_213>Highlights</section_header_level_1>
<unordered_list><list_item><loc_118><loc_218><loc_198><loc_226>GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g81>GLYPH<g75>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g72>GLYPH<g3> GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g73>GLYPH<g82>GLYPH<g85>GLYPH<g80>GLYPH<g68>GLYPH<g81>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g92>GLYPH<g82>GLYPH<g88>GLYPH<g85> GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86></list_item>
<list_item><loc_118><loc_232><loc_212><loc_246>GLYPH<g115>GLYPH<g3> GLYPH<g40>GLYPH<g68>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g74>GLYPH<g85>GLYPH<g72>GLYPH<g68>GLYPH<g87>GLYPH<g72>GLYPH<g85>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g87>GLYPH<g88>GLYPH<g85> GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g55>GLYPH<g3> GLYPH<g83>GLYPH<g85>GLYPH<g82>GLYPH<g77>GLYPH<g72>GLYPH<g70>GLYPH<g87>GLYPH<g86> GLYPH<g3> GLYPH<g87>GLYPH<g75>GLYPH<g85>GLYPH<g82>GLYPH<g88>GLYPH<g74>GLYPH<g75>GLYPH<g3> GLYPH<g80>GLYPH<g82>GLYPH<g71>GLYPH<g72>GLYPH<g85> GLYPH<g81>GLYPH<g76>GLYPH<g93>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g71>GLYPH<g68>GLYPH<g87>GLYPH<g68>GLYPH<g69>GLYPH<g68>GLYPH<g86>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71> GLYPH<g3> GLYPH<g68>GLYPH<g83>GLYPH<g83>GLYPH<g79>GLYPH<g76>GLYPH<g70>GLYPH<g68>GLYPH<g87>GLYPH<g76>GLYPH<g82>GLYPH<g81>GLYPH<g86></list_item>
<list_item><loc_118><loc_252><loc_204><loc_260>GLYPH<g115>GLYPH<g3> GLYPH<g53>GLYPH<g72>GLYPH<g79>GLYPH<g92>GLYPH<g3> GLYPH<g82>GLYPH<g81>GLYPH<g3> GLYPH<g44>GLYPH<g37>GLYPH<g48>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g3> GLYPH<g70>GLYPH<g82>GLYPH<g81>GLYPH<g86>GLYPH<g88>GLYPH<g79>GLYPH<g87>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g15>GLYPH<g3> GLYPH<g86>GLYPH<g78>GLYPH<g76>GLYPH<g79>GLYPH<g79>GLYPH<g86> GLYPH<g3> GLYPH<g86>GLYPH<g75>GLYPH<g68>GLYPH<g85>GLYPH<g76>GLYPH<g81>GLYPH<g74>GLYPH<g3> GLYPH<g68>GLYPH<g81>GLYPH<g71>GLYPH<g3> GLYPH<g85>GLYPH<g72>GLYPH<g81>GLYPH<g82>GLYPH<g90>GLYPH<g81>GLYPH<g3> GLYPH<g86>GLYPH<g72>GLYPH<g85>GLYPH<g89>GLYPH<g76>GLYPH<g70>GLYPH<g72>GLYPH<g86></list_item>
<list_item><loc_118><loc_266><loc_191><loc_274>GLYPH<g115>GLYPH<g3> GLYPH<g55> GLYPH<g68>GLYPH<g78>GLYPH<g72>GLYPH<g3> GLYPH<g68>GLYPH<g71>GLYPH<g89>GLYPH<g68>GLYPH<g81>GLYPH<g87>GLYPH<g68>GLYPH<g74>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g68>GLYPH<g70>GLYPH<g70>GLYPH<g72>GLYPH<g86>GLYPH<g86>GLYPH<g3> GLYPH<g87>GLYPH<g82>GLYPH<g3> GLYPH<g68> GLYPH<g3> GLYPH<g90>GLYPH<g82>GLYPH<g85>GLYPH<g79>GLYPH<g71>GLYPH<g90>GLYPH<g76>GLYPH<g71>GLYPH<g72>GLYPH<g3> GLYPH<g86>GLYPH<g82>GLYPH<g88>GLYPH<g85>GLYPH<g70>GLYPH<g72>GLYPH<g3> GLYPH<g82>GLYPH<g73>GLYPH<g3> GLYPH<g72>GLYPH<g91>GLYPH<g83>GLYPH<g72>GLYPH<g85>GLYPH<g87>GLYPH<g76>GLYPH<g86>GLYPH<g72></list_item>
</unordered_list>
<picture><loc_52><loc_381><loc_211><loc_434></picture>
<text><loc_377><loc_87><loc_414><loc_92>Power Services</text>
<section_header_level_1><loc_229><loc_151><loc_379><loc_175>DB2 for i Center of Excellence</section_header_level_1>
<text><loc_229><loc_175><loc_395><loc_181>Expert help to achieve your business requirements</text>
<section_header_level_1><loc_229><loc_199><loc_362><loc_205>We build confident, satisfied clients</section_header_level_1>
<text><loc_229><loc_207><loc_399><loc_218>No one else has the vast consulting experiences, skills sharing and renown service offerings to do what we can do for you.</text>
<text><loc_229><loc_226><loc_301><loc_230>Because no one else is IBM.</text>
<text><loc_229><loc_238><loc_409><loc_268>With combined experiences and direct access to development groups, we're the experts in IBM DB2® for i. The DB2 for i Center of Excellence (CoE) can help you achieve-perhaps reexamine and exceed-your business requirements and gain more confidence and satisfaction in IBM product data management products and solutions.</text>
<section_header_level_1><loc_229><loc_276><loc_355><loc_282>Who we are, some of what we do</section_header_level_1>
<text><loc_229><loc_284><loc_355><loc_288>Global CoE engagements cover topics including:</text>
<unordered_list><list_item><loc_229><loc_296><loc_328><loc_301>r Database performance and scalability</list_item>
<list_item><loc_229><loc_303><loc_347><loc_307>r Advanced SQL knowledge and skills transfer</list_item>
<list_item><loc_229><loc_309><loc_320><loc_314>r Business intelligence and analytics</list_item>
<list_item><loc_229><loc_315><loc_278><loc_320>r DB2 Web Query</list_item>
<list_item><loc_229><loc_322><loc_412><loc_327>r Query/400 modernization for better reporting and analysis capabilities</list_item>
<list_item><loc_229><loc_328><loc_346><loc_333>r Database modernization and re-engineering</list_item>
<list_item><loc_229><loc_335><loc_327><loc_339>r Data-centric architecture and design</list_item>
<list_item><loc_229><loc_341><loc_381><loc_346>r Extremely large database and overcoming limits to growth</list_item>
<list_item><loc_229><loc_348><loc_312><loc_352>r ISV education and enablement</list_item>
</unordered_list>
<page_break>
<section_header_level_1><loc_53><loc_47><loc_124><loc_61>Preface</section_header_level_1>
<text><loc_112><loc_84><loc_447><loc_127>This IBMfi Redpaper™ publication provides information about the IBM i 7.2 feature of IBM DB2fi for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.</text>
<text><loc_112><loc_135><loc_446><loc_164>This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.</text>
<text><loc_112><loc_202><loc_447><loc_216>This paper was produced by the IBM DB2 for i Center of Excellence team in partnership with the International Technical Support Organization (ITSO), Rochester, Minnesota US.</text>
<picture><loc_116><loc_237><loc_205><loc_318></picture>
<text><loc_215><loc_237><loc_442><loc_326>Jim Bainbridge is a senior DB2 consultant on the DB2 for i Center of Excellence team in the IBM Lab Services and Training organization. His primary role is training and implementation services for IBM DB2 Web Query for i and business analytics. Jim began his career with IBM 30 years ago in the IBM Rochester Development Lab, where he developed cooperative processing products that paired IBM PCs with IBM S/36 and AS/.400 systems. In the years since, Jim has held numerous technical roles, including independent software vendors technical support on a broad range of IBM technologies and products, and supporting customers in the IBM Executive Briefing Center and IBM Project Office.</text>
<picture><loc_119><loc_333><loc_206><loc_401></picture>
<text><loc_215><loc_333><loc_442><loc_430>Hernando Bedoya is a Senior IT Specialist at STG Lab Services and Training in Rochester, Minnesota. He writes extensively and teaches IBM classes worldwide in all areas of DB2 for i. Before joining STG Lab Services, he worked in the ITSO for nine years writing multiple IBM Redbooksfi publications. He also worked for IBM Colombia as an IBM AS/400fi IT Specialist doing presales support for the Andean countries. He has 28 years of experience in the computing field and has taught database classes in Colombian universities. He holds a Master's degree in Computer Science from EAFIT, Colombia. His areas of expertise are database technology, performance, and data warehousing. Hernando can be contacted at hbedoya@us.ibm.com .</text>
<section_header_level_1><loc_53><loc_182><loc_102><loc_191>Authors</section_header_level_1>
<page_footer><loc_53><loc_477><loc_210><loc_482>' Copyright IBM Corp. 2014. All rights reserved.</page_footer>
<page_footer><loc_440><loc_477><loc_447><loc_482>xi</page_footer>
<page_break>
<picture><loc_26><loc_45><loc_196><loc_150></picture>
<text><loc_409><loc_59><loc_427><loc_82>1</text>
<text><loc_66><loc_170><loc_94><loc_174>Chapter 1.</text>
<section_header_level_1><loc_112><loc_161><loc_447><loc_196>Securing and protecting IBM DB2 data</section_header_level_1>
<text><loc_112><loc_220><loc_447><loc_271>Recent news headlines are filled with reports of data breaches and cyber-attacks impacting global businesses of all sizes. The Identity Theft Resource Center$^{1}$ reports that almost 5000 data breaches have occurred since 2005, exposing over 600 million records of data. The financial cost of these data breaches is skyrocketing. Studies from the Ponemon Institute$^{2}$ revealed that the average cost of a data breach increased in 2013 by 15% globally and resulted in a brand equity loss of $9.4 million per attack. The average cost that is incurred for each lost record containing sensitive information increased more than 9% to $145 per record.</text>
<text><loc_112><loc_279><loc_431><loc_308>Businesses must make a serious effort to secure their data and recognize that securing information assets is a cost of doing business. In many parts of the world and in many industries, securing the data is required by law and subject to audits. Data security is no longer an option; it is a requirement.</text>
<text><loc_112><loc_316><loc_447><loc_329>This chapter describes how you can secure and protect data in DB2 for i. The following topics are covered in this chapter:</text>
<unordered_list><list_item><loc_112><loc_334><loc_204><loc_340>GLYPH<SM590000> Security fundamentals</list_item>
<list_item><loc_112><loc_342><loc_231><loc_348>GLYPH<SM590000> Current state of IBM i security</list_item>
<list_item><loc_112><loc_350><loc_216><loc_355>GLYPH<SM590000> DB2 for i security controls</list_item>
</unordered_list>
<footnote><loc_112><loc_453><loc_211><loc_458>$^{1 }$http://www.idtheftcenter.org</footnote>
<footnote><loc_112><loc_459><loc_191><loc_464>$^{2 }$http://www.ponemon.org /</footnote>
<page_footer><loc_53><loc_477><loc_210><loc_482>' Copyright IBM Corp. 2014. All rights reserved.</page_footer>
<page_footer><loc_443><loc_477><loc_447><loc_482>1</page_footer>
<page_break>
<section_header_level_1><loc_53><loc_47><loc_218><loc_56>1.1 Security fundamentals</section_header_level_1>
<text><loc_112><loc_67><loc_445><loc_81>Before reviewing database security techniques, there are two fundamental steps in securing information assets that must be described:</text>
<unordered_list><list_item><loc_112><loc_85><loc_447><loc_114>GLYPH<SM590000> First, and most important, is the definition of a company's security policy . Without a security policy, there is no definition of what are acceptable practices for using, accessing, and storing information by who, what, when, where, and how. A security policy should minimally address three things: confidentiality, integrity, and availability.</list_item>
<list_item><loc_124><loc_119><loc_447><loc_170>The monitoring and assessment of adherence to the security policy determines whether your security strategy is working. Often, IBM security consultants are asked to perform security assessments for companies without regard to the security policy. Although these assessments can be useful for observing how the system is defined and how data is being accessed, they cannot determine the level of security without a security policy. Without a security policy, it really is not an assessment as much as it is a baseline for monitoring the changes in the security settings that are captured.</list_item>
</unordered_list>
<text><loc_124><loc_175><loc_443><loc_181>A security policy is what defines whether the system and its settings are secure (or not).</text>
<unordered_list><list_item><loc_112><loc_186><loc_447><loc_237>GLYPH<SM590000> The second fundamental in securing data assets is the use of resource security . If implemented properly, resource security prevents data breaches from both internal and external intrusions. Resource security controls are closely tied to the part of the security policy that defines who should have access to what information resources. A hacker might be good enough to get through your company firewalls and sift his way through to your system, but if they do not have explicit access to your database, the hacker cannot compromise your information assets.</list_item>
</unordered_list>
<text><loc_112><loc_245><loc_437><loc_259>With your eyes now open to the importance of securing information assets, the rest of this chapter reviews the methods that are available for securing database resources on IBM i.</text>
<section_header_level_1><loc_53><loc_277><loc_264><loc_286>1.2 Current state of IBM i security</section_header_level_1>
<text><loc_112><loc_297><loc_447><loc_326>Because of the inherently secure nature of IBM i, many clients rely on the default system settings to protect their business data that is stored in DB2 for i. In most cases, this means no data protection because the default setting for the Create default public authority (QCRTAUT) system value is *CHANGE.</text>
<text><loc_112><loc_334><loc_447><loc_370>Even more disturbing is that many IBM i clients remain in this state, despite the news headlines and the significant costs that are involved with databases being compromised. This default security configuration makes it quite challenging to implement basic security policies. A tighter implementation is required if you really want to protect one of your company's most valuable assets, which is the data.</text>
<text><loc_112><loc_378><loc_447><loc_429>Traditionally, IBM i applications have employed menu-based security to counteract this default configuration that gives all users access to the data. The theory is that data is protected by the menu options controlling what database operations that the user can perform. This approach is ineffective, even if the user profile is restricted from running interactive commands. The reason is that in today's connected world there are a multitude of interfaces into the system, from web browsers to PC clients, that bypass application menus. If there are no object-level controls, users of these newer interfaces have an open door to your data.</text>
<page_footer><loc_53><loc_477><loc_59><loc_482>2</page_footer>
<page_footer><loc_72><loc_477><loc_269><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<page_break>
<text><loc_112><loc_45><loc_445><loc_96>Many businesses are trying to limit data access to a need-to-know basis. This security goal means that users should be given access only to the minimum set of data that is required to perform their job. Often, users with object-level access are given access to row and column values that are beyond what their business task requires because that object-level security provides an all-or-nothing solution. For example, object-level controls allow a manager to access data about all employees. Most security policies limit a manager to accessing data only for the employees that they manage.</text>
<section_header_level_1><loc_53><loc_109><loc_246><loc_117>1.3.1 Existing row and column control</section_header_level_1>
<text><loc_112><loc_126><loc_442><loc_162>Some IBM i clients have tried augmenting the all-or-nothing object-level security with SQL views (or logical files) and application logic, as shown in Figure 1-2. However, application-based logic is easy to bypass with all of the different data access interfaces that are provided by the IBM i operating system, such as Open Database Connectivity (ODBC) and System i Navigator.</text>
<text><loc_112><loc_170><loc_447><loc_199>Using SQL views to limit access to a subset of the data in a table also has its own set of challenges. First, there is the complexity of managing all of the SQL view objects that are used for securing data access. Second, scaling a view-based security solution can be difficult as the amount of data grows and the number of users increases.</text>
<text><loc_112><loc_207><loc_447><loc_228>Even if you are willing to live with these performance and management issues, a user with *ALLOBJ access still can directly access all of the data in the underlying DB2 table and easily bypass the security controls that are built into an SQL view.</text>
<picture><loc_111><loc_237><loc_446><loc_435><caption><loc_112><loc_437><loc_259><loc_442>Figure 1-2 Existing row and column controls</caption></picture>
<page_footer><loc_53><loc_477><loc_59><loc_482>4</page_footer>
<page_footer><loc_72><loc_477><loc_269><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<page_break>
<section_header_level_1><loc_53><loc_45><loc_274><loc_53>2.1.6 Change Function Usage CL command</section_header_level_1>
<text><loc_112><loc_62><loc_447><loc_67>The following CL commands can be used to work with, display, or change function usage IDs:</text>
<unordered_list><list_item><loc_112><loc_72><loc_246><loc_78>GLYPH<SM590000> Work Function Usage ( WRKFCNUSG )</list_item>
<list_item><loc_112><loc_80><loc_256><loc_86>GLYPH<SM590000> Change Function Usage ( CHGFCNUSG )</list_item>
<list_item><loc_112><loc_87><loc_254><loc_93>GLYPH<SM590000> Display Function Usage ( DSPFCNUSG )</list_item>
</unordered_list>
<text><loc_112><loc_101><loc_419><loc_115>For example, the following CHGFCNUSG command shows granting authorization to user HBEDOYA to administer and manage RCAC rules:</text>
<text><loc_112><loc_120><loc_361><loc_125>CHGFCNUSG FCNID(QIBM_DB_SECADM) USER(HBEDOYA) USAGE(*ALLOWED)</text>
<section_header_level_1><loc_53><loc_138><loc_445><loc_146>2.1.7 Verifying function usage IDs for RCAC with the FUNCTION_USAGE view</section_header_level_1>
<text><loc_112><loc_155><loc_424><loc_168>The FUNCTION_USAGE view contains function usage configuration details. Table 2-1 describes the columns in the FUNCTION_USAGE view.</text>
<otsl><loc_111><loc_183><loc_446><loc_279><ched>Column name<ched>Data type<ched>Description<nl><fcel>FUNCTION_ID<fcel>VARCHAR(30)<fcel>ID of the function.<nl><fcel>USER_NAME<fcel>VARCHAR(10)<fcel>Name of the user profile that has a usage setting for this  function.<nl><fcel>USAGE<fcel>VARCHAR(7)<fcel>Usage setting: GLYPH<SM590000> ALLOWED: The user profile is allowed to use the function. GLYPH<SM590000> DENIED: The user profile is not allowed to use the function.<nl><fcel>USER_TYPE<fcel>VARCHAR(5)<fcel>Type of user profile: GLYPH<SM590000> USER: The user profile is a user. GLYPH<SM590000> GROUP: The user profile is a group.<nl><caption><loc_112><loc_176><loc_232><loc_182>Table 2-1 FUNCTION_USAGE view</caption></otsl>
<text><loc_112><loc_286><loc_447><loc_299>To discover who has authorization to define and manage RCAC, you can use the query that is shown in Example 2-1.</text>
<caption><loc_112><loc_307><loc_378><loc_312>Example 2-1 Query to determine who has authority to define and manage RCAC</caption>
<text><loc_112><loc_318><loc_140><loc_324>SELECT</text>
<text><loc_149><loc_318><loc_206><loc_324>function_id,</text>
<text><loc_136><loc_326><loc_197><loc_331>user_name,</text>
<text><loc_140><loc_333><loc_181><loc_339>usage,</text>
<text><loc_137><loc_341><loc_193><loc_346>user_type</text>
<text><loc_112><loc_348><loc_131><loc_354>FROM</text>
<text><loc_146><loc_348><loc_214><loc_354>function_usage</text>
<text><loc_112><loc_356><loc_133><loc_361>WHERE</text>
<text><loc_145><loc_356><loc_271><loc_361>function_id=’QIBM_DB_SECADM’</text>
<text><loc_112><loc_363><loc_146><loc_369>ORDER BY</text>
<text><loc_155><loc_363><loc_197><loc_369>user_name;</text>
<section_header_level_1><loc_53><loc_392><loc_204><loc_401>2.2 Separation of duties</section_header_level_1>
<text><loc_112><loc_412><loc_447><loc_448>Separation of duties helps businesses comply with industry regulations or organizational requirements and simplifies the management of authorities. Separation of duties is commonly used to prevent fraudulent activities or errors by a single person. It provides the ability for administrative functions to be divided across individuals without overlapping responsibilities, so that one user does not possess unlimited authority, such as with the *ALLOBJ authority.</text>
<page_footer><loc_53><loc_477><loc_64><loc_482>10</page_footer>
<page_footer><loc_76><loc_477><loc_273><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<page_break>
<text><loc_112><loc_45><loc_443><loc_89>For example, assume that a business has assigned the duty to manage security on IBM i to Theresa. Before release IBM i 7.2, to grant privileges, Theresa had to have the same privileges Theresa was granting to others. Therefore, to grant *USE privileges to the PAYROLL table, Theresa had to have *OBJMGT and *USE authority (or a higher level of authority, such as *ALLOBJ). This requirement allowed Theresa to access the data in the PAYROLL table even though Theresa's job description was only to manage its security.</text>
<text><loc_112><loc_97><loc_447><loc_125>In IBM i 7.2, the QIBM_DB_SECADM function usage grants authorities, revokes authorities, changes ownership, or changes the primary group without giving access to the object or, in the case of a database table, to the data that is in the table or allowing other operations on the table.</text>
<text><loc_112><loc_134><loc_440><loc_147>QIBM_DB_SECADM function usage can be granted only by a user with *SECADM special authority and can be given to a user or a group.</text>
<text><loc_112><loc_155><loc_446><loc_176>QIBM_DB_SECADM also is responsible for administering RCAC, which restricts which rows a user is allowed to access in a table and whether a user is allowed to see information in certain columns of a table.</text>
<text><loc_112><loc_184><loc_441><loc_213>A preferred practice is that the RCAC administrator has the QIBM_DB_SECADM function usage ID, but absolutely no other data privileges. The result is that the RCAC administrator can deploy and maintain the RCAC constructs, but cannot grant themselves unauthorized access to data itself.</text>
<text><loc_112><loc_221><loc_444><loc_234>Table 2-2 shows a comparison of the different function usage IDs and *JOBCTL authority to the different CL commands and DB2 for i tools.</text>
<otsl><loc_53><loc_248><loc_447><loc_456><rhed>User action<fcel>*JOBCTL<fcel>QIBM_DB_SECADM<fcel>QIBM_DB_SQLADM<fcel>QIBM_DB_SYSMON<fcel>No Authority<nl><rhed>SET CURRENT DEGREE  (SQL statement)<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>CHGQRYA  command targeting a different user’s job<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>STRDBMON  or  ENDDBMON  commands targeting a different user’s job<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>STRDBMON  or  ENDDBMON  commands targeting a job that matches the current user<fcel>X<ecel><fcel>X<fcel>X<fcel>X<nl><rhed>QUSRJOBI() API format 900 or System i Navigator’s SQL Details for Job<fcel>X<ecel><fcel>X<fcel>X<ecel><nl><rhed>Visual Explain within Run SQL scripts<fcel>X<ecel><fcel>X<fcel>X<fcel>X<nl><rhed>Visual Explain outside of Run SQL scripts<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>ANALYZE PLAN CACHE procedure<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>DUMP PLAN CACHE procedure<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>MODIFY PLAN CACHE procedure<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>MODIFY PLAN CACHE PROPERTIES procedure (currently does not check authority)<fcel>X<ecel><fcel>X<ecel><ecel><nl><rhed>CHANGE PLAN CACHE SIZE procedure (currently does not check authority)<fcel>X<ecel><fcel>X<ecel><ecel><nl><caption><loc_53><loc_242><loc_320><loc_247>Table 2-2 Comparison of the different function usage IDs and *JOBCTL authority</caption></otsl>
<page_footer><loc_290><loc_477><loc_428><loc_482>Chapter 2. Roles and separation of duties</page_footer>
<page_footer><loc_438><loc_477><loc_447><loc_482>11</page_footer>
<page_break>
<caption><loc_112><loc_45><loc_432><loc_59>The SQL CREATE PERMISSION statement that is shown in Figure 3-1 is used to define and initially enable or disable the row access rules.</caption>
<picture><loc_111><loc_68><loc_446><loc_259><caption><loc_112><loc_261><loc_279><loc_267>Figure 3-1 CREATE PERMISSION SQL statement</caption></picture>
<section_header_level_1><loc_112><loc_278><loc_176><loc_285>Column mask</section_header_level_1>
<text><loc_112><loc_287><loc_443><loc_316>A column mask is a database object that manifests a column value access control rule for a specific column in a specific table. It uses a CASE expression that describes what you see when you access the column. For example, a teller can see only the last four digits of a tax identification number.</text>
<page_footer><loc_282><loc_477><loc_428><loc_482>Chapter 3. Row and Column Access Control</page_footer>
<page_footer><loc_438><loc_477><loc_447><loc_482>15</page_footer>
<page_break>
<caption><loc_112><loc_45><loc_337><loc_51>Table 3-1 summarizes these special registers and their values.</caption>
<otsl><loc_110><loc_65><loc_443><loc_129><ched>Special register<ched>Corresponding value<nl><fcel>USER or SESSION_USER<fcel>The effective user of the thread excluding adopted authority.<nl><fcel>CURRENT_USER<fcel>The effective user of the thread including adopted authority. When no adopted  authority is present, this has the same value as USER.<nl><fcel>SYSTEM_USER<fcel>The authorization ID that initiated the connection.<nl><caption><loc_112><loc_59><loc_304><loc_64>Table 3-1 Special registers and their corresponding values</caption></otsl>
<text><loc_112><loc_135><loc_440><loc_149>Figure 3-5 shows the difference in the special register values when an adopted authority is used:</text>
<unordered_list><list_item><loc_112><loc_154><loc_336><loc_160>GLYPH<SM590000> A user connects to the server using the user profile ALICE.</list_item>
<list_item><loc_112><loc_164><loc_370><loc_170>GLYPH<SM590000> USER and CURRENT USER initially have the same value of ALICE.</list_item>
<list_item><loc_112><loc_175><loc_442><loc_189>GLYPH<SM590000> ALICE calls an SQL procedure that is named proc1, which is owned by user profile JOE and was created to adopt JOE's authority when it is called.</list_item>
<list_item><loc_112><loc_194><loc_447><loc_214>GLYPH<SM590000> While the procedure is running, the special register USER still contains the value of ALICE because it excludes any adopted authority. The special register CURRENT USER contains the value of JOE because it includes any adopted authority.</list_item>
<list_item><loc_112><loc_219><loc_447><loc_233>GLYPH<SM590000> When proc1 ends, the session reverts to its original state with both USER and CURRENT USER having the value of ALICE.</list_item>
</unordered_list>
<picture><loc_111><loc_243><loc_246><loc_375><caption><loc_112><loc_377><loc_279><loc_382>Figure 3-5 Special registers and adopted authority</caption></picture>
<section_header_level_1><loc_53><loc_395><loc_202><loc_402>3.2.2 Built-in global variables</section_header_level_1>
<text><loc_112><loc_411><loc_423><loc_425>Built-in global variables are provided with the database manager and are used in SQL statements to retrieve scalar values that are associated with the variables.</text>
<text><loc_112><loc_433><loc_435><loc_454>IBM DB2 for i supports nine different built-in global variables that are read only and maintained by the system. These global variables can be used to identify attributes of the database connection and used as part of the RCAC logic.</text>
<page_footer><loc_282><loc_477><loc_428><loc_482>Chapter 3. Row and Column Access Control</page_footer>
<page_footer><loc_438><loc_477><loc_447><loc_482>19</page_footer>
<page_break>
<text><loc_112><loc_45><loc_280><loc_51>Table 3-2 lists the nine built-in global variables.</text>
<otsl><loc_52><loc_66><loc_448><loc_187><ched>Global variable<ched>Type<ched>Description<nl><fcel>CLIENT_HOST<fcel>VARCHAR(255)<fcel>Host name of the current client as returned by the system<nl><fcel>CLIENT_IPADDR<fcel>VARCHAR(128)<fcel>IP address of the current client as returned by the system<nl><fcel>CLIENT_PORT<fcel>INTEGER<fcel>Port used by the current client to communicate with the server<nl><fcel>PACKAGE_NAME<fcel>VARCHAR(128)<fcel>Name of the currently running package<nl><fcel>PACKAGE_SCHEMA<fcel>VARCHAR(128)<fcel>Schema name of the currently running package<nl><fcel>PACKAGE_VERSION<fcel>VARCHAR(64)<fcel>Version identifier of the currently running package<nl><fcel>ROUTINE_SCHEMA<fcel>VARCHAR(128)<fcel>Schema name of the currently running routine<nl><fcel>ROUTINE_SPECIFIC_NAME<fcel>VARCHAR(128)<fcel>Name of the currently running routine<nl><fcel>ROUTINE_TYPE<fcel>CHAR(1)<fcel>Type of the currently running routine<nl><caption><loc_53><loc_59><loc_164><loc_64>Table 3-2 Built-in global variables</caption></otsl>
<section_header_level_1><loc_53><loc_203><loc_314><loc_213>3.3 VERIFY_GROUP_FOR_USER function</section_header_level_1>
<text><loc_112><loc_224><loc_447><loc_275>The VERIFY_GROUP_FOR_USER function was added in IBM i 7.2. Although it is primarily intended for use with RCAC permissions and masks, it can be used in other SQL statements. The first parameter must be one of these three special registers: SESSION_USER, USER, or CURRENT_USER. The second and subsequent parameters are a list of user or group profiles. Each of these values must be 1 - 10 characters in length. These values are not validated for their existence, which means that you can specify the names of user profiles that do not exist without receiving any kind of error.</text>
<text><loc_112><loc_283><loc_447><loc_304>If a special register value is in the list of user profiles or it is a member of a group profile included in the list, the function returns a long integer value of 1. Otherwise, it returns a value of 0. It never returns the null value.</text>
<text><loc_112><loc_312><loc_375><loc_318>Here is an example of using the VERIFY_GROUP_FOR_USER function:</text>
<unordered_list><list_item><loc_112><loc_323><loc_332><loc_329>1. There are user profiles for MGR, JANE, JUDY, and TONY.</list_item>
<list_item><loc_112><loc_334><loc_324><loc_339>2. The user profile JANE specifies a group profile of MGR.</list_item>
<list_item><loc_112><loc_344><loc_438><loc_358>3. If a user is connected to the server using user profile JANE, all of the following function invocations return a value of 1:</list_item>
</unordered_list>
<code<loc_124><loc_363><loc_368><loc_405><_unknown_>VERIFY_GROUP_FOR_USER (CURRENT_USER, 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR', 'STEVE') The following function invocation returns a value of 0: VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JUDY', 'TONY')</code
<page_footer><loc_53><loc_477><loc_64><loc_482>20</page_footer>
<page_footer><loc_76><loc_477><loc_273><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<page_break>
<text><loc_112><loc_45><loc_136><loc_51>RETURN</text>
<text><loc_112><loc_53><loc_128><loc_58>CASE</text>
<code<loc_112><loc_60><loc_426><loc_164><_unknown_>WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR', 'EMP' ) = 1 THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 9999 || '-' || MONTH ( EMPLOYEES . DATE_OF_BIRTH ) || '-' || DAY (EMPLOYEES.DATE_OF_BIRTH )) ELSE NULL END ENABLE ;</code
<unordered_list><list_item><loc_112><loc_174><loc_447><loc_187>2. The other column to mask in this example is the TAX_ID information. In this example, the rules to enforce include the following ones:</list_item>
<list_item><loc_124><loc_192><loc_383><loc_198>-Human Resources can see the unmasked TAX_ID of the employees.</list_item>
<list_item><loc_124><loc_203><loc_330><loc_209>-Employees can see only their own unmasked TAX_ID.</list_item>
<list_item><loc_124><loc_214><loc_445><loc_227>-Managers see a masked version of TAX_ID with the first five characters replaced with the X character (for example, XXX-XX-1234).</list_item>
<list_item><loc_124><loc_232><loc_433><loc_238>-Any other person sees the entire TAX_ID as masked, for example, XXX-XX-XXXX.</list_item>
<list_item><loc_124><loc_243><loc_433><loc_249>To implement this column mask, run the SQL statement that is shown in Example 3-9.</list_item>
</unordered_list>
<code<loc_112><loc_267><loc_430><loc_432><_unknown_>CREATE MASK HR_SCHEMA.MASK_TAX_ID_ON_EMPLOYEES ON HR_SCHEMA.EMPLOYEES AS EMPLOYEES FOR COLUMN TAX_ID RETURN CASE WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR' ) = 1 THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( EMPLOYEES . TAX_ID , 8 , 4 ) ) WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'EMP' ) = 1 THEN EMPLOYEES . TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ;</code
<caption><loc_112><loc_257><loc_288><loc_262>Example 3-9 Creating a mask on the TAX_ID column</caption>
<page_footer><loc_282><loc_477><loc_428><loc_482>Chapter 3. Row and Column Access Control</page_footer>
<page_footer><loc_438><loc_477><loc_447><loc_482>27</page_footer>
<unordered_list><page_break>
<list_item><loc_112><loc_45><loc_368><loc_51>3. Figure 3-10 shows the masks that are created in the HR_SCHEMA.</list_item>
</unordered_list>
<picture><loc_52><loc_60><loc_447><loc_107><caption><loc_53><loc_110><loc_239><loc_115>Figure 3-10 Column masks shown in System i Navigator</caption></picture>
<section_header_level_1><loc_53><loc_128><loc_167><loc_135>3.6.6 Activating RCAC</section_header_level_1>
<text><loc_112><loc_144><loc_447><loc_165>Now that you have created the row permission and the two column masks, RCAC must be activated. The row permission and the two column masks are enabled (last clause in the scripts), but now you must activate RCAC on the table. To do so, complete the following steps:</text>
<unordered_list><list_item><loc_112><loc_170><loc_335><loc_176>1. Run the SQL statements that are shown in Example 3-10.</list_item>
</unordered_list>
<section_header_level_1><loc_112><loc_184><loc_307><loc_189>Example 3-10 Activating RCAC on the EMPLOYEES table</section_header_level_1>
<unordered_list><list_item><loc_112><loc_195><loc_308><loc_200>/* Active Row Access Control (permissions) */</list_item>
<list_item><loc_112><loc_202><loc_290><loc_208>/* Active Column Access Control (masks)</list_item>
</unordered_list>
<text><loc_299><loc_202><loc_308><loc_208>*/</text>
<text><loc_112><loc_210><loc_238><loc_216>ALTER TABLE HR_SCHEMA.EMPLOYEES</text>
<text><loc_112><loc_218><loc_222><loc_223>ACTIVATE ROW ACCESS CONTROL</text>
<text><loc_112><loc_225><loc_238><loc_231>ACTIVATE COLUMN ACCESS CONTROL;</text>
<unordered_list><list_item><loc_112><loc_240><loc_442><loc_261>2. Look at the definition of the EMPLOYEE table, as shown in Figure 3-11. To do this, from the main navigation pane of System i Navigator, click Schemas  HR_SCHEMA  Tables , right-click the EMPLOYEES table, and click Definition .</list_item>
</unordered_list>
<picture><loc_52><loc_270><loc_433><loc_408><caption><loc_53><loc_410><loc_284><loc_415>Figure 3-11 Selecting the EMPLOYEES table from System i Navigator</caption></picture>
<page_footer><loc_53><loc_477><loc_64><loc_482>28</page_footer>
<page_footer><loc_76><loc_477><loc_273><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<unordered_list><page_break>
<list_item><loc_112><loc_45><loc_420><loc_66>2. Figure 4-68 shows the Visual Explain of the same SQL statement, but with RCAC enabled. It is clear that the implementation of the SQL statement is more complex because the row permission rule becomes part of the WHERE clause.</list_item>
<list_item><loc_112><loc_320><loc_447><loc_341>3. Compare the advised indexes that are provided by the Optimizer without RCAC and with RCAC enabled. Figure 4-69 shows the index advice for the SQL statement without RCAC enabled. The index being advised is for the ORDER BY clause.</list_item>
</unordered_list>
<picture><loc_112><loc_75><loc_446><loc_301><caption><loc_112><loc_303><loc_267><loc_309>Figure 4-68 Visual Explain with RCAC enabled</caption></picture>
<picture><loc_53><loc_349><loc_414><loc_419><caption><loc_53><loc_421><loc_186><loc_427>Figure 4-69 Index advice with no RCAC</caption></picture>
<page_footer><loc_175><loc_477><loc_428><loc_482>Chapter 4. Implementing Row and Column Access Control: Banking example</page_footer>
<page_footer><loc_438><loc_477><loc_447><loc_482>77</page_footer>
<page_break>
<code<loc_53><loc_45><loc_409><loc_446><_unknown_>THEN C . CUSTOMER_TAX_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( C . CUSTOMER_TAX_ID , 8 , 4 ) ) WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_DRIVERS_LICENSE_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_DRIVERS_LICENSE_NUMBER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER ELSE '*************' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_LOGIN_ID_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_LOGIN_ID RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_LOGIN_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_LOGIN_ID ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ANSWER_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION_ANSWER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER ELSE '*****' END ENABLE ; ALTER TABLE BANK_SCHEMA.CUSTOMERS ACTIVATE ROW ACCESS CONTROL ACTIVATE COLUMN ACCESS CONTROL ;</code
<page_footer><loc_53><loc_477><loc_69><loc_482>124</page_footer>
<page_footer><loc_81><loc_477><loc_278><loc_482>Row and Column Access Control Support in IBM DB2 for i</page_footer>
<page_break>
<text><loc_235><loc_18><loc_338><loc_32>Back cover</text>
<section_header_level_1><loc_22><loc_46><loc_365><loc_89>Row and Column Access Control Support in IBM DB2 for i</section_header_level_1>
<text><loc_22><loc_153><loc_104><loc_168>Implement roles and separation of duties</text>
<text><loc_22><loc_179><loc_98><loc_204>Leverage row permissions on the database</text>
<text><loc_125><loc_153><loc_338><loc_204>This IBM Redpaper publication provides information about the IBM i 7.2 feature of IBM DB2 for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.</text>
<text><loc_22><loc_215><loc_99><loc_239>Protect columns by defining column masks</text>
<text><loc_125><loc_209><loc_338><loc_245>This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.</text>
<picture><loc_396><loc_16><loc_463><loc_34></picture>
<picture><loc_388><loc_51><loc_484><loc_120></picture>
<text><loc_382><loc_156><loc_457><loc_191>INTERNATIONAL TECHNICAL SUPPORT ORGANIZATION</text>
<text><loc_382><loc_222><loc_480><loc_244>BUILDING TECHNICAL INFORMATION BASED ON PRACTICAL EXPERIENCE</text>
<text><loc_382><loc_252><loc_480><loc_342>IBM Redbooks are developed by the IBM International Technical Support Organization. Experts from IBM, Customers and Partners from around the world create timely technical information based on realistic scenarios. Specific recommendations are provided to help you implement IT solutions more effectively in your environment.</text>
<text><loc_382><loc_365><loc_466><loc_380>For more information: ibm.com /redbooks</text>
<page_footer><loc_140><loc_399><loc_189><loc_404>REDP-5110-00</page_footer>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/redp5110_sampled.md
================================================
Front cover

<!-- image -->

## Row and Column Access Control Support in IBM DB2 for i

<!-- image -->

Front cover

## Contents

| Notices                                                                                                                                                                   | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Trademarks                                                                                                                                                                | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii          |
| DB2 for i Center of Excellence                                                                                                                                            | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix                                          |
| Preface                                                                                                                                                                   | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi    |
| Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi                            |                                                                                                                                         |
| Now you can become a published author, too!                                                                                                                               | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii                                                                |
| Comments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                             | xiii                                                                                                                                    |
| Stay connected to IBM Redbooks                                                                                                                                            | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv                                             |
| Chapter 1. Securing and protecting IBM DB2 data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                | 1                                                                                                                                       |
| 1.1 Security fundamentals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2                                          |                                                                                                                                         |
| 1.2 Current state of IBM i security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                             | 2                                                                                                                                       |
| 1.3 DB2 for i security controls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3                                         |                                                                                                                                         |
| 1.3.1 Existing row and column control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                       | 4                                                                                                                                       |
| 1.3.2 New controls: Row and Column Access Control. . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                    | 5                                                                                                                                       |
| Chapter 2. Roles and separation of duties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                         | 7                                                                                                                                       |
| 2.1 Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                 | 8                                                                                                                                       |
| 2.1.1 DDM and DRDA application server access: QIBM\_DB\_DDMDRDA . . . . . . . . . . .                                                                                       | 8                                                                                                                                       |
| 2.1.2 Toolbox application server access: QIBM\_DB\_ZDA. . . . . . . . . . . . . . . . . . . . . . . .                                                                       | 8                                                                                                                                       |
| 2.1.3 Database Administrator function: QIBM\_DB\_SQLADM . . . . . . . . . . . . . . . . . . . . .                                                                           | 9                                                                                                                                       |
| 2.1.4 Database Information function: QIBM\_DB\_SYSMON                                                                                                                       | . . . . . . . . . . . . . . . . . . . . . . 9                                                                                           |
| 2.1.5 Security Administrator function: QIBM\_DB\_SECADM . . . . . . . . . . . . . . . . . . . . . .                                                                         | 9                                                                                                                                       |
| 2.1.6 Change Function Usage CL command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                  | 10                                                                                                                                      |
| 2.1.7 Verifying function usage IDs for RCAC with the FUNCTION\_USAGE view . . . . .                                                                                        | 10                                                                                                                                      |
| 2.2 Separation of duties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10                                         |                                                                                                                                         |
| Chapter 3. Row and Column Access Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                | 13                                                                                                                                      |
| 3.1 Explanation of RCAC and the concept of access control . . . . . . . . . . . . . . . . . . . . . . .                                                                   | 14                                                                                                                                      |
| 3.1.1 Row permission and column mask definitions                                                                                                                          | . . . . . . . . . . . . . . . . . . . . . . . . . . . 14                                                                                |
| 3.1.2 Enabling and activating RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                          | 16                                                                                                                                      |
| 3.2 Special registers and built-in global variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                       | 18                                                                                                                                      |
| 3.2.1 Special registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                               | 18                                                                                                                                      |
| 3.2.2 Built-in global variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                 | 19                                                                                                                                      |
| 3.3 VERIFY\_GROUP\_FOR\_USER function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                | 20                                                                                                                                      |
| 3.4 Establishing and controlling accessibility by using the RCAC rule text . . . . . . . . . . . . .                                                                      | 21                                                                                                                                      |
| . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                                           | 22                                                                                                                                      |
| 3.5 SELECT, INSERT, and UPDATE behavior with RCAC 3.6 Human resources example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 22                                                                                                                                      |
| 3.6.1 Assigning the QIBM\_DB\_SECADM function ID to the consultants. . . . . . . . . . . .                                                                                  | 23                                                                                                                                      |
| 3.6.2 Creating group profiles for the users and their roles . . . . . . . . . . . . . . . . . . . . . . .                                                                 | 23                                                                                                                                      |
| 3.6.3 Demonstrating data access without RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                  | 24                                                                                                                                      |
| 3.6.4 Defining and creating row permissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                             | 25                                                                                                                                      |
| 3.6.5 Defining and creating column masks                                                                                                                                  | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26                                                                  |
| 3.6.6 Activating RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                 | 28                                                                                                                                      |
| 3.6.7 Demonstrating data access with RCAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                 | 29                                                                                                                                      |
| 3.6.8 Demonstrating data access with a view and RCAC . . . . . . . . . . . . . . . . . . . . . . .                                                                        | 32                                                                                                                                      |

DB2 for i Center of Excellence

Solution Brief IBM Systems Lab Services and Training

<!-- image -->

## Highlights

- GLYPH&lt;g115&gt;GLYPH&lt;g3&gt; GLYPH&lt;g40&gt;GLYPH&lt;g81&gt;GLYPH&lt;g75&gt;GLYPH&lt;g68&gt;GLYPH&lt;g81&gt;GLYPH&lt;g70&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g87&gt;GLYPH&lt;g75&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g83&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g73&gt;GLYPH&lt;g82&gt;GLYPH&lt;g85&gt;GLYPH&lt;g80&gt;GLYPH&lt;g68&gt;GLYPH&lt;g81&gt;GLYPH&lt;g70&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g73&gt;GLYPH&lt;g3&gt; GLYPH&lt;g92&gt;GLYPH&lt;g82&gt;GLYPH&lt;g88&gt;GLYPH&lt;g85&gt; GLYPH&lt;g3&gt; GLYPH&lt;g71&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g68&gt;GLYPH&lt;g69&gt;GLYPH&lt;g68&gt;GLYPH&lt;g86&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g83&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g76&gt;GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g86&gt;
- GLYPH&lt;g115&gt;GLYPH&lt;g3&gt; GLYPH&lt;g40&gt;GLYPH&lt;g68&gt;GLYPH&lt;g85&gt; GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g74&gt;GLYPH&lt;g85&gt;GLYPH&lt;g72&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g3&gt; GLYPH&lt;g85&gt;GLYPH&lt;g72&gt;GLYPH&lt;g87&gt;GLYPH&lt;g88&gt;GLYPH&lt;g85&gt; GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g44&gt;GLYPH&lt;g55&gt;GLYPH&lt;g3&gt; GLYPH&lt;g83&gt;GLYPH&lt;g85&gt;GLYPH&lt;g82&gt;GLYPH&lt;g77&gt;GLYPH&lt;g72&gt;GLYPH&lt;g70&gt;GLYPH&lt;g87&gt;GLYPH&lt;g86&gt; GLYPH&lt;g3&gt; GLYPH&lt;g87&gt;GLYPH&lt;g75&gt;GLYPH&lt;g85&gt;GLYPH&lt;g82&gt;GLYPH&lt;g88&gt;GLYPH&lt;g74&gt;GLYPH&lt;g75&gt;GLYPH&lt;g3&gt; GLYPH&lt;g80&gt;GLYPH&lt;g82&gt;GLYPH&lt;g71&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt; GLYPH&lt;g81&gt;GLYPH&lt;g76&gt;GLYPH&lt;g93&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g76&gt;GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g73&gt;GLYPH&lt;g3&gt; GLYPH&lt;g71&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g68&gt;GLYPH&lt;g69&gt;GLYPH&lt;g68&gt;GLYPH&lt;g86&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g68&gt;GLYPH&lt;g81&gt;GLYPH&lt;g71&gt; GLYPH&lt;g3&gt; GLYPH&lt;g68&gt;GLYPH&lt;g83&gt;GLYPH&lt;g83&gt;GLYPH&lt;g79&gt;GLYPH&lt;g76&gt;GLYPH&lt;g70&gt;GLYPH&lt;g68&gt;GLYPH&lt;g87&gt;GLYPH&lt;g76&gt;GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g86&gt;
- GLYPH&lt;g115&gt;GLYPH&lt;g3&gt; GLYPH&lt;g53&gt;GLYPH&lt;g72&gt;GLYPH&lt;g79&gt;GLYPH&lt;g92&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g44&gt;GLYPH&lt;g37&gt;GLYPH&lt;g48&gt;GLYPH&lt;g3&gt; GLYPH&lt;g72&gt;GLYPH&lt;g91&gt;GLYPH&lt;g83&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g87&gt;GLYPH&lt;g3&gt; GLYPH&lt;g70&gt;GLYPH&lt;g82&gt;GLYPH&lt;g81&gt;GLYPH&lt;g86&gt;GLYPH&lt;g88&gt;GLYPH&lt;g79&gt;GLYPH&lt;g87&gt;GLYPH&lt;g76&gt;GLYPH&lt;g81&gt;GLYPH&lt;g74&gt;GLYPH&lt;g15&gt;GLYPH&lt;g3&gt; GLYPH&lt;g86&gt;GLYPH&lt;g78&gt;GLYPH&lt;g76&gt;GLYPH&lt;g79&gt;GLYPH&lt;g79&gt;GLYPH&lt;g86&gt; GLYPH&lt;g3&gt; GLYPH&lt;g86&gt;GLYPH&lt;g75&gt;GLYPH&lt;g68&gt;GLYPH&lt;g85&gt;GLYPH&lt;g76&gt;GLYPH&lt;g81&gt;GLYPH&lt;g74&gt;GLYPH&lt;g3&gt; GLYPH&lt;g68&gt;GLYPH&lt;g81&gt;GLYPH&lt;g71&gt;GLYPH&lt;g3&gt; GLYPH&lt;g85&gt;GLYPH&lt;g72&gt;GLYPH&lt;g81&gt;GLYPH&lt;g82&gt;GLYPH&lt;g90&gt;GLYPH&lt;g81&gt;GLYPH&lt;g3&gt; GLYPH&lt;g86&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g89&gt;GLYPH&lt;g76&gt;GLYPH&lt;g70&gt;GLYPH&lt;g72&gt;GLYPH&lt;g86&gt;
- GLYPH&lt;g115&gt;GLYPH&lt;g3&gt; GLYPH&lt;g55&gt; GLYPH&lt;g68&gt;GLYPH&lt;g78&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g68&gt;GLYPH&lt;g71&gt;GLYPH&lt;g89&gt;GLYPH&lt;g68&gt;GLYPH&lt;g81&gt;GLYPH&lt;g87&gt;GLYPH&lt;g68&gt;GLYPH&lt;g74&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g73&gt;GLYPH&lt;g3&gt; GLYPH&lt;g68&gt;GLYPH&lt;g70&gt;GLYPH&lt;g70&gt;GLYPH&lt;g72&gt;GLYPH&lt;g86&gt;GLYPH&lt;g86&gt;GLYPH&lt;g3&gt; GLYPH&lt;g87&gt;GLYPH&lt;g82&gt;GLYPH&lt;g3&gt; GLYPH&lt;g68&gt; GLYPH&lt;g3&gt; GLYPH&lt;g90&gt;GLYPH&lt;g82&gt;GLYPH&lt;g85&gt;GLYPH&lt;g79&gt;GLYPH&lt;g71&gt;GLYPH&lt;g90&gt;GLYPH&lt;g76&gt;GLYPH&lt;g71&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g86&gt;GLYPH&lt;g82&gt;GLYPH&lt;g88&gt;GLYPH&lt;g85&gt;GLYPH&lt;g70&gt;GLYPH&lt;g72&gt;GLYPH&lt;g3&gt; GLYPH&lt;g82&gt;GLYPH&lt;g73&gt;GLYPH&lt;g3&gt; GLYPH&lt;g72&gt;GLYPH&lt;g91&gt;GLYPH&lt;g83&gt;GLYPH&lt;g72&gt;GLYPH&lt;g85&gt;GLYPH&lt;g87&gt;GLYPH&lt;g76&gt;GLYPH&lt;g86&gt;GLYPH&lt;g72&gt;

<!-- image -->

Power Services

## DB2 for i Center of Excellence

Expert help to achieve your business requirements

## We build confident, satisfied clients

No one else has the vast consulting experiences, skills sharing and renown service offerings to do what we can do for you.

Because no one else is IBM.

With combined experiences and direct access to development groups, we're the experts in IBM DB2® for i. The DB2 for i Center of Excellence (CoE) can help you achieve-perhaps reexamine and exceed-your business requirements and gain more confidence and satisfaction in IBM product data management products and solutions.

## Who we are, some of what we do

Global CoE engagements cover topics including:

- r Database performance and scalability
- r Advanced SQL knowledge and skills transfer
- r Business intelligence and analytics
- r DB2 Web Query
- r Query/400 modernization for better reporting and analysis capabilities
- r Database modernization and re-engineering
- r Data-centric architecture and design
- r Extremely large database and overcoming limits to growth
- r ISV education and enablement

## Preface

This IBMfi Redpaper™ publication provides information about the IBM i 7.2 feature of IBM DB2fi for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.

This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.

This paper was produced by the IBM DB2 for i Center of Excellence team in partnership with the International Technical Support Organization (ITSO), Rochester, Minnesota US.

<!-- image -->

Jim Bainbridge is a senior DB2 consultant on the DB2 for i Center of Excellence team in the IBM Lab Services and Training organization. His primary role is training and implementation services for IBM DB2 Web Query for i and business analytics. Jim began his career with IBM 30 years ago in the IBM Rochester Development Lab, where he developed cooperative processing products that paired IBM PCs with IBM S/36 and AS/.400 systems. In the years since, Jim has held numerous technical roles, including independent software vendors technical support on a broad range of IBM technologies and products, and supporting customers in the IBM Executive Briefing Center and IBM Project Office.

<!-- image -->

Hernando Bedoya is a Senior IT Specialist at STG Lab Services and Training in Rochester, Minnesota. He writes extensively and teaches IBM classes worldwide in all areas of DB2 for i. Before joining STG Lab Services, he worked in the ITSO for nine years writing multiple IBM Redbooksfi publications. He also worked for IBM Colombia as an IBM AS/400fi IT Specialist doing presales support for the Andean countries. He has 28 years of experience in the computing field and has taught database classes in Colombian universities. He holds a Master's degree in Computer Science from EAFIT, Colombia. His areas of expertise are database technology, performance, and data warehousing. Hernando can be contacted at hbedoya@us.ibm.com .

## Authors

<!-- image -->

1

Chapter 1.

## Securing and protecting IBM DB2 data

Recent news headlines are filled with reports of data breaches and cyber-attacks impacting global businesses of all sizes. The Identity Theft Resource Center$^{1}$ reports that almost 5000 data breaches have occurred since 2005, exposing over 600 million records of data. The financial cost of these data breaches is skyrocketing. Studies from the Ponemon Institute$^{2}$ revealed that the average cost of a data breach increased in 2013 by 15% globally and resulted in a brand equity loss of $9.4 million per attack. The average cost that is incurred for each lost record containing sensitive information increased more than 9% to $145 per record.

Businesses must make a serious effort to secure their data and recognize that securing information assets is a cost of doing business. In many parts of the world and in many industries, securing the data is required by law and subject to audits. Data security is no longer an option; it is a requirement.

This chapter describes how you can secure and protect data in DB2 for i. The following topics are covered in this chapter:

- GLYPH&lt;SM590000&gt; Security fundamentals
- GLYPH&lt;SM590000&gt; Current state of IBM i security
- GLYPH&lt;SM590000&gt; DB2 for i security controls

## 1.1 Security fundamentals

Before reviewing database security techniques, there are two fundamental steps in securing information assets that must be described:

- GLYPH&lt;SM590000&gt; First, and most important, is the definition of a company's security policy . Without a security policy, there is no definition of what are acceptable practices for using, accessing, and storing information by who, what, when, where, and how. A security policy should minimally address three things: confidentiality, integrity, and availability.
- The monitoring and assessment of adherence to the security policy determines whether your security strategy is working. Often, IBM security consultants are asked to perform security assessments for companies without regard to the security policy. Although these assessments can be useful for observing how the system is defined and how data is being accessed, they cannot determine the level of security without a security policy. Without a security policy, it really is not an assessment as much as it is a baseline for monitoring the changes in the security settings that are captured.

A security policy is what defines whether the system and its settings are secure (or not).

- GLYPH&lt;SM590000&gt; The second fundamental in securing data assets is the use of resource security . If implemented properly, resource security prevents data breaches from both internal and external intrusions. Resource security controls are closely tied to the part of the security policy that defines who should have access to what information resources. A hacker might be good enough to get through your company firewalls and sift his way through to your system, but if they do not have explicit access to your database, the hacker cannot compromise your information assets.

With your eyes now open to the importance of securing information assets, the rest of this chapter reviews the methods that are available for securing database resources on IBM i.

## 1.2 Current state of IBM i security

Because of the inherently secure nature of IBM i, many clients rely on the default system settings to protect their business data that is stored in DB2 for i. In most cases, this means no data protection because the default setting for the Create default public authority (QCRTAUT) system value is *CHANGE.

Even more disturbing is that many IBM i clients remain in this state, despite the news headlines and the significant costs that are involved with databases being compromised. This default security configuration makes it quite challenging to implement basic security policies. A tighter implementation is required if you really want to protect one of your company's most valuable assets, which is the data.

Traditionally, IBM i applications have employed menu-based security to counteract this default configuration that gives all users access to the data. The theory is that data is protected by the menu options controlling what database operations that the user can perform. This approach is ineffective, even if the user profile is restricted from running interactive commands. The reason is that in today's connected world there are a multitude of interfaces into the system, from web browsers to PC clients, that bypass application menus. If there are no object-level controls, users of these newer interfaces have an open door to your data.

Many businesses are trying to limit data access to a need-to-know basis. This security goal means that users should be given access only to the minimum set of data that is required to perform their job. Often, users with object-level access are given access to row and column values that are beyond what their business task requires because that object-level security provides an all-or-nothing solution. For example, object-level controls allow a manager to access data about all employees. Most security policies limit a manager to accessing data only for the employees that they manage.

## 1.3.1 Existing row and column control

Some IBM i clients have tried augmenting the all-or-nothing object-level security with SQL views (or logical files) and application logic, as shown in Figure 1-2. However, application-based logic is easy to bypass with all of the different data access interfaces that are provided by the IBM i operating system, such as Open Database Connectivity (ODBC) and System i Navigator.

Using SQL views to limit access to a subset of the data in a table also has its own set of challenges. First, there is the complexity of managing all of the SQL view objects that are used for securing data access. Second, scaling a view-based security solution can be difficult as the amount of data grows and the number of users increases.

Even if you are willing to live with these performance and management issues, a user with *ALLOBJ access still can directly access all of the data in the underlying DB2 table and easily bypass the security controls that are built into an SQL view.

Figure 1-2 Existing row and column controls

<!-- image -->

## 2.1.6 Change Function Usage CL command

The following CL commands can be used to work with, display, or change function usage IDs:

- GLYPH&lt;SM590000&gt; Work Function Usage ( WRKFCNUSG )
- GLYPH&lt;SM590000&gt; Change Function Usage ( CHGFCNUSG )
- GLYPH&lt;SM590000&gt; Display Function Usage ( DSPFCNUSG )

For example, the following CHGFCNUSG command shows granting authorization to user HBEDOYA to administer and manage RCAC rules:

CHGFCNUSG FCNID(QIBM\_DB\_SECADM) USER(HBEDOYA) USAGE(*ALLOWED)

## 2.1.7 Verifying function usage IDs for RCAC with the FUNCTION\_USAGE view

The FUNCTION\_USAGE view contains function usage configuration details. Table 2-1 describes the columns in the FUNCTION\_USAGE view.

Table 2-1 FUNCTION\_USAGE view

| Column name   | Data type   | Description                                                                                                                                                           |
|---------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| FUNCTION\_ID   | VARCHAR(30) | ID of the function.                                                                                                                                                   |
| USER\_NAME     | VARCHAR(10) | Name of the user profile that has a usage setting for this  function.                                                                                                 |
| USAGE         | VARCHAR(7)  | Usage setting: GLYPH&lt;SM590000&gt; ALLOWED: The user profile is allowed to use the function. GLYPH&lt;SM590000&gt; DENIED: The user profile is not allowed to use the function. |
| USER\_TYPE     | VARCHAR(5)  | Type of user profile: GLYPH&lt;SM590000&gt; USER: The user profile is a user. GLYPH&lt;SM590000&gt; GROUP: The user profile is a group.                                           |

To discover who has authorization to define and manage RCAC, you can use the query that is shown in Example 2-1.

SELECT

function\_id,

user\_name,

usage,

user\_type

FROM

function\_usage

WHERE

function\_id=’QIBM\_DB\_SECADM’

ORDER BY

user\_name;

## 2.2 Separation of duties

Separation of duties helps businesses comply with industry regulations or organizational requirements and simplifies the management of authorities. Separation of duties is commonly used to prevent fraudulent activities or errors by a single person. It provides the ability for administrative functions to be divided across individuals without overlapping responsibilities, so that one user does not possess unlimited authority, such as with the *ALLOBJ authority.

For example, assume that a business has assigned the duty to manage security on IBM i to Theresa. Before release IBM i 7.2, to grant privileges, Theresa had to have the same privileges Theresa was granting to others. Therefore, to grant *USE privileges to the PAYROLL table, Theresa had to have *OBJMGT and *USE authority (or a higher level of authority, such as *ALLOBJ). This requirement allowed Theresa to access the data in the PAYROLL table even though Theresa's job description was only to manage its security.

In IBM i 7.2, the QIBM\_DB\_SECADM function usage grants authorities, revokes authorities, changes ownership, or changes the primary group without giving access to the object or, in the case of a database table, to the data that is in the table or allowing other operations on the table.

QIBM\_DB\_SECADM function usage can be granted only by a user with *SECADM special authority and can be given to a user or a group.

QIBM\_DB\_SECADM also is responsible for administering RCAC, which restricts which rows a user is allowed to access in a table and whether a user is allowed to see information in certain columns of a table.

A preferred practice is that the RCAC administrator has the QIBM\_DB\_SECADM function usage ID, but absolutely no other data privileges. The result is that the RCAC administrator can deploy and maintain the RCAC constructs, but cannot grant themselves unauthorized access to data itself.

Table 2-2 shows a comparison of the different function usage IDs and *JOBCTL authority to the different CL commands and DB2 for i tools.

Table 2-2 Comparison of the different function usage IDs and *JOBCTL authority

| User action                                                                    | *JOBCTL   | QIBM\_DB\_SECADM   | QIBM\_DB\_SQLADM   | QIBM\_DB\_SYSMON   | No Authority   |
|--------------------------------------------------------------------------------|-----------|------------------|------------------|------------------|----------------|
| SET CURRENT DEGREE  (SQL statement)                                            | X         |                  | X                |                  |                |
| CHGQRYA  command targeting a different user’s job                              | X         |                  | X                |                  |                |
| STRDBMON  or  ENDDBMON  commands targeting a different user’s job              | X         |                  | X                |                  |                |
| STRDBMON  or  ENDDBMON  commands targeting a job that matches the current user | X         |                  | X                | X                | X              |
| QUSRJOBI() API format 900 or System i Navigator’s SQL Details for Job          | X         |                  | X                | X                |                |
| Visual Explain within Run SQL scripts                                          | X         |                  | X                | X                | X              |
| Visual Explain outside of Run SQL scripts                                      | X         |                  | X                |                  |                |
| ANALYZE PLAN CACHE procedure                                                   | X         |                  | X                |                  |                |
| DUMP PLAN CACHE procedure                                                      | X         |                  | X                |                  |                |
| MODIFY PLAN CACHE procedure                                                    | X         |                  | X                |                  |                |
| MODIFY PLAN CACHE PROPERTIES procedure (currently does not check authority)    | X         |                  | X                |                  |                |
| CHANGE PLAN CACHE SIZE procedure (currently does not check authority)          | X         |                  | X                |                  |                |

Figure 3-1 CREATE PERMISSION SQL statement

<!-- image -->

## Column mask

A column mask is a database object that manifests a column value access control rule for a specific column in a specific table. It uses a CASE expression that describes what you see when you access the column. For example, a teller can see only the last four digits of a tax identification number.

Table 3-1 Special registers and their corresponding values

| Special register     | Corresponding value                                                                                                                   |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------|
| USER or SESSION\_USER | The effective user of the thread excluding adopted authority.                                                                         |
| CURRENT\_USER         | The effective user of the thread including adopted authority. When no adopted  authority is present, this has the same value as USER. |
| SYSTEM\_USER          | The authorization ID that initiated the connection.                                                                                   |

Figure 3-5 shows the difference in the special register values when an adopted authority is used:

- GLYPH&lt;SM590000&gt; A user connects to the server using the user profile ALICE.
- GLYPH&lt;SM590000&gt; USER and CURRENT USER initially have the same value of ALICE.
- GLYPH&lt;SM590000&gt; ALICE calls an SQL procedure that is named proc1, which is owned by user profile JOE and was created to adopt JOE's authority when it is called.
- GLYPH&lt;SM590000&gt; While the procedure is running, the special register USER still contains the value of ALICE because it excludes any adopted authority. The special register CURRENT USER contains the value of JOE because it includes any adopted authority.
- GLYPH&lt;SM590000&gt; When proc1 ends, the session reverts to its original state with both USER and CURRENT USER having the value of ALICE.

Figure 3-5 Special registers and adopted authority

<!-- image -->

## 3.2.2 Built-in global variables

Built-in global variables are provided with the database manager and are used in SQL statements to retrieve scalar values that are associated with the variables.

IBM DB2 for i supports nine different built-in global variables that are read only and maintained by the system. These global variables can be used to identify attributes of the database connection and used as part of the RCAC logic.

Table 3-2 lists the nine built-in global variables.

Table 3-2 Built-in global variables

| Global variable       | Type         | Description                                                    |
|-----------------------|--------------|----------------------------------------------------------------|
| CLIENT\_HOST           | VARCHAR(255) | Host name of the current client as returned by the system      |
| CLIENT\_IPADDR         | VARCHAR(128) | IP address of the current client as returned by the system     |
| CLIENT\_PORT           | INTEGER      | Port used by the current client to communicate with the server |
| PACKAGE\_NAME          | VARCHAR(128) | Name of the currently running package                          |
| PACKAGE\_SCHEMA        | VARCHAR(128) | Schema name of the currently running package                   |
| PACKAGE\_VERSION       | VARCHAR(64)  | Version identifier of the currently running package            |
| ROUTINE\_SCHEMA        | VARCHAR(128) | Schema name of the currently running routine                   |
| ROUTINE\_SPECIFIC\_NAME | VARCHAR(128) | Name of the currently running routine                          |
| ROUTINE\_TYPE          | CHAR(1)      | Type of the currently running routine                          |

## 3.3 VERIFY\_GROUP\_FOR\_USER function

The VERIFY\_GROUP\_FOR\_USER function was added in IBM i 7.2. Although it is primarily intended for use with RCAC permissions and masks, it can be used in other SQL statements. The first parameter must be one of these three special registers: SESSION\_USER, USER, or CURRENT\_USER. The second and subsequent parameters are a list of user or group profiles. Each of these values must be 1 - 10 characters in length. These values are not validated for their existence, which means that you can specify the names of user profiles that do not exist without receiving any kind of error.

If a special register value is in the list of user profiles or it is a member of a group profile included in the list, the function returns a long integer value of 1. Otherwise, it returns a value of 0. It never returns the null value.

Here is an example of using the VERIFY\_GROUP\_FOR\_USER function:

- 1. There are user profiles for MGR, JANE, JUDY, and TONY.
- 2. The user profile JANE specifies a group profile of MGR.
- 3. If a user is connected to the server using user profile JANE, all of the following function invocations return a value of 1:

```
VERIFY_GROUP_FOR_USER (CURRENT_USER, 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR') VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JANE', 'MGR', 'STEVE') The following function invocation returns a value of 0: VERIFY_GROUP_FOR_USER (CURRENT_USER, 'JUDY', 'TONY')
```

RETURN

CASE

```
WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR', 'EMP' ) = 1 THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . DATE_OF_BIRTH WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 9999 || '-' || MONTH ( EMPLOYEES . DATE_OF_BIRTH ) || '-' || DAY (EMPLOYEES.DATE_OF_BIRTH )) ELSE NULL END ENABLE ;
```

- 2. The other column to mask in this example is the TAX\_ID information. In this example, the rules to enforce include the following ones:
- -Human Resources can see the unmasked TAX\_ID of the employees.
- -Employees can see only their own unmasked TAX\_ID.
- -Managers see a masked version of TAX\_ID with the first five characters replaced with the X character (for example, XXX-XX-1234).
- -Any other person sees the entire TAX\_ID as masked, for example, XXX-XX-XXXX.
- To implement this column mask, run the SQL statement that is shown in Example 3-9.

```
CREATE MASK HR_SCHEMA.MASK_TAX_ID_ON_EMPLOYEES ON HR_SCHEMA.EMPLOYEES AS EMPLOYEES FOR COLUMN TAX_ID RETURN CASE WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'HR' ) = 1 THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER = EMPLOYEES . USER_ID THEN EMPLOYEES . TAX_ID WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'MGR' ) = 1 AND SESSION_USER <> EMPLOYEES . USER_ID THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( EMPLOYEES . TAX_ID , 8 , 4 ) ) WHEN VERIFY_GROUP_FOR_USER ( SESSION_USER , 'EMP' ) = 1 THEN EMPLOYEES . TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ;
```

- 3. Figure 3-10 shows the masks that are created in the HR\_SCHEMA.

Figure 3-10 Column masks shown in System i Navigator

<!-- image -->

## 3.6.6 Activating RCAC

Now that you have created the row permission and the two column masks, RCAC must be activated. The row permission and the two column masks are enabled (last clause in the scripts), but now you must activate RCAC on the table. To do so, complete the following steps:

- 1. Run the SQL statements that are shown in Example 3-10.

## Example 3-10 Activating RCAC on the EMPLOYEES table

- /* Active Row Access Control (permissions) */
- /* Active Column Access Control (masks)

*/

ALTER TABLE HR\_SCHEMA.EMPLOYEES

ACTIVATE ROW ACCESS CONTROL

ACTIVATE COLUMN ACCESS CONTROL;

- 2. Look at the definition of the EMPLOYEE table, as shown in Figure 3-11. To do this, from the main navigation pane of System i Navigator, click Schemas  HR\_SCHEMA  Tables , right-click the EMPLOYEES table, and click Definition .

Figure 3-11 Selecting the EMPLOYEES table from System i Navigator

<!-- image -->

- 2. Figure 4-68 shows the Visual Explain of the same SQL statement, but with RCAC enabled. It is clear that the implementation of the SQL statement is more complex because the row permission rule becomes part of the WHERE clause.
- 3. Compare the advised indexes that are provided by the Optimizer without RCAC and with RCAC enabled. Figure 4-69 shows the index advice for the SQL statement without RCAC enabled. The index being advised is for the ORDER BY clause.

Figure 4-68 Visual Explain with RCAC enabled

<!-- image -->

Figure 4-69 Index advice with no RCAC

<!-- image -->

```
THEN C . CUSTOMER_TAX_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN ( 'XXX-XX-' CONCAT QSYS2 . SUBSTR ( C . CUSTOMER_TAX_ID , 8 , 4 ) ) WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_TAX_ID ELSE 'XXX-XX-XXXX' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_DRIVERS_LICENSE_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_DRIVERS_LICENSE_NUMBER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'TELLER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_DRIVERS_LICENSE_NUMBER ELSE '*************' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_LOGIN_ID_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_LOGIN_ID RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_LOGIN_ID WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_LOGIN_ID ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION ELSE '*****' END ENABLE ; CREATE MASK BANK_SCHEMA.MASK_SECURITY_QUESTION_ANSWER_ON_CUSTOMERS ON BANK_SCHEMA.CUSTOMERS AS C FOR COLUMN CUSTOMER_SECURITY_QUESTION_ANSWER RETURN CASE WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'ADMIN' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER WHEN QSYS2 . VERIFY_GROUP_FOR_USER ( SESSION_USER , 'CUSTOMER' ) = 1 THEN C . CUSTOMER_SECURITY_QUESTION_ANSWER ELSE '*****' END ENABLE ; ALTER TABLE BANK_SCHEMA.CUSTOMERS ACTIVATE ROW ACCESS CONTROL ACTIVATE COLUMN ACCESS CONTROL ;
```

Back cover

## Row and Column Access Control Support in IBM DB2 for i

Implement roles and separation of duties

Leverage row permissions on the database

This IBM Redpaper publication provides information about the IBM i 7.2 feature of IBM DB2 for i Row and Column Access Control (RCAC). It offers a broad description of the function and advantages of controlling access to data in a comprehensive and transparent way. This publication helps you understand the capabilities of RCAC and provides examples of defining, creating, and implementing the row permissions and column masks in a relational database environment.

Protect columns by defining column masks

This paper is intended for database engineers, data-centric application developers, and security officers who want to design and implement RCAC as a part of their data control and governance policy. A solid background in IBM i object level security, DB2 for i relational database concepts, and SQL is assumed.

<!-- image -->

<!-- image -->

INTERNATIONAL TECHNICAL SUPPORT ORGANIZATION

BUILDING TECHNICAL INFORMATION BASED ON PRACTICAL EXPERIENCE

IBM Redbooks are developed by the IBM International Technical Support Organization. Experts from IBM, Customers and Partners from around the world create timely technical information based on realistic scenarios. Specific recommendations are provided to help you implement IT solutions more effectively in your environment.

For more information: ibm.com /redbooks

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_01.doctags.txt
================================================
<doctag><section_header_level_1><loc_183><loc_46><loc_426><loc_55>Pythonو R ةغلب ةجمربلا للاخ نم تلاكشملا لحو ةيجاتنلإا نيسحت</section_header_level_1>
<text><loc_74><loc_64><loc_427><loc_99>Python و R ةغلب ةجمربلا ربتعت ةلاعف لولح داجيإ يف دعاستو ةيجاتنلإا ززعت نأ نكمي يتلا ةيوقلا تاودلأا نم ءاملعلاو نيللحملا ىلع لهسي امم ،تانايبلا ليلحتل ةيلاثم اهلعجت ةديرف تازيمPython و R نم لك كلتمي .تلاكشملل ناك اذإ .ةلاعفو ةعيرس ةقيرطب ةدقعم تلايلحت ءارجإ مهسي نأ نكمي تاغللا هذه مادختسا نإف ،ةيليلحت ةيلقع كيدل .لمعلا جئاتن نيسحت يف ريبك لكشب</text>
<text><loc_170><loc_126><loc_170><loc_134>ً</text>
<text><loc_416><loc_135><loc_416><loc_143>ً</text>
<text><loc_82><loc_108><loc_427><loc_143>جارختساو تانايبلا نم ةلئاه تايمك ةجلاعم نكمملا نم حبصي ،ةجمربلا تاراهم عم يليلحتلا ريكفتلا عمتجي امدنع ذيفنتلPython و R مادختسا نيجمربملل نكمي .اهنم تاهجوتلاو طامنلأا ةجذمنلا لثم ،ةمدقتم ةيليلحت تايلمع ةقد رثكأ تارارق ذاختا ىلإ ا ضيأ يدؤي نأ نكمي لب ،تقولا رفوي طقف سيل اذه .ةريبكلا تانايبلا ليلحتو ةيئاصحلإا تانايبلا ىلع ةمئاق تاجاتنتسا ىلع ءانب .</text>
<text><loc_76><loc_152><loc_427><loc_186>ليلحتلا نم ،تاقيبطتلا نم ةعساو ةعومجم معدت ةينغ تاودأو تابتكمPython و R نم لك رفوت ،كلذ ىلع ةولاع ىلع .ةفلتخملا تلاكشملل ةركتبم لولح ريوطتل تابتكملا هذه نم ةدافتسلاا نيمدختسملل نكمي .يللآا ملعتلا ىلإ ينايبلا R رفوت امنيب ،ةءافكب تانايبلا ةرادلإ Python يف pandas ةبتكم مادختسا نكمي ،لاثملا ليبس مسرلل ةيوق تاودأ .نيللحملاو نيثحابلل ةيلاثم اهلعجي امم ،يئاصحلإا ليلحتلاو ينايبلا</text>
<text><loc_79><loc_195><loc_427><loc_221>Python و R ةغلب ةجمربلا يدؤت نأ نكمي ،ةياهنلا يف ةركتبم لولح ريفوتو ةيجاتنلإا نيسحت ىلإ ةيليلحت ةيلقع عم اهل نوكت نأ نكمي ةبسانملا ةيجمربلا بيلاسلأا قيبطتو لاعف لكشب تانايبلا ليلحت ىلع ةردقلا نإ .ةدقعملا تلاكشملل .ينهملاو يصخشلا ءادلأا ىلع ىدملا ةديعب ةيباجيإ تاريثأت</text>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_01.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "right_to_left_01", "origin": {"mimetype": "application/pdf", "binary_hash": 11705364559529254676, "filename": "right_to_left_01.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/texts/2"}, {"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/texts/5"}, {"cref": "#/texts/6"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 223.85000999999997, "t": 719.4619800000002, "r": 521.98181, "b": 704.4510500000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 59]}], "orig": "Python\u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0644\u0644\u0627\u062e \u0646\u0645 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0627 \u0644\u062d\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a", "text": "Python\u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0644\u0644\u0627\u062e \u0646\u0645 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0627 \u0644\u062d\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a", "level": 1}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 90.744003, "t": 689.992, "r": 522.19, "b": 635.30804, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 345]}], "orig": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0631\u0628\u062a\u0639\u062a \u0629\u0644\u0627\u0639\u0641 \u0644\u0648\u0644\u062d \u062f\u0627\u062c\u064a\u0625 \u064a\u0641 \u062f\u0639\u0627\u0633\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0632\u0632\u0639\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u064a\u062a\u0644\u0627 \u0629\u064a\u0648\u0642\u0644\u0627 \u062a\u0627\u0648\u062f\u0644\u0623\u0627 \u0646\u0645 \u0621\u0627\u0645\u0644\u0639\u0644\u0627\u0648 \u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627 \u0649\u0644\u0639 \u0644\u0647\u0633\u064a \u0627\u0645\u0645 \u060c\u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u062a \u0629\u062f\u064a\u0631\u0641 \u062a\u0627\u0632\u064a\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0643\u0644\u062a\u0645\u064a .\u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0646\u0627\u0643 \u0627\u0630\u0625 .\u0629\u0644\u0627\u0639\u0641\u0648 \u0629\u0639\u064a\u0631\u0633 \u0629\u0642\u064a\u0631\u0637\u0628 \u0629\u062f\u0642\u0639\u0645 \u062a\u0644\u0627\u064a\u0644\u062d\u062a \u0621\u0627\u0631\u062c\u0625 \u0645\u0647\u0633\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u062a\u0627\u063a\u0644\u0644\u0627 \u0647\u0630\u0647 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0625\u0641 \u060c\u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0643\u064a\u062f\u0644 .\u0644\u0645\u0639\u0644\u0627 \u062c\u0626\u0627\u062a\u0646 \u0646\u064a\u0633\u062d\u062a \u064a\u0641 \u0631\u064a\u0628\u0643 \u0644\u0643\u0634\u0628", "text": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u0631\u0628\u062a\u0639\u062a \u0629\u0644\u0627\u0639\u0641 \u0644\u0648\u0644\u062d \u062f\u0627\u062c\u064a\u0625 \u064a\u0641 \u062f\u0639\u0627\u0633\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0632\u0632\u0639\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u064a\u062a\u0644\u0627 \u0629\u064a\u0648\u0642\u0644\u0627 \u062a\u0627\u0648\u062f\u0644\u0623\u0627 \u0646\u0645 \u0621\u0627\u0645\u0644\u0639\u0644\u0627\u0648 \u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627 \u0649\u0644\u0639 \u0644\u0647\u0633\u064a \u0627\u0645\u0645 \u060c\u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u062a \u0629\u062f\u064a\u0631\u0641 \u062a\u0627\u0632\u064a\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0643\u0644\u062a\u0645\u064a .\u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0646\u0627\u0643 \u0627\u0630\u0625 .\u0629\u0644\u0627\u0639\u0641\u0648 \u0629\u0639\u064a\u0631\u0633 \u0629\u0642\u064a\u0631\u0637\u0628 \u0629\u062f\u0642\u0639\u0645 \u062a\u0644\u0627\u064a\u0644\u062d\u062a \u0621\u0627\u0631\u062c\u0625 \u0645\u0647\u0633\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u062a\u0627\u063a\u0644\u0644\u0627 \u0647\u0630\u0647 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0625\u0641 \u060c\u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0643\u064a\u062f\u0644 .\u0644\u0645\u0639\u0644\u0627 \u062c\u0626\u0627\u062a\u0646 \u0646\u064a\u0633\u062d\u062a \u064a\u0641 \u0631\u064a\u0628\u0643 \u0644\u0643\u0634\u0628"}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 208.10402, "t": 592.67206, "r": 208.10402, "b": 579.38806, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "\u064b", "text": "\u064b"}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 509.34990999999997, "t": 578.03198, "r": 509.34990999999997, "b": 564.74799, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "\u064b", "text": "\u064b"}, {"self_ref": "#/texts/4", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 99.863998, "t": 620.75201, "r": 522.23792, "b": 566.06799, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 348]}], "orig": "\u062c\u0627\u0631\u062e\u062a\u0633\u0627\u0648 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0646\u0645 \u0629\u0644\u0626\u0627\u0647 \u062a\u0627\u064a\u0645\u0643 \u0629\u062c\u0644\u0627\u0639\u0645 \u0646\u0643\u0645\u0645\u0644\u0627 \u0646\u0645 \u062d\u0628\u0635\u064a \u060c\u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u062a\u0627\u0631\u0627\u0647\u0645 \u0639\u0645 \u064a\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0631\u064a\u0643\u0641\u062a\u0644\u0627 \u0639\u0645\u062a\u062c\u064a \u0627\u0645\u062f\u0646\u0639 \u0630\u064a\u0641\u0646\u062a\u0644Python \u0648 R \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u064a\u062c\u0645\u0631\u0628\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u0627\u0647\u0646\u0645 \u062a\u0627\u0647\u062c\u0648\u062a\u0644\u0627\u0648 \u0637\u0627\u0645\u0646\u0644\u0623\u0627 \u0629\u062c\u0630\u0645\u0646\u0644\u0627 \u0644\u062b\u0645 \u060c\u0629\u0645\u062f\u0642\u062a\u0645 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u062a\u0627\u064a\u0644\u0645\u0639 \u0629\u0642\u062f \u0631\u062b\u0643\u0623 \u062a\u0627\u0631\u0627\u0631\u0642 \u0630\u0627\u062e\u062a\u0627 \u0649\u0644\u0625 \u0627 \u0636\u064a\u0623 \u064a\u062f\u0624\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u0644\u0628 \u060c\u062a\u0642\u0648\u0644\u0627 \u0631\u0641\u0648\u064a \u0637\u0642\u0641 \u0633\u064a\u0644 \u0627\u0630\u0647 .\u0629\u0631\u064a\u0628\u0643\u0644\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0648 \u0629\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0649\u0644\u0639 \u0629\u0645\u0626\u0627\u0642 \u062a\u0627\u062c\u0627\u062a\u0646\u062a\u0633\u0627 \u0649\u0644\u0639 \u0621\u0627\u0646\u0628 .", "text": "\u062c\u0627\u0631\u062e\u062a\u0633\u0627\u0648 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0646\u0645 \u0629\u0644\u0626\u0627\u0647 \u062a\u0627\u064a\u0645\u0643 \u0629\u062c\u0644\u0627\u0639\u0645 \u0646\u0643\u0645\u0645\u0644\u0627 \u0646\u0645 \u062d\u0628\u0635\u064a \u060c\u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u062a\u0627\u0631\u0627\u0647\u0645 \u0639\u0645 \u064a\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0631\u064a\u0643\u0641\u062a\u0644\u0627 \u0639\u0645\u062a\u062c\u064a \u0627\u0645\u062f\u0646\u0639 \u0630\u064a\u0641\u0646\u062a\u0644Python \u0648 R \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u064a\u062c\u0645\u0631\u0628\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u0627\u0647\u0646\u0645 \u062a\u0627\u0647\u062c\u0648\u062a\u0644\u0627\u0648 \u0637\u0627\u0645\u0646\u0644\u0623\u0627 \u0629\u062c\u0630\u0645\u0646\u0644\u0627 \u0644\u062b\u0645 \u060c\u0629\u0645\u062f\u0642\u062a\u0645 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u062a\u0627\u064a\u0644\u0645\u0639 \u0629\u0642\u062f \u0631\u062b\u0643\u0623 \u062a\u0627\u0631\u0627\u0631\u0642 \u0630\u0627\u062e\u062a\u0627 \u0649\u0644\u0625 \u0627 \u0636\u064a\u0623 \u064a\u062f\u0624\u064a \u0646\u0623 \u0646\u0643\u0645\u064a \u0644\u0628 \u060c\u062a\u0642\u0648\u0644\u0627 \u0631\u0641\u0648\u064a \u0637\u0642\u0641 \u0633\u064a\u0644 \u0627\u0630\u0647 .\u0629\u0631\u064a\u0628\u0643\u0644\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a\u0648 \u0629\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0649\u0644\u0639 \u0629\u0645\u0626\u0627\u0642 \u062a\u0627\u062c\u0627\u062a\u0646\u062a\u0633\u0627 \u0649\u0644\u0639 \u0621\u0627\u0646\u0628 ."}, {"self_ref": "#/texts/5", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 92.903999, "t": 551.63202, "r": 522.10596, "b": 496.91799999999995, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 375]}], "orig": "\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0646\u0645 \u060c\u062a\u0627\u0642\u064a\u0628\u0637\u062a\u0644\u0627 \u0646\u0645 \u0629\u0639\u0633\u0627\u0648 \u0629\u0639\u0648\u0645\u062c\u0645 \u0645\u0639\u062f\u062a \u0629\u064a\u0646\u063a \u062a\u0627\u0648\u062f\u0623\u0648 \u062a\u0627\u0628\u062a\u0643\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0631\u0641\u0648\u062a \u060c\u0643\u0644\u0630 \u0649\u0644\u0639 \u0629\u0648\u0644\u0627\u0639 \u0649\u0644\u0639 .\u0629\u0641\u0644\u062a\u062e\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0648\u0637\u062a\u0644 \u062a\u0627\u0628\u062a\u0643\u0645\u0644\u0627 \u0647\u0630\u0647 \u0646\u0645 \u0629\u062f\u0627\u0641\u062a\u0633\u0644\u0627\u0627 \u0646\u064a\u0645\u062f\u062e\u062a\u0633\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u064a\u0644\u0644\u0622\u0627 \u0645\u0644\u0639\u062a\u0644\u0627 \u0649\u0644\u0625 \u064a\u0646\u0627\u064a\u0628\u0644\u0627 R \u0631\u0641\u0648\u062a \u0627\u0645\u0646\u064a\u0628 \u060c\u0629\u0621\u0627\u0641\u0643\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0629\u0631\u0627\u062f\u0644\u0625 Python \u064a\u0641 pandas \u0629\u0628\u062a\u0643\u0645 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0643\u0645\u064a \u060c\u0644\u0627\u062b\u0645\u0644\u0627 \u0644\u064a\u0628\u0633 \u0645\u0633\u0631\u0644\u0644 \u0629\u064a\u0648\u0642 \u062a\u0627\u0648\u062f\u0623 .\u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627\u0648 \u0646\u064a\u062b\u062d\u0627\u0628\u0644\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u064a \u0627\u0645\u0645 \u060c\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u0644\u064a\u0644\u062d\u062a\u0644\u0627\u0648 \u064a\u0646\u0627\u064a\u0628\u0644\u0627", "text": "\u0644\u064a\u0644\u062d\u062a\u0644\u0627 \u0646\u0645 \u060c\u062a\u0627\u0642\u064a\u0628\u0637\u062a\u0644\u0627 \u0646\u0645 \u0629\u0639\u0633\u0627\u0648 \u0629\u0639\u0648\u0645\u062c\u0645 \u0645\u0639\u062f\u062a \u0629\u064a\u0646\u063a \u062a\u0627\u0648\u062f\u0623\u0648 \u062a\u0627\u0628\u062a\u0643\u0645Python \u0648 R \u0646\u0645 \u0644\u0643 \u0631\u0641\u0648\u062a \u060c\u0643\u0644\u0630 \u0649\u0644\u0639 \u0629\u0648\u0644\u0627\u0639 \u0649\u0644\u0639 .\u0629\u0641\u0644\u062a\u062e\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0648\u0637\u062a\u0644 \u062a\u0627\u0628\u062a\u0643\u0645\u0644\u0627 \u0647\u0630\u0647 \u0646\u0645 \u0629\u062f\u0627\u0641\u062a\u0633\u0644\u0627\u0627 \u0646\u064a\u0645\u062f\u062e\u062a\u0633\u0645\u0644\u0644 \u0646\u0643\u0645\u064a .\u064a\u0644\u0644\u0622\u0627 \u0645\u0644\u0639\u062a\u0644\u0627 \u0649\u0644\u0625 \u064a\u0646\u0627\u064a\u0628\u0644\u0627 R \u0631\u0641\u0648\u062a \u0627\u0645\u0646\u064a\u0628 \u060c\u0629\u0621\u0627\u0641\u0643\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0629\u0631\u0627\u062f\u0644\u0625 Python \u064a\u0641 pandas \u0629\u0628\u062a\u0643\u0645 \u0645\u0627\u062f\u062e\u062a\u0633\u0627 \u0646\u0643\u0645\u064a \u060c\u0644\u0627\u062b\u0645\u0644\u0627 \u0644\u064a\u0628\u0633 \u0645\u0633\u0631\u0644\u0644 \u0629\u064a\u0648\u0642 \u062a\u0627\u0648\u062f\u0623 .\u0646\u064a\u0644\u0644\u062d\u0645\u0644\u0627\u0648 \u0646\u064a\u062b\u062d\u0627\u0628\u0644\u0644 \u0629\u064a\u0644\u0627\u062b\u0645 \u0627\u0647\u0644\u0639\u062c\u064a \u0627\u0645\u0645 \u060c\u064a\u0626\u0627\u0635\u062d\u0644\u0625\u0627 \u0644\u064a\u0644\u062d\u062a\u0644\u0627\u0648 \u064a\u0646\u0627\u064a\u0628\u0644\u0627"}, {"self_ref": "#/texts/6", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 96.863998, "t": 482.362, "r": 522.07404, "b": 441.478, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 267]}], "orig": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u064a\u062f\u0624\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u060c\u0629\u064a\u0627\u0647\u0646\u0644\u0627 \u064a\u0641 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0641\u0648\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a \u0649\u0644\u0625 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0639\u0645 \u0627\u0647\u0644 \u0646\u0648\u0643\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u0629\u0628\u0633\u0627\u0646\u0645\u0644\u0627 \u0629\u064a\u062c\u0645\u0631\u0628\u0644\u0627 \u0628\u064a\u0644\u0627\u0633\u0644\u0623\u0627 \u0642\u064a\u0628\u0637\u062a\u0648 \u0644\u0627\u0639\u0641 \u0644\u0643\u0634\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a \u0649\u0644\u0639 \u0629\u0631\u062f\u0642\u0644\u0627 \u0646\u0625 .\u0629\u062f\u0642\u0639\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 .\u064a\u0646\u0647\u0645\u0644\u0627\u0648 \u064a\u0635\u062e\u0634\u0644\u0627 \u0621\u0627\u062f\u0644\u0623\u0627 \u0649\u0644\u0639 \u0649\u062f\u0645\u0644\u0627 \u0629\u062f\u064a\u0639\u0628 \u0629\u064a\u0628\u0627\u062c\u064a\u0625 \u062a\u0627\u0631\u064a\u062b\u0623\u062a", "text": "Python \u0648 R \u0629\u063a\u0644\u0628 \u0629\u062c\u0645\u0631\u0628\u0644\u0627 \u064a\u062f\u0624\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u060c\u0629\u064a\u0627\u0647\u0646\u0644\u0627 \u064a\u0641 \u0629\u0631\u0643\u062a\u0628\u0645 \u0644\u0648\u0644\u062d \u0631\u064a\u0641\u0648\u062a\u0648 \u0629\u064a\u062c\u0627\u062a\u0646\u0644\u0625\u0627 \u0646\u064a\u0633\u062d\u062a \u0649\u0644\u0625 \u0629\u064a\u0644\u064a\u0644\u062d\u062a \u0629\u064a\u0644\u0642\u0639 \u0639\u0645 \u0627\u0647\u0644 \u0646\u0648\u0643\u062a \u0646\u0623 \u0646\u0643\u0645\u064a \u0629\u0628\u0633\u0627\u0646\u0645\u0644\u0627 \u0629\u064a\u062c\u0645\u0631\u0628\u0644\u0627 \u0628\u064a\u0644\u0627\u0633\u0644\u0623\u0627 \u0642\u064a\u0628\u0637\u062a\u0648 \u0644\u0627\u0639\u0641 \u0644\u0643\u0634\u0628 \u062a\u0627\u0646\u0627\u064a\u0628\u0644\u0627 \u0644\u064a\u0644\u062d\u062a \u0649\u0644\u0639 \u0629\u0631\u062f\u0642\u0644\u0627 \u0646\u0625 .\u0629\u062f\u0642\u0639\u0645\u0644\u0627 \u062a\u0644\u0627\u0643\u0634\u0645\u0644\u0644 .\u064a\u0646\u0647\u0645\u0644\u0627\u0648 \u064a\u0635\u062e\u0634\u0644\u0627 \u0621\u0627\u062f\u0644\u0623\u0627 \u0649\u0644\u0639 \u0649\u062f\u0645\u0644\u0627 \u0629\u062f\u064a\u0639\u0628 \u0629\u064a\u0628\u0627\u062c\u064a\u0625 \u062a\u0627\u0631\u064a\u062b\u0623\u062a"}], "pictures": [], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 612.0, "height": 792.0}, "image": null, "page_no": 1}}}

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_01.md
================================================
## Pythonو R ةغلب ةجمربلا للاخ نم تلاكشملا لحو ةيجاتنلإا نيسحت

Python و R ةغلب ةجمربلا ربتعت ةلاعف لولح داجيإ يف دعاستو ةيجاتنلإا ززعت نأ نكمي يتلا ةيوقلا تاودلأا نم ءاملعلاو نيللحملا ىلع لهسي امم ،تانايبلا ليلحتل ةيلاثم اهلعجت ةديرف تازيمPython و R نم لك كلتمي .تلاكشملل ناك اذإ .ةلاعفو ةعيرس ةقيرطب ةدقعم تلايلحت ءارجإ مهسي نأ نكمي تاغللا هذه مادختسا نإف ،ةيليلحت ةيلقع كيدل .لمعلا جئاتن نيسحت يف ريبك لكشب

ً

ً

جارختساو تانايبلا نم ةلئاه تايمك ةجلاعم نكمملا نم حبصي ،ةجمربلا تاراهم عم يليلحتلا ريكفتلا عمتجي امدنع ذيفنتلPython و R مادختسا نيجمربملل نكمي .اهنم تاهجوتلاو طامنلأا ةجذمنلا لثم ،ةمدقتم ةيليلحت تايلمع ةقد رثكأ تارارق ذاختا ىلإ ا ضيأ يدؤي نأ نكمي لب ،تقولا رفوي طقف سيل اذه .ةريبكلا تانايبلا ليلحتو ةيئاصحلإا تانايبلا ىلع ةمئاق تاجاتنتسا ىلع ءانب .

ليلحتلا نم ،تاقيبطتلا نم ةعساو ةعومجم معدت ةينغ تاودأو تابتكمPython و R نم لك رفوت ،كلذ ىلع ةولاع ىلع .ةفلتخملا تلاكشملل ةركتبم لولح ريوطتل تابتكملا هذه نم ةدافتسلاا نيمدختسملل نكمي .يللآا ملعتلا ىلإ ينايبلا R رفوت امنيب ،ةءافكب تانايبلا ةرادلإ Python يف pandas ةبتكم مادختسا نكمي ،لاثملا ليبس مسرلل ةيوق تاودأ .نيللحملاو نيثحابلل ةيلاثم اهلعجي امم ،يئاصحلإا ليلحتلاو ينايبلا

Python و R ةغلب ةجمربلا يدؤت نأ نكمي ،ةياهنلا يف ةركتبم لولح ريفوتو ةيجاتنلإا نيسحت ىلإ ةيليلحت ةيلقع عم اهل نوكت نأ نكمي ةبسانملا ةيجمربلا بيلاسلأا قيبطتو لاعف لكشب تانايبلا ليلحت ىلع ةردقلا نإ .ةدقعملا تلاكشملل .ينهملاو يصخشلا ءادلأا ىلع ىدملا ةديعب ةيباجيإ تاريثأت

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_02.doctags.txt
================================================
<doctag><text><loc_40><loc_478><loc_49><loc_486>11</text>
<text><loc_57><loc_125><loc_367><loc_249>،هيلعو ملا ةوا رملا لاول خواهييع ووص عضت ةيرص م لا ةموكح لا نإف ةو اب لأا نم ددي قي حت ىاي لمعلخب خال ةير وام جلا سي ئر د يسلا فياكت ا دو ه :خاسعر ىاي ويولولأا ةومئخق سعر ىا ي يرصملا نخسنلإا ءخهب فام عضو ، تخ ووومن تحدووعم قووي حت ىوو اي لو وم علا ،ليوواعللاو ةحووصلا تحخووجم اووف ةووصخل ىوووواي خوووو حلا ا وووو و ،تخوووو ي خل لا فوووواذع اووووف ةامخوووو و ةمادلووووسمو ةوووويوق وو يلودلاو ةوويمياقلإا تخيدوو حلل ا ءوووض اووف يرووصملا امووو لا نووملأا تاددووحم ،ة وو ام ةووعبخلم رارملووساو ،ةيووسخيسلا ة رخوواملا ر ي وو و لت د ووواو ةاووصاومو تخ ايوووو لاو ةوووفخ لا تخووو ام ريوووولت ، خوووهرلإا ةوووحفخ كمو ر ار لوووسحاو نوووملأا لي هخووو م وووسري ي ووولا وووو حهل ا ىووواي لدووولعملا اهيدووو لا خووولبلاو ،اه،وووولا .اعملجملا ماسلاو ةه،اوملا</text>
<text><loc_63><loc_258><loc_370><loc_277>رول لا لاول ةيرو ص م لا ةو موكحلا امخونرب دالوسي ،قبس خمل خً فوو 2024( -)2026 اتلآا وحهلا ىاي اهو ،ةسيئر ةيجيتارلسا اد هع ةعبرع قي حت :</text>
<text><loc_58><loc_301><loc_367><loc_317>نــــــــم ما ةــــــــيا م رـ صم لا يم وقل ا اــــسن ا ءاــــ نب رــــــــــــــــــــصم لا عاـــــصت ا ءاـــــ نب يــــــــــــــــــــــسبا نت قتسظا ق يقحت را ر يــــــــــــــــــــــــساي سلا</text>
<text><loc_61><loc_344><loc_367><loc_385>خهلوسحخب امخونرب لا ت خفدالوسم ديدحت لت دق هن ع ىلإ رخ لإا ردجت لكواب د روووصم ةو ووي ر تخ فدال ووو س م ىووو اي سيوووئر 2023 ر اوو وو حلا تخووو ساو تخيوووصوتو ، كيال ا تخ اووصيل اه،ووولا امخوونربلاو ،تارا ووو لا ت خ فدا لوو سمو ،اه،ووولا ،ةوو ي ا ةيه، ولا تخ ي جيتا رلسحا فالبمو .</text>
<picture><loc_375><loc_119><loc_500><loc_386></picture>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_02.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "right_to_left_02", "origin": {"mimetype": "application/pdf", "binary_hash": 6694727290501120405, "filename": "right_to_left_02.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/texts/1"}, {"cref": "#/texts/2"}, {"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/pictures/0"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 47.9520001778084, "t": 37.827721130754185, "r": 58.751999217855335, "b": 23.787720082223927, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "11", "text": "11"}, {"self_ref": "#/texts/1", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 67.9919972521177, "t": 632.2632421854628, "r": 437.42722162200187, "b": 422.5932914607237, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 745]}], "orig": "\u060c\u0647\u064a\u0644\u0639\u0648 \u0645\u0644\u0627 \u0629\u0648\u0627 \u0631\u0645\u0644\u0627 \u0644\u0627\u0648\u0644 \u062e\u0648\u0627\u0647\u064a\u064a\u0639 \u0648\u0648\u0635 \u0639\u0636\u062a \u0629\u064a\u0631\u0635 \u0645 \u0644\u0627 \u0629\u0645\u0648\u0643\u062d \u0644\u0627 \u0646\u0625\u0641 \u0629\u0648 \u0627\u0628 \u0644\u0623\u0627 \u0646\u0645 \u062f\u062f\u064a \u0642\u064a \u062d\u062a \u0649\u0627\u064a \u0644\u0645\u0639\u0644\u062e\u0628 \u062e\u0627\u0644 \u0629\u064a\u0631 \u0648\u0627\u0645 \u062c\u0644\u0627 \u0633\u064a \u0626\u0631 \u062f \u064a\u0633\u0644\u0627 \u0641\u064a\u0627\u0643\u062a \u0627 \u062f\u0648 \u0647 :\u062e\u0627\u0633\u0639\u0631 \u0649\u0627\u064a \u0648\u064a\u0648\u0644\u0648\u0644\u0623\u0627 \u0629\u0648\u0645\u0626\u062e\u0642 \u0633\u0639\u0631 \u0649\u0627 \u064a \u064a\u0631\u0635\u0645\u0644\u0627 \u0646\u062e\u0633\u0646\u0644\u0625\u0627 \u0621\u062e\u0647\u0628 \u0641\u0627\u0645 \u0639\u0636\u0648 \u060c \u062a\u062e \u0648\u0648\u0648\u0645\u0646 \u062a\u062d\u062f\u0648\u0648\u0639\u0645 \u0642\u0648\u0648\u064a \u062d\u062a \u0649\u0648\u0648 \u0627\u064a \u0644\u0648 \u0648\u0645 \u0639\u0644\u0627 \u060c\u0644\u064a\u0648\u0648\u0627\u0639\u0644\u0644\u0627\u0648 \u0629\u062d\u0648\u0648\u0635\u0644\u0627 \u062a\u062d\u062e\u0648\u0648\u062c\u0645 \u0627\u0648\u0648\u0641 \u0629\u0648\u0648\u0635\u062e\u0644 \u0649\u0648\u0648\u0648\u0648\u0627\u064a \u062e\u0648\u0648\u0648\u0648 \u062d\u0644\u0627 \u0627 \u0648\u0648\u0648\u0648 \u0648 \u060c\u062a\u062e\u0648\u0648\u0648\u0648 \u064a \u062e\u0644 \u0644\u0627 \u0641\u0648\u0648\u0648\u0648\u0627\u0630\u0639 \u0627\u0648\u0648\u0648\u0648\u0641 \u0629\u0627\u0645\u062e\u0648\u0648\u0648\u0648 \u0648 \u0629\u0645\u0627\u062f\u0644\u0648\u0648\u0648\u0648\u0633\u0645\u0648 \u0629\u0648\u0648\u0648\u0648\u064a\u0648\u0642 \u0648\u0648 \u064a\u0644\u0648\u062f\u0644\u0627\u0648 \u0629\u0648\u0648\u064a\u0645\u064a\u0627\u0642\u0644\u0625\u0627 \u062a\u062e\u064a\u062f\u0648\u0648 \u062d\u0644\u0644 \u0627 \u0621\u0648\u0648\u0648\u0636 \u0627\u0648\u0648\u0641 \u064a\u0631\u0648\u0648\u0635\u0645\u0644\u0627 \u0627\u0645\u0648\u0648\u0648 \u0644\u0627 \u0646\u0648\u0648\u0645\u0644\u0623\u0627 \u062a\u0627\u062f\u062f\u0648\u0648\u062d\u0645 \u060c\u0629 \u0648\u0648 \u0627\u0645 \u0629\u0648\u0648\u0639\u0628\u062e\u0644\u0645 \u0631\u0627\u0631\u0645\u0644\u0648\u0648\u0633\u0627\u0648 \u060c\u0629\u064a\u0648\u0648\u0633\u062e\u064a\u0633\u0644\u0627 \u0629 \u0631\u062e\u0648\u0648\u0627\u0645\u0644\u0627 \u0631 \u064a \u0648\u0648 \u0648 \u0644\u062a \u062f \u0648\u0648\u0648\u0627\u0648 \u0629\u0627\u0648\u0648\u0635\u0627\u0648\u0645\u0648 \u062a\u062e \u0627\u064a\u0648\u0648\u0648\u0648 \u0644\u0627\u0648 \u0629\u0648\u0648\u0648\u0641\u062e \u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0627\u0645 \u0631\u064a\u0648\u0648\u0648\u0648\u0644\u062a \u060c \u062e\u0648\u0648\u0648\u0647\u0631\u0644\u0625\u0627 \u0629\u0648\u0648\u0648\u062d\u0641\u062e \u0643\u0645\u0648 \u0631 \u0627\u0631 \u0644\u0648\u0648\u0648\u0633\u062d\u0627\u0648 \u0646\u0648\u0648\u0648\u0645\u0644\u0623\u0627 \u0644\u064a \u0647\u062e\u0648\u0648\u0648 \u0645 \u0648\u0648\u0648\u0633\u0631\u064a \u064a \u0648\u0648\u0648\u0644\u0627 \u0648\u0648\u0648\u0648 \u062d\u0647\u0644 \u0627 \u0649\u0648\u0648\u0648\u0627\u064a \u0644\u062f\u0648\u0648\u0648\u0644\u0639\u0645\u0644\u0627 \u0627\u0647\u064a\u062f\u0648\u0648\u0648 \u0644\u0627 \u062e\u0648\u0648\u0648\u0644\u0628\u0644\u0627\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0648\u0644\u0627 .\u0627\u0639\u0645\u0644\u062c\u0645\u0644\u0627 \u0645\u0627\u0633\u0644\u0627\u0648 \u0629\u0647\u060c\u0627\u0648\u0645\u0644\u0627", "text": "\u060c\u0647\u064a\u0644\u0639\u0648 \u0645\u0644\u0627 \u0629\u0648\u0627 \u0631\u0645\u0644\u0627 \u0644\u0627\u0648\u0644 \u062e\u0648\u0627\u0647\u064a\u064a\u0639 \u0648\u0648\u0635 \u0639\u0636\u062a \u0629\u064a\u0631\u0635 \u0645 \u0644\u0627 \u0629\u0645\u0648\u0643\u062d \u0644\u0627 \u0646\u0625\u0641 \u0629\u0648 \u0627\u0628 \u0644\u0623\u0627 \u0646\u0645 \u062f\u062f\u064a \u0642\u064a \u062d\u062a \u0649\u0627\u064a \u0644\u0645\u0639\u0644\u062e\u0628 \u062e\u0627\u0644 \u0629\u064a\u0631 \u0648\u0627\u0645 \u062c\u0644\u0627 \u0633\u064a \u0626\u0631 \u062f \u064a\u0633\u0644\u0627 \u0641\u064a\u0627\u0643\u062a \u0627 \u062f\u0648 \u0647 :\u062e\u0627\u0633\u0639\u0631 \u0649\u0627\u064a \u0648\u064a\u0648\u0644\u0648\u0644\u0623\u0627 \u0629\u0648\u0645\u0626\u062e\u0642 \u0633\u0639\u0631 \u0649\u0627 \u064a \u064a\u0631\u0635\u0645\u0644\u0627 \u0646\u062e\u0633\u0646\u0644\u0625\u0627 \u0621\u062e\u0647\u0628 \u0641\u0627\u0645 \u0639\u0636\u0648 \u060c \u062a\u062e \u0648\u0648\u0648\u0645\u0646 \u062a\u062d\u062f\u0648\u0648\u0639\u0645 \u0642\u0648\u0648\u064a \u062d\u062a \u0649\u0648\u0648 \u0627\u064a \u0644\u0648 \u0648\u0645 \u0639\u0644\u0627 \u060c\u0644\u064a\u0648\u0648\u0627\u0639\u0644\u0644\u0627\u0648 \u0629\u062d\u0648\u0648\u0635\u0644\u0627 \u062a\u062d\u062e\u0648\u0648\u062c\u0645 \u0627\u0648\u0648\u0641 \u0629\u0648\u0648\u0635\u062e\u0644 \u0649\u0648\u0648\u0648\u0648\u0627\u064a \u062e\u0648\u0648\u0648\u0648 \u062d\u0644\u0627 \u0627 \u0648\u0648\u0648\u0648 \u0648 \u060c\u062a\u062e\u0648\u0648\u0648\u0648 \u064a \u062e\u0644 \u0644\u0627 \u0641\u0648\u0648\u0648\u0648\u0627\u0630\u0639 \u0627\u0648\u0648\u0648\u0648\u0641 \u0629\u0627\u0645\u062e\u0648\u0648\u0648\u0648 \u0648 \u0629\u0645\u0627\u062f\u0644\u0648\u0648\u0648\u0648\u0633\u0645\u0648 \u0629\u0648\u0648\u0648\u0648\u064a\u0648\u0642 \u0648\u0648 \u064a\u0644\u0648\u062f\u0644\u0627\u0648 \u0629\u0648\u0648\u064a\u0645\u064a\u0627\u0642\u0644\u0625\u0627 \u062a\u062e\u064a\u062f\u0648\u0648 \u062d\u0644\u0644 \u0627 \u0621\u0648\u0648\u0648\u0636 \u0627\u0648\u0648\u0641 \u064a\u0631\u0648\u0648\u0635\u0645\u0644\u0627 \u0627\u0645\u0648\u0648\u0648 \u0644\u0627 \u0646\u0648\u0648\u0645\u0644\u0623\u0627 \u062a\u0627\u062f\u062f\u0648\u0648\u062d\u0645 \u060c\u0629 \u0648\u0648 \u0627\u0645 \u0629\u0648\u0648\u0639\u0628\u062e\u0644\u0645 \u0631\u0627\u0631\u0645\u0644\u0648\u0648\u0633\u0627\u0648 \u060c\u0629\u064a\u0648\u0648\u0633\u062e\u064a\u0633\u0644\u0627 \u0629 \u0631\u062e\u0648\u0648\u0627\u0645\u0644\u0627 \u0631 \u064a \u0648\u0648 \u0648 \u0644\u062a \u062f \u0648\u0648\u0648\u0627\u0648 \u0629\u0627\u0648\u0648\u0635\u0627\u0648\u0645\u0648 \u062a\u062e \u0627\u064a\u0648\u0648\u0648\u0648 \u0644\u0627\u0648 \u0629\u0648\u0648\u0648\u0641\u062e \u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0627\u0645 \u0631\u064a\u0648\u0648\u0648\u0648\u0644\u062a \u060c \u062e\u0648\u0648\u0648\u0647\u0631\u0644\u0625\u0627 \u0629\u0648\u0648\u0648\u062d\u0641\u062e \u0643\u0645\u0648 \u0631 \u0627\u0631 \u0644\u0648\u0648\u0648\u0633\u062d\u0627\u0648 \u0646\u0648\u0648\u0648\u0645\u0644\u0623\u0627 \u0644\u064a \u0647\u062e\u0648\u0648\u0648 \u0645 \u0648\u0648\u0648\u0633\u0631\u064a \u064a \u0648\u0648\u0648\u0644\u0627 \u0648\u0648\u0648\u0648 \u062d\u0647\u0644 \u0627 \u0649\u0648\u0648\u0648\u0627\u064a \u0644\u062f\u0648\u0648\u0648\u0644\u0639\u0645\u0644\u0627 \u0627\u0647\u064a\u062f\u0648\u0648\u0648 \u0644\u0627 \u062e\u0648\u0648\u0648\u0644\u0628\u0644\u0627\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0648\u0644\u0627 .\u0627\u0639\u0645\u0644\u062c\u0645\u0644\u0627 \u0645\u0627\u0633\u0644\u0627\u0648 \u0629\u0647\u060c\u0627\u0648\u0645\u0644\u0627"}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 74.51999727632386, "t": 408.00330141029247, "r": 440.883241634817, "b": 376.3233013007883, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 135]}], "orig": "\u0631\u0648\u0644 \u0644\u0627 \u0644\u0627\u0648\u0644 \u0629\u064a\u0631\u0648 \u0635 \u0645 \u0644\u0627 \u0629\u0648 \u0645\u0648\u0643\u062d\u0644\u0627 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u062f\u0627\u0644\u0648\u0633\u064a \u060c\u0642\u0628\u0633 \u062e\u0645\u0644 \u062e\u064b \u0641\u0648\u0648 2024( -)2026 \u0627\u062a\u0644\u0622\u0627 \u0648\u062d\u0647\u0644\u0627 \u0649\u0627\u064a \u0627\u0647\u0648 \u060c\u0629\u0633\u064a\u0626\u0631 \u0629\u064a\u062c\u064a\u062a\u0627\u0631\u0644\u0633\u0627 \u0627\u062f \u0647\u0639 \u0629\u0639\u0628\u0631\u0639 \u0642\u064a \u062d\u062a :", "text": "\u0631\u0648\u0644 \u0644\u0627 \u0644\u0627\u0648\u0644 \u0629\u064a\u0631\u0648 \u0635 \u0645 \u0644\u0627 \u0629\u0648 \u0645\u0648\u0643\u062d\u0644\u0627 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u062f\u0627\u0644\u0648\u0633\u064a \u060c\u0642\u0628\u0633 \u062e\u0645\u0644 \u062e\u064b \u0641\u0648\u0648 2024( -)2026 \u0627\u062a\u0644\u0622\u0627 \u0648\u062d\u0647\u0644\u0627 \u0649\u0627\u064a \u0627\u0647\u0648 \u060c\u0629\u0633\u064a\u0626\u0631 \u0629\u064a\u062c\u064a\u062a\u0627\u0631\u0644\u0633\u0627 \u0627\u062f \u0647\u0639 \u0629\u0639\u0628\u0631\u0639 \u0642\u064a \u062d\u062a :"}, {"self_ref": "#/texts/3", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 69.09600125621141, "t": 334.49329115619986, "r": 437.3132016215791, "b": 307.8032810639438, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 196]}], "orig": "\u0646\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0645 \u0645\u0627 \u0629\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u064a\u0627 \u0645 \u0631\u0640 \u0635\u0645 \u0644\u0627 \u064a\u0645 \u0648\u0642\u0644 \u0627 \u0627\u0640\u0640\u0640\u0640\u0633\u0646 \u0627 \u0621\u0627\u0640\u0640\u0640\u0640 \u0646\u0628 \u0631\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0635\u0645 \u0644\u0627 \u0639\u0627\u0640\u0640\u0640\u0640\u0640\u0635\u062a \u0627 \u0621\u0627\u0640\u0640\u0640\u0640\u0640 \u0646\u0628 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0628\u0627 \u0646\u062a \u0642\u062a\u0633\u0638\u0627 \u0642 \u064a\u0642\u062d\u062a \u0631\u0627 \u0631 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0627\u064a \u0633\u0644\u0627", "text": "\u0646\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0645 \u0645\u0627 \u0629\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u064a\u0627 \u0645 \u0631\u0640 \u0635\u0645 \u0644\u0627 \u064a\u0645 \u0648\u0642\u0644 \u0627 \u0627\u0640\u0640\u0640\u0640\u0633\u0646 \u0627 \u0621\u0627\u0640\u0640\u0640\u0640 \u0646\u0628 \u0631\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0635\u0645 \u0644\u0627 \u0639\u0627\u0640\u0640\u0640\u0640\u0640\u0635\u062a \u0627 \u0621\u0627\u0640\u0640\u0640\u0640\u0640 \u0646\u0628 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0628\u0627 \u0646\u062a \u0642\u062a\u0633\u0638\u0627 \u0642 \u064a\u0642\u062d\u062a \u0631\u0627 \u0631 \u064a\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0633\u0627\u064a \u0633\u0644\u0627"}, {"self_ref": "#/texts/4", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 72.94919627049924, "t": 263.09326090940056, "r": 437.29059162149525, "b": 193.95328067041328, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 280]}], "orig": "\u062e\u0647\u0644\u0648\u0633\u062d\u062e\u0628 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u0644\u0627 \u062a \u062e\u0641\u062f\u0627\u0644\u0648\u0633\u0645 \u062f\u064a\u062f\u062d\u062a \u0644\u062a \u062f\u0642 \u0647\u0646 \u0639 \u0649\u0644\u0625 \u0631\u062e \u0644\u0625\u0627 \u0631\u062f\u062c\u062a \u0644\u0643\u0648\u0627\u0628 \u062f \u0631\u0648\u0648\u0648\u0635\u0645 \u0629\u0648 \u0648\u0648\u064a \u0631 \u062a\u062e \u0641\u062f\u0627\u0644 \u0648\u0648\u0648 \u0633 \u0645 \u0649\u0648\u0648\u0648 \u0627\u064a \u0633\u064a\u0648\u0648\u0648\u0626\u0631 2023 \u0631 \u0627\u0648\u0648 \u0648\u0648 \u062d\u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0633\u0627\u0648 \u062a\u062e\u064a\u0648\u0648\u0648\u0635\u0648\u062a\u0648 \u060c \u0643\u064a\u0627\u0644 \u0627 \u062a\u062e \u0627\u0648\u0648\u0635\u064a\u0644 \u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u0627\u0645\u062e\u0648\u0648\u0646\u0631\u0628\u0644\u0627\u0648 \u060c\u062a\u0627\u0631\u0627 \u0648\u0648\u0648 \u0644\u0627 \u062a \u062e \u0641\u062f\u0627 \u0644\u0648\u0648 \u0633\u0645\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u060c\u0629\u0648\u0648 \u064a \u0627 \u0629\u064a\u0647\u060c \u0648\u0644\u0627 \u062a\u062e \u064a \u062c\u064a\u062a\u0627 \u0631\u0644\u0633\u062d\u0627 \u0641\u0627\u0644\u0628\u0645\u0648 .", "text": "\u062e\u0647\u0644\u0648\u0633\u062d\u062e\u0628 \u0627\u0645\u062e\u0648\u0646\u0631\u0628 \u0644\u0627 \u062a \u062e\u0641\u062f\u0627\u0644\u0648\u0633\u0645 \u062f\u064a\u062f\u062d\u062a \u0644\u062a \u062f\u0642 \u0647\u0646 \u0639 \u0649\u0644\u0625 \u0631\u062e \u0644\u0625\u0627 \u0631\u062f\u062c\u062a \u0644\u0643\u0648\u0627\u0628 \u062f \u0631\u0648\u0648\u0648\u0635\u0645 \u0629\u0648 \u0648\u0648\u064a \u0631 \u062a\u062e \u0641\u062f\u0627\u0644 \u0648\u0648\u0648 \u0633 \u0645 \u0649\u0648\u0648\u0648 \u0627\u064a \u0633\u064a\u0648\u0648\u0648\u0626\u0631 2023 \u0631 \u0627\u0648\u0648 \u0648\u0648 \u062d\u0644\u0627 \u062a\u062e\u0648\u0648\u0648 \u0633\u0627\u0648 \u062a\u062e\u064a\u0648\u0648\u0648\u0635\u0648\u062a\u0648 \u060c \u0643\u064a\u0627\u0644 \u0627 \u062a\u062e \u0627\u0648\u0648\u0635\u064a\u0644 \u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u0627\u0645\u062e\u0648\u0648\u0646\u0631\u0628\u0644\u0627\u0648 \u060c\u062a\u0627\u0631\u0627 \u0648\u0648\u0648 \u0644\u0627 \u062a \u062e \u0641\u062f\u0627 \u0644\u0648\u0648 \u0633\u0645\u0648 \u060c\u0627\u0647\u060c\u0648\u0648\u0648\u0644\u0627 \u060c\u0629\u0648\u0648 \u064a \u0627 \u0629\u064a\u0647\u060c \u0648\u0644\u0627 \u062a\u062e \u064a \u062c\u064a\u062a\u0627 \u0631\u0644\u0633\u062d\u0627 \u0641\u0627\u0644\u0628\u0645\u0648 ."}], "pictures": [{"self_ref": "#/pictures/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "picture", "prov": [{"page_no": 1, "bbox": {"l": 446.4657287597656, "t": 641.2087554931641, "r": 595.0, "b": 191.27679443359375, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [], "references": [], "footnotes": [], "image": null, "annotations": []}], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 595.2000122070312, "height": 841.9199829101562}, "image": null, "page_no": 1}}}

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_02.md
================================================
11

،هيلعو ملا ةوا رملا لاول خواهييع ووص عضت ةيرص م لا ةموكح لا نإف ةو اب لأا نم ددي قي حت ىاي لمعلخب خال ةير وام جلا سي ئر د يسلا فياكت ا دو ه :خاسعر ىاي ويولولأا ةومئخق سعر ىا ي يرصملا نخسنلإا ءخهب فام عضو ، تخ ووومن تحدووعم قووي حت ىوو اي لو وم علا ،ليوواعللاو ةحووصلا تحخووجم اووف ةووصخل ىوووواي خوووو حلا ا وووو و ،تخوووو ي خل لا فوووواذع اووووف ةامخوووو و ةمادلووووسمو ةوووويوق وو يلودلاو ةوويمياقلإا تخيدوو حلل ا ءوووض اووف يرووصملا امووو لا نووملأا تاددووحم ،ة وو ام ةووعبخلم رارملووساو ،ةيووسخيسلا ة رخوواملا ر ي وو و لت د ووواو ةاووصاومو تخ ايوووو لاو ةوووفخ لا تخووو ام ريوووولت ، خوووهرلإا ةوووحفخ كمو ر ار لوووسحاو نوووملأا لي هخووو م وووسري ي ووولا وووو حهل ا ىووواي لدووولعملا اهيدووو لا خووولبلاو ،اه،وووولا .اعملجملا ماسلاو ةه،اوملا

رول لا لاول ةيرو ص م لا ةو موكحلا امخونرب دالوسي ،قبس خمل خً فوو 2024( -)2026 اتلآا وحهلا ىاي اهو ،ةسيئر ةيجيتارلسا اد هع ةعبرع قي حت :

نــــــــم ما ةــــــــيا م رـ صم لا يم وقل ا اــــسن ا ءاــــ نب رــــــــــــــــــــصم لا عاـــــصت ا ءاـــــ نب يــــــــــــــــــــــسبا نت قتسظا ق يقحت را ر يــــــــــــــــــــــــساي سلا

خهلوسحخب امخونرب لا ت خفدالوسم ديدحت لت دق هن ع ىلإ رخ لإا ردجت لكواب د روووصم ةو ووي ر تخ فدال ووو س م ىووو اي سيوووئر 2023 ر اوو وو حلا تخووو ساو تخيوووصوتو ، كيال ا تخ اووصيل اه،ووولا امخوونربلاو ،تارا ووو لا ت خ فدا لوو سمو ،اه،ووولا ،ةوو ي ا ةيه، ولا تخ ي جيتا رلسحا فالبمو .

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_03.doctags.txt
================================================
<doctag><section_header_level_1><loc_58><loc_37><loc_225><loc_48>یلخاد یلااک - یلصا رازاب رد شريذپ همانديما</section_header_level_1>
<picture><loc_326><loc_21><loc_405><loc_61></picture>
<section_header_level_1><loc_314><loc_82><loc_403><loc_93>لااک درادناتسا -2-5</section_header_level_1>
<text><loc_385><loc_96><loc_436><loc_106>درادناتسا مان</text>
<text><loc_56><loc_96><loc_222><loc_125>یرگ هتخير شور هب هدش ديلوت لاشمش و هشمش فرصم دروم هتسويپ یا هزاس یاهدلاوف رد - قباطم تسويپ زيلانآ</text>
<text><loc_354><loc_128><loc_436><loc_138>یلم درادناتسا هرامش</text>
<text><loc_199><loc_128><loc_223><loc_136>20300</text>
<text><loc_342><loc_142><loc_436><loc_152>؟تسا یرابجا درادناتسا</text>
<checkbox_unselected><loc_166><loc_141><loc_222><loc_149>ريخ       یلب</checkbox_unselected>
<text><loc_327><loc_155><loc_436><loc_165>درادناتسا هدننکرداص عجرم</text>
<text><loc_140><loc_154><loc_222><loc_163>ناريا درادناتسا یلم نامزاس</text>
<text><loc_245><loc_169><loc_436><loc_192>ذخا ار روکذم درادناتسا ،لوصحم هدننکديلوت ايآ ؟تسا هدومن</text>
<checkbox_selected><loc_166><loc_168><loc_175><loc_176>ريخ</checkbox_selected>
<checkbox_unselected><loc_199><loc_168><loc_208><loc_176>یلب</checkbox_unselected>
<section_header_level_1><loc_344><loc_209><loc_425><loc_219>سروب رد شريذپ -3</section_header_level_1>
<text><loc_340><loc_222><loc_414><loc_232>کرادم هئارا خيرات</text>
<text><loc_116><loc_221><loc_158><loc_230>1403/09/19</text>
<text><loc_358><loc_236><loc_414><loc_246>شريذپ خيرات</text>
<text><loc_116><loc_235><loc_158><loc_243>1403/10/04</text>
<text><loc_308><loc_249><loc_414><loc_259>هضرع هتيمک هسلج هرامش</text>
<text><loc_130><loc_248><loc_144><loc_257>436</text>
<text><loc_335><loc_263><loc_414><loc_273>همانديما جرد خيرات</text>
<text><loc_116><loc_262><loc_158><loc_270>1403/10/05</text>
<text><loc_355><loc_276><loc_414><loc_286>شريذپ رواشم</text>
<text><loc_103><loc_275><loc_171><loc_283>سروب نومرآ یرازگراک</text>
<text><loc_236><loc_291><loc_414><loc_314>رد لااک شريذپ زا سپ هياپ تميق نييعت ةوحن سروب</text>
<text><loc_92><loc_290><loc_179><loc_298>یناهج  یاه تميق ساسا رب</text>
<text><loc_224><loc_317><loc_414><loc_340>شورف /شورف لک /ديلوت زا هضرع دصرد لقادح یلخاد</text>
<text><loc_72><loc_316><loc_202><loc_325>نت 47.500 اي هنايلاس ديلوت زا %50 لقادح</text>
<text><loc_340><loc_344><loc_414><loc_354>ليوحت زاجم یاطخ</text>
<text><loc_90><loc_343><loc_184><loc_351>ليوحت لباق هلومحم نيرخآ 5%</text>
<page_footer><loc_224><loc_463><loc_247><loc_469>Page 7</page_footer>
</doctag>

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_03.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "right_to_left_03", "origin": {"mimetype": "application/pdf", "binary_hash": 10326044566005236748, "filename": "right_to_left_03.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}, {"cref": "#/pictures/0"}, {"cref": "#/texts/2"}, {"cref": "#/groups/0"}, {"cref": "#/texts/14"}, {"cref": "#/groups/1"}, {"cref": "#/texts/31"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [{"self_ref": "#/groups/0", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/3"}, {"cref": "#/texts/4"}, {"cref": "#/texts/5"}, {"cref": "#/texts/6"}, {"cref": "#/texts/7"}, {"cref": "#/texts/8"}, {"cref": "#/texts/9"}, {"cref": "#/texts/10"}, {"cref": "#/texts/11"}, {"cref": "#/texts/12"}, {"cref": "#/texts/13"}], "content_layer": "body", "name": "group", "label": "key_value_area"}, {"self_ref": "#/groups/1", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/15"}, {"cref": "#/texts/16"}, {"cref": "#/texts/17"}, {"cref": "#/texts/18"}, {"cref": "#/texts/19"}, {"cref": "#/texts/20"}, {"cref": "#/texts/21"}, {"cref": "#/texts/22"}, {"cref": "#/texts/23"}, {"cref": "#/texts/24"}, {"cref": "#/texts/25"}, {"cref": "#/texts/26"}, {"cref": "#/texts/27"}, {"cref": "#/texts/28"}, {"cref": "#/texts/29"}, {"cref": "#/texts/30"}], "content_layer": "body", "name": "group", "label": "key_value_area"}], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 68.78399669083697, "t": 779.3882381741187, "r": 267.65960879695194, "b": 761.0098882171737, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 42]}], "orig": "\u06cc\u0644\u062e\u0627\u062f \u06cc\u0644\u0627\u0627\u06a9 - \u06cc\u0644\u0635\u0627 \u0631\u0627\u0632\u0627\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e \u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627", "text": "\u06cc\u0644\u062e\u0627\u062f \u06cc\u0644\u0627\u0627\u06a9 - \u06cc\u0644\u0635\u0627 \u0631\u0627\u0632\u0627\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e \u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627", "level": 1}, {"self_ref": "#/texts/1", "parent": {"cref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 388.989988251609, "t": 750.2512182423783, "r": 481.6045778353348, "b": 736.9811382734663, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 21]}], "orig": "\u0646\u0627\u0631\u064a\u0627 \u06cc\u0644\u0627\u0627\u06a9 \u0633\u0631\u0648\u0628 \u062a\u06a9\u0631\u0634", "text": "\u0646\u0627\u0631\u064a\u0627 \u06cc\u0644\u0627\u0627\u06a9 \u0633\u0631\u0648\u0628 \u062a\u06a9\u0631\u0634"}, {"self_ref": "#/texts/2", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 373.9899883190294, "t": 703.4050283521253, "r": 479.52999784465936, "b": 685.3749983943645, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "\u0644\u0627\u0627\u06a9 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 -2-5", "text": "\u0644\u0627\u0627\u06a9 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 -2-5", "level": 1}, {"self_ref": "#/texts/3", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 458.7399879381041, "t": 679.6162084078558, "r": 519.2383976661823, "b": 662.7401084473915, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0645\u0627\u0646", "text": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0645\u0627\u0646"}, {"self_ref": "#/texts/4", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 66.26399970216359, "t": 681.171998404211, "r": 264.81795880972436, "b": 631.5399785204845, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 97]}], "orig": "\u06cc\u0631\u06af \u0647\u062a\u062e\u064a\u0631 \u0634\u0648\u0631 \u0647\u0628 \u0647\u062f\u0634 \u062f\u064a\u0644\u0648\u062a \u0644\u0627\u0634\u0645\u0634 \u0648 \u0647\u0634\u0645\u0634 \u0641\u0631\u0635\u0645 \u062f\u0631\u0648\u0645 \u0647\u062a\u0633\u0648\u064a\u067e \u06cc\u0627 \u0647\u0632\u0627\u0633 \u06cc\u0627\u0647\u062f\u0644\u0627\u0648\u0641 \u0631\u062f - \u0642\u0628\u0627\u0637\u0645 \u062a\u0633\u0648\u064a\u067e \u0632\u064a\u0644\u0627\u0646\u0622", "text": "\u06cc\u0631\u06af \u0647\u062a\u062e\u064a\u0631 \u0634\u0648\u0631 \u0647\u0628 \u0647\u062f\u0634 \u062f\u064a\u0644\u0648\u062a \u0644\u0627\u0634\u0645\u0634 \u0648 \u0647\u0634\u0645\u0634 \u0641\u0631\u0635\u0645 \u062f\u0631\u0648\u0645 \u0647\u062a\u0633\u0648\u064a\u067e \u06cc\u0627 \u0647\u0632\u0627\u0633 \u06cc\u0627\u0647\u062f\u0644\u0627\u0648\u0641 \u0631\u062f - \u0642\u0628\u0627\u0637\u0645 \u062a\u0633\u0648\u064a\u067e \u0632\u064a\u0644\u0627\u0646\u0622"}, {"self_ref": "#/texts/5", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 420.9099981081384, "t": 625.7362085340809, "r": 519.1619876665258, "b": 608.8601085736167, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "\u06cc\u0644\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u0631\u0627\u0645\u0634", "text": "\u06cc\u0644\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u0631\u0627\u0645\u0634"}, {"self_ref": "#/texts/6", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 236.80999893561153, "t": 627.2919885304362, "r": 265.01000880886113, "b": 613.2999885632154, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "20300", "text": "20300"}, {"self_ref": "#/texts/7", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 406.9899881707045, "t": 603.0262485872838, "r": 519.1415376666176, "b": 586.1501486268197, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 21]}], "orig": "\u061f\u062a\u0633\u0627 \u06cc\u0631\u0627\u0628\u062c\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627", "text": "\u061f\u062a\u0633\u0627 \u06cc\u0631\u0627\u0628\u062c\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627"}, {"self_ref": "#/texts/8", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "checkbox_unselected", "prov": [{"page_no": 1, "bbox": {"l": 197.32999911306206, "t": 604.5820285836392, "r": 264.91399880929265, "b": 590.5900286164182, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "\u0631\u064a\u062e       \u06cc\u0644\u0628", "text": "\u0631\u064a\u062e       \u06cc\u0644\u0628"}, {"self_ref": "#/texts/9", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 389.4699982494516, "t": 580.3462486404165, "r": 519.2136776662934, "b": 563.4701486799523, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 24]}], "orig": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u062f\u0646\u0646\u06a9\u0631\u062f\u0627\u0635 \u0639\u062c\u0631\u0645", "text": "\u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u0647\u062f\u0646\u0646\u06a9\u0631\u062f\u0627\u0635 \u0639\u062c\u0631\u0645"}, {"self_ref": "#/texts/10", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 166.5799992512739, "t": 581.9020386367717, "r": 264.77599880991295, "b": 567.9100286695509, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 26]}], "orig": "\u0646\u0627\u0631\u064a\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u06cc\u0644\u0645 \u0646\u0627\u0645\u0632\u0627\u0633", "text": "\u0646\u0627\u0631\u064a\u0627 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u06cc\u0644\u0645 \u0646\u0627\u0645\u0632\u0627\u0633"}, {"self_ref": "#/texts/11", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 292.129998686965, "t": 557.6661986935493, "r": 519.2351676661968, "b": 518.5901487850932, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 55]}], "orig": "\u0630\u062e\u0627 \u0627\u0631 \u0631\u0648\u06a9\u0630\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u060c\u0644\u0648\u0635\u062d\u0645 \u0647\u062f\u0646\u0646\u06a9\u062f\u064a\u0644\u0648\u062a \u0627\u064a\u0622 \u061f\u062a\u0633\u0627 \u0647\u062f\u0648\u0645\u0646", "text": "\u0630\u062e\u0627 \u0627\u0631 \u0631\u0648\u06a9\u0630\u0645 \u062f\u0631\u0627\u062f\u0646\u0627\u062a\u0633\u0627 \u060c\u0644\u0648\u0635\u062d\u0645 \u0647\u062f\u0646\u0646\u06a9\u062f\u064a\u0644\u0648\u062a \u0627\u064a\u0622 \u061f\u062a\u0633\u0627 \u0647\u062f\u0648\u0645\u0646"}, {"self_ref": "#/texts/12", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "checkbox_selected", "prov": [{"page_no": 1, "bbox": {"l": 197.32999911306206, "t": 559.2219786899045, "r": 208.04769906488926, "b": 545.2299787226838, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "\u0631\u064a\u062e", "text": "\u0631\u064a\u062e"}, {"self_ref": "#/texts/13", "parent": {"cref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "checkbox_unselected", "prov": [{"page_no": 1, "bbox": {"l": 236.62821893642857, "t": 559.2219786899045, "r": 247.34591888825577, "b": 545.2299787226838, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "\u06cc\u0644\u0628", "text": "\u06cc\u0644\u0628"}, {"self_ref": "#/texts/14", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 409.0299981615353, "t": 490.58620885069837, "r": 505.7644977267433, "b": 473.71013889023413, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "\u0633\u0631\u0648\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e -3", "text": "\u0633\u0631\u0648\u0628 \u0631\u062f \u0634\u0631\u064a\u0630\u067e -3", "level": 1}, {"self_ref": "#/texts/15", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 405.30999817825557, "t": 467.88619890387787, "r": 492.6107177858655, "b": 451.01012894341363, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 17]}], "orig": "\u06a9\u0631\u0627\u062f\u0645 \u0647\u0626\u0627\u0631\u0627 \u062e\u064a\u0631\u0627\u062a", "text": "\u06a9\u0631\u0627\u062f\u0645 \u0647\u0626\u0627\u0631\u0627 \u062e\u064a\u0631\u0627\u062a"}, {"self_ref": "#/texts/16", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 137.89998938018175, "t": 469.4620089001862, "r": 187.8199891558066, "b": 455.4699989329655, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "1403/09/19", "text": "1403/09/19"}, {"self_ref": "#/texts/17", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 426.309998083867, "t": 445.2062089570106, "r": 492.59463778593783, "b": 428.3301389965463, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "\u0634\u0631\u064a\u0630\u067e \u062e\u064a\u0631\u0627\u062a", "text": "\u0634\u0631\u064a\u0630\u067e \u062e\u064a\u0631\u0627\u062a"}, {"self_ref": "#/texts/18", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 137.89998938018175, "t": 446.7620189533657, "r": 187.8199891558066, "b": 432.7700189861449, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "1403/10/04", "text": "1403/10/04"}, {"self_ref": "#/texts/19", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 367.14998834977314, "t": 422.5261790101433, "r": 492.68526778553047, "b": 405.65011904967906, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 21]}], "orig": "\u0647\u0636\u0631\u0639 \u0647\u062a\u064a\u0645\u06a9 \u0647\u0633\u0644\u062c \u0647\u0631\u0627\u0645\u0634", "text": "\u0647\u0636\u0631\u0639 \u0647\u062a\u064a\u0645\u06a9 \u0647\u0633\u0644\u062c \u0647\u0631\u0627\u0645\u0634"}, {"self_ref": "#/texts/20", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 154.69999930467083, "t": 423.96200900677957, "r": 171.19999923050838, "b": 409.96999903955884, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "436", "text": "436"}, {"self_ref": "#/texts/21", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 399.42998820468443, "t": 399.72619906355703, "r": 492.62752778578994, "b": 382.8501291030928, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 18]}], "orig": "\u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627 \u062c\u0631\u062f \u062e\u064a\u0631\u0627\u062a", "text": "\u0647\u0645\u0627\u0646\u062f\u064a\u0645\u0627 \u062c\u0631\u062f \u062e\u064a\u0631\u0627\u062a"}, {"self_ref": "#/texts/22", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 137.89998938018175, "t": 401.2820090599123, "r": 187.8199891558066, "b": 387.29000909269143, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "1403/10/05", "text": "1403/10/05"}, {"self_ref": "#/texts/23", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 422.82998809950857, "t": 377.04619911668976, "r": 492.6789577855588, "b": 360.17013915622545, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "\u0634\u0631\u064a\u0630\u067e \u0631\u0648\u0627\u0634\u0645", "text": "\u0634\u0631\u064a\u0630\u067e \u0631\u0648\u0627\u0634\u0645"}, {"self_ref": "#/texts/24", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 122.05999945137766, "t": 378.6020191130449, "r": 203.6480090846645, "b": 364.6100191458242, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "\u0633\u0631\u0648\u0628 \u0646\u0648\u0645\u0631\u0622 \u06cc\u0631\u0627\u0632\u06af\u0631\u0627\u06a9", "text": "\u0633\u0631\u0648\u0628 \u0646\u0648\u0645\u0631\u0622 \u06cc\u0631\u0627\u0632\u06af\u0631\u0627\u06a9"}, {"self_ref": "#/texts/25", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 281.3299887355078, "t": 352.6861891737582, "r": 492.70525778544066, "b": 313.730129265021, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 45]}], "orig": "\u0631\u062f \u0644\u0627\u0627\u06a9 \u0634\u0631\u064a\u0630\u067e \u0632\u0627 \u0633\u067e \u0647\u064a\u0627\u067e \u062a\u0645\u064a\u0642 \u0646\u064a\u064a\u0639\u062a \u0629\u0648\u062d\u0646 \u0633\u0631\u0648\u0628", "text": "\u0631\u062f \u0644\u0627\u0627\u06a9 \u0634\u0631\u064a\u0630\u067e \u0632\u0627 \u0633\u067e \u0647\u064a\u0627\u067e \u062a\u0645\u064a\u0642 \u0646\u064a\u064a\u0639\u062a \u0629\u0648\u062d\u0646 \u0633\u0631\u0648\u0628"}, {"self_ref": "#/texts/26", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 109.21999950908952, "t": 354.24199917011344, "r": 213.67396903960088, "b": 340.2499992028926, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 23]}], "orig": "\u06cc\u0646\u0627\u0647\u062c  \u06cc\u0627\u0647 \u062a\u0645\u064a\u0642 \u0633\u0627\u0633\u0627 \u0631\u0628", "text": "\u06cc\u0646\u0627\u0647\u062c  \u06cc\u0627\u0647 \u062a\u0645\u064a\u0642 \u0633\u0627\u0633\u0627 \u0631\u0628"}, {"self_ref": "#/texts/27", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 266.5700088018494, "t": 307.7761792789694, "r": 492.7008677854604, "b": 268.82012937023217, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 45]}], "orig": "\u0634\u0648\u0631\u0641 /\u0634\u0648\u0631\u0641 \u0644\u06a9 /\u062f\u064a\u0644\u0648\u062a \u0632\u0627 \u0647\u0636\u0631\u0639 \u062f\u0635\u0631\u062f \u0644\u0642\u0627\u062f\u062d \u06cc\u0644\u062e\u0627\u062f", "text": "\u0634\u0648\u0631\u0641 /\u0634\u0648\u0631\u0641 \u0644\u06a9 /\u062f\u064a\u0644\u0648\u062a \u0632\u0627 \u0647\u0636\u0631\u0639 \u062f\u0635\u0631\u062f \u0644\u0642\u0627\u062f\u062d \u06cc\u0644\u062e\u0627\u062f"}, {"self_ref": "#/texts/28", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 85.4639966158655, "t": 309.3319992753245, "r": 240.36199891964634, "b": 295.33999930810376, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 39]}], "orig": "\u0646\u062a 47.500 \u0627\u064a \u0647\u0646\u0627\u064a\u0644\u0627\u0633 \u062f\u064a\u0644\u0648\u062a \u0632\u0627 %50 \u0644\u0642\u0627\u062f\u062d", "text": "\u0646\u062a 47.500 \u0627\u064a \u0647\u0646\u0627\u064a\u0644\u0627\u0633 \u062f\u064a\u0644\u0648\u062a \u0632\u0627 %50 \u0644\u0642\u0627\u062f\u062d"}, {"self_ref": "#/texts/29", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 404.2300081831098, "t": 262.8962093841102, "r": 492.6399177857343, "b": 246.02010942364598, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "\u0644\u064a\u0648\u062d\u062a \u0632\u0627\u062c\u0645 \u06cc\u0627\u0637\u062e", "text": "\u0644\u064a\u0648\u062d\u062a \u0632\u0627\u062c\u0645 \u06cc\u0627\u0637\u062e"}, {"self_ref": "#/texts/30", "parent": {"cref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 106.93999951933742, "t": 264.4519993804654, "r": 218.89399901613845, "b": 250.45998941324467, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 26]}], "orig": "\u0644\u064a\u0648\u062d\u062a \u0644\u0628\u0627\u0642 \u0647\u0644\u0648\u0645\u062d\u0645 \u0646\u064a\u0631\u062e\u0622 5%", "text": "\u0644\u064a\u0648\u062d\u062a \u0644\u0628\u0627\u0642 \u0647\u0644\u0648\u0645\u062d\u0645 \u0646\u064a\u0631\u062e\u0622 5%"}, {"self_ref": "#/texts/31", "parent": {"cref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 1, "bbox": {"l": 267.2900087986132, "t": 62.54999885346342, "r": 294.5899986759081, "b": 52.00199887817439, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "Page 7", "text": "Page 7"}], "pictures": [{"self_ref": "#/pictures/0", "parent": {"cref": "#/body"}, "children": [{"cref": "#/texts/1"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 1, "bbox": {"l": 388.5767822265625, "t": 806.0041046142578, "r": 482.4759216308594, "b": 739.034423828125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [], "references": [], "footnotes": [], "image": null, "annotations": []}], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 595.3200073242188, "height": 842.0399780273438}, "image": null, "page_no": 1}}}

================================================
File: tests/data/groundtruth/docling_v2/right_to_left_03.md
================================================
## یلخاد یلااک - یلصا رازاب رد شريذپ همانديما

<!-- image -->

## لااک درادناتسا -2-5

درادناتسا مان

یرگ هتخير شور هب هدش ديلوت لاشمش و هشمش فرصم دروم هتسويپ یا هزاس یاهدلاوف رد - قباطم تسويپ زيلانآ

یلم درادناتسا هرامش

20300

؟تسا یرابجا درادناتسا

ريخ       یلب

درادناتسا هدننکرداص عجرم

ناريا درادناتسا یلم نامزاس

ذخا ار روکذم درادناتسا ،لوصحم هدننکديلوت ايآ ؟تسا هدومن

ريخ

یلب

## سروب رد شريذپ -3

کرادم هئارا خيرات

1403/09/19

شريذپ خيرات

1403/10/04

هضرع هتيمک هسلج هرامش

436

همانديما جرد خيرات

1403/10/05

شريذپ رواشم

سروب نومرآ یرازگراک

رد لااک شريذپ زا سپ هياپ تميق نييعت ةوحن سروب

یناهج  یاه تميق ساسا رب

شورف /شورف لک /ديلوت زا هضرع دصرد لقادح یلخاد

نت 47.500 اي هنايلاس ديلوت زا %50 لقادح

ليوحت زاجم یاطخ

ليوحت لباق هلومحم نيرخآ 5%

================================================
File: tests/data/groundtruth/docling_v2/tablecell.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: list: group list
    item-2 at level 2: list_item: Hello world1
    item-3 at level 2: list_item: Hello2
  item-4 at level 1: paragraph: 
  item-5 at level 1: paragraph: Some text before
  item-6 at level 1: table with [3x3]
  item-7 at level 1: paragraph: 
  item-8 at level 1: paragraph: 
  item-9 at level 1: paragraph: Some text after

================================================
File: tests/data/groundtruth/docling_v2/tablecell.docx.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "tablecell",
  "origin": {
    "mimetype": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "binary_hash": 1111850039819445035,
    "filename": "tablecell.docx"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/groups/0"
      },
      {
        "$ref": "#/texts/2"
      },
      {
        "$ref": "#/texts/3"
      },
      {
        "$ref": "#/tables/0"
      },
      {
        "$ref": "#/texts/4"
      },
      {
        "$ref": "#/texts/5"
      },
      {
        "$ref": "#/texts/6"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/0"
        },
        {
          "$ref": "#/texts/1"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Hello world1",
      "text": "Hello world1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "Hello2",
      "text": "Hello2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Some text before",
      "text": "Some text before"
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Some text after",
      "text": "Some text after"
    }
  ],
  "pictures": [],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Tab1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Tab2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Tab3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "A",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "B",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "C",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "D",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "E",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "F",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 3,
        "num_cols": 3,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Tab1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Tab2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Tab3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "A",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "B",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "C",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "D",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "E",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "F",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/tablecell.docx.md
================================================
- Hello world1
- Hello2

Some text before

| Tab1   | Tab2   | Tab3   |
|--------|--------|--------|
| A      | B      | C      |
| D      | E      | F      |

Some text after

================================================
File: tests/data/groundtruth/docling_v2/test-01.xlsx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: section: group sheet: Sheet1
    item-2 at level 2: table with [7x3]
  item-3 at level 1: section: group sheet: Sheet2
    item-4 at level 2: table with [9x4]
    item-5 at level 2: table with [5x3]
    item-6 at level 2: table with [5x3]
  item-7 at level 1: section: group sheet: Sheet3
    item-8 at level 2: table with [7x3]
    item-9 at level 2: table with [7x3]
    item-10 at level 2: picture

================================================
File: tests/data/groundtruth/docling_v2/test-01.xlsx.md
================================================
|   first  |   second  |   third |
|----------|-----------|---------|
|        1 |         5 |       9 |
|        2 |         4 |       6 |
|        3 |         3 |       3 |
|        4 |         2 |       0 |
|        5 |         1 |      -3 |
|        6 |         0 |      -6 |

|   col-1 |   col-2 |   col-3 |   col-4 |
|---------|---------|---------|---------|
|       1 |       2 |       3 |       4 |
|       2 |       4 |       6 |       8 |
|       3 |       6 |       9 |      12 |
|       4 |       8 |      12 |      16 |
|       5 |      10 |      15 |      20 |
|       6 |      12 |      18 |      24 |
|       7 |      14 |      21 |      28 |
|       8 |      16 |      24 |      32 |

|   col-1 |   col-2 |   col-3 |
|---------|---------|---------|
|       1 |       2 |       3 |
|       2 |       4 |       6 |
|       3 |       6 |       9 |
|       4 |       8 |      12 |

|   col-1 |   col-2 |   col-3 |
|---------|---------|---------|
|       1 |       2 |       3 |
|       2 |       4 |       6 |
|       3 |       6 |       9 |
|       4 |       8 |      12 |

| first    | header   | header   |
|----------|----------|----------|
| first    | second   | third    |
| 1        | 2        | 3        |
| 3        | 4        | 5        |
| 3        | 6        | 7        |
| 8        | 9        | 9        |
| 10       | 9        | 9        |

| first (f)   | header (f)   | header (f)   |
|-------------|--------------|--------------|
| first (f)   | second       | third        |
| 1           | 2            | 3            |
| 3           | 4            | 5            |
| 3           | 6            | 7            |
| 8           | 9            | 9            |
| 10          | 9            | 9            |

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v2/test_01.asciidoc.md
================================================
# Sample Document Title

## Section 1

This is some introductory text in section 1.

## Subsection 1.1

- * First list item

- * Second list item

This is some introductory text in section 1.1.

- - A dash list item

## Section 2

This is some text in section 2.

| Header 1   | Header 2   |
|------------|------------|
| Value 1    | Value 2    |
| Value 3    | Value 4    |

================================================
File: tests/data/groundtruth/docling_v2/test_02.asciidoc.md
================================================
2nd Sample Document Title

This is an abstract.

 Section 1: Testing nestedlists

    - First item
    - Nested item 1
    - Nested item 2
    - Second item
    - Nested ordered item 1
    - Nested ordered item 2
    - Deeper nested unordered item
    - Third item
    - Nested ordered item 1
    - Nested ordered item 2
    - Deeper nested unordered item
    - Nested ordered item 2

 Section 2

bla bla

bla bla bla

 Section 3: test image

image::images/example1.png[Example Image, width=200, height=150, align=center]

.An example caption for the image

image::images/example2.png[Example Image, width=200, height=150, align=center]

 Section 4: test tables


| Header 1   | Header 2   |
|------------|------------|
| Value 1    | Value 2    |
| Value 3    | Value 4    |

.Caption for the table 1

|===


| Header 1   | Header 2   |
|------------|------------|
| Value 1    | Value 2    |
| Value 3    | Value 4    |

.Caption for the table 2

|===


| Column 1 Heading   | Column 2 Heading   | Column 3 Heading       |
|--------------------|--------------------|------------------------|
| Cell 1             | Cell 2             | Cell 3                 |
| Cell 4             | Cell 5 colspan=2   | Cell spans two columns |

.Caption for the table 3

|===


| Column 1 Heading   | Column 2 Heading   | Column 3 Heading   |
|--------------------|--------------------|--------------------|
| Rowspan=2          | Cell 2             | Cell 3             |
| Cell 5             | Cell 6             |                    |

.Caption for the table 4

|===


| Col 1               | Col 2                              | Col 3   | Col 4   |
|---------------------|------------------------------------|---------|---------|
| Rowspan=2.Colspan=2 | Cell spanning 2 rows and 2 columns | Col 3   | Col 4   |
| Col 3               | Col 4                              |         |         |
| Col 1               | Col 2                              | Col 3   | Col 4   |

 SubSubSection 2.1.1

================================================
File: tests/data/groundtruth/docling_v2/test_emf_docx.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: paragraph: Test with three images in unusual formats
  item-2 at level 1: paragraph: Raster in emf:
  item-3 at level 1: picture
  item-4 at level 1: paragraph: Vector in emf:
  item-5 at level 1: picture
  item-6 at level 1: paragraph: Raster in webp:
  item-7 at level 1: picture

================================================
File: tests/data/groundtruth/docling_v2/test_emf_docx.docx.md
================================================
Test with three images in unusual formats

Raster in emf:

<!-- image -->

Vector in emf:

<!-- image -->

Raster in webp:

<!-- image -->

================================================
File: tests/data/groundtruth/docling_v2/unit_test_01.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Title
    item-2 at level 2: section_header: section-1
      item-3 at level 3: section_header: section-1.1
    item-4 at level 2: section_header: section-2
      item-5 at level 3: section: group header-3
        item-6 at level 4: section_header: section-2.0.1
      item-7 at level 3: section_header: section-2.2
      item-8 at level 3: section_header: section-2.3

================================================
File: tests/data/groundtruth/docling_v2/unit_test_01.html.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "unit_test_01",
  "origin": {
    "mimetype": "text/html",
    "binary_hash": 11574357959810932112,
    "filename": "unit_test_01.html"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [
        {
          "$ref": "#/texts/4"
        }
      ],
      "content_layer": "body",
      "name": "header-3",
      "label": "section"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/3"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Title",
      "text": "Title"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/2"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-1",
      "text": "section-1",
      "level": 2
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-1.1",
      "text": "section-1.1",
      "level": 3
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/groups/0"
        },
        {
          "$ref": "#/texts/5"
        },
        {
          "$ref": "#/texts/6"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-2",
      "text": "section-2",
      "level": 2
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-2.0.1",
      "text": "section-2.0.1",
      "level": 4
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-2.2",
      "text": "section-2.2",
      "level": 3
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/texts/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "section-2.3",
      "text": "section-2.3",
      "level": 3
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "form_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/unit_test_01.html.md
================================================
# Title

## section-1

### section-1.1

## section-2

#### section-2.0.1

### section-2.2

### section-2.3

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Test Document
    item-2 at level 2: paragraph: 
    item-3 at level 2: section_header: Section 1
      item-4 at level 3: paragraph: 
      item-5 at level 3: paragraph: Paragraph 1.1
      item-6 at level 3: paragraph: 
      item-7 at level 3: paragraph: Paragraph 1.2
      item-8 at level 3: paragraph: 
      item-9 at level 3: section_header: Section 1.1
        item-10 at level 4: paragraph: 
        item-11 at level 4: paragraph: Paragraph 1.1.1
        item-12 at level 4: paragraph: 
        item-13 at level 4: paragraph: Paragraph 1.1.2
        item-14 at level 4: paragraph: 
      item-15 at level 3: section_header: Section 1.2
        item-16 at level 4: paragraph: 
        item-17 at level 4: paragraph: Paragraph 1.1.1
        item-18 at level 4: paragraph: 
        item-19 at level 4: paragraph: Paragraph 1.1.2
        item-20 at level 4: paragraph: 
        item-21 at level 4: section_header: Section 1.2.3
          item-22 at level 5: paragraph: 
          item-23 at level 5: paragraph: Paragraph 1.2.3.1
          item-24 at level 5: paragraph: 
          item-25 at level 5: paragraph: Paragraph 1.2.3.1
          item-26 at level 5: paragraph: 
          item-27 at level 5: paragraph: 
    item-28 at level 2: section_header: Section 2
      item-29 at level 3: paragraph: 
      item-30 at level 3: paragraph: Paragraph 2.1
      item-31 at level 3: paragraph: 
      item-32 at level 3: paragraph: Paragraph 2.2
      item-33 at level 3: paragraph: 
      item-34 at level 3: section: group header-2
        item-35 at level 4: section_header: Section 2.1.1
          item-36 at level 5: paragraph: 
          item-37 at level 5: paragraph: Paragraph 2.1.1.1
          item-38 at level 5: paragraph: 
          item-39 at level 5: paragraph: Paragraph 2.1.1.1
          item-40 at level 5: paragraph: 
      item-41 at level 3: section_header: Section 2.1
        item-42 at level 4: paragraph: 
        item-43 at level 4: paragraph: Paragraph 2.1.1
        item-44 at level 4: paragraph: 
        item-45 at level 4: paragraph: Paragraph 2.1.2
        item-46 at level 4: paragraph: 
        item-47 at level 4: paragraph: 

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers.docx.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "unit_test_headers",
  "origin": {
    "mimetype": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "binary_hash": 15606343257915737103,
    "filename": "unit_test_headers.docx"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [
        {
          "$ref": "#/texts/33"
        }
      ],
      "content_layer": "body",
      "name": "header-2",
      "label": "section"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/2"
        },
        {
          "$ref": "#/texts/27"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Test Document",
      "text": "Test Document"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/texts/5"
        },
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/texts/7"
        },
        {
          "$ref": "#/texts/8"
        },
        {
          "$ref": "#/texts/14"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1",
      "text": "Section 1",
      "level": 1
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1",
      "text": "Paragraph 1.1"
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2",
      "text": "Paragraph 1.2"
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/9"
        },
        {
          "$ref": "#/texts/10"
        },
        {
          "$ref": "#/texts/11"
        },
        {
          "$ref": "#/texts/12"
        },
        {
          "$ref": "#/texts/13"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.1",
      "text": "Section 1.1",
      "level": 2
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.1",
      "text": "Paragraph 1.1.1"
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.2",
      "text": "Paragraph 1.1.2"
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/14",
      "parent": {
        "$ref": "#/texts/2"
      },
      "children": [
        {
          "$ref": "#/texts/15"
        },
        {
          "$ref": "#/texts/16"
        },
        {
          "$ref": "#/texts/17"
        },
        {
          "$ref": "#/texts/18"
        },
        {
          "$ref": "#/texts/19"
        },
        {
          "$ref": "#/texts/20"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.2",
      "text": "Section 1.2",
      "level": 2
    },
    {
      "self_ref": "#/texts/15",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/16",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.1",
      "text": "Paragraph 1.1.1"
    },
    {
      "self_ref": "#/texts/17",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/18",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.2",
      "text": "Paragraph 1.1.2"
    },
    {
      "self_ref": "#/texts/19",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/20",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [
        {
          "$ref": "#/texts/21"
        },
        {
          "$ref": "#/texts/22"
        },
        {
          "$ref": "#/texts/23"
        },
        {
          "$ref": "#/texts/24"
        },
        {
          "$ref": "#/texts/25"
        },
        {
          "$ref": "#/texts/26"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.2.3",
      "text": "Section 1.2.3",
      "level": 3
    },
    {
      "self_ref": "#/texts/21",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/22",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2.3.1",
      "text": "Paragraph 1.2.3.1"
    },
    {
      "self_ref": "#/texts/23",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/24",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2.3.1",
      "text": "Paragraph 1.2.3.1"
    },
    {
      "self_ref": "#/texts/25",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/26",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/27",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/28"
        },
        {
          "$ref": "#/texts/29"
        },
        {
          "$ref": "#/texts/30"
        },
        {
          "$ref": "#/texts/31"
        },
        {
          "$ref": "#/texts/32"
        },
        {
          "$ref": "#/groups/0"
        },
        {
          "$ref": "#/texts/39"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2",
      "text": "Section 2",
      "level": 1
    },
    {
      "self_ref": "#/texts/28",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/29",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1",
      "text": "Paragraph 2.1"
    },
    {
      "self_ref": "#/texts/30",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/31",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.2",
      "text": "Paragraph 2.2"
    },
    {
      "self_ref": "#/texts/32",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/33",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [
        {
          "$ref": "#/texts/34"
        },
        {
          "$ref": "#/texts/35"
        },
        {
          "$ref": "#/texts/36"
        },
        {
          "$ref": "#/texts/37"
        },
        {
          "$ref": "#/texts/38"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2.1.1",
      "text": "Section 2.1.1",
      "level": 3
    },
    {
      "self_ref": "#/texts/34",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/35",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1.1",
      "text": "Paragraph 2.1.1.1"
    },
    {
      "self_ref": "#/texts/36",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/37",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1.1",
      "text": "Paragraph 2.1.1.1"
    },
    {
      "self_ref": "#/texts/38",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/39",
      "parent": {
        "$ref": "#/texts/27"
      },
      "children": [
        {
          "$ref": "#/texts/40"
        },
        {
          "$ref": "#/texts/41"
        },
        {
          "$ref": "#/texts/42"
        },
        {
          "$ref": "#/texts/43"
        },
        {
          "$ref": "#/texts/44"
        },
        {
          "$ref": "#/texts/45"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2.1",
      "text": "Section 2.1",
      "level": 2
    },
    {
      "self_ref": "#/texts/40",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/41",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1",
      "text": "Paragraph 2.1.1"
    },
    {
      "self_ref": "#/texts/42",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/43",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.2",
      "text": "Paragraph 2.1.2"
    },
    {
      "self_ref": "#/texts/44",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/45",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers.docx.md
================================================
# Test Document

## Section 1

Paragraph 1.1

Paragraph 1.2

### Section 1.1

Paragraph 1.1.1

Paragraph 1.1.2

### Section 1.2

Paragraph 1.1.1

Paragraph 1.1.2

#### Section 1.2.3

Paragraph 1.2.3.1

Paragraph 1.2.3.1

## Section 2

Paragraph 2.1

Paragraph 2.2

#### Section 2.1.1

Paragraph 2.1.1.1

Paragraph 2.1.1.1

### Section 2.1

Paragraph 2.1.1

Paragraph 2.1.2

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers_numbered.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: title: Test Document
    item-2 at level 2: paragraph: 
    item-3 at level 2: section_header: Section 1
  item-4 at level 1: paragraph: 
  item-5 at level 1: paragraph: Paragraph 1.1
  item-6 at level 1: paragraph: 
  item-7 at level 1: paragraph: Paragraph 1.2
  item-8 at level 1: paragraph: 
  item-9 at level 1: section: group header-0
    item-10 at level 2: section: group header-1
      item-11 at level 3: section_header: Section 1.1
        item-12 at level 4: paragraph: 
        item-13 at level 4: paragraph: Paragraph 1.1.1
        item-14 at level 4: paragraph: 
        item-15 at level 4: paragraph: Paragraph 1.1.2
        item-16 at level 4: paragraph: 
      item-17 at level 3: section_header: Section 1.2
        item-18 at level 4: paragraph: 
        item-19 at level 4: paragraph: Paragraph 1.1.1
        item-20 at level 4: paragraph: 
        item-21 at level 4: paragraph: Paragraph 1.1.2
        item-22 at level 4: paragraph: 
        item-23 at level 4: section_header: Section 1.2.3
          item-24 at level 5: paragraph: 
          item-25 at level 5: paragraph: Paragraph 1.2.3.1
          item-26 at level 5: paragraph: 
          item-27 at level 5: paragraph: Paragraph 1.2.3.1
          item-28 at level 5: paragraph: 
          item-29 at level 5: paragraph: 
    item-30 at level 2: section_header: Section 2
  item-31 at level 1: paragraph: 
  item-32 at level 1: paragraph: Paragraph 2.1
  item-33 at level 1: paragraph: 
  item-34 at level 1: paragraph: Paragraph 2.2
  item-35 at level 1: paragraph: 
  item-36 at level 1: section: group header-0
    item-37 at level 2: section: group header-1
      item-38 at level 3: section: group header-2
        item-39 at level 4: section_header: Section 2.1.1
          item-40 at level 5: paragraph: 
          item-41 at level 5: paragraph: Paragraph 2.1.1.1
          item-42 at level 5: paragraph: 
          item-43 at level 5: paragraph: Paragraph 2.1.1.1
          item-44 at level 5: paragraph: 
      item-45 at level 3: section_header: Section 2.1
        item-46 at level 4: paragraph: 
        item-47 at level 4: paragraph: Paragraph 2.1.1
        item-48 at level 4: paragraph: 
        item-49 at level 4: paragraph: Paragraph 2.1.2
        item-50 at level 4: paragraph: 
        item-51 at level 4: paragraph: 

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers_numbered.docx.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "unit_test_headers_numbered",
  "origin": {
    "mimetype": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "binary_hash": 7684538628968220703,
    "filename": "unit_test_headers_numbered.docx"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      },
      {
        "$ref": "#/texts/3"
      },
      {
        "$ref": "#/texts/4"
      },
      {
        "$ref": "#/texts/5"
      },
      {
        "$ref": "#/texts/6"
      },
      {
        "$ref": "#/texts/7"
      },
      {
        "$ref": "#/groups/0"
      },
      {
        "$ref": "#/texts/28"
      },
      {
        "$ref": "#/texts/29"
      },
      {
        "$ref": "#/texts/30"
      },
      {
        "$ref": "#/texts/31"
      },
      {
        "$ref": "#/texts/32"
      },
      {
        "$ref": "#/groups/2"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/groups/1"
        },
        {
          "$ref": "#/texts/27"
        }
      ],
      "content_layer": "body",
      "name": "header-0",
      "label": "section"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [
        {
          "$ref": "#/texts/8"
        },
        {
          "$ref": "#/texts/14"
        }
      ],
      "content_layer": "body",
      "name": "header-1",
      "label": "section"
    },
    {
      "self_ref": "#/groups/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/groups/3"
        }
      ],
      "content_layer": "body",
      "name": "header-0",
      "label": "section"
    },
    {
      "self_ref": "#/groups/3",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [
        {
          "$ref": "#/groups/4"
        },
        {
          "$ref": "#/texts/39"
        }
      ],
      "content_layer": "body",
      "name": "header-1",
      "label": "section"
    },
    {
      "self_ref": "#/groups/4",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [
        {
          "$ref": "#/texts/33"
        }
      ],
      "content_layer": "body",
      "name": "header-2",
      "label": "section"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/2"
        }
      ],
      "content_layer": "body",
      "label": "title",
      "prov": [],
      "orig": "Test Document",
      "text": "Test Document"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1",
      "text": "Section 1",
      "level": 1
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1",
      "text": "Paragraph 1.1"
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2",
      "text": "Paragraph 1.2"
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/texts/9"
        },
        {
          "$ref": "#/texts/10"
        },
        {
          "$ref": "#/texts/11"
        },
        {
          "$ref": "#/texts/12"
        },
        {
          "$ref": "#/texts/13"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.1",
      "text": "Section 1.1",
      "level": 2
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.1",
      "text": "Paragraph 1.1.1"
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.2",
      "text": "Paragraph 1.1.2"
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/texts/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/14",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/texts/15"
        },
        {
          "$ref": "#/texts/16"
        },
        {
          "$ref": "#/texts/17"
        },
        {
          "$ref": "#/texts/18"
        },
        {
          "$ref": "#/texts/19"
        },
        {
          "$ref": "#/texts/20"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.2",
      "text": "Section 1.2",
      "level": 2
    },
    {
      "self_ref": "#/texts/15",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/16",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.1",
      "text": "Paragraph 1.1.1"
    },
    {
      "self_ref": "#/texts/17",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/18",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.1.2",
      "text": "Paragraph 1.1.2"
    },
    {
      "self_ref": "#/texts/19",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/20",
      "parent": {
        "$ref": "#/texts/14"
      },
      "children": [
        {
          "$ref": "#/texts/21"
        },
        {
          "$ref": "#/texts/22"
        },
        {
          "$ref": "#/texts/23"
        },
        {
          "$ref": "#/texts/24"
        },
        {
          "$ref": "#/texts/25"
        },
        {
          "$ref": "#/texts/26"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 1.2.3",
      "text": "Section 1.2.3",
      "level": 3
    },
    {
      "self_ref": "#/texts/21",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/22",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2.3.1",
      "text": "Paragraph 1.2.3.1"
    },
    {
      "self_ref": "#/texts/23",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/24",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 1.2.3.1",
      "text": "Paragraph 1.2.3.1"
    },
    {
      "self_ref": "#/texts/25",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/26",
      "parent": {
        "$ref": "#/texts/20"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/27",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2",
      "text": "Section 2",
      "level": 1
    },
    {
      "self_ref": "#/texts/28",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/29",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1",
      "text": "Paragraph 2.1"
    },
    {
      "self_ref": "#/texts/30",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/31",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.2",
      "text": "Paragraph 2.2"
    },
    {
      "self_ref": "#/texts/32",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/33",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [
        {
          "$ref": "#/texts/34"
        },
        {
          "$ref": "#/texts/35"
        },
        {
          "$ref": "#/texts/36"
        },
        {
          "$ref": "#/texts/37"
        },
        {
          "$ref": "#/texts/38"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2.1.1",
      "text": "Section 2.1.1",
      "level": 3
    },
    {
      "self_ref": "#/texts/34",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/35",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1.1",
      "text": "Paragraph 2.1.1.1"
    },
    {
      "self_ref": "#/texts/36",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/37",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1.1",
      "text": "Paragraph 2.1.1.1"
    },
    {
      "self_ref": "#/texts/38",
      "parent": {
        "$ref": "#/texts/33"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/39",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [
        {
          "$ref": "#/texts/40"
        },
        {
          "$ref": "#/texts/41"
        },
        {
          "$ref": "#/texts/42"
        },
        {
          "$ref": "#/texts/43"
        },
        {
          "$ref": "#/texts/44"
        },
        {
          "$ref": "#/texts/45"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Section 2.1",
      "text": "Section 2.1",
      "level": 2
    },
    {
      "self_ref": "#/texts/40",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/41",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1",
      "text": "Paragraph 2.1.1"
    },
    {
      "self_ref": "#/texts/42",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/43",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.2",
      "text": "Paragraph 2.1.2"
    },
    {
      "self_ref": "#/texts/44",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/45",
      "parent": {
        "$ref": "#/texts/39"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/unit_test_headers_numbered.docx.md
================================================
# Test Document

## Section 1

Paragraph 1.1

Paragraph 1.2

### Section 1.1

Paragraph 1.1.1

Paragraph 1.1.2

### Section 1.2

Paragraph 1.1.1

Paragraph 1.1.2

#### Section 1.2.3

Paragraph 1.2.3.1

Paragraph 1.2.3.1

## Section 2

Paragraph 2.1

Paragraph 2.2

#### Section 2.1.1

Paragraph 2.1.1.1

Paragraph 2.1.1.1

### Section 2.1

Paragraph 2.1.1

Paragraph 2.1.2

================================================
File: tests/data/groundtruth/docling_v2/unit_test_lists.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: section: group header-0
    item-2 at level 2: section_header: Test Document
      item-3 at level 3: paragraph: 
      item-4 at level 3: paragraph: 
      item-5 at level 3: paragraph: Paragraph 2.1.1
      item-6 at level 3: paragraph: 
      item-7 at level 3: paragraph: Paragraph 2.1.2
      item-8 at level 3: paragraph: 
      item-9 at level 3: section: group header-2
        item-10 at level 4: section_header: Test 1:
          item-11 at level 5: list: group list
            item-12 at level 6: list_item: List item 1
            item-13 at level 6: list_item: List item 2
            item-14 at level 6: list_item: List item 3
          item-15 at level 5: paragraph: 
        item-16 at level 4: section_header: Test 2:
          item-17 at level 5: list: group list
            item-18 at level 6: list_item: List item a
            item-19 at level 6: list_item: List item b
            item-20 at level 6: list_item: List item c
          item-21 at level 5: paragraph: 
        item-22 at level 4: section_header: Test 3:
          item-23 at level 5: list: group list
            item-24 at level 6: list_item: List item 1
            item-25 at level 6: list_item: List item 2
            item-26 at level 6: list: group list
              item-27 at level 7: list_item: List item 1.1
              item-28 at level 7: list_item: List item 1.2
              item-29 at level 7: list_item: List item 1.3
            item-30 at level 6: list_item: List item 3
          item-31 at level 5: paragraph: 
        item-32 at level 4: section_header: Test 4:
          item-33 at level 5: list: group list
            item-34 at level 6: list_item: List item 1
            item-35 at level 6: list: group list
              item-36 at level 7: list_item: List item 1.1
            item-37 at level 6: list_item: List item 2
          item-38 at level 5: paragraph: 
        item-39 at level 4: section_header: Test 5:
          item-40 at level 5: list: group list
            item-41 at level 6: list_item: List item 1
            item-42 at level 6: list: group list
              item-43 at level 7: list_item: List item 1.1
              item-44 at level 7: list: group list
                item-45 at level 8: list_item: List item 1.1.1
            item-46 at level 6: list_item: List item 3
          item-47 at level 5: paragraph: 
        item-48 at level 4: section_header: Test 6:
          item-49 at level 5: list: group list
            item-50 at level 6: list_item: List item 1
            item-51 at level 6: list_item: List item 2
            item-52 at level 6: list: group list
              item-53 at level 7: list_item: List item 1.1
              item-54 at level 7: list_item: List item 1.2
              item-55 at level 7: list: group list
                item-56 at level 8: list_item: List item 1.2.1
            item-57 at level 6: list_item: List item 3
          item-58 at level 5: paragraph: 
          item-59 at level 5: paragraph: 
          item-60 at level 5: paragraph: 

================================================
File: tests/data/groundtruth/docling_v2/unit_test_lists.docx.json
================================================
{
  "schema_name": "DoclingDocument",
  "version": "1.1.0",
  "name": "unit_test_lists",
  "origin": {
    "mimetype": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "binary_hash": 13601004233111293776,
    "filename": "unit_test_lists.docx"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/groups/0"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/0"
        }
      ],
      "content_layer": "body",
      "name": "header-0",
      "label": "section"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [
        {
          "$ref": "#/texts/7"
        },
        {
          "$ref": "#/texts/12"
        },
        {
          "$ref": "#/texts/17"
        },
        {
          "$ref": "#/texts/25"
        },
        {
          "$ref": "#/texts/30"
        },
        {
          "$ref": "#/texts/36"
        }
      ],
      "content_layer": "body",
      "name": "header-2",
      "label": "section"
    },
    {
      "self_ref": "#/groups/2",
      "parent": {
        "$ref": "#/texts/7"
      },
      "children": [
        {
          "$ref": "#/texts/8"
        },
        {
          "$ref": "#/texts/9"
        },
        {
          "$ref": "#/texts/10"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/3",
      "parent": {
        "$ref": "#/texts/12"
      },
      "children": [
        {
          "$ref": "#/texts/13"
        },
        {
          "$ref": "#/texts/14"
        },
        {
          "$ref": "#/texts/15"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/4",
      "parent": {
        "$ref": "#/texts/17"
      },
      "children": [
        {
          "$ref": "#/texts/18"
        },
        {
          "$ref": "#/texts/19"
        },
        {
          "$ref": "#/groups/5"
        },
        {
          "$ref": "#/texts/23"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/5",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [
        {
          "$ref": "#/texts/20"
        },
        {
          "$ref": "#/texts/21"
        },
        {
          "$ref": "#/texts/22"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/6",
      "parent": {
        "$ref": "#/texts/25"
      },
      "children": [
        {
          "$ref": "#/texts/26"
        },
        {
          "$ref": "#/groups/7"
        },
        {
          "$ref": "#/texts/28"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/7",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [
        {
          "$ref": "#/texts/27"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/8",
      "parent": {
        "$ref": "#/texts/30"
      },
      "children": [
        {
          "$ref": "#/texts/31"
        },
        {
          "$ref": "#/groups/9"
        },
        {
          "$ref": "#/texts/34"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/9",
      "parent": {
        "$ref": "#/groups/8"
      },
      "children": [
        {
          "$ref": "#/texts/32"
        },
        {
          "$ref": "#/groups/10"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/10",
      "parent": {
        "$ref": "#/groups/9"
      },
      "children": [
        {
          "$ref": "#/texts/33"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/11",
      "parent": {
        "$ref": "#/texts/36"
      },
      "children": [
        {
          "$ref": "#/texts/37"
        },
        {
          "$ref": "#/texts/38"
        },
        {
          "$ref": "#/groups/12"
        },
        {
          "$ref": "#/texts/42"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/12",
      "parent": {
        "$ref": "#/groups/11"
      },
      "children": [
        {
          "$ref": "#/texts/39"
        },
        {
          "$ref": "#/texts/40"
        },
        {
          "$ref": "#/groups/13"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/13",
      "parent": {
        "$ref": "#/groups/12"
      },
      "children": [
        {
          "$ref": "#/texts/41"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [
        {
          "$ref": "#/texts/1"
        },
        {
          "$ref": "#/texts/2"
        },
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/texts/5"
        },
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/groups/1"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test Document",
      "text": "Test Document",
      "level": 1
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.1",
      "text": "Paragraph 2.1.1"
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "Paragraph 2.1.2",
      "text": "Paragraph 2.1.2"
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/texts/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/2"
        },
        {
          "$ref": "#/texts/11"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 1:",
      "text": "Test 1:",
      "level": 3
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1",
      "text": "List item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 2",
      "text": "List item 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 3",
      "text": "List item 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/texts/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/3"
        },
        {
          "$ref": "#/texts/16"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 2:",
      "text": "Test 2:",
      "level": 3
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item a",
      "text": "List item a",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/14",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item b",
      "text": "List item b",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/15",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item c",
      "text": "List item c",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/16",
      "parent": {
        "$ref": "#/texts/12"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/17",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/4"
        },
        {
          "$ref": "#/texts/24"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 3:",
      "text": "Test 3:",
      "level": 3
    },
    {
      "self_ref": "#/texts/18",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1",
      "text": "List item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/19",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 2",
      "text": "List item 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/20",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.1",
      "text": "List item 1.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/21",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.2",
      "text": "List item 1.2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/22",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.3",
      "text": "List item 1.3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/23",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 3",
      "text": "List item 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/24",
      "parent": {
        "$ref": "#/texts/17"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/25",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/6"
        },
        {
          "$ref": "#/texts/29"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 4:",
      "text": "Test 4:",
      "level": 3
    },
    {
      "self_ref": "#/texts/26",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1",
      "text": "List item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/27",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.1",
      "text": "List item 1.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/28",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 2",
      "text": "List item 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/29",
      "parent": {
        "$ref": "#/texts/25"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/30",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/8"
        },
        {
          "$ref": "#/texts/35"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 5:",
      "text": "Test 5:",
      "level": 3
    },
    {
      "self_ref": "#/texts/31",
      "parent": {
        "$ref": "#/groups/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1",
      "text": "List item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/32",
      "parent": {
        "$ref": "#/groups/9"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.1",
      "text": "List item 1.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/33",
      "parent": {
        "$ref": "#/groups/10"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.1.1",
      "text": "List item 1.1.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/34",
      "parent": {
        "$ref": "#/groups/8"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 3",
      "text": "List item 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/35",
      "parent": {
        "$ref": "#/texts/30"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/36",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [
        {
          "$ref": "#/groups/11"
        },
        {
          "$ref": "#/texts/43"
        },
        {
          "$ref": "#/texts/44"
        },
        {
          "$ref": "#/texts/45"
        }
      ],
      "content_layer": "body",
      "label": "section_header",
      "prov": [],
      "orig": "Test 6:",
      "text": "Test 6:",
      "level": 3
    },
    {
      "self_ref": "#/texts/37",
      "parent": {
        "$ref": "#/groups/11"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1",
      "text": "List item 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/38",
      "parent": {
        "$ref": "#/groups/11"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 2",
      "text": "List item 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/39",
      "parent": {
        "$ref": "#/groups/12"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.1",
      "text": "List item 1.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/40",
      "parent": {
        "$ref": "#/groups/12"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.2",
      "text": "List item 1.2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/41",
      "parent": {
        "$ref": "#/groups/13"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 1.2.1",
      "text": "List item 1.2.1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/42",
      "parent": {
        "$ref": "#/groups/11"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [],
      "orig": "List item 3",
      "text": "List item 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/43",
      "parent": {
        "$ref": "#/texts/36"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/44",
      "parent": {
        "$ref": "#/texts/36"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    },
    {
      "self_ref": "#/texts/45",
      "parent": {
        "$ref": "#/texts/36"
      },
      "children": [],
      "content_layer": "body",
      "label": "paragraph",
      "prov": [],
      "orig": "",
      "text": ""
    }
  ],
  "pictures": [],
  "tables": [],
  "key_value_items": [],
  "pages": {}
}

================================================
File: tests/data/groundtruth/docling_v2/unit_test_lists.docx.md
================================================
## Test Document

Paragraph 2.1.1

Paragraph 2.1.2

#### Test 1:

- List item 1
- List item 2
- List item 3

#### Test 2:

- List item a
- List item b
- List item c

#### Test 3:

- List item 1
- List item 2
    - List item 1.1
    - List item 1.2
    - List item 1.3
- List item 3

#### Test 4:

- List item 1
    - List item 1.1
- List item 2

#### Test 5:

- List item 1
    - List item 1.1
        - List item 1.1.1
- List item 3

#### Test 6:

- List item 1
- List item 2
    - List item 1.1
    - List item 1.2
        - List item 1.2.1
- List item 3

================================================
File: tests/data/groundtruth/docling_v2/wiki.md.md
================================================
# IBM

International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.

It is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.

IBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.

IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed "International Business Machines" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the IBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]

IBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s, IBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.

As one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming language, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing Awards.[16]

## 1910s–1950s

IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the Computing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington, D.C.; and Toronto, Canada.[22]

Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]: 105  He implemented sales conventions, "generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker".[25][26] His favorite slogan, "THINK", became a mantra for each company's employees.[25] During Watson's first four years, revenues reached $9 million ($158 million today) and the company's operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the clumsy hyphenated name "Computing-Tabulating-Recording Company" and chose to replace it with the more expansive title "International Business Machines" which had previously been used as the name of CTR's Canadian Division;[27] the name was changed on February 14, 1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.

## 1960s–1980s

In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.


================================================
File: tests/data/groundtruth/docling_v2/wiki_duck.html.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: paragraph: Main menu
  item-2 at level 1: paragraph: Navigation
  item-3 at level 1: list: group list
    item-4 at level 2: list_item: Main page
    item-5 at level 2: list_item: Contents
    item-6 at level 2: list_item: Current events
    item-7 at level 2: list_item: Random article
    item-8 at level 2: list_item: About Wikipedia
    item-9 at level 2: list_item: Contact us
  item-10 at level 1: paragraph: Contribute
  item-11 at level 1: list: group list
    item-12 at level 2: list_item: Help
    item-13 at level 2: list_item: Learn to edit
    item-14 at level 2: list_item: Community portal
    item-15 at level 2: list_item: Recent changes
    item-16 at level 2: list_item: Upload file
  item-17 at level 1: picture
  item-18 at level 1: picture
  item-19 at level 1: picture
  item-20 at level 1: list: group list
  item-21 at level 1: list: group list
    item-22 at level 2: list_item: Donate
  item-23 at level 1: list: group list
  item-24 at level 1: list: group list
    item-25 at level 2: list_item: Create account
    item-26 at level 2: list_item: Log in
  item-27 at level 1: list: group list
    item-28 at level 2: list_item: Create account
    item-29 at level 2: list_item: Log in
  item-30 at level 1: paragraph: Pages for logged out editors
  item-31 at level 1: list: group list
    item-32 at level 2: list_item: Contributions
    item-33 at level 2: list_item: Talk
  item-34 at level 1: section: group header-1
    item-35 at level 2: section_header: Contents
      item-36 at level 3: list: group list
        item-37 at level 4: list_item: (Top)
        item-38 at level 4: list_item: 1 Etymology
          item-39 at level 5: list: group list
        item-40 at level 4: list_item: 2 Taxonomy
          item-41 at level 5: list: group list
        item-42 at level 4: list_item: 3 Morphology
          item-43 at level 5: list: group list
        item-44 at level 4: list_item: 4 Distribution and habitat
          item-45 at level 5: list: group list
        item-46 at level 4: list_item: 5 Behaviour Toggle Behaviour subsection
          item-47 at level 5: list: group list
            item-48 at level 6: list_item: 5.1 Feeding
              item-49 at level 7: list: group list
            item-50 at level 6: list_item: 5.2 Breeding
              item-51 at level 7: list: group list
            item-52 at level 6: list_item: 5.3 Communication
              item-53 at level 7: list: group list
            item-54 at level 6: list_item: 5.4 Predators
              item-55 at level 7: list: group list
        item-56 at level 4: list_item: 6 Relationship with humans Toggle Relationship with humans subsection
          item-57 at level 5: list: group list
            item-58 at level 6: list_item: 6.1 Hunting
              item-59 at level 7: list: group list
            item-60 at level 6: list_item: 6.2 Domestication
              item-61 at level 7: list: group list
            item-62 at level 6: list_item: 6.3 Heraldry
              item-63 at level 7: list: group list
            item-64 at level 6: list_item: 6.4 Cultural references
              item-65 at level 7: list: group list
        item-66 at level 4: list_item: 7 See also
          item-67 at level 5: list: group list
        item-68 at level 4: list_item: 8 Notes Toggle Notes subsection
          item-69 at level 5: list: group list
            item-70 at level 6: list_item: 8.1 Citations
              item-71 at level 7: list: group list
            item-72 at level 6: list_item: 8.2 Sources
              item-73 at level 7: list: group list
        item-74 at level 4: list_item: 9 External links
          item-75 at level 5: list: group list
  item-76 at level 1: title: Duck
    item-77 at level 2: list: group list
      item-78 at level 3: list_item: Acèh
      item-79 at level 3: list_item: Afrikaans
      item-80 at level 3: list_item: Alemannisch
      item-81 at level 3: list_item: አማርኛ
      item-82 at level 3: list_item: Ænglisc
      item-83 at level 3: list_item: العربية
      item-84 at level 3: list_item: Aragonés
      item-85 at level 3: list_item: ܐܪܡܝܐ
      item-86 at level 3: list_item: Armãneashti
      item-87 at level 3: list_item: Asturianu
      item-88 at level 3: list_item: Atikamekw
      item-89 at level 3: list_item: Авар
      item-90 at level 3: list_item: Aymar aru
      item-91 at level 3: list_item: تۆرکجه
      item-92 at level 3: list_item: Basa Bali
      item-93 at level 3: list_item: বাংলা
      item-94 at level 3: list_item: 閩南語 / Bân-lâm-gú
      item-95 at level 3: list_item: Беларуская
      item-96 at level 3: list_item: Беларуская (тарашкевіца)
      item-97 at level 3: list_item: Bikol Central
      item-98 at level 3: list_item: Български
      item-99 at level 3: list_item: Brezhoneg
      item-100 at level 3: list_item: Буряад
      item-101 at level 3: list_item: Català
      item-102 at level 3: list_item: Чӑвашла
      item-103 at level 3: list_item: Čeština
      item-104 at level 3: list_item: ChiShona
      item-105 at level 3: list_item: Cymraeg
      item-106 at level 3: list_item: Dagbanli
      item-107 at level 3: list_item: Dansk
      item-108 at level 3: list_item: Deitsch
      item-109 at level 3: list_item: Deutsch
      item-110 at level 3: list_item: डोटेली
      item-111 at level 3: list_item: Ελληνικά
      item-112 at level 3: list_item: Emiliàn e rumagnòl
      item-113 at level 3: list_item: Español
      item-114 at level 3: list_item: Esperanto
      item-115 at level 3: list_item: Euskara
      item-116 at level 3: list_item: فارسی
      item-117 at level 3: list_item: Français
      item-118 at level 3: list_item: Gaeilge
      item-119 at level 3: list_item: Galego
      item-120 at level 3: list_item: ГӀалгӀай
      item-121 at level 3: list_item: 贛語
      item-122 at level 3: list_item: گیلکی
      item-123 at level 3: list_item: 𐌲𐌿𐍄𐌹𐍃𐌺
      item-124 at level 3: list_item: गोंयची कोंकणी / Gõychi Konknni
      item-125 at level 3: list_item: 客家語 / Hak-kâ-ngî
      item-126 at level 3: list_item: 한국어
      item-127 at level 3: list_item: Hausa
      item-128 at level 3: list_item: Հայերեն
      item-129 at level 3: list_item: हिन्दी
      item-130 at level 3: list_item: Hrvatski
      item-131 at level 3: list_item: Ido
      item-132 at level 3: list_item: Bahasa Indonesia
      item-133 at level 3: list_item: Iñupiatun
      item-134 at level 3: list_item: Íslenska
      item-135 at level 3: list_item: Italiano
      item-136 at level 3: list_item: עברית
      item-137 at level 3: list_item: Jawa
      item-138 at level 3: list_item: ಕನ್ನಡ
      item-139 at level 3: list_item: Kapampangan
      item-140 at level 3: list_item: ქართული
      item-141 at level 3: list_item: कॉशुर / کٲشُر
      item-142 at level 3: list_item: Қазақша
      item-143 at level 3: list_item: Ikirundi
      item-144 at level 3: list_item: Kongo
      item-145 at level 3: list_item: Kreyòl ayisyen
      item-146 at level 3: list_item: Кырык мары
      item-147 at level 3: list_item: ລາວ
      item-148 at level 3: list_item: Latina
      item-149 at level 3: list_item: Latviešu
      item-150 at level 3: list_item: Lietuvių
      item-151 at level 3: list_item: Li Niha
      item-152 at level 3: list_item: Ligure
      item-153 at level 3: list_item: Limburgs
      item-154 at level 3: list_item: Lingála
      item-155 at level 3: list_item: Malagasy
      item-156 at level 3: list_item: മലയാളം
      item-157 at level 3: list_item: मराठी
      item-158 at level 3: list_item: مازِرونی
      item-159 at level 3: list_item: Bahasa Melayu
      item-160 at level 3: list_item: ꯃꯤꯇꯩ ꯂꯣꯟ
      item-161 at level 3: list_item: 閩東語 / Mìng-dĕ̤ng-ngṳ̄
      item-162 at level 3: list_item: Мокшень
      item-163 at level 3: list_item: Монгол
      item-164 at level 3: list_item: မြန်မာဘာသာ
      item-165 at level 3: list_item: Nederlands
      item-166 at level 3: list_item: Nedersaksies
      item-167 at level 3: list_item: नेपाली
      item-168 at level 3: list_item: नेपाल भाषा
      item-169 at level 3: list_item: 日本語
      item-170 at level 3: list_item: Нохчийн
      item-171 at level 3: list_item: Norsk nynorsk
      item-172 at level 3: list_item: Occitan
      item-173 at level 3: list_item: Oromoo
      item-174 at level 3: list_item: ਪੰਜਾਬੀ
      item-175 at level 3: list_item: Picard
      item-176 at level 3: list_item: Plattdüütsch
      item-177 at level 3: list_item: Polski
      item-178 at level 3: list_item: Português
      item-179 at level 3: list_item: Qırımtatarca
      item-180 at level 3: list_item: Română
      item-181 at level 3: list_item: Русский
      item-182 at level 3: list_item: Саха тыла
      item-183 at level 3: list_item: ᱥᱟᱱᱛᱟᱲᱤ
      item-184 at level 3: list_item: Sardu
      item-185 at level 3: list_item: Scots
      item-186 at level 3: list_item: Seeltersk
      item-187 at level 3: list_item: Shqip
      item-188 at level 3: list_item: Sicilianu
      item-189 at level 3: list_item: සිංහල
      item-190 at level 3: list_item: Simple English
      item-191 at level 3: list_item: سنڌي
      item-192 at level 3: list_item: کوردی
      item-193 at level 3: list_item: Српски / srpski
      item-194 at level 3: list_item: Srpskohrvatski / српскохрватски
      item-195 at level 3: list_item: Sunda
      item-196 at level 3: list_item: Svenska
      item-197 at level 3: list_item: Tagalog
      item-198 at level 3: list_item: தமிழ்
      item-199 at level 3: list_item: Taqbaylit
      item-200 at level 3: list_item: Татарча / tatarça
      item-201 at level 3: list_item: ไทย
      item-202 at level 3: list_item: Türkçe
      item-203 at level 3: list_item: Українська
      item-204 at level 3: list_item: ئۇيغۇرچە / Uyghurche
      item-205 at level 3: list_item: Vahcuengh
      item-206 at level 3: list_item: Tiếng Việt
      item-207 at level 3: list_item: Walon
      item-208 at level 3: list_item: 文言
      item-209 at level 3: list_item: Winaray
      item-210 at level 3: list_item: 吴语
      item-211 at level 3: list_item: 粵語
      item-212 at level 3: list_item: Žemaitėška
      item-213 at level 3: list_item: 中文
    item-214 at level 2: list: group list
      item-215 at level 3: list_item: Article
      item-216 at level 3: list_item: Talk
    item-217 at level 2: list: group list
    item-218 at level 2: list: group list
      item-219 at level 3: list_item: Read
      item-220 at level 3: list_item: View source
      item-221 at level 3: list_item: View history
    item-222 at level 2: paragraph: Tools
    item-223 at level 2: paragraph: Actions
    item-224 at level 2: list: group list
      item-225 at level 3: list_item: Read
      item-226 at level 3: list_item: View source
      item-227 at level 3: list_item: View history
    item-228 at level 2: paragraph: General
    item-229 at level 2: list: group list
      item-230 at level 3: list_item: What links here
      item-231 at level 3: list_item: Related changes
      item-232 at level 3: list_item: Upload file
      item-233 at level 3: list_item: Special pages
      item-234 at level 3: list_item: Permanent link
      item-235 at level 3: list_item: Page information
      item-236 at level 3: list_item: Cite this page
      item-237 at level 3: list_item: Get shortened URL
      item-238 at level 3: list_item: Download QR code
      item-239 at level 3: list_item: Wikidata item
    item-240 at level 2: paragraph: Print/export
    item-241 at level 2: list: group list
      item-242 at level 3: list_item: Download as PDF
      item-243 at level 3: list_item: Printable version
    item-244 at level 2: paragraph: In other projects
    item-245 at level 2: list: group list
      item-246 at level 3: list_item: Wikimedia Commons
      item-247 at level 3: list_item: Wikiquote
    item-248 at level 2: paragraph: Appearance
    item-249 at level 2: picture
    item-250 at level 2: paragraph: From Wikipedia, the free encyclopedia
    item-251 at level 2: paragraph: Common name for many species of bird
    item-252 at level 2: paragraph: This article is about the bird.  ... as a food, see . For other uses, see .
    item-253 at level 2: paragraph: "Duckling" redirects here. For other uses, see .
    item-254 at level 2: table with [13x2]
    item-255 at level 2: paragraph: Duck is the common name for nume ... und in both fresh water and sea water.
    item-256 at level 2: paragraph: Ducks are sometimes confused wit ...  divers, grebes, gallinules and coots.
    item-257 at level 2: section_header: Etymology
      item-258 at level 3: paragraph: The word duck comes from Old Eng ... h duiken and German tauchen 'to dive'.
      item-259 at level 3: picture
        item-259 at level 4: caption: Pacific black duck displaying the characteristic upending "duck"
      item-260 at level 3: paragraph: This word replaced Old English e ... nskrit ātí 'water bird', among others.
      item-261 at level 3: paragraph: A duckling is a young duck in do ... , is sometimes labelled as a duckling.
      item-262 at level 3: paragraph: A male is called a drake and the ... a duck, or in ornithology a hen.[3][4]
      item-263 at level 3: picture
        item-263 at level 4: caption: Male mallard.
      item-264 at level 3: picture
        item-264 at level 4: caption: Wood ducks.
    item-265 at level 2: section_header: Taxonomy
      item-266 at level 3: paragraph: All ducks belong to the biologic ... ationships between various species.[9]
      item-267 at level 3: picture
        item-267 at level 4: caption: Mallard landing in approach
      item-268 at level 3: paragraph: In most modern classifications,  ... all size and stiff, upright tails.[14]
      item-269 at level 3: paragraph: A number of other species called ...  shelducks in the tribe Tadornini.[15]
    item-270 at level 2: section_header: Morphology
      item-271 at level 3: picture
        item-271 at level 4: caption: Male Mandarin duck
      item-272 at level 3: paragraph: The overall body plan of ducks i ... is moult typically precedes migration.
      item-273 at level 3: paragraph: The drakes of northern species o ... rkscrew shaped vagina to prevent rape.
    item-274 at level 2: section_header: Distribution and habitat
      item-275 at level 3: picture
        item-275 at level 4: caption: Flying steamer ducks in Ushuaia, Argentina
      item-276 at level 3: paragraph: Ducks have a cosmopolitan distri ... endemic to such far-flung islands.[21]
      item-277 at level 3: picture
        item-277 at level 4: caption: Female mallard in Cornwall, England
      item-278 at level 3: paragraph: Some duck species, mainly those  ... t form after localised heavy rain.[23]
    item-279 at level 2: section_header: Behaviour
      item-280 at level 3: section_header: Feeding
        item-281 at level 4: picture
          item-281 at level 5: caption: Pecten along the bill
        item-282 at level 4: picture
          item-282 at level 5: caption: Mallard duckling preening
        item-283 at level 4: paragraph: Ducks eat food sources such as g ... amphibians, worms, and small molluscs.
        item-284 at level 4: paragraph: Dabbling ducks feed on the surfa ... thers and to hold slippery food items.
        item-285 at level 4: paragraph: Diving ducks and sea ducks forag ... ave more difficulty taking off to fly.
        item-286 at level 4: paragraph: A few specialized species such a ... apted to catch and swallow large fish.
        item-287 at level 4: paragraph: The others have the characterist ... e nostrils come out through hard horn.
        item-288 at level 4: paragraph: The Guardian published an articl ...  the ducks and pollutes waterways.[25]
      item-289 at level 3: section_header: Breeding
        item-290 at level 4: picture
          item-290 at level 5: caption: A Muscovy duckling
        item-291 at level 4: paragraph: Ducks generally only have one pa ... st and led her ducklings to water.[28]
      item-292 at level 3: section_header: Communication
        item-293 at level 4: paragraph: Female mallard ducks (as well as ... laying calls or quieter contact calls.
        item-294 at level 4: paragraph: A common urban legend claims tha ... annel television show MythBusters.[32]
      item-295 at level 3: section_header: Predators
        item-296 at level 4: picture
          item-296 at level 5: caption: Ringed teal
        item-297 at level 4: paragraph: Ducks have many predators. Duckl ... or large birds, such as hawks or owls.
        item-298 at level 4: paragraph: Adult ducks are fast fliers, but ... its speed and strength to catch ducks.
    item-299 at level 2: section_header: Relationship with humans
      item-300 at level 3: section_header: Hunting
        item-301 at level 4: paragraph: Humans have hunted ducks since p ...  evidence of this is uncommon.[35][42]
        item-302 at level 4: paragraph: In many areas, wild ducks (inclu ... inated by pollutants such as PCBs.[44]
      item-303 at level 3: section_header: Domestication
        item-304 at level 4: picture
          item-304 at level 5: caption: Indian Runner ducks, a common breed of domestic ducks
        item-305 at level 4: paragraph: Ducks have many economic uses, b ... it weighs less than 1 kg (2.2 lb).[48]
      item-306 at level 3: section_header: Heraldry
        item-307 at level 4: picture
          item-307 at level 5: caption: Three black-colored ducks in the coat of arms of Maaninka[49]
        item-308 at level 4: paragraph: Ducks appear on several coats of ... the coat of arms of Föglö (Åland).[51]
      item-309 at level 3: section_header: Cultural references
        item-310 at level 4: paragraph: In 2002, psychologist Richard Wi ... 54] and was made into a movie in 1986.
        item-311 at level 4: paragraph: The 1992 Disney film The Mighty  ...  Ducks minor league baseball team.[55]
    item-312 at level 2: section_header: See also
      item-313 at level 3: list: group list
        item-314 at level 4: list_item: Birds portal
      item-315 at level 3: list: group list
        item-316 at level 4: list_item: Domestic duck
        item-317 at level 4: list_item: Duck as food
        item-318 at level 4: list_item: Duck test
        item-319 at level 4: list_item: Duck breeds
        item-320 at level 4: list_item: Fictional ducks
        item-321 at level 4: list_item: Rubber duck
    item-322 at level 2: section_header: Notes
      item-323 at level 3: section_header: Citations
        item-324 at level 4: ordered_list: group ordered list
          item-325 at level 5: list_item: ^ "Duckling". The American Herit ... n Company. 2006. Retrieved 2015-05-22.
          item-326 at level 5: list_item: ^ "Duckling". Kernerman English  ...  Ltd. 2000–2006. Retrieved 2015-05-22.
          item-327 at level 5: list_item: ^ Dohner, Janet Vorwald (2001).  ... University Press. ISBN 978-0300138139.
          item-328 at level 5: list_item: ^ Visca, Curt; Visca, Kelley (20 ...  Publishing Group. ISBN 9780823961566.
          item-329 at level 5: list_item: ^ a b c d Carboneras 1992, p. 536.
          item-330 at level 5: list_item: ^ Livezey 1986, pp. 737–738.
          item-331 at level 5: list_item: ^ Madsen, McHugh & de Kloet 1988, p. 452.
          item-332 at level 5: list_item: ^ Donne-Goussé, Laudet & Hänni 2002, pp. 353–354.
          item-333 at level 5: list_item: ^ a b c d e f Carboneras 1992, p. 540.
          item-334 at level 5: list_item: ^ Elphick, Dunning & Sibley 2001, p. 191.
          item-335 at level 5: list_item: ^ Kear 2005, p. 448.
          item-336 at level 5: list_item: ^ Kear 2005, p. 622–623.
          item-337 at level 5: list_item: ^ Kear 2005, p. 686.
          item-338 at level 5: list_item: ^ Elphick, Dunning & Sibley 2001, p. 193.
          item-339 at level 5: list_item: ^ a b c d e f g Carboneras 1992, p. 537.
          item-340 at level 5: list_item: ^ American Ornithologists' Union 1998, p. xix.
          item-341 at level 5: list_item: ^ American Ornithologists' Union 1998.
          item-342 at level 5: list_item: ^ Carboneras 1992, p. 538.
          item-343 at level 5: list_item: ^ Christidis & Boles 2008, p. 62.
          item-344 at level 5: list_item: ^ Shirihai 2008, pp. 239, 245.
          item-345 at level 5: list_item: ^ a b Pratt, Bruner & Berrett 1987, pp. 98–107.
          item-346 at level 5: list_item: ^ Fitter, Fitter & Hosking 2000, pp. 52–3.
          item-347 at level 5: list_item: ^ "Pacific Black Duck". www.wiresnr.org. Retrieved 2018-04-27.
          item-348 at level 5: list_item: ^ Ogden, Evans. "Dabbling Ducks". CWE. Retrieved 2006-11-02.
          item-349 at level 5: list_item: ^ Karl Mathiesen (16 March 2015) ...  Guardian. Retrieved 13 November 2016.
          item-350 at level 5: list_item: ^ Rohwer, Frank C.; Anderson, Mi ... 4615-6787-5_4. ISBN 978-1-4615-6789-9.
          item-351 at level 5: list_item: ^ Smith, Cyndi M.; Cooke, Fred;  ... 093/condor/102.1.201. hdl:10315/13797.
          item-352 at level 5: list_item: ^ "If You Find An Orphaned Duckl ... l on 2018-09-23. Retrieved 2018-12-22.
          item-353 at level 5: list_item: ^ Carver, Heather (2011). The Du ...  9780557901562.[self-published source]
          item-354 at level 5: list_item: ^ Titlow, Budd (2013-09-03). Bir ... man & Littlefield. ISBN 9780762797707.
          item-355 at level 5: list_item: ^ Amos, Jonathan (2003-09-08). " ... kers". BBC News. Retrieved 2006-11-02.
          item-356 at level 5: list_item: ^ "Mythbusters Episode 8". 12 December 2003.
          item-357 at level 5: list_item: ^ Erlandson 1994, p. 171.
          item-358 at level 5: list_item: ^ Jeffries 2008, pp. 168, 243.
          item-359 at level 5: list_item: ^ a b Sued-Badillo 2003, p. 65.
          item-360 at level 5: list_item: ^ Thorpe 1996, p. 68.
          item-361 at level 5: list_item: ^ Maisels 1999, p. 42.
          item-362 at level 5: list_item: ^ Rau 1876, p. 133.
          item-363 at level 5: list_item: ^ Higman 2012, p. 23.
          item-364 at level 5: list_item: ^ Hume 2012, p. 53.
          item-365 at level 5: list_item: ^ Hume 2012, p. 52.
          item-366 at level 5: list_item: ^ Fieldhouse 2002, p. 167.
          item-367 at level 5: list_item: ^ Livingston, A. D. (1998-01-01) ... Editions, Limited. ISBN 9781853263774.
          item-368 at level 5: list_item: ^ "Study plan for waterfowl inju ...  on 2022-10-09. Retrieved 2 July 2019.
          item-369 at level 5: list_item: ^ "FAOSTAT". www.fao.org. Retrieved 2019-10-25.
          item-370 at level 5: list_item: ^ "Anas platyrhynchos, Domestic  ... . Digimorph.org. Retrieved 2012-12-23.
          item-371 at level 5: list_item: ^ Sy Montgomery. "Mallard; Encyc ...  Britannica.com. Retrieved 2012-12-23.
          item-372 at level 5: list_item: ^ Glenday, Craig (2014). Guinnes ... ited. pp. 135. ISBN 978-1-908843-15-9.
          item-373 at level 5: list_item: ^ Suomen kunnallisvaakunat (in F ... tto. 1982. p. 147. ISBN 951-773-085-3.
          item-374 at level 5: list_item: ^ "Lubānas simbolika" (in Latvian). Retrieved September 9, 2021.
          item-375 at level 5: list_item: ^ "Föglö" (in Swedish). Retrieved September 9, 2021.
          item-376 at level 5: list_item: ^ Young, Emma. "World's funniest ... w Scientist. Retrieved 7 January 2019.
          item-377 at level 5: list_item: ^ "Howard the Duck (character)". Grand Comics Database.
          item-378 at level 5: list_item: ^ Sanderson, Peter; Gilbert, Lau ... luding this bad-tempered talking duck.
          item-379 at level 5: list_item: ^ "The Duck". University of Oregon Athletics. Retrieved 2022-01-20.
      item-380 at level 3: section_header: Sources
        item-381 at level 4: list: group list
          item-382 at level 5: list_item: American Ornithologists' Union ( ... (PDF) from the original on 2022-10-09.
          item-383 at level 5: list_item: Carboneras, Carlos (1992). del H ... Lynx Edicions. ISBN 978-84-87334-10-8.
          item-384 at level 5: list_item: Christidis, Les; Boles, Walter E ... ro Publishing. ISBN 978-0-643-06511-6.
          item-385 at level 5: list_item: Donne-Goussé, Carole; Laudet, Vi ... /S1055-7903(02)00019-2. PMID 12099792.
          item-386 at level 5: list_item: Elphick, Chris; Dunning, John B. ... istopher Helm. ISBN 978-0-7136-6250-4.
          item-387 at level 5: list_item: Erlandson, Jon M. (1994). Early  ... usiness Media. ISBN 978-1-4419-3231-0.
          item-388 at level 5: list_item: Fieldhouse, Paul (2002). Food, F ... ara: ABC-CLIO. ISBN 978-1-61069-412-4.
          item-389 at level 5: list_item: Fitter, Julian; Fitter, Daniel;  ... versity Press. ISBN 978-0-691-10295-5.
          item-390 at level 5: list_item: Higman, B. W. (2012). How Food M ...  Wiley & Sons. ISBN 978-1-4051-8947-7.
          item-391 at level 5: list_item: Hume, Julian H. (2012). Extinct  ... istopher Helm. ISBN 978-1-4729-3744-5.
          item-392 at level 5: list_item: Jeffries, Richard (2008). Holoce ... Alabama Press. ISBN 978-0-8173-1658-7.
          item-393 at level 5: list_item: Kear, Janet, ed. (2005). Ducks,  ... versity Press. ISBN 978-0-19-861009-0.
          item-394 at level 5: list_item: Livezey, Bradley C. (October 198 ... (PDF) from the original on 2022-10-09.
          item-395 at level 5: list_item: Madsen, Cort S.; McHugh, Kevin P ... (PDF) from the original on 2022-10-09.
          item-396 at level 5: list_item: Maisels, Charles Keith (1999). E ... on: Routledge. ISBN 978-0-415-10975-8.
          item-397 at level 5: list_item: Pratt, H. Douglas; Bruner, Phill ...  University Press. ISBN 0-691-02399-9.
          item-398 at level 5: list_item: Rau, Charles (1876). Early Man i ... ork: Harper & Brothers. LCCN 05040168.
          item-399 at level 5: list_item: Shirihai, Hadoram (2008). A Comp ... versity Press. ISBN 978-0-691-13666-0.
          item-400 at level 5: list_item: Sued-Badillo, Jalil (2003). Auto ... Paris: UNESCO. ISBN 978-92-3-103832-7.
          item-401 at level 5: list_item: Thorpe, I. J. (1996). The Origin ... rk: Routledge. ISBN 978-0-415-08009-5.
    item-402 at level 2: section_header: External links
      item-403 at level 3: list: group list
        item-404 at level 4: list_item: Definitions from Wiktionary
        item-405 at level 4: list_item: Media from Commons
        item-406 at level 4: list_item: Quotations from Wikiquote
        item-407 at level 4: list_item: Recipes from Wikibooks
        item-408 at level 4: list_item: Taxa from Wikispecies
        item-409 at level 4: list_item: Data from Wikidata
      item-410 at level 3: list: group list
        item-411 at level 4: list_item: list of books (useful looking abstracts)
        item-412 at level 4: list_item: Ducks on postage stamps Archived 2013-05-13 at the Wayback Machine
        item-413 at level 4: list_item: Ducks at a Distance, by Rob Hine ... uide to identification of US waterfowl
      item-414 at level 3: table with [3x2]
      item-415 at level 3: picture
      item-416 at level 3: paragraph: Retrieved from ""
      item-417 at level 3: paragraph: :
      item-418 at level 3: list: group list
        item-419 at level 4: list_item: Ducks
        item-420 at level 4: list_item: Game birds
        item-421 at level 4: list_item: Bird common names
      item-422 at level 3: paragraph: Hidden categories:
      item-423 at level 3: list: group list
        item-424 at level 4: list_item: All accuracy disputes
        item-425 at level 4: list_item: Accuracy disputes from February 2020
        item-426 at level 4: list_item: CS1 Finnish-language sources (fi)
        item-427 at level 4: list_item: CS1 Latvian-language sources (lv)
        item-428 at level 4: list_item: CS1 Swedish-language sources (sv)
        item-429 at level 4: list_item: Articles with short description
        item-430 at level 4: list_item: Short description is different from Wikidata
        item-431 at level 4: list_item: Wikipedia indefinitely move-protected pages
        item-432 at level 4: list_item: Wikipedia indefinitely semi-protected pages
        item-433 at level 4: list_item: Articles with 'species' microformats
        item-434 at level 4: list_item: Articles containing Old English (ca. 450-1100)-language text
        item-435 at level 4: list_item: Articles containing Dutch-language text
        item-436 at level 4: list_item: Articles containing German-language text
        item-437 at level 4: list_item: Articles containing Norwegian-language text
        item-438 at level 4: list_item: Articles containing Lithuanian-language text
        item-439 at level 4: list_item: Articles containing Ancient Greek (to 1453)-language text
        item-440 at level 4: list_item: All articles with self-published sources
        item-441 at level 4: list_item: Articles with self-published sources from February 2020
        item-442 at level 4: list_item: All articles with unsourced statements
        item-443 at level 4: list_item: Articles with unsourced statements from January 2022
        item-444 at level 4: list_item: CS1: long volume value
        item-445 at level 4: list_item: Pages using Sister project links with wikidata mismatch
        item-446 at level 4: list_item: Pages using Sister project links with hidden wikidata
        item-447 at level 4: list_item: Webarchive template wayback links
        item-448 at level 4: list_item: Articles with Project Gutenberg links
        item-449 at level 4: list_item: Articles containing video clips
      item-450 at level 3: list: group list
        item-451 at level 4: list_item: This page was last edited on 21 September 2024, at 12:11 (UTC).
        item-452 at level 4: list_item: Text is available under the Crea ... tion, Inc., a non-profit organization.
      item-453 at level 3: list: group list
        item-454 at level 4: list_item: Privacy policy
        item-455 at level 4: list_item: About Wikipedia
        item-456 at level 4: list_item: Disclaimers
        item-457 at level 4: list_item: Contact Wikipedia
        item-458 at level 4: list_item: Code of Conduct
        item-459 at level 4: list_item: Developers
        item-460 at level 4: list_item: Statistics
        item-461 at level 4: list_item: Cookie statement
        item-462 at level 4: list_item: Mobile view
      item-463 at level 3: list: group list
      item-464 at level 3: list: group list
  item-465 at level 1: caption: Pacific black duck displaying the characteristic upending "duck"
  item-466 at level 1: caption: Male mallard.
  item-467 at level 1: caption: Wood ducks.
  item-468 at level 1: caption: Mallard landing in approach
  item-469 at level 1: caption: Male Mandarin duck
  item-470 at level 1: caption: Flying steamer ducks in Ushuaia, Argentina
  item-471 at level 1: caption: Female mallard in Cornwall, England
  item-472 at level 1: caption: Pecten along the bill
  item-473 at level 1: caption: Mallard duckling preening
  item-474 at level 1: caption: A Muscovy duckling
  item-475 at level 1: caption: Ringed teal
  item-476 at level 1: caption: Indian Runner ducks, a common breed of domestic ducks
  item-477 at level 1: caption: Three black-colored ducks in the coat of arms of Maaninka[49]

================================================
File: tests/data/groundtruth/docling_v2/wiki_duck.html.md
================================================
Main menu

Navigation

- Main page
- Contents
- Current events
- Random article
- About Wikipedia
- Contact us

Contribute

- Help
- Learn to edit
- Community portal
- Recent changes
- Upload file

<!-- image -->

<!-- image -->

<!-- image -->

    - Donate
        - Create account
        - Log in
        - Create account
        - Log in

Pages for logged out editors

        - Contributions
        - Talk

## Contents

        - (Top)
        - 1 Etymology
        - 2 Taxonomy
        - 3 Morphology
        - 4 Distribution and habitat
        - 5 Behaviour Toggle Behaviour subsection
            - 5.1 Feeding
            - 5.2 Breeding
            - 5.3 Communication
            - 5.4 Predators
    - 6 Relationship with humans Toggle Relationship with humans subsection
        - 6.1 Hunting
        - 6.2 Domestication
        - 6.3 Heraldry
        - 6.4 Cultural references
- 7 See also
- 8 Notes Toggle Notes subsection
    - 8.1 Citations
    - 8.2 Sources
- 9 External links

# Duck

- Acèh
- Afrikaans
- Alemannisch
- አማርኛ
- Ænglisc
- العربية
- Aragonés
- ܐܪܡܝܐ
- Armãneashti
- Asturianu
- Atikamekw
- Авар
- Aymar aru
- تۆرکجه
- Basa Bali
- বাংলা
- 閩南語 / Bân-lâm-gú
- Беларуская
- Беларуская (тарашкевіца)
- Bikol Central
- Български
- Brezhoneg
- Буряад
- Català
- Чӑвашла
- Čeština
- ChiShona
- Cymraeg
- Dagbanli
- Dansk
- Deitsch
- Deutsch
- डोटेली
- Ελληνικά
- Emiliàn e rumagnòl
- Español
- Esperanto
- Euskara
- فارسی
- Français
- Gaeilge
- Galego
- ГӀалгӀай
- 贛語
- گیلکی
- 𐌲𐌿𐍄𐌹𐍃𐌺
- गोंयची कोंकणी / Gõychi Konknni
- 客家語 / Hak-kâ-ngî
- 한국어
- Hausa
- Հայերեն
- हिन्दी
- Hrvatski
- Ido
- Bahasa Indonesia
- Iñupiatun
- Íslenska
- Italiano
- עברית
- Jawa
- ಕನ್ನಡ
- Kapampangan
- ქართული
- कॉशुर / کٲشُر
- Қазақша
- Ikirundi
- Kongo
- Kreyòl ayisyen
- Кырык мары
- ລາວ
- Latina
- Latviešu
- Lietuvių
- Li Niha
- Ligure
- Limburgs
- Lingála
- Malagasy
- മലയാളം
- मराठी
- مازِرونی
- Bahasa Melayu
- ꯃꯤꯇꯩ ꯂꯣꯟ
- 閩東語 / Mìng-dĕ̤ng-ngṳ̄
- Мокшень
- Монгол
- မြန်မာဘာသာ
- Nederlands
- Nedersaksies
- नेपाली
- नेपाल भाषा
- 日本語
- Нохчийн
- Norsk nynorsk
- Occitan
- Oromoo
- ਪੰਜਾਬੀ
- Picard
- Plattdüütsch
- Polski
- Português
- Qırımtatarca
- Română
- Русский
- Саха тыла
- ᱥᱟᱱᱛᱟᱲᱤ
- Sardu
- Scots
- Seeltersk
- Shqip
- Sicilianu
- සිංහල
- Simple English
- سنڌي
- کوردی
- Српски / srpski
- Srpskohrvatski / српскохрватски
- Sunda
- Svenska
- Tagalog
- தமிழ்
- Taqbaylit
- Татарча / tatarça
- ไทย
- Türkçe
- Українська
- ئۇيغۇرچە / Uyghurche
- Vahcuengh
- Tiếng Việt
- Walon
- 文言
- Winaray
- 吴语
- 粵語
- Žemaitėška
- 中文

- Article
- Talk

    - Read
    - View source
    - View history

Tools

Actions

    - Read
    - View source
    - View history

General

    - What links here
    - Related changes
    - Upload file
    - Special pages
    - Permanent link
    - Page information
    - Cite this page
    - Get shortened URL
    - Download QR code
    - Wikidata item

Print/export

    - Download as PDF
    - Printable version

In other projects

    - Wikimedia Commons
    - Wikiquote

Appearance

<!-- image -->

From Wikipedia, the free encyclopedia

Common name for many species of bird

This article is about the bird. For duck as a food, see . For other uses, see .

"Duckling" redirects here. For other uses, see .

| Duck                           | Duck                           |
|--------------------------------|--------------------------------|
|                                |                                |
| Bufflehead (Bucephala albeola) | Bufflehead (Bucephala albeola) |
| Scientific classification      | Scientific classification      |
| Domain:                        | Eukaryota                      |
| Kingdom:                       | Animalia                       |
| Phylum:                        | Chordata                       |
| Class:                         | Aves                           |
| Order:                         | Anseriformes                   |
| Superfamily:                   | Anatoidea                      |
| Family:                        | Anatidae                       |
| Subfamilies                    | Subfamilies                    |
| See text                       | See text                       |

Duck is the common name for numerous species of waterfowl in the family Anatidae. Ducks are generally smaller and shorter-necked than swans and geese, which are members of the same family. Divided among several subfamilies, they are a form taxon; they do not represent a monophyletic group (the group of all descendants of a single common ancestral species), since swans and geese are not considered ducks. Ducks are mostly aquatic birds, and may be found in both fresh water and sea water.

Ducks are sometimes confused with several types of unrelated water birds with similar forms, such as loons or divers, grebes, gallinules and coots.

## Etymology

The word duck comes from Old English dūce 'diver', a derivative of the verb *dūcan 'to duck, bend down low as if to get under something, or dive', because of the way many species in the dabbling duck group feed by upending; compare with Dutch duiken and German tauchen 'to dive'.

Pacific black duck displaying the characteristic upending "duck"

<!-- image -->

This word replaced Old English ened /ænid 'duck', possibly to avoid confusion with other words, such as ende 'end' with similar forms. Other Germanic languages still have similar words for duck, for example, Dutch eend, German Ente and Norwegian and. The word ened /ænid was inherited from Proto-Indo-European; cf. Latin anas "duck", Lithuanian ántis 'duck', Ancient Greek νῆσσα /νῆττα (nēssa /nētta) 'duck', and Sanskrit ātí 'water bird', among others.

A duckling is a young duck in downy plumage[1] or baby duck,[2] but in the food trade a young domestic duck which has just reached adult size and bulk and its meat is still fully tender, is sometimes labelled as a duckling.

A male is called a drake and the female is called a duck, or in ornithology a hen.[3][4]

Male mallard.

<!-- image -->

Wood ducks.

<!-- image -->

## Taxonomy

All ducks belong to the biological order Anseriformes, a group that contains the ducks, geese and swans, as well as the screamers, and the magpie goose.[5] All except the screamers belong to the biological family Anatidae.[5] Within the family, ducks are split into a variety of subfamilies and 'tribes'. The number and composition of these subfamilies and tribes is the cause of considerable disagreement among taxonomists.[5] Some base their decisions on morphological characteristics, others on shared behaviours or genetic studies.[6][7] The number of suggested subfamilies containing ducks ranges from two to five.[8][9] The significant level of hybridisation that occurs among wild ducks complicates efforts to tease apart the relationships between various species.[9]

Mallard landing in approach

<!-- image -->

In most modern classifications, the so-called 'true ducks' belong to the subfamily Anatinae, which is further split into a varying number of tribes.[10] The largest of these, the Anatini, contains the 'dabbling' or 'river' ducks – named for their method of feeding primarily at the surface of fresh water.[11] The 'diving ducks', also named for their primary feeding method, make up the tribe Aythyini.[12] The 'sea ducks' of the tribe Mergini are diving ducks which specialise on fish and shellfish and spend a majority of their lives in saltwater.[13] The tribe Oxyurini contains the 'stifftails', diving ducks notable for their small size and stiff, upright tails.[14]

A number of other species called ducks are not considered to be 'true ducks', and are typically placed in other subfamilies or tribes. The whistling ducks are assigned either to a tribe (Dendrocygnini) in the subfamily Anatinae or the subfamily Anserinae,[15] or to their own subfamily (Dendrocygninae) or family (Dendrocyganidae).[9][16] The freckled duck of Australia is either the sole member of the tribe Stictonettini in the subfamily Anserinae,[15] or in its own family, the Stictonettinae.[9] The shelducks make up the tribe Tadornini in the family Anserinae in some classifications,[15] and their own subfamily, Tadorninae, in others,[17] while the steamer ducks are either placed in the family Anserinae in the tribe Tachyerini[15] or lumped with the shelducks in the tribe Tadorini.[9] The perching ducks make up in the tribe Cairinini in the subfamily Anserinae in some classifications, while that tribe is eliminated in other classifications and its members assigned to the tribe Anatini.[9] The torrent duck is generally included in the subfamily Anserinae in the monotypic tribe Merganettini,[15] but is sometimes included in the tribe Tadornini.[18] The pink-eared duck is sometimes included as a true duck either in the tribe Anatini[15] or the tribe Malacorhynchini,[19] and other times is included with the shelducks in the tribe Tadornini.[15]

## Morphology

Male Mandarin duck

<!-- image -->

The overall body plan of ducks is elongated and broad, and they are also relatively long-necked, albeit not as long-necked as the geese and swans. The body shape of diving ducks varies somewhat from this in being more rounded. The bill is usually broad and contains serrated pectens, which are particularly well defined in the filter-feeding species. In the case of some fishing species the bill is long and strongly serrated. The scaled legs are strong and well developed, and generally set far back on the body, more so in the highly aquatic species. The wings are very strong and are generally short and pointed, and the flight of ducks requires fast continuous strokes, requiring in turn strong wing muscles. Three species of steamer duck are almost flightless, however. Many species of duck are temporarily flightless while moulting; they seek out protected habitat with good food supplies during this period. This moult typically precedes migration.

The drakes of northern species often have extravagant plumage, but that is moulted in summer to give a more female-like appearance, the "eclipse" plumage. Southern resident species typically show less sexual dimorphism, although there are exceptions such as the paradise shelduck of New Zealand, which is both strikingly sexually dimorphic and in which the female's plumage is brighter than that of the male. The plumage of juvenile birds generally resembles that of the female. Female ducks have evolved to have a corkscrew shaped vagina to prevent rape.

## Distribution and habitat

Flying steamer ducks in Ushuaia, Argentina

<!-- image -->

Ducks have a cosmopolitan distribution, and are found on every continent except Antarctica.[5] Several species manage to live on subantarctic islands, including South Georgia and the Auckland Islands.[20] Ducks have reached a number of isolated oceanic islands, including the Hawaiian Islands, Micronesia and the Galápagos Islands, where they are often vagrants and less often residents.[21][22] A handful are endemic to such far-flung islands.[21]

Female mallard in Cornwall, England

<!-- image -->

Some duck species, mainly those breeding in the temperate and Arctic Northern Hemisphere, are migratory; those in the tropics are generally not. Some ducks, particularly in Australia where rainfall is erratic, are nomadic, seeking out the temporary lakes and pools that form after localised heavy rain.[23]

## Behaviour

### Feeding

Pecten along the bill

<!-- image -->

Mallard duckling preening

<!-- image -->

Ducks eat food sources such as grasses, aquatic plants, fish, insects, small amphibians, worms, and small molluscs.

Dabbling ducks feed on the surface of water or on land, or as deep as they can reach by up-ending without completely submerging.[24] Along the edge of the bill, there is a comb-like structure called a pecten. This strains the water squirting from the side of the bill and traps any food. The pecten is also used to preen feathers and to hold slippery food items.

Diving ducks and sea ducks forage deep underwater. To be able to submerge more easily, the diving ducks are heavier than dabbling ducks, and therefore have more difficulty taking off to fly.

A few specialized species such as the mergansers are adapted to catch and swallow large fish.

The others have the characteristic wide flat bill adapted to dredging-type jobs such as pulling up waterweed, pulling worms and small molluscs out of mud, searching for insect larvae, and bulk jobs such as dredging out, holding, turning head first, and swallowing a squirming frog. To avoid injury when digging into sediment it has no cere, but the nostrils come out through hard horn.

The Guardian published an article advising that ducks should not be fed with bread because it damages the health of the ducks and pollutes waterways.[25]

### Breeding

A Muscovy duckling

<!-- image -->

Ducks generally only have one partner at a time, although the partnership usually only lasts one year.[26] Larger species and the more sedentary species (like fast-river specialists) tend to have pair-bonds that last numerous years.[27] Most duck species breed once a year, choosing to do so in favourable conditions (spring/summer or wet seasons). Ducks also tend to make a nest before breeding, and, after hatching, lead their ducklings to water. Mother ducks are very caring and protective of their young, but may abandon some of their ducklings if they are physically stuck in an area they cannot get out of (such as nesting in an enclosed courtyard) or are not prospering due to genetic defects or sickness brought about by hypothermia, starvation, or disease. Ducklings can also be orphaned by inconsistent late hatching where a few eggs hatch after the mother has abandoned the nest and led her ducklings to water.[28]

### Communication

Female mallard ducks (as well as several other species in the genus Anas, such as the American and Pacific black ducks, spot-billed duck, northern pintail and common teal) make the classic "quack" sound while males make a similar but raspier sound that is sometimes written as "breeeeze",[29][self-published source?] but, despite widespread misconceptions, most species of duck do not "quack".[30] In general, ducks make a range of calls, including whistles, cooing, yodels and grunts. For example, the scaup – which are diving ducks – make a noise like "scaup" (hence their name). Calls may be loud displaying calls or quieter contact calls.

A common urban legend claims that duck quacks do not echo; however, this has been proven to be false. This myth was first debunked by the Acoustics Research Centre at the University of Salford in 2003 as part of the British Association's Festival of Science.[31] It was also debunked in one of the earlier episodes of the popular Discovery Channel television show MythBusters.[32]

### Predators

Ringed teal

<!-- image -->

Ducks have many predators. Ducklings are particularly vulnerable, since their inability to fly makes them easy prey not only for predatory birds but also for large fish like pike, crocodilians, predatory testudines such as the alligator snapping turtle, and other aquatic hunters, including fish-eating birds such as herons. Ducks' nests are raided by land-based predators, and brooding females may be caught unaware on the nest by mammals, such as foxes, or large birds, such as hawks or owls.

Adult ducks are fast fliers, but may be caught on the water by large aquatic predators including big fish such as the North American muskie and the European pike. In flight, ducks are safe from all but a few predators such as humans and the peregrine falcon, which uses its speed and strength to catch ducks.

## Relationship with humans

### Hunting

Humans have hunted ducks since prehistoric times. Excavations of middens in California dating to 7800 – 6400 BP have turned up bones of ducks, including at least one now-extinct flightless species.[33] Ducks were captured in "significant numbers" by Holocene inhabitants of the lower Ohio River valley, suggesting they took advantage of the seasonal bounty provided by migrating waterfowl.[34] Neolithic hunters in locations as far apart as the Caribbean,[35] Scandinavia,[36] Egypt,[37] Switzerland,[38] and China relied on ducks as a source of protein for some or all of the year.[39] Archeological evidence shows that Māori people in New Zealand hunted the flightless Finsch's duck, possibly to extinction, though rat predation may also have contributed to its fate.[40] A similar end awaited the Chatham duck, a species with reduced flying capabilities which went extinct shortly after its island was colonised by Polynesian settlers.[41] It is probable that duck eggs were gathered by Neolithic hunter-gathers as well, though hard evidence of this is uncommon.[35][42]

In many areas, wild ducks (including ducks farmed and released into the wild) are hunted for food or sport,[43] by shooting, or by being trapped using duck decoys. Because an idle floating duck or a duck squatting on land cannot react to fly or move quickly, "a sitting duck" has come to mean "an easy target". These ducks may be contaminated by pollutants such as PCBs.[44]

### Domestication

Indian Runner ducks, a common breed of domestic ducks

<!-- image -->

Ducks have many economic uses, being farmed for their meat, eggs, and feathers (particularly their down). Approximately 3 billion ducks are slaughtered each year for meat worldwide.[45] They are also kept and bred by aviculturists and often displayed in zoos. Almost all the varieties of domestic ducks are descended from the mallard (Anas platyrhynchos), apart from the Muscovy duck (Cairina moschata).[46][47] The Call duck is another example of a domestic duck breed. Its name comes from its original use established by hunters, as a decoy to attract wild mallards from the sky, into traps set for them on the ground. The call duck is the world's smallest domestic duck breed, as it weighs less than 1 kg (2.2 lb).[48]

### Heraldry

Three black-colored ducks in the coat of arms of Maaninka[49]

<!-- image -->

Ducks appear on several coats of arms, including the coat of arms of Lubāna (Latvia)[50] and the coat of arms of Föglö (Åland).[51]

### Cultural references

In 2002, psychologist Richard Wiseman and colleagues at the University of Hertfordshire, UK, finished a year-long LaughLab experiment, concluding that of all animals, ducks attract the most humor and silliness; he said, "If you're going to tell a joke involving an animal, make it a duck."[52] The word "duck" may have become an inherently funny word in many languages, possibly because ducks are seen as silly in their looks or behavior. Of the many ducks in fiction, many are cartoon characters, such as Walt Disney's Donald Duck, and Warner Bros.' Daffy Duck. Howard the Duck started as a comic book character in 1973[53][54] and was made into a movie in 1986.

The 1992 Disney film The Mighty Ducks, starring Emilio Estevez, chose the duck as the mascot for the fictional youth hockey team who are protagonists of the movie, based on the duck being described as a fierce fighter. This led to the duck becoming the nickname and mascot for the eventual National Hockey League professional team of the Anaheim Ducks, who were founded with the name the Mighty Ducks of Anaheim.[citation needed] The duck is also the nickname of the University of Oregon sports teams as well as the Long Island Ducks minor league baseball team.[55]

## See also

- Birds portal

- Domestic duck
- Duck as food
- Duck test
- Duck breeds
- Fictional ducks
- Rubber duck

## Notes

### Citations

1. ^ "Duckling". The American Heritage Dictionary of the English Language, Fourth Edition. Houghton Mifflin Company. 2006. Retrieved 2015-05-22.
2. ^ "Duckling". Kernerman English Multilingual Dictionary (Beta Version). K. Dictionaries Ltd. 2000–2006. Retrieved 2015-05-22.
3. ^ Dohner, Janet Vorwald (2001). The Encyclopedia of Historic and Endangered Livestock and Poultry Breeds. Yale University Press. ISBN 978-0300138139.
4. ^ Visca, Curt; Visca, Kelley (2003). How to Draw Cartoon Birds. The Rosen Publishing Group. ISBN 9780823961566.
5. ^ a b c d Carboneras 1992, p. 536.
6. ^ Livezey 1986, pp. 737–738.
7. ^ Madsen, McHugh &amp; de Kloet 1988, p. 452.
8. ^ Donne-Goussé, Laudet &amp; Hänni 2002, pp. 353–354.
9. ^ a b c d e f Carboneras 1992, p. 540.
10. ^ Elphick, Dunning &amp; Sibley 2001, p. 191.
11. ^ Kear 2005, p. 448.
12. ^ Kear 2005, p. 622–623.
13. ^ Kear 2005, p. 686.
14. ^ Elphick, Dunning &amp; Sibley 2001, p. 193.
15. ^ a b c d e f g Carboneras 1992, p. 537.
16. ^ American Ornithologists' Union 1998, p. xix.
17. ^ American Ornithologists' Union 1998.
18. ^ Carboneras 1992, p. 538.
19. ^ Christidis &amp; Boles 2008, p. 62.
20. ^ Shirihai 2008, pp. 239, 245.
21. ^ a b Pratt, Bruner &amp; Berrett 1987, pp. 98–107.
22. ^ Fitter, Fitter &amp; Hosking 2000, pp. 52–3.
23. ^ "Pacific Black Duck". www.wiresnr.org. Retrieved 2018-04-27.
24. ^ Ogden, Evans. "Dabbling Ducks". CWE. Retrieved 2006-11-02.
25. ^ Karl Mathiesen (16 March 2015). "Don't feed the ducks bread, say conservationists". The Guardian. Retrieved 13 November 2016.
26. ^ Rohwer, Frank C.; Anderson, Michael G. (1988). "Female-Biased Philopatry, Monogamy, and the Timing of Pair Formation in Migratory Waterfowl". Current Ornithology. pp. 187–221. doi:10.1007/978-1-4615-6787-5\_4. ISBN 978-1-4615-6789-9.
27. ^ Smith, Cyndi M.; Cooke, Fred; Robertson, Gregory J.; Goudie, R. Ian; Boyd, W. Sean (2000). "Long-Term Pair Bonds in Harlequin Ducks". The Condor. 102 (1): 201–205. doi:10.1093/condor/102.1.201. hdl:10315/13797.
28. ^ "If You Find An Orphaned Duckling - Wildlife Rehabber". wildliferehabber.com. Archived from the original on 2018-09-23. Retrieved 2018-12-22.
29. ^ Carver, Heather (2011). The Duck Bible. Lulu.com. ISBN 9780557901562.[self-published source]
30. ^ Titlow, Budd (2013-09-03). Bird Brains: Inside the Strange Minds of Our Fine Feathered Friends. Rowman &amp; Littlefield. ISBN 9780762797707.
31. ^ Amos, Jonathan (2003-09-08). "Sound science is quackers". BBC News. Retrieved 2006-11-02.
32. ^ "Mythbusters Episode 8". 12 December 2003.
33. ^ Erlandson 1994, p. 171.
34. ^ Jeffries 2008, pp. 168, 243.
35. ^ a b Sued-Badillo 2003, p. 65.
36. ^ Thorpe 1996, p. 68.
37. ^ Maisels 1999, p. 42.
38. ^ Rau 1876, p. 133.
39. ^ Higman 2012, p. 23.
40. ^ Hume 2012, p. 53.
41. ^ Hume 2012, p. 52.
42. ^ Fieldhouse 2002, p. 167.
43. ^ Livingston, A. D. (1998-01-01). Guide to Edible Plants and Animals. Wordsworth Editions, Limited. ISBN 9781853263774.
44. ^ "Study plan for waterfowl injury assessment: Determining PCB concentrations in Hudson river resident waterfowl" (PDF). New York State Department of Environmental Conservation. US Department of Commerce. December 2008. p. 3. Archived (PDF) from the original on 2022-10-09. Retrieved 2 July 2019.
45. ^ "FAOSTAT". www.fao.org. Retrieved 2019-10-25.
46. ^ "Anas platyrhynchos, Domestic Duck; DigiMorph Staff - The University of Texas at Austin". Digimorph.org. Retrieved 2012-12-23.
47. ^ Sy Montgomery. "Mallard; Encyclopædia Britannica". Britannica.com. Retrieved 2012-12-23.
48. ^ Glenday, Craig (2014). Guinness World Records. Guinness World Records Limited. pp. 135. ISBN 978-1-908843-15-9.
49. ^ Suomen kunnallisvaakunat (in Finnish). Suomen Kunnallisliitto. 1982. p. 147. ISBN 951-773-085-3.
50. ^ "Lubānas simbolika" (in Latvian). Retrieved September 9, 2021.
51. ^ "Föglö" (in Swedish). Retrieved September 9, 2021.
52. ^ Young, Emma. "World's funniest joke revealed". New Scientist. Retrieved 7 January 2019.
53. ^ "Howard the Duck (character)". Grand Comics Database.
54. ^ Sanderson, Peter; Gilbert, Laura (2008). "1970s". Marvel Chronicle A Year by Year History. London, United Kingdom: Dorling Kindersley. p. 161. ISBN 978-0756641238. December saw the debut of the cigar-smoking Howard the Duck. In this story by writer Steve Gerber and artist Val Mayerik, various beings from different realities had begun turning up in the Man-Thing's Florida swamp, including this bad-tempered talking duck.
55. ^ "The Duck". University of Oregon Athletics. Retrieved 2022-01-20.

### Sources

- American Ornithologists' Union (1998). Checklist of North American Birds (PDF). Washington, DC: American Ornithologists' Union. ISBN 978-1-891276-00-2. Archived (PDF) from the original on 2022-10-09.
- Carboneras, Carlos (1992). del Hoyo, Josep; Elliott, Andrew; Sargatal, Jordi (eds.). Handbook of the Birds of the World. Vol. 1: Ostrich to Ducks. Barcelona: Lynx Edicions. ISBN 978-84-87334-10-8.
- Christidis, Les; Boles, Walter E., eds. (2008). Systematics and Taxonomy of Australian Birds. Collingwood, VIC: Csiro Publishing. ISBN 978-0-643-06511-6.
- Donne-Goussé, Carole; Laudet, Vincent; Hänni, Catherine (July 2002). "A molecular phylogeny of Anseriformes based on mitochondrial DNA analysis". Molecular Phylogenetics and Evolution. 23 (3): 339–356. Bibcode:2002MolPE..23..339D. doi:10.1016/S1055-7903(02)00019-2. PMID 12099792.
- Elphick, Chris; Dunning, John B. Jr.; Sibley, David, eds. (2001). The Sibley Guide to Bird Life and Behaviour. London: Christopher Helm. ISBN 978-0-7136-6250-4.
- Erlandson, Jon M. (1994). Early Hunter-Gatherers of the California Coast. New York, NY: Springer Science &amp; Business Media. ISBN 978-1-4419-3231-0.
- Fieldhouse, Paul (2002). Food, Feasts, and Faith: An Encyclopedia of Food Culture in World Religions. Vol. I: A–K. Santa Barbara: ABC-CLIO. ISBN 978-1-61069-412-4.
- Fitter, Julian; Fitter, Daniel; Hosking, David (2000). Wildlife of the Galápagos. Princeton, NJ: Princeton University Press. ISBN 978-0-691-10295-5.
- Higman, B. W. (2012). How Food Made History. Chichester, UK: John Wiley &amp; Sons. ISBN 978-1-4051-8947-7.
- Hume, Julian H. (2012). Extinct Birds. London: Christopher Helm. ISBN 978-1-4729-3744-5.
- Jeffries, Richard (2008). Holocene Hunter-Gatherers of the Lower Ohio River Valley. Tuscaloosa: University of Alabama Press. ISBN 978-0-8173-1658-7.
- Kear, Janet, ed. (2005). Ducks, Geese and Swans: Species Accounts (Cairina to Mergus). Bird Families of the World. Oxford: Oxford University Press. ISBN 978-0-19-861009-0.
- Livezey, Bradley C. (October 1986). "A phylogenetic analysis of recent Anseriform genera using morphological characters" (PDF). The Auk. 103 (4): 737–754. doi:10.1093/auk/103.4.737. Archived (PDF) from the original on 2022-10-09.
- Madsen, Cort S.; McHugh, Kevin P.; de Kloet, Siwo R. (July 1988). "A partial classification of waterfowl (Anatidae) based on single-copy DNA" (PDF). The Auk. 105 (3): 452–459. doi:10.1093/auk/105.3.452. Archived (PDF) from the original on 2022-10-09.
- Maisels, Charles Keith (1999). Early Civilizations of the Old World. London: Routledge. ISBN 978-0-415-10975-8.
- Pratt, H. Douglas; Bruner, Phillip L.; Berrett, Delwyn G. (1987). A Field Guide to the Birds of Hawaii and the Tropical Pacific. Princeton, NJ: Princeton University Press. ISBN 0-691-02399-9.
- Rau, Charles (1876). Early Man in Europe. New York: Harper &amp; Brothers. LCCN 05040168.
- Shirihai, Hadoram (2008). A Complete Guide to Antarctic Wildlife. Princeton, NJ, US: Princeton University Press. ISBN 978-0-691-13666-0.
- Sued-Badillo, Jalil (2003). Autochthonous Societies. General History of the Caribbean. Paris: UNESCO. ISBN 978-92-3-103832-7.
- Thorpe, I. J. (1996). The Origins of Agriculture in Europe. New York: Routledge. ISBN 978-0-415-08009-5.

## External links

- Definitions from Wiktionary
- Media from Commons
- Quotations from Wikiquote
- Recipes from Wikibooks
- Taxa from Wikispecies
- Data from Wikidata

- list of books (useful looking abstracts)
- Ducks on postage stamps Archived 2013-05-13 at the Wayback Machine
- Ducks at a Distance, by Rob Hines at Project Gutenberg - A modern illustrated guide to identification of US waterfowl

| Authority control databases    | Authority control databases                  |
|--------------------------------|----------------------------------------------|
| National                       | United StatesFranceBnF dataJapanLatviaIsrael |
| Other                          | IdRef                                        |

<!-- image -->

Retrieved from ""

:

- Ducks
- Game birds
- Bird common names

Hidden categories:

- All accuracy disputes
- Accuracy disputes from February 2020
- CS1 Finnish-language sources (fi)
- CS1 Latvian-language sources (lv)
- CS1 Swedish-language sources (sv)
- Articles with short description
- Short description is different from Wikidata
- Wikipedia indefinitely move-protected pages
- Wikipedia indefinitely semi-protected pages
- Articles with 'species' microformats
- Articles containing Old English (ca. 450-1100)-language text
- Articles containing Dutch-language text
- Articles containing German-language text
- Articles containing Norwegian-language text
- Articles containing Lithuanian-language text
- Articles containing Ancient Greek (to 1453)-language text
- All articles with self-published sources
- Articles with self-published sources from February 2020
- All articles with unsourced statements
- Articles with unsourced statements from January 2022
- CS1: long volume value
- Pages using Sister project links with wikidata mismatch
- Pages using Sister project links with hidden wikidata
- Webarchive template wayback links
- Articles with Project Gutenberg links
- Articles containing video clips

- This page was last edited on 21 September 2024, at 12:11 (UTC).
- Text is available under the Creative Commons Attribution-ShareAlike License 4.0;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

- Privacy policy
- About Wikipedia
- Disclaimers
- Contact Wikipedia
- Code of Conduct
- Developers
- Statistics
- Cookie statement
- Mobile view

================================================
File: tests/data/groundtruth/docling_v2/word_sample.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: paragraph: Summer activities
  item-2 at level 1: title: Swimming in the lake
    item-3 at level 2: paragraph: Duck
    item-4 at level 2: picture
    item-5 at level 2: paragraph: Figure 1: This is a cute duckling
    item-6 at level 2: section_header: Let’s swim!
      item-7 at level 3: paragraph: To get started with swimming, fi ...  down in a water and try not to drown:
      item-8 at level 3: list: group list
        item-9 at level 4: list_item: You can relax and look around
        item-10 at level 4: list_item: Paddle about
        item-11 at level 4: list_item: Enjoy summer warmth
      item-12 at level 3: paragraph: Also, don’t forget:
      item-13 at level 3: list: group list
        item-14 at level 4: list_item: Wear sunglasses
        item-15 at level 4: list_item: Don’t forget to drink water
        item-16 at level 4: list_item: Use sun cream
      item-17 at level 3: paragraph: Hmm, what else…
      item-18 at level 3: section_header: Let’s eat
        item-19 at level 4: paragraph: After we had a good day of swimm ... , it’s important to eat something nice
        item-20 at level 4: paragraph: I like to eat leaves
        item-21 at level 4: paragraph: Here are some interesting things a respectful duck could eat:
        item-22 at level 4: table with [4x3]
        item-23 at level 4: paragraph: 
        item-24 at level 4: paragraph: And let’s add another list in the end:
        item-25 at level 4: list: group list
          item-26 at level 5: list_item: Leaves
          item-27 at level 5: list_item: Berries
          item-28 at level 5: list_item: Grain

================================================
File: tests/data/groundtruth/docling_v2/word_sample.docx.md
================================================
Summer activities

# Swimming in the lake

Duck

<!-- image -->

Figure 1: This is a cute duckling

## Let’s swim!

To get started with swimming, first lay down in a water and try not to drown:

- You can relax and look around
- Paddle about
- Enjoy summer warmth

Also, don’t forget:

- Wear sunglasses
- Don’t forget to drink water
- Use sun cream

Hmm, what else…

### Let’s eat

After we had a good day of swimming in the lake, it’s important to eat something nice

I like to eat leaves

Here are some interesting things a respectful duck could eat:

|         | Food                             |   Calories per portion |
|---------|----------------------------------|------------------------|
| Leaves  | Ash, Elm, Maple                  |                     50 |
| Berries | Blueberry, Strawberry, Cranberry |                    150 |
| Grain   | Corn, Buckwheat, Barley          |                    200 |

And let’s add another list in the end:

- Leaves
- Berries
- Grain

================================================
File: tests/data/groundtruth/docling_v2/word_sample.md
================================================
Summer activities

# Swimming in the lake

Duck

<!-- image -->

Figure 1: This is a cute duckling

## Let’s swim!

To get started with swimming, first lay down in a water and try not to drown:

- You can relax and look around
- Paddle about
- Enjoy summer warmth

Also, don’t forget:

- Wear sunglasses
- Don’t forget to drink water
- Use sun cream

Hmm, what else…

### Let’s eat

After we had a good day of swimming in the lake, it’s important to eat something nice

I like to eat leaves

Here are some interesting things a respectful duck could eat:

|         | Food                             |   Calories per portion |
|---------|----------------------------------|------------------------|
| Leaves  | Ash, Elm, Maple                  |                     50 |
| Berries | Blueberry, Strawberry, Cranberry |                    150 |
| Grain   | Corn, Buckwheat, Barley          |                    200 |

And let’s add another list in the end:

- Leaves
- Berries
- Grain

================================================
File: tests/data/groundtruth/docling_v2/word_tables.docx.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="icon" type="image/png"
    href="https://ds4sd.github.io/docling/assets/logo.png"/>
    <meta charset="UTF-8">
    <title>
    Powered by Docling
    </title>
    <style>
    html {
    background-color: LightGray;
    }
    body {
    margin: 0 auto;
    width:800px;
    padding: 30px;
    background-color: White;
    font-family: Arial, sans-serif;
    box-shadow: 10px 10px 10px grey;
    }
    figure{
    display: block;
    width: 100%;
    margin: 0px;
    margin-top: 10px;
    margin-bottom: 10px;
    }
    img {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
    max-width: 640px;
    max-height: 640px;
    }
    table {
    min-width:500px;
    background-color: White;
    border-collapse: collapse;
    cell-padding: 5px;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
    }
    th, td {
    border: 1px solid black;
    padding: 8px;
    }
    th {
    font-weight: bold;
    }
    table tr:nth-child(even) td{
    background-color: LightGray;
    }
    math annotation {
    display: none;
    }
    .formula-not-decoded {
    background: repeating-linear-gradient(
    45deg, /* Angle of the stripes */
    LightGray, /* First color */
    LightGray 10px, /* Length of the first color */
    White 10px, /* Second color */
    White 20px /* Length of the second color */
    );
    margin: 0;
    text-align: center;
    }
    </style>
    </head>
<h2>Test with tables</h2>
<p>A uniform table</p>
<table><tbody><tr><td>Header 0.0</td><td>Header 0.1</td><td>Header 0.2</td></tr><tr><td>Cell 1.0</td><td>Cell 1.1</td><td>Cell 1.2</td></tr><tr><td>Cell 2.0</td><td>Cell 2.1</td><td>Cell 2.2</td></tr></tbody></table>
<p></p>
<p>A non-uniform table with horizontal spans</p>
<table><tbody><tr><td>Header 0.0</td><td>Header 0.1</td><td>Header 0.2</td></tr><tr><td>Cell 1.0</td><td colspan="2">Merged Cell 1.1 1.2</td></tr><tr><td>Cell 2.0</td><td colspan="2">Merged Cell 2.1 2.2</td></tr></tbody></table>
<p></p>
<p>A non-uniform table with horizontal spans in inner columns</p>
<table><tbody><tr><td>Header 0.0</td><td>Header 0.1</td><td>Header 0.2</td><td>Header 0.3</td></tr><tr><td>Cell 1.0</td><td colspan="2">Merged Cell 1.1 1.2</td><td>Cell 1.3</td></tr><tr><td>Cell 2.0</td><td colspan="2">Merged Cell 2.1 2.2</td><td>Cell 2.3</td></tr></tbody></table>
<p></p>
<p>A non-uniform table with vertical spans</p>
<table><tbody><tr><td>Header 0.0</td><td>Header 0.1</td><td>Header 0.2</td></tr><tr><td>Cell 1.0</td><td rowspan="2">Merged Cell 1.1 2.1</td><td>Cell 1.2</td></tr><tr><td>Cell 2.0</td><td>Cell 2.2</td></tr><tr><td>Cell 3.0</td><td rowspan="2">Merged Cell 3.1 4.1</td><td>Cell 3.2</td></tr><tr><td>Cell 4.0</td><td>Cell 4.2</td></tr></tbody></table>
<p></p>
<p>A non-uniform table with all kinds of spans and empty cells</p>
<table><tbody><tr><td>Header 0.0</td><td>Header 0.1</td><td>Header 0.2</td><td></td><td></td></tr><tr><td>Cell 1.0</td><td rowspan="2">Merged Cell 1.1 2.1</td><td>Cell 1.2</td><td></td><td></td></tr><tr><td>Cell 2.0</td><td>Cell 2.2</td><td></td><td></td></tr><tr><td>Cell 3.0</td><td rowspan="2">Merged Cell 3.1 4.1</td><td>Cell 3.2</td><td rowspan="3"></td><td></td></tr><tr><td>Cell 4.0</td><td>Cell 4.2</td><td rowspan="2">Merged Cell 4.4 5.4</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="5"></td></tr><tr><td></td><td></td><td></td><td></td><td>Cell 8.4</td></tr></tbody></table>
<p></p>
<p></p>
</html>

================================================
File: tests/data/groundtruth/docling_v2/word_tables.docx.itxt
================================================
item-0 at level 0: unspecified: group _root_
  item-1 at level 1: section: group header-0
    item-2 at level 2: section_header: Test with tables
      item-3 at level 3: paragraph: A uniform table
      item-4 at level 3: table with [3x3]
      item-5 at level 3: paragraph: 
      item-6 at level 3: paragraph: A non-uniform table with horizontal spans
      item-7 at level 3: table with [3x3]
      item-8 at level 3: paragraph: 
      item-9 at level 3: paragraph: A non-uniform table with horizontal spans in inner columns
      item-10 at level 3: table with [3x4]
      item-11 at level 3: paragraph: 
      item-12 at level 3: paragraph: A non-uniform table with vertical spans
      item-13 at level 3: table with [5x3]
      item-14 at level 3: paragraph: 
      item-15 at level 3: paragraph: A non-uniform table with all kinds of spans and empty cells
      item-16 at level 3: table with [9x5]
      item-17 at level 3: paragraph: 
      item-18 at level 3: paragraph: 

================================================
File: tests/data/groundtruth/docling_v2/word_tables.docx.md
================================================
## Test with tables

A uniform table

| Header 0.0   | Header 0.1   | Header 0.2   |
|--------------|--------------|--------------|
| Cell 1.0     | Cell 1.1     | Cell 1.2     |
| Cell 2.0     | Cell 2.1     | Cell 2.2     |

A non-uniform table with horizontal spans

| Header 0.0   | Header 0.1          | Header 0.2          |
|--------------|---------------------|---------------------|
| Cell 1.0     | Merged Cell 1.1 1.2 | Merged Cell 1.1 1.2 |
| Cell 2.0     | Merged Cell 2.1 2.2 | Merged Cell 2.1 2.2 |

A non-uniform table with horizontal spans in inner columns

| Header 0.0   | Header 0.1          | Header 0.2          | Header 0.3   |
|--------------|---------------------|---------------------|--------------|
| Cell 1.0     | Merged Cell 1.1 1.2 | Merged Cell 1.1 1.2 | Cell 1.3     |
| Cell 2.0     | Merged Cell 2.1 2.2 | Merged Cell 2.1 2.2 | Cell 2.3     |

A non-uniform table with vertical spans

| Header 0.0   | Header 0.1          | Header 0.2   |
|--------------|---------------------|--------------|
| Cell 1.0     | Merged Cell 1.1 2.1 | Cell 1.2     |
| Cell 2.0     | Merged Cell 1.1 2.1 | Cell 2.2     |
| Cell 3.0     | Merged Cell 3.1 4.1 | Cell 3.2     |
| Cell 4.0     | Merged Cell 3.1 4.1 | Cell 4.2     |

A non-uniform table with all kinds of spans and empty cells

| Header 0.0   | Header 0.1          | Header 0.2   |    |                     |
|--------------|---------------------|--------------|----|---------------------|
| Cell 1.0     | Merged Cell 1.1 2.1 | Cell 1.2     |    |                     |
| Cell 2.0     | Merged Cell 1.1 2.1 | Cell 2.2     |    |                     |
| Cell 3.0     | Merged Cell 3.1 4.1 | Cell 3.2     |    |                     |
| Cell 4.0     | Merged Cell 3.1 4.1 | Cell 4.2     |    | Merged Cell 4.4 5.4 |
|              |                     |              |    | Merged Cell 4.4 5.4 |
|              |                     |              |    |                     |
|              |                     |              |    |                     |
|              |                     |              |    | Cell 8.4            |

================================================
File: tests/data/html/example_01.html
================================================
<html>
    <body>
        <h1>Introduction</h1>
        <p>This is the first paragraph of the introduction.</p>
        <h2>Background</h2>
        <p>Some background information here.</p>
        <img src="image1.png" alt="Example image"/>
        <ul>
            <li>First item in unordered list</li>
            <li>Second item in unordered list</li>
        </ul>
        <ol>
            <li>First item in ordered list</li>
            <li>Second item in ordered list</li>
        </ol>
    </body>
</html>


================================================
File: tests/data/html/example_02.html
================================================
<html>
    <body>
        <h1>Introduction</h1>
        <p>This is the first paragraph of the introduction.</p>
        <h2>Background</h2>
        <p>Some background information here.</p>
        <ul>
            <li>First item in unordered list</li>
            <li>Second item in unordered list</li>
        </ul>
        <ol>
            <li>First item in ordered list</li>
            <li>Second item in ordered list</li>
        </ol>
    </body>
</html>


================================================
File: tests/data/html/example_03.html
================================================
<html>
    <head>
        <style>
            table {
                border-collapse: collapse; /* Ensures borders don't double up */
                width: 100%;
            }
            th, td {
                border: 1px solid black; /* Adds a black border around cells */
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2; /* Light gray background for header */
            }
        </style>
    </head>  
    <body>
        <h1>Example Document</h1>
        <h2>Introduction</h2>
        <p>This is the first paragraph of the introduction.</p>
        <h2>Background</h2>
        <p>Some background information here.</p>
        <ul>
            <li>First item in unordered list
                <ul>
                    <li>Nested item 1</li>
                    <li>Nested item 2</li>
                </ul>
            </li>
            <li>Second item in unordered list</li>
        </ul>
        <ol>
            <li>First item in ordered list
                <ol>
                    <li>Nested ordered item 1</li>
                    <li>Nested ordered item 2</li>
                </ol>
            </li>
            <li>Second item in ordered list</li>
        </ol>
        <h2>Data Table</h2>
        <table>
            <tr>
                <th>Header 1</th>
                <th>Header 2</th>
                <th>Header 3</th>
            </tr>
            <tr>
                <td>Row 1, Col 1</td>
                <td>Row 1, Col 2</td>
                <td>Row 1, Col 3</td>
            </tr>
            <tr>
                <td>Row 2, Col 1</td>
                <td>Row 2, Col 2</td>
                <td>Row 2, Col 3</td>
            </tr>
            <tr>
                <td>Row 3, Col 1</td>
                <td>Row 3, Col 2</td>
                <td>Row 3, Col 3</td>
            </tr>
        </table>
    </body>
</html>


================================================
File: tests/data/html/example_04.html
================================================
<html>
    <body>
        <h1>Data Table with Rowspan and Colspan</h1>
        <table>
            <tr>
                <th>Header 1</th>
                <th colspan="2">Header 2 & 3 (colspan)</th>
            </tr>
            <tr>
                <td rowspan="2">Row 1 & 2, Col 1 (rowspan)</td>
                <td>Row 1, Col 2</td>
                <td>Row 1, Col 3</td>
            </tr>
            <tr>
                <td colspan="2">Row 2, Col 2 & 3 (colspan)</td>
            </tr>
            <tr>
                <td>Row 3, Col 1</td>
                <td>Row 3, Col 2</td>
                <td>Row 3, Col 3</td>
            </tr>
        </table>
    </body>
</html>


================================================
File: tests/data/html/example_05.html
================================================
<h1>Omitted html and body tags</h1>
<table>
    <tr>
        <th>Header 1</th>
        <th colspan="2">Header 2 & 3 (colspan)</th>
    </tr>
    <tr>
        <td rowspan="2">Row 1 & 2, Col 1 (rowspan)</td>
        <td>Row 1, Col 2</td>
        <td>Row 1, Col 3</td>
    </tr>
    <tr>
        <td colspan="2">Row 2, Col 2 & 3 (colspan)</td>
    </tr>
    <tr>
        <td>Row 3, Col 1</td>
        <td>Row 3, Col 2</td>
        <td>Row 3, Col 3</td>
    </tr>
</table>


================================================
File: tests/data/html/example_06.html
================================================
<html>
<head>
    <title>Sample HTML File</title>
</head>
<body>
    <div>This is a div with text.</div>
    <div>This is another div with text.</div>
    <p>This is a regular paragraph.</p>
    <div>This is a third div<br/>with a new line.</div>
    <div><p>This is a fourth div with a <b>bold</b> paragraph.</p></div>
</body>
</html>


================================================
File: tests/data/html/unit_test_01.html
================================================
<html>
    <body>
        <h1>Title</h1>
        <h2>section-1</h2>
	<h3>section-1.1</h3>
	<h2>section-2</h2>
	<h4>section-2.0.1</h4>
	<h3>section-2.2</h3>
	<h3>section-2.3</h3>
    </body>
</html>


================================================
File: tests/data/jats/bmj_sample.xml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN" "JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1" xml:lang="en"
xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" >
<front>
<journal-meta>
<journal-id journal-id-type="pmc">bmj</journal-id>
<journal-id journal-id-type="pubmed">BMJ</journal-id>
<journal-id journal-id-type="publisher">BMJ</journal-id>
<issn>0959-8138</issn>
<publisher>
<publisher-name>BMJ</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="other">jBMJ.v324.i7342.pg880</article-id>
<article-id pub-id-type="pmid">11950738</article-id>
<article-categories>
<subj-group>
<subject>Primary care</subject>
<subj-group>
<subject>190</subject>
<subject>10</subject>
<subject>218</subject>
<subject>219</subject>
<subject>355</subject>
<subject>357</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>Evolving general practice consultation in Britain: issues of length and
context</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Freeman</surname>
<given-names>George K</given-names>
</name>
<role>professor of general practice</role>
<xref ref-type="aff" rid="aff-a"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Horder</surname>
<given-names>John P</given-names>
</name>
<role>past president</role>
<xref ref-type="aff" rid="aff-b"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Howie</surname>
<given-names>John G R</given-names>
</name>
<role>emeritus professor of general practice</role>
<xref ref-type="aff" rid="aff-c"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hungin</surname>
<given-names>A Pali</given-names>
</name>
<role>professor of general practice</role>
<xref ref-type="aff" rid="aff-d"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hill</surname>
<given-names>Alison P</given-names>
</name>
<role>general practitioner</role>
<xref ref-type="aff" rid="aff-e"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shah</surname>
<given-names>Nayan C</given-names>
</name>
<role>general practitioner</role>
<xref ref-type="aff" rid="aff-b"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wilson</surname>
<given-names>Andrew</given-names>
</name>
<role>senior lecturer</role>
<xref ref-type="aff" rid="aff-f"/>
</contrib>
</contrib-group>
<aff id="aff-a">Centre for Primary Care and Social Medicine, Imperial College of Science,
Technology and Medicine, London W6 8RP</aff>
<aff id="aff-b">Royal College of General Practitioners, London SW7 1PU</aff>
<aff id="aff-c">Department of General Practice, University of Edinburgh, Edinburgh EH8 9DX</aff>
<aff id="aff-d">Centre for Health Studies, University of Durham, Durham DH1 3HN</aff>
<aff id="aff-e">Kilburn Park Medical Centre, London NW6</aff>
<aff id="aff-f">Department of General Practice and Primary Health Care, University of Leicester,
Leicester LE5 4PW</aff>
<author-notes>
<fn fn-type="con">
<p>Contributors: GKF wrote the paper and revised it after repeated and detailed comments from
all of the other authors and feedback from the first referee and from the <italic>BMJ</italic>
editorial panel. All other authors gave detailed and repeated comments and cristicisms. GKF is
the guarantor of the paper.</p>
</fn>
<fn>
<p>Correspondence to: G Freeman <email>g.freeman@ic.ac.uk</email> </p>
</fn>
</author-notes>
<pub-date date-type="pub" publication-format="print" iso-8601-date="2002-04-13">
<day>13</day>
<month>4</month>
<year>2002</year>
</pub-date>
<volume>324</volume>
<issue>7342</issue>
<fpage>880</fpage>
<lpage>882</lpage>
<history>
<date date-type="accepted" iso-8601-date="2002-02-07" publication-format="print">
<day>7</day>
<month>2</month>
<year>2002</year>
</date>
</history>
<permissions>
<copyright-statement>Copyright &#x00A9; 2002, BMJ</copyright-statement>
<copyright-year>2002, </copyright-year>
</permissions>
</article-meta>
</front>
<body>
<p>In 1999 Shah<xref ref-type="bibr" rid="B1">1</xref> and others said that the Royal College of
General Practitioners should advocate longer consultations in general practice as a matter of
policy. The college set up a working group chaired by A P Hungin, and a systematic review of
literature on consultation length in general practice was commissioned. The working group agreed
that the available evidence would be hard to interpret without discussion of the changing context
within which consultations now take place. For many years general practitioners and those who
have surveyed patients' opinions in the United Kingdom have complained about short consultation
time, despite a steady increase in actual mean length. Recently Mechanic pointed out that this is
also true in the United States.<xref ref-type="bibr" rid="B2">2</xref> Is there any justification
for a further increase in mean time allocated per consultation in general practice?</p>
<p>We report on the outcome of extensive debate among a group of general practitioners with an
interest in the process of care, with reference to the interim findings of the commissioned
systematic review and our personal databases. The review identified 14 relevant papers. <boxed-text>
<sec>
<title>Summary points</title>
<p> <list list-type="bullet">
<list-item>
<p>Longer consultations are associated with a range of better patient outcomes</p>
</list-item>
<list-item>
<p>Modern consultations in general practice deal with patients with more serious and chronic
conditions</p>
</list-item>
<list-item>
<p>Increasing patient participation means more complex interaction, which demands extra
time</p>
</list-item>
<list-item>
<p>Difficulties with access and with loss of continuity add to perceived stress and poor
performance and lead to further pressure on time</p>
</list-item>
<list-item>
<p>Longer consultations should be a professional priority, combined with increased use of
technology and more flexible practice management to maximise interpersonal continuity</p>
</list-item>
<list-item>
<p>Research on implementation is needed</p>
</list-item>
</list> </p>
</sec>
</boxed-text> </p>
<sec sec-type="subjects">
<title>Longer consultations: benefits for patients</title>
<p>The systematic review consistently showed that doctors with longer consultation times
prescribe less and offer more advice on lifestyle and other health promoting activities. Longer
consultations have been significantly associated with better recognition and handling of
psychosocial problems<xref ref-type="bibr" rid="B3">3</xref> and with better patient
enablement.<xref ref-type="bibr" rid="B4">4</xref> Also clinical care for some chronic illnesses
is better in practices with longer booked intervals between one appointment and the next.<xref
ref-type="bibr" rid="B5">5</xref> It is not clear whether time is itself the main influence or
whether some doctors insist on more time.</p>
<p>A national survey in 1998 reported that most (87&#x0025;) patients were satisfied with the
length of their most recent consultation.<xref ref-type="bibr" rid="B6">6</xref> Satisfaction
with any service will be high if expectations are met or exceeded. But expectations are modified
by previous experience.<xref ref-type="bibr" rid="B7">7</xref> The result is that primary care
patients are likely to be satisfied with what they are used to unless the context modifies the
effects of their own experience.</p>
</sec>
<sec>
<title>Context of modern consultations</title>
<p>Shorter consultations were more appropriate when the population was younger, when even a brief
absence from employment due to sickness required a doctor's note, and when many simple remedies
were available only on prescription. Recently at least five important influences have increased
the content and hence the potential length of the consultation.</p>
</sec>
<sec>
<title>Participatory consultation style</title>
<p>The most effective consultations are those in which doctors most directly acknowledge and
perhaps respond to patients' problems and concerns. In addition, for patients to be committed to
taking advantage of medical advice they must agree with both the goals and methods proposed. A
landmark publication in the United Kingdom was <italic>Meetings Between Experts</italic>, which
argued that while doctors are the experts about medical problems in general patients are the
experts on how they themselves experience these problems.<xref ref-type="bibr" rid="B8">8</xref>
New emphasis on teaching consulting skills in general practice advocated specific attention to
the patient's agenda, beliefs, understanding, and agreement. Currently the General Medical
Council, aware that communication difficulties underlie many complaints about doctors, has
further emphasised the importance of involving patients in consultations in its revised guidance
to medical schools.<xref ref-type="bibr" rid="B9">9</xref> More patient involvement should give
a better outcome, but this participatory style usually lengthens consultations.</p>
</sec>
<sec>
<title>Extended professional agenda</title>
<p>The traditional consultation in general practice was brief.<xref ref-type="bibr" rid="B2"
>2</xref> The patient presented symptoms and the doctor prescribed treatment. In 1957 Balint
gave new insights into the meaning of symptoms.<xref ref-type="bibr" rid="B10">10</xref> By 1979
an enhanced model of consultation was presented, in which the doctors dealt with ongoing as well
as presenting problems and added health promotion and education about future appropriate use of
services.<xref ref-type="bibr" rid="B11">11</xref> Now, with an ageing population and more
community care of chronic illness, there are more issues to be considered at each consultation.
Ideas of what constitutes good general practice are more complex.<xref ref-type="bibr" rid="B12"
>12</xref> Good practice now includes both extended care of chronic medical problems&#x2014;for
example, coronary heart disease<xref ref-type="bibr" rid="B13">13</xref>&#x2014;and a public
health role. At first this model was restricted to those who lead change (&#x201C;early
adopters&#x201D;) and enthusiasts<xref ref-type="bibr" rid="B14">14</xref> but now it is
embedded in professional and managerial expectations of good practice.</p>
<p>Adequate time is essential. It may be difficult for an elderly patient with several active
problems to undress, be examined, and get adequate professional consideration in under 15
minutes. Here the doctor is faced with the choice of curtailing the consultation or of reducing
the time available for the next patient. Having to cope with these situations often contributes
to professional dissatisfaction.<xref ref-type="bibr" rid="B15">15</xref> This combination of
more care, more options, and more genuine discussion of those options with informed patient
choice inevitably leads to pressure on time.</p>
</sec>
<sec>
<title>Access problems</title>
<p>In a service free at the point of access, rising demand will tend to increase rationing by
delay. But attempts to improve access by offering more consultations at short notice squeeze
consultation times.</p>
<p>While appointment systems can and should reduce queuing time for consultations, they have long
tended to be used as a brake on total demand.<xref ref-type="bibr" rid="B16">16</xref> This may
seriously erode patients' confidence in being able to see their doctor or nurse when they need
to. Patients are offered appointments further ahead but may keep these even if their symptoms
have remitted &#x201C;just in case.&#x201D; Availability of consultations is thus blocked.
Receptionists are then inappropriately blamed for the inadequate access to doctors.</p>
<p>In response to perception of delay, the government has set targets in the NHS plan of
&#x201C;guaranteed access to a primary care professional within 24 hours and to a primary care
doctor within 48 hours.&#x201D; Implementation is currently being negotiated.</p>
<p>Virtually all patients think that they would not consult unless it was absolutely necessary.
They do not think they are wasting NHS time and do not like being made to feel so. But
underlying general practitioners' willingness to make patients wait several days is their
perception that few of the problems are urgent. Patients and general practitioners evidently do
not agree about the urgency of so called minor problems. To some extent general practice in the
United Kingdom may have scored an &#x201C;own goal&#x201D; by setting up perceived access
barriers (appointment systems and out of hours cooperatives) in the attempt to increase
professional standards and control demand in a service that is free at the point of access.</p>
<p>A further government initiative has been to bypass general practice with new
services&#x2014;notably, walk-in centres (primary care clinics in which no appointment is
needed) and NHS Direct (a professional telephone helpline giving advice on simple remedies and
access to services). Introduced widely and rapidly, these services each potentially provide
significant features of primary care&#x2014;namely, quick access to skilled health advice and
first line treatment.</p>
</sec>
<sec>
<title>Loss of interpersonal continuity</title>
<p>If a patient has to consult several different professionals, particularly over a short period
of time, there is inevitable duplication of stories, risk of naive diagnoses, potential for
conflicting advice, and perhaps loss of trust. Trust is essential if patients are to accept the
&#x201C;wait and see&#x201D; management policy which is, or should be, an important part of the
management of self limiting conditions, which are often on the boundary between illness and
non-illness.<xref ref-type="bibr" rid="B17">17</xref> Such duplication again increases pressure
for more extra (unscheduled) consultations resulting in late running and professional
frustration.<xref ref-type="bibr" rid="B18">18</xref> </p>
<p>Mechanic described how loss of longitudinal (and perhaps personal and relational<xref
ref-type="bibr" rid="B19">19</xref>) continuity influences the perception and use of time
through an inability to build on previous consultations.<xref ref-type="bibr" rid="B2">2</xref>
Knowing the doctor well, particularly in smaller practices, is associated with enhanced patient
enablement in shorter time.<xref ref-type="bibr" rid="B4">4</xref> Though Mechanic pointed out
that three quarters of UK patients have been registered with their general practitioner five
years or more, this may be misleading. Practices are growing, with larger teams and more
registered patients. Being registered with a doctor in a larger practice is usually no guarantee
that the patient will be able to see the same doctor or the doctor of his or her choice, who may
be different. Thus the system does not encourage adequate personal continuity. This adds to
pressure on time and reduces both patient and professional satisfaction.</p>
</sec>
<sec>
<title>Health service reforms</title>
<p>Finally, for the past 15 years the NHS has experienced unprecedented change with a succession
of major administrative reforms. Recent reforms have focused on an NHS led by primary care,
including the aim of shifting care from the secondary specialist sector to primary care. One
consequence is increased demand for primary care of patients with more serious and less stable
problems. With the limited piloting of reforms we do not know whether such major redirection can
be achieved without greatly altering the delicate balance between expectations (of both patients
and staff) and what is delivered.</p>
</sec>
<sec>
<title>The future</title>
<p>We think that the way ahead must embrace both longer mean consultation times and more
flexibility. More time is needed for high quality consultations with patients with major and
complex problems of all kinds. But patients also need access to simpler services and advice.
This should be more appropriate (and cost less) when it is given by professionals who know the
patient and his or her medical history and social circumstances. For doctors, the higher quality
associated with longer consultations may lead to greater professional satisfaction and, if these
longer consultations are combined with more realistic scheduling, to reduced levels of
stress.<xref ref-type="bibr" rid="B20">20</xref> They will also find it easier to develop
further the care of chronic disease.</p>
<p>The challenge posed to general practice by walk-in centres and NHS Direct is considerable, and
the diversion of funding from primary care is large. The risk of waste and duplication increases
as more layers of complexity are added to a primary care service that started out as something
familiar, simple, and local and which is still envied in other developed countries.<xref
ref-type="bibr" rid="B21">21</xref> Access needs to be simple, and the advantages of personal
knowledge and trust in minimising duplication and overmedicalisation need to be exploited.</p>
<p>We must ensure better communication and access so that patients can more easily deal with
minor issues and queries with someone they know and trust and avoid the formality and
inconvenience of a full face to face consultation. Too often this has to be with a different
professional, unfamiliar with the nuances of the case. There should be far more managerial
emphasis on helping patients to interact with their chosen practitioner<xref ref-type="bibr"
rid="B22">22</xref>; such a programme has been described.<xref ref-type="bibr" rid="B23"
>23</xref> Modern information systems make it much easier to record which doctor(s) a patient
prefers to see and to monitor how often this is achieved. The telephone is hardly modern but is
underused. Email avoids the problems inherent in arranging simultaneous availability necessary
for telephone consultations but at the cost of reducing the communication of emotions. There is
a place for both.<xref ref-type="bibr" rid="B2">2</xref> Access without prior appointment is a
valued feature of primary care, and we need to know more about the right balance between planned
and ad hoc consulting.</p>
</sec>
<sec>
<title>Next steps</title>
<p>General practitioners do not behave in a uniform way. They can be categorised as slow, medium,
and fast and react in different ways to changes in consulting speed.<xref ref-type="bibr"
rid="B18">18</xref> They are likely to have differing views about a widespread move to lengthen
consultation time. We do not need further confirmation that longer consultations are desirable
and necessary, but research could show us the best way to learn how to introduce them with
minimal disruption to the way in which patients and practices like primary care to be
provided.<xref ref-type="bibr" rid="B24">24</xref> We also need to learn how to make the most of
available time in complex consultations.</p>
<p>Devising appropriate incentives and helping practices move beyond just reacting to demand in
the traditional way by working harder and faster is perhaps our greatest challenge in the United
Kingdom. The new primary are trusts need to work together with the growing primary care research
networks to carry out the necessary development work. In particular, research is needed on how a
primary care team can best provide the right balance of quick access and interpersonal knowledge
and trust.</p>
</sec>
</body>
<back>
<ack>
<p>We thank the other members of the working group: Susan Childs, Paul Freeling, Iona Heath,
Marshall Marinker, and Bonnie Sibbald. We also thank Fenny Green of the Royal College of General
Practitioners for administrative help.</p>
</ack>
<ref-list>
<ref id="B1">
<label>1</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Shah</surname>
<given-names>NC</given-names>
</name> </person-group>
<article-title>Viewpoint: Consultation time&#x2014;time for a change? Still the
&#x201C;perfunctory work of perfunctory men!&#x201D;</article-title>
<source>Br J Gen Pract</source>
<year iso-8601-date="1999">1999</year>
<volume>49</volume>
<fpage>497</fpage>
</element-citation>
</ref>
<ref id="B2">
<label>2</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Mechanic</surname>
<given-names>D</given-names>
</name> </person-group>
<article-title>How should hamsters run? Some observations about sufficient patient time in
primary care</article-title>
<source>BMJ</source>
<year iso-8601-date="2001">2001</year>
<volume>323</volume>
<fpage>266</fpage>
<lpage>268</lpage>
<pub-id pub-id-type="pmid">11485957</pub-id>
</element-citation>
</ref>
<ref id="B3">
<label>3</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Howie</surname>
<given-names>JGR</given-names>
</name> <name>
<surname>Porter</surname>
<given-names>AMD</given-names>
</name> <name>
<surname>Heaney</surname>
<given-names>DJ</given-names>
</name> <name>
<surname>Hopton</surname>
<given-names>JL</given-names>
</name> </person-group>
<article-title>Long to short consultation ratio: a proxy measure of quality of care for general
practice</article-title>
<source>Br J Gen Pract</source>
<year iso-8601-date="1991">1991</year>
<volume>41</volume>
<fpage>48</fpage>
<lpage>54</lpage>
<pub-id pub-id-type="pmid">2031735</pub-id>
</element-citation>
</ref>
<ref id="B4">
<label>4</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Howie</surname>
<given-names>JGR</given-names>
</name> <name>
<surname>Heaney</surname>
<given-names>DJ</given-names>
</name> <name>
<surname>Maxwell</surname>
<given-names>M</given-names>
</name> <name>
<surname>Walker</surname>
<given-names>JJ</given-names>
</name> <name>
<surname>Freeman</surname>
<given-names>GK</given-names>
</name> <name>
<surname>Rai</surname>
<given-names>H</given-names>
</name> </person-group>
<article-title>Quality at general practice consultations: cross-sectional
survey</article-title>
<source>BMJ</source>
<year iso-8601-date="1999">1999</year>
<volume>319</volume>
<fpage>738</fpage>
<lpage>743</lpage>
<pub-id pub-id-type="pmid">10487999</pub-id>
</element-citation>
</ref>
<ref id="B5">
<label>5</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Kaplan</surname>
<given-names>SH</given-names>
</name> <name>
<surname>Greenfield</surname>
<given-names>S</given-names>
</name> <name>
<surname>Ware</surname>
<given-names>JE</given-names>
</name> </person-group>
<article-title>Assessing the effects of physician-patient interactions on the outcome of
chronic disease</article-title>
<source>Med Care</source>
<year iso-8601-date="1989">1989</year>
<volume>27</volume>
<supplement>suppl 3</supplement>
<fpage>110</fpage>
<lpage>125</lpage>
</element-citation>
</ref>
<ref id="B6">
<label>6</label>
<element-citation publication-type="book" publication-format="print">
<person-group person-group-type="editor"> <name>
<surname>Airey</surname>
<given-names>C</given-names>
</name> <name>
<surname>Erens</surname>
<given-names>B</given-names>
</name> </person-group>
<source>National surveys of NHS patients: general practice, 1998</source>
<year iso-8601-date="1999">1999</year>
<publisher-loc>London</publisher-loc>
<publisher-name>NHS Executive</publisher-name>
</element-citation>
</ref>
<ref id="B7">
<label>7</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Hart</surname>
<given-names>JT</given-names>
</name> </person-group>
<article-title>Expectations of health care: promoted, managed or shared?</article-title>
<source>Health Expect</source>
<year iso-8601-date="1998">1998</year>
<volume>1</volume>
<fpage>3</fpage>
<lpage>13</lpage>
<pub-id pub-id-type="pmid">11281857</pub-id>
</element-citation>
</ref>
<ref id="B8">
<label>8</label>
<element-citation publication-type="book" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Tuckett</surname>
<given-names>D</given-names>
</name> <name>
<surname>Boulton</surname>
<given-names>M</given-names>
</name> <name>
<surname>Olson</surname>
<given-names>C</given-names>
</name> <name>
<surname>Williams</surname>
<given-names>A</given-names>
</name> </person-group>
<source>Meetings between experts: an approach to sharing ideas in medical
consultations</source>
<year iso-8601-date="1985">1985</year>
<publisher-loc>London</publisher-loc>
<publisher-name>Tavistock Publications</publisher-name>
</element-citation>
</ref>
<ref id="B9">
<label>9</label>
<mixed-citation publication-type="webpage" publication-format="web">General Medical Council.
<source>Draft recommendations on undergraduate medical education</source>. July 2001.
www.gmc-uk.org/med_ed/tomorrowsdoctors/index.htm (accessed 2 Jan 2002).</mixed-citation>
</ref>
<ref id="B10">
<label>10</label>
<element-citation publication-type="book" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Balint</surname>
<given-names>M</given-names>
</name> </person-group>
<source>The doctor, his patient and the illness</source>
<year iso-8601-date="1957">1957</year>
<publisher-loc>London</publisher-loc>
<publisher-name>Tavistock</publisher-name>
</element-citation>
</ref>
<ref id="B11">
<label>11</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Stott</surname>
<given-names>NCH</given-names>
</name> <name>
<surname>Davies</surname>
<given-names>RH</given-names>
</name> </person-group>
<article-title>The exceptional potential in each primary care consultation</article-title>
<source>J R Coll Gen Pract</source>
<year iso-8601-date="1979">1979</year>
<volume>29</volume>
<fpage>210</fpage>
<lpage>205</lpage>
</element-citation>
</ref>
<ref id="B12">
<label>12</label>
<element-citation publication-type="book" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Hill</surname>
<given-names>AP</given-names>
</name> </person-group>
<person-group person-group-type="editor"> <name>
<surname>Hill</surname>
<given-names>AP</given-names>
</name> </person-group>
<article-title>Challenges for primary care</article-title>
<source>What's gone wrong with health care? Challenges for the new millennium</source>
<year iso-8601-date="2000">2000</year>
<publisher-loc>London</publisher-loc>
<publisher-name>King's Fund</publisher-name>
<fpage>75</fpage>
<lpage>86</lpage>
</element-citation>
</ref>
<ref id="B13">
<label>13</label>
<element-citation publication-type="book" publication-format="print">
<collab>Department of Health</collab>
<source>National service framework for coronary heart disease</source>
<year iso-8601-date="2000">2000</year>
<publisher-loc>London</publisher-loc>
<publisher-name>Department of Health</publisher-name>
</element-citation>
</ref>
<ref id="B14">
<label>14</label>
<element-citation publication-type="book" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Hart</surname>
<given-names>JT</given-names>
</name> </person-group>
<source>A new kind of doctor: the general practitioner's part in the health of the
community</source>
<year iso-8601-date="1988">1988</year>
<publisher-loc>London</publisher-loc>
<publisher-name>Merlin Press</publisher-name>
</element-citation>
</ref>
<ref id="B15">
<label>15</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Morrison</surname>
<given-names>I</given-names>
</name> <name>
<surname>Smith</surname>
<given-names>R</given-names>
</name> </person-group>
<article-title>Hamster health care</article-title>
<source>BMJ</source>
<year iso-8601-date="2000">2000</year>
<volume>321</volume>
<fpage>1541</fpage>
<lpage>1542</lpage>
<pub-id pub-id-type="pmid">11124164</pub-id>
</element-citation>
</ref>
<ref id="B16">
<label>16</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Arber</surname>
<given-names>S</given-names>
</name> <name>
<surname>Sawyer</surname>
<given-names>L</given-names>
</name> </person-group>
<article-title>Do appointment systems work?</article-title>
<source>BMJ</source>
<year iso-8601-date="1982">1982</year>
<volume>284</volume>
<fpage>478</fpage>
<lpage>480</lpage>
<pub-id pub-id-type="pmid">6800503</pub-id>
</element-citation>
</ref>
<ref id="B17">
<label>17</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Hjortdahl</surname>
<given-names>P</given-names>
</name> <name>
<surname>Borchgrevink</surname>
<given-names>CF</given-names>
</name> </person-group>
<article-title>Continuity of care: influence of general practitioners' knowledge about their
patients on use of resources in consultations</article-title>
<source>BMJ</source>
<year iso-8601-date="1991">1991</year>
<volume>303</volume>
<fpage>1181</fpage>
<lpage>1184</lpage>
<pub-id pub-id-type="pmid">1747619</pub-id>
</element-citation>
</ref>
<ref id="B18">
<label>18</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Howie</surname>
<given-names>JGR</given-names>
</name> <name>
<surname>Hopton</surname>
<given-names>JL</given-names>
</name> <name>
<surname>Heaney</surname>
<given-names>DJ</given-names>
</name> <name>
<surname>Porter</surname>
<given-names>AMD</given-names>
</name> </person-group>
<article-title>Attitudes to medical care, the organization of work, and stress among general
practitioners</article-title>
<source>Br J Gen Pract</source>
<year iso-8601-date="1992">1992</year>
<volume>42</volume>
<fpage>181</fpage>
<lpage>185</lpage>
<pub-id pub-id-type="pmid">1389427</pub-id>
</element-citation>
</ref>
<ref id="B19">
<label>19</label>
<element-citation publication-type="book" publication-format="web">
<person-group person-group-type="author"> <name>
<surname>Freeman</surname>
<given-names>G</given-names>
</name> <name>
<surname>Shepperd</surname>
<given-names>S</given-names>
</name> <name>
<surname>Robinson</surname>
<given-names>I</given-names>
</name> <name>
<surname>Ehrich</surname>
<given-names>K</given-names>
</name> <name>
<surname>Richards</surname>
<given-names>SC</given-names>
</name> <name>
<surname>Pitman</surname>
<given-names>P</given-names>
</name> </person-group>
<source>Continuity of care: report of a scoping exercise for the national co-ordinating centre
for NHS Service Delivery and Organisation R&#x0026;D (NCCSDO), Summer 2000</source>
<year iso-8601-date="2001">2001</year>
<publisher-loc>London</publisher-loc>
<publisher-name>NCCSDO</publisher-name>
<comment><ext-link ext-link-type="url" xmlns:xlink="http://www.w3.org/1999/xlink"
xlink:href="http://www.sdo.lshtm.ac.uk/continuityofcare.htm"
>www.sdo.lshtm.ac.uk/continuityofcare.htm</ext-link> (accessed 2 Jan 2002)</comment>
</element-citation>
</ref>
<ref id="B20">
<label>20</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Wilson</surname>
<given-names>A</given-names>
</name> <name>
<surname>McDonald</surname>
<given-names>P</given-names>
</name> <name>
<surname>Hayes</surname>
<given-names>L</given-names>
</name> <name>
<surname>Cooney</surname>
<given-names>J</given-names>
</name> </person-group>
<article-title>Longer booking intervals in general practice: effects on doctors' stress and
arousal</article-title>
<source>Br J Gen Pract</source>
<year iso-8601-date="1991">1991</year>
<volume>41</volume>
<fpage>184</fpage>
<lpage>187</lpage>
<pub-id pub-id-type="pmid">1878267</pub-id>
</element-citation>
</ref>
<ref id="B21">
<label>21</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>De Maeseneer</surname>
<given-names>J</given-names>
</name> <name>
<surname>Hjortdahl</surname>
<given-names>P</given-names>
</name> <name>
<surname>Starfield</surname>
<given-names>B</given-names>
</name> </person-group>
<article-title>Fix what's wrong, not what's right, with general practice in
Britain</article-title>
<source>BMJ</source>
<year iso-8601-date="2000">2000</year>
<volume>320</volume>
<fpage>1616</fpage>
<lpage>1617</lpage>
<pub-id pub-id-type="pmid">10856043</pub-id>
</element-citation>
</ref>
<ref id="B22">
<label>22</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Freeman</surname>
<given-names>G</given-names>
</name> <name>
<surname>Hjortdahl</surname>
<given-names>P</given-names>
</name> </person-group>
<article-title>What future for continuity of care in general practice?</article-title>
<source>BMJ</source>
<year iso-8601-date="1997">1997</year>
<volume>314</volume>
<fpage>1870</fpage>
<lpage>1873</lpage>
<pub-id pub-id-type="pmid">9224130</pub-id>
</element-citation>
</ref>
<ref id="B23">
<label>23</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Kibbe</surname>
<given-names>DC</given-names>
</name> <name>
<surname>Bentz</surname>
<given-names>E</given-names>
</name> <name>
<surname>McLaughlin</surname>
<given-names>CP</given-names>
</name> </person-group>
<article-title>Continuous quality improvement for continuity of care</article-title>
<source>J Fam Pract</source>
<year iso-8601-date="1993">1993</year>
<volume>36</volume>
<fpage>304</fpage>
<lpage>308</lpage>
<pub-id pub-id-type="pmid">8454977</pub-id>
</element-citation>
</ref>
<ref id="B24">
<label>24</label>
<element-citation publication-type="journal" publication-format="print">
<person-group person-group-type="author"> <name>
<surname>Williams</surname>
<given-names>M</given-names>
</name> <name>
<surname>Neal</surname>
<given-names>RD</given-names>
</name> </person-group>
<article-title>Time for a change? The process of lengthening booking intervals in general
practice</article-title>
<source>Br J Gen Pract</source>
<year iso-8601-date="1998">1998</year>
<volume>48</volume>
<fpage>1783</fpage>
<lpage>1786</lpage>
<pub-id pub-id-type="pmid">10198490</pub-id>
</element-citation>
</ref>
</ref-list>
<fn-group>
<fn id="fn1">
<p>Funding: Meetings of the working group in 1999-2000 were funded by the
<funding-source>Scientific Foundation Board of the RCGP</funding-source>.</p>
</fn>
<fn id="fn2">
<p>Competing interests: None declared.</p>
</fn>
</fn-group>
</back>
</article>


================================================
File: tests/data/md/blocks.md
================================================
Unordered list:

- foo

Empty unordered list:

-

Ordered list:

1. bar

Empty ordered list:

1.

Heading:

# my heading

Empty heading:

#

Indented code block:

    print("Hi!")

Empty indented code block:

    

Fenced code block:

```python
print("Hello world!")
```

Empty fenced code block:

```

```


================================================
File: tests/data/md/duck.md
================================================
Summer activities

# Swimming in the lake

Duck


Figure 1: This is a cute duckling

## Let’s swim!

To get started with swimming, first lay down in a water and try not to drown:

- You can relax and look around
- Paddle about
- Enjoy summer warmth

Also, don’t forget:

- Wear sunglasses
- Don’t forget to drink water
- Use sun cream

Hmm, what else…

## Let’s eat

After we had a good day of swimming in the lake,
it’s important to eat
something nice

I like to eat leaves


Here are some interesting things a respectful duck could eat:

|         | Food                             |   Calories per portion |
|---------|----------------------------------|------------------------|
| Leaves  | Ash, Elm, Maple                  |                     50 |
| Berries | Blueberry, Strawberry, Cranberry |                    150 |
| Grain   | Corn, Buckwheat, Barley          |                    200 |

And let’s add another list in the end:

- Leaves
- Berries
- Grain

And here my listing in code:

```
Leaves

Berries
Grain
```


================================================
File: tests/data/md/ending_with_table.md
================================================
| Character | Name in German | Name in French | Name in Italian |
|---|---|---|---|
| Scrooge McDuck | Dagobert Duck | Balthazar Picsou | Paperone |
| Huey | Tick | Riri | Qui |
| Dewey | Trick | Fifi | Quo |
| Louie | Track | Loulou | Qua |


================================================
File: tests/data/md/mixed.md
================================================
# Title

Some text

## Famous ducks

Here is a table:

<table>
  <tr>
    <th>Character</th>
    <th>Name in German</th>
    <th>Name in French</th>
    <th>Name in Italian</th>
  </tr>
  <tr>
    <td>Scrooge McDuck</td>
    <td>Dagobert Duck</td>
    <td>Balthazar Picsou</td>
    <td>Paperone</td>
  </tr>
  <tr>
    <td>Huey</td>
    <td>Tick</td>
    <td>Riri</td>
    <td>Qui</td>
  </tr>
  <tr>
    <td>Dewey</td>
    <td>Trick</td>
    <td>Fifi</td>
    <td>Quo</td>
  </tr>
  <tr>
    <td>Louie</td>
    <td>Track</td>
    <td>Loulou</td>
    <td>Qua</td>
  </tr>
</table>

And here is more HTML:

<p>Some paragraph.</p>

<div>
    <p>Now a div — almost there...</p>
    <ul>
        <li>foo</li>
        <li>bar</li>
    </ul>
</div>

The end!


================================================
File: tests/data/md/nested.md
================================================
# Nesting

A list featuring nesting:

- abc
	- abc123
		- abc1234
			- abc12345
				- a.
				- b.
		- abcd1234：
			- abcd12345：
				- a.
				- b.
- def：
	- def1234：
		- def12345。

- after one empty line
	- foo


- afer two empty lines
	- bar
* changing symbol

A nested HTML list:

<ul>
    <li>First item</li>
    <li>Second item with subitems:
        <ul>
            <li>Subitem 1</li>
            <li>Subitem 2</li>
        </ul>
    </li>
    <li>Last list item</li>
</ul>

<!--
Table nesting apparently not yet suported by HTML backend:

<table>
  <tr>
    <td>Cell</td>
    <td>Nested Table
      <table>
        <tr>
          <td>Cell 1</td>
		  <>
        </tr>
        <tr>
          <td>Cell 2</td>
        </tr>
        <tr>
          <td>Cell 3</td>
        </tr>
        <tr>
          <td>Cell 4</td>
        </tr>
      </table>
    </td>
  </tr>
  <tr><td>additional row</td></tr>
</table>
-->


================================================
File: tests/data/md/wiki.md
================================================
# IBM

International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.

It is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.

IBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.

IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed "International Business Machines" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the IBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]

IBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s, IBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.

As one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming language, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing Awards.[16]

## 1910s–1950s

IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the Computing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington, D.C.; and Toronto, Canada.[22]

Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]: 105  He implemented sales conventions, "generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker".[25][26] His favorite slogan, "THINK", became a mantra for each company's employees.[25] During Watson's first four years, revenues reached $9 million ($158 million today) and the company's operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the clumsy hyphenated name "Computing-Tabulating-Recording Company" and chose to replace it with the more expansive title "International Business Machines" which had previously been used as the name of CTR's Canadian Division;[27] the name was changed on February 14, 1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.

## 1960s–1980s

In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.


================================================
File: tests/data/uspto/ipgD0701016.xml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="USD0701016-20140318.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20140304" date-publ="20140318">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>D0701016</doc-number>
<kind>S1</kind>
<date>20140318</date>
</document-id>
</publication-reference>
<application-reference appl-type="design">
<document-id>
<country>US</country>
<doc-number>29414573</doc-number>
<date>20120229</date>
</document-id>
</application-reference>
<us-application-series-code>29</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EM</country>
<doc-number>001184956-0001</doc-number>
<date>20091210</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<length-of-grant>14</length-of-grant>
</us-term-of-grant>
<classification-locarno>
<edition>10</edition>
<main-classification>0101</main-classification>
</classification-locarno>
<classification-national>
<country>US</country>
<main-classification>D 1101</main-classification>
</classification-national>
<invention-title id="d2e71">Cheese in form of a triangular pyramid</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>D14842</doc-number>
<kind>S</kind>
<name>Griscom, Jr.</name>
<date>18840300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D24100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>D94174</doc-number>
<kind>S</kind>
<name>Vogt</name>
<date>19341200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 9714</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2819167</doc-number>
<kind>A</kind>
<name>Irmscher</name>
<date>19580100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426 83</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2952394</doc-number>
<kind>A</kind>
<name>Schneider</name>
<date>19600900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>229113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2992829</doc-number>
<kind>A</kind>
<name>Hopkins</name>
<date>19610700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273157 R</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>D192941</doc-number>
<kind>S</kind>
<name>Schneider</name>
<date>19620500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 9707</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>3032251</doc-number>
<kind>A</kind>
<name>Jarund</name>
<date>19620500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>229116</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>3074612</doc-number>
<kind>A</kind>
<name>Schneider</name>
<date>19630100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426 85</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>D199935</doc-number>
<kind>S</kind>
<name>Dykes</name>
<date>19641200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 9707</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>3223532</doc-number>
<kind>A</kind>
<name>Pinkalla et al.</name>
<date>19651200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426602</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>3347363</doc-number>
<kind>A</kind>
<name>Dykes et al.</name>
<date>19671000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>206436</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>3379360</doc-number>
<kind>A</kind>
<name>Crossley</name>
<date>19680400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383 66</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>3404988</doc-number>
<kind>A</kind>
<name>Rausing</name>
<date>19681000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426115</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>3420366</doc-number>
<kind>A</kind>
<name>Doyle</name>
<date>19690100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>206436</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>3648319</doc-number>
<kind>A</kind>
<name>Mitchell</name>
<date>19720300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 15118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>3659360</doc-number>
<kind>A</kind>
<name>Zeischegg</name>
<date>19720500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434403</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>3662486</doc-number>
<kind>A</kind>
<name>Freedman</name>
<date>19720500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>446120</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>3734388</doc-number>
<kind>A</kind>
<name>Hopkins</name>
<date>19730500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383210</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>3791570</doc-number>
<kind>A</kind>
<name>Hopkins</name>
<date>19740200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383210</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>3887190</doc-number>
<kind>A</kind>
<name>Ameri</name>
<date>19750600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273264</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>3923238</doc-number>
<kind>A</kind>
<name>Thomas</name>
<date>19751200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>2291982</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>3925959</doc-number>
<kind>A</kind>
<name>Dykes et al.</name>
<date>19751200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 53451</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>4334871</doc-number>
<kind>A</kind>
<name>Roane</name>
<date>19820600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434211</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>4515370</doc-number>
<kind>A</kind>
<name>Garcia</name>
<date>19850500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273258</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>4659086</doc-number>
<kind>A</kind>
<name>Colborne</name>
<date>19870400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273242</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>4723382</doc-number>
<kind>A</kind>
<name>Lalvani</name>
<date>19880200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 52 811</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>5026068</doc-number>
<kind>A</kind>
<name>Weisser</name>
<date>19910600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>5076793</doc-number>
<kind>A</kind>
<name>Aghevli et al.</name>
<date>19911200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434196</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>5188285</doc-number>
<kind>A</kind>
<name>Nilsson et al.</name>
<date>19930200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>229242</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>5249966</doc-number>
<kind>A</kind>
<name>Hiigli</name>
<date>19931000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434211</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>5409235</doc-number>
<kind>A</kind>
<name>Ameri</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273242</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>D367672</doc-number>
<kind>S</kind>
<name>Hahn</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D19 49</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>D368152</doc-number>
<kind>S</kind>
<name>Cahill et al.</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 1199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>5690331</doc-number>
<kind>A</kind>
<name>Sides</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>273146</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>D412537</doc-number>
<kind>S</kind>
<name>Underwood</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D21373</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>D414903</doc-number>
<kind>S</kind>
<name>Baiera et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D30160</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>D427260</doc-number>
<kind>S</kind>
<name>Burr</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D21684</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>D432011</doc-number>
<kind>S</kind>
<name>Millon</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 9697</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>6214392</doc-number>
<kind>B1</kind>
<name>Ramirez</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426106</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>6629603</doc-number>
<kind>B1</kind>
<name>Fontaine</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>206438</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>6735809</doc-number>
<kind>B2</kind>
<name>Parks</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 15118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>D497238</doc-number>
<kind>S</kind>
<name>Maniak</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 1129</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>7247075</doc-number>
<kind>B2</kind>
<name>von Oech</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>446 92</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>D639016</doc-number>
<kind>S</kind>
<name>Rea et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 1199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>7967510</doc-number>
<kind>B2</kind>
<name>Martuch</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383207</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>D644101</doc-number>
<kind>S</kind>
<name>Caldwell et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>D 9430</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>8408793</doc-number>
<kind>B2</kind>
<name>Martuch</name>
<date>20130400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383207</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2005/0158442</doc-number>
<kind>A1</kind>
<name>Westermann et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2006/0034551</doc-number>
<kind>A1</kind>
<name>Linneweil</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383 612</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2008/0037912</doc-number>
<kind>A1</kind>
<name>Martuch</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>383205</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2009/0263546</doc-number>
<kind>A1</kind>
<name>Rea et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>426 83</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2010/0176007</doc-number>
<kind>A1</kind>
<name>Rea et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>206  5</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2012/0273560</doc-number>
<kind>A1</kind>
<name>Rupp</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>229 8705</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>EP</country>
<doc-number>0482574</doc-number>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>EP</country>
<doc-number>0577989</doc-number>
<date>19940100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>JP</country>
<doc-number>61105664</doc-number>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
</us-references-cited>
<number-of-claims>1</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>D 1100-199</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D 7602</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D 7672-677</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D28 88</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D28  81</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 33</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 63</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 67</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 70</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 84</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11108</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11115</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 56</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 49</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11 82</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11134</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11157</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D11158</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D21803</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D21386</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D24101</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D24104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D19 42</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>D30160</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>273261</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 76</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 87</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 89</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 90</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 91</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 92</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426100</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426101</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426105</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426132</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426134</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426138</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426139</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426421</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426515</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426122</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426144</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426660</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426512</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426393</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426635</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426805</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426 94- 95</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426128</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426293</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426383</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426438-439</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426446</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426450</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426502-504</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426516</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426543</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426549-550</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426556</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426249</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426250</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>426802</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>29363465</doc-number>
<date>20100609</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>29414573</doc-number>
</document-id>
</child-doc>
</relation>
</division>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Rupp</last-name>
<first-name>Ludwig</first-name>
<address>
<city>Lochau</city>
<country>AT</country>
</address>
</addressbook>
<residence>
<country>AT</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Rupp</last-name>
<first-name>Ludwig</first-name>
<address>
<city>Lochau</city>
<country>AT</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Workman Nydegger</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Rupp AG</orgname>
<role>03</role>
<address>
<country>AT</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Fox</last-name>
<first-name>Barbara</first-name>
<department>2911</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="130.22mm" wi="99.91mm" file="USD0701016-20140318-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="247.99mm" wi="154.60mm" file="USD0701016-20140318-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="240.71mm" wi="164.68mm" file="USD0701016-20140318-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="238.25mm" wi="158.41mm" file="USD0701016-20140318-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="238.84mm" wi="164.68mm" file="USD0701016-20140318-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<p id="p-0001" num="0001"><figref idref="DRAWINGS">FIG. 1</figref> is a perspective view of cheese in form of a triangular pyramid showing the new design;</p>
<p id="p-0002" num="0002"><figref idref="DRAWINGS">FIG. 2</figref> is another perspective view thereof;</p>
<p id="p-0003" num="0003"><figref idref="DRAWINGS">FIG. 3</figref> is another perspective view thereof;</p>
<p id="p-0004" num="0004"><figref idref="DRAWINGS">FIG. 4</figref> is a front elevational view thereof;</p>
<p id="p-0005" num="0005"><figref idref="DRAWINGS">FIG. 5</figref> is a rear elevational view thereof;</p>
<p id="p-0006" num="0006"><figref idref="DRAWINGS">FIG. 6</figref> is a top view thereof;</p>
<p id="p-0007" num="0007"><figref idref="DRAWINGS">FIG. 7</figref> is another front view thereof; and,</p>
<p id="p-0008" num="0008"><figref idref="DRAWINGS">FIG. 8</figref> is a right side view thereof.</p>
<p id="p-0009" num="0009">The broken lines shown in the drawings represent portions of the cheese in form of a triangular pyramid which form no part of the claimed design.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
</description>
<us-claim-statement>CLAIM</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>The ornamental design for cheese in form of a triangular pyramid, as shown and described.</claim-text>
</claim>
</claims>
</us-patent-grant>

================================================
File: tests/data/uspto/pa20010031492.xml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v15-2001-01-31.dtd" [
<!ENTITY US20010031492A1-20011018-M00001.NB SYSTEM "US20010031492A1-20011018-M00001.NB" NDATA NB>
<!ENTITY US20010031492A1-20011018-M00001.TIF SYSTEM "US20010031492A1-20011018-M00001.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00001.TIF SYSTEM "US20010031492A1-20011018-D00001.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00002.TIF SYSTEM "US20010031492A1-20011018-D00002.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00003.TIF SYSTEM "US20010031492A1-20011018-D00003.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00004.TIF SYSTEM "US20010031492A1-20011018-D00004.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00005.TIF SYSTEM "US20010031492A1-20011018-D00005.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00006.TIF SYSTEM "US20010031492A1-20011018-D00006.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00007.TIF SYSTEM "US20010031492A1-20011018-D00007.TIF" NDATA TIF>
<!ENTITY US20010031492A1-20011018-D00008.TIF SYSTEM "US20010031492A1-20011018-D00008.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20010031492</doc-number>
<kind-code>A1</kind-code>
<document-date>20011018</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09728320</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20001201</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>9811845.8</doc-number>
</priority-application-number>
<filing-date>19980602</filing-date>
<country-code>GB</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>C12N001/20</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>C12N005/06</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>435</class>
<subclass>252100</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>435</class>
<subclass>325000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>435</class>
<subclass>252330</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Assay reagent</title-of-invention>
</technical-information>
<continuity-data>
<continuations>
<continuation-of>
<parent-child>
<child>
<document-id>
<doc-number>09728320</doc-number>
<kind-code>A1</kind-code>
<document-date>20001201</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>PCT/GB99/01730</doc-number>
<document-date>19990601</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>UNKNOWN</parent-status>
</parent-child>
</continuation-of>
</continuations>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Jay</given-name>
<family-name>Lewington</family-name>
</name>
<residence>
<residence-non-us>
<city>Bisley</city>
<country-code>GB</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Katherine</given-name>
<family-name>Isles</family-name>
</name>
<residence>
<residence-non-us>
<city>Oxon</city>
<country-code>GB</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Sandy</given-name>
<family-name>Primrose</family-name>
</name>
<residence>
<residence-non-us>
<city>High Wycombe</city>
<country-code>GB</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>Azur Environmental</organization-name>
<assignee-type>03</assignee-type>
</assignee>
<correspondence-address>
<name-1>FITCH EVEN TABIN AND FLANNERY</name-1>
<name-2></name-2>
<address>
<address-1>120 SOUTH LA SALLE STREET</address-1>
<address-2>SUITE 1600</address-2>
<city>CHICAGO</city>
<state>IL</state>
<postalcode>606033406</postalcode>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A cell-derived assay reagent prepared from cells which have been killed by treatment with an antibiotic selected from the bleomycin-phleomycin family of antibiotics but which retain a signal-generating metabolic activity such as bioluminescence. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is a continuation of PCT/GB99/01730, filed Jun. 1, 1999 designating the United States (the disclosure of which is incorporated herein by reference) and claiming priority from British application serial no. 9811845.8, filed Jun. 2, 1998. </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The invention relates to a cell-derived assay reagent, in particular to an assay reagent prepared from cells which have been killed but which retain a signal-generating metabolic activity such as bioluminescence and also to assay methods using the cell-derived reagent such as, for example, toxicity testing methods. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The use of bacteria with a signal-generating metabolic activity as indicators of toxicity is well established. UK patent number GB 2005018 describes a method of assaying a liquid sample for toxic substances which involves contacting a suspension of bioluminescent microorganisms with a sample suspected of containing a toxic substance and observing the change in the light output of the bioluminescent organisms as a result of contact with the suspected toxic substance. Furthermore, a toxicity monitoring system embodying the same assay principle, which is manufactured and sold under the Trade Mark Microtox&reg;, is in routine use in both environmental laboratories and for a variety of industrial applications. An improved toxicity assay method using bioluminescent bacteria, which can be used in a wider range of test conditions than the method of GB 2005018, is described in International patent application number WO 95/10767. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> The assay methods known in the prior art may utilize naturally occurring bioluminescent organisms, including <highlight><italic>Photobacterium phosphoreum </italic></highlight>and <highlight><italic>Vibrio fischeri. </italic></highlight>However, recent interest has focused on the use of genetically modified microorganisms which have been engineered to express bioluminescence. These genetically modified bioluminescent microorganisms usually express lux genes, encoding the enzyme luciferase, which have been cloned from a naturally occurring bioluminescent microorganism (E. A. Meighen (1994) Genetics of Bacterial Bioluminescence. <highlight><italic>Ann. Rev. Genet. </italic></highlight>28: 117-139; Stewart, G. S. A. B. Jassin, S. A. A. and Denyer, S. P. (1993), Engineering Microbial bioluminescence and biosensor applications. In Molecular Diagnosis. Eds R. Rapley and M. R. Walker Blackwell Scientific Pubs/Oxford). A process for producing genetically modified bioluminescent microorganisms expressing lux genes cloned from <highlight><italic>Vibrio harveyi </italic></highlight>is described in U.S. Pat. No. 4,581,335. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> The use of genetically modified bioluminescent microorganisms in toxicity testing applications has several advantages over the use of naturally occurring microorganisms. For example, it is possible to engineer microorganisms with different sensitivities to a range of different toxic substances or to a single toxic substance. However, genetically modified microorganisms are subject to marketing restrictions as a result of government legislation and there is major concern relating to the deliberate release of genetically modified microorganisms into the environment as components of commercial products. This is particularly relevant with regard to toxicity testing which is often performed in the field rather than within the laboratory. The potential risk from release of potentially pathogenic genetically modified microorganisms into the environment where they may continue to grow in an uncontrollable manner has led to the introduction of legal restrictions on the use of genetically modified organisms in the field in many countries. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> It has been suggested, to avoid the problems discussed above, to use genetically modified bioluminescent microorganisms which have been treated so that they retain the metabolic function of bioluminescence but an no longer reproduce. The use of radiation (gamma-radiation), X-rays or an electron beam) to kill bioluminescent cells whilst retaining the metabolic function of bioluminescence is demonstrated in International patent application number WO 95/07346. It is an object of the present invention to provide an alternative method of killing bioluminescent cells whilst retaining the metabolic function of bioluminescence which does not require the use of radiation and, as such, can be easily carried out without the need for specialized radiation equipment and containment facilities and without the risk to laboratory personnel associated with the use of radiation. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Accordingly, in a first aspect the invention provides a method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of cells with signal-generating metabolic activity with a member of the bleomycin/phleomycin family of antibiotics. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Bleomycin and phleomycin are closely related glycopeptide antibiotics that are isolated in the form of copper chelates from cultures of <highlight><italic>Streptomyces verticillus. </italic></highlight>They represent a group of proteins with molecular weights ranging from 1000 to 1000 kda that are potent antibiotics and anti-tumour agents. So far more than 200 members of the bleomycin/phleomycin family have been isolated and characterised as complex basic glycopeptides. Family members resemble each other with respect to their physicochemical properties and their structure, indicating that functionally they all behave in the same manner. Furthermore, the chemical structure of the active moiety is conserved between family members and consists of 5 amino acids, L-glucose, 3-O-carbamoyl-D-mannose and a terminal cation. The various different bleomycin/phleomycin family members differ from each other in the nature of the terminal cation moiety, which is usually an amine. A preferred bleomycin/phleomycin antibiotic for use in the method of the invention is phleomycin D<highlight><bold>1</bold></highlight>, sold under the trade name Zeocin&trade;. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Bleomycin and phleomycin are strong, selective inhibitors of DNA synthesis in intact bacteria and in mammalian cells. Bleomycin can be observed to attack purified DNA in vitro when incubated under appropriate conditions and analysis of the bleomycin damaged DNA shows that both single-stranded and double-stranded cleavages occur, the latter being the result of staggered single strand breaks formed approximately two base pairs apart in the complementary strands. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> In in vivo systems, after being taken up by the cell, bleomycin enters the cell nucleus, binds to DNA (by virtue of the interaction between its positively charged terminal amine moiety and a negatively charged phosphate group of the DNA backbone) and causes strand scission. Bleomycin causes strand scission of DNA in viruses, bacteria and eukaryotic cell systems. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> The present inventors have surprisingly found that treatment of a culture of cells with signal-generating metabolic activity with a bleomycin/phleomycin antibiotic renders the culture non-viable whilst retaining a level of signal-generating metabolic activity suitable for use in toxicity testing applications. In the context of this application the term non-viable is taken to mean that the cells are unable to reproduce. The process of rendering cells non-viable whilst retaining signal-generating metabolic activity may hereinafter be referred to as &lsquo;inactivation&rsquo; and cells which have been rendered non-viable according to the method of the invention may be referred to as &lsquo;inactivated&rsquo;. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Because of the broad spectrum of action of the bleomycin/phleomycin family of antibiotics the method of the invention is equally applicable to bacterial cells and to eukaryotic cells with signal generating metabolic activity. Preferably the signal-generating metabolic activity is bioluminescence but other signal-generating metabolic activities which are reporters of toxic damage could be used with equivalent effect. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The method of the invention is preferred for use with bacteria or eukaryotic cells that have been genetically modified to express a signal-generating metabolic activity. The examples given below relate to <highlight><italic>E. coil </italic></highlight>which have been engineered to express bioluminescence by transformation with a plasmid carrying lux genes. The eukaryotic equivalent would be cells transfected with a vector containing nucleic acid encoding a eukaryotic luciferase enzyme (abbreviated luc) such as, for example, luciferase from the firefly <highlight><italic>Photinus pyralis. </italic></highlight>A suitable plasmid vector containing cDNA encoding firefly luciferase under the control of an SV40 viral promoter is available from Promega Corporation, Madison Wis., USA. However, in connection with the present invention it is advantageous to use recombinant cells containing the entire eukaryotic luc operon so as to avoid the need to add an exogenous substrate ( e.g. luciferin) in order to generate light output. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The optimum concentration of bleomycin/phleomycin antibiotic and contact time required to render a culture of cells non-viable whilst retaining a useful level of signal-generating metabolic activity may vary according to the cell type but can be readily determined by routine experiment. In general, the lower the concentration of antibiotic used the longer the contact time required for cell inactivation. In connection with the production of assay reagents for use in toxicity testing applications, it is generally advantageous to keep the concentration of antibiotic low (e.g. around 1-1.5 mg/ml) and increase the contact time for inactivation. As will be shown in Example 1, treatment with Zeocin&trade; at a concentration of 1.5 mg/ml for 3 to 5 hours is sufficient to completely inactivate a culture of recombinant <highlight><italic>E. coli. </italic></highlight></paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> In the case of bacteria, the contact time required to inactivate a culture of bacterial cells is found to vary according to the stage of growth of the bacterial culture at the time the antibiotic is administered. Although the method of the invention can be used on bacteria at all stages of growth it is generally preferable to perform the method on bacterial cells in an exponential growth phase because the optimum antibiotic contact time has been observed to be shortest when the antibiotic is administered to bacterial cells in an exponential growth phase. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> Following treatment with bleomycin/phleomycin antibiotic the non-viable preparation of cells is preferably stabilised for ease of storage or shipment. The cells can be stabilised using known techniques such as, for example, freeze drying (lyophilization) or other cell preservation techniques known in the art. Stabilization by freeze drying has the added advantage that the freeze drying procedure itself can render cells non-viable. Thus, any cells in the preparation which remain viable after treatment of the culture with bleomycin/phleomycin antibiotic will be rendered non-viable by freeze drying. It is thought that freeze drying inactivates any remaining viable cells by enhancing the effect of antibiotic, such that sub-lethally injured cells in the culture are more sensitive to the stresses applied during freeze drying. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Prior to use the stabilised cell preparation is reconstituted using a reconstitution buffer to form an assay reagent. This reconstituted assay reagent may then be used directly in assays for analytes, for example in toxicity testing applications. It is preferable that the stabilised (i.e. freeze dried) assay reagent be reconstituted immediately prior to use, but after reconstitution it is generally necessary to allow sufficient time prior to use for the reconstituted reagent to reach a stable, high level of signal-generating activity. Suitable reconstitution buffers preferably contain an osmotically potent non-salt compound such as sucrose, dextran or polyethylene glycol, although salt based stabilisers may also be used. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> Whilst the assay reagent of the invention is particularly suitable for use in toxicity testing applications it is to be understood that the invention is not limited to assay reagents for use in toxicity testing. The cell inactivation method of the invention can be used to inactivate any recombinant cells (prokaryotic or eukaryotic) with a signal generating metabolic activity that is not dependent upon cell viability. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> In a further aspect the invention provides a method of assaying a potentially toxic analyte comprising the steps of, </paragraph>
<paragraph id="P-0020" lvl="2"><number>&lsqb;0020&rsqb;</number> (a) contacting a sample to be assayed for the analyte with a sample of assay reagent comprising a non-viable preparation of cells with a signal-generating metabolic activity; </paragraph>
<paragraph id="P-0021" lvl="2"><number>&lsqb;0021&rsqb;</number> (b) measuring the level of signal generated; and </paragraph>
<paragraph id="P-0022" lvl="2"><number>&lsqb;0022&rsqb;</number> (c) using the measurement obtained as an indicator of the toxicity of the analyte. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> In a still further aspect, the invention provides a kit for performing the above-stated assay comprising an assay reagent with signal generating metabolic activity and means for contacting the assay reagent with a sample to be assayed for an analyte. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The analytes tested using the assay of the invention are usually toxic substances, but it is to be understood that the precise nature of the analyte to be tested is not material to the invention. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> Toxicity is a general term used to describe an adverse effect on biological system and the term &lsquo;toxic substances&rsquo; includes both toxicants (synthetic chemicals that are toxic) and toxins (natural poisons). Toxicity is usually expressed as an effective concentration (EC) or inhibitory concentration (IC) value. The EC/IC value is usually denoted as a percentage response e.g. EC<highlight><subscript>50</subscript></highlight>, EC<highlight><subscript>10 </subscript></highlight>which denotes the concentration (dose) of a particular substance which affects the designated criteria for assessing toxicity (i.e. a behavioural trait or death) in the indicated proportion of the population tested. For example, an EC<highlight><subscript>50 </subscript></highlight>of 10 ppm indicates that 50% of the population will be affected by a concentration of 10 ppm. In the case of a toxicity assay based on the use of a bioluminescent assay reagent, the EC<highlight><subscript>50 </subscript></highlight>value is usually the concentration of sample substance causing a 50% change in light output.</paragraph>
</summary-of-invention>
<brief-description-of-drawings>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The present invention will be further understood by way of the following Examples with reference to the accompanying Figures in which: </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a graph to show the effect of Zeocin&trade; treatment on viable count and light output of recombinant bioluminescent <highlight><italic>E. coil </italic></highlight>cells. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a graph to show the light output from five separate vials of reconstituted assay reagent. The assay reagent was prepared from recombinant bioluminescent <highlight><italic>E. coil </italic></highlight>exposed to 1.5 mg/ml Zeocin&trade; for 300 minutes. Five vials were used to reduce discrepancies resulting from vial to vial variation. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> FIGS. <highlight><bold>3</bold></highlight> to <highlight><bold>8</bold></highlight> are graphs to show the effect of Zeocin&trade; treatment on the sensitivity of bioluminescent assay reagent to toxicant (ZnSO<highlight><subscript>4</subscript></highlight>): </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference>: Control cells, lag phase. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>: Zeocin&trade; treated cells, lag phase. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference>: Control cells, mid-exponential growth. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference>: Zeocin&trade; treated cells, mid-exponential growth. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference>: Control cells, stationary phase. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference>: Zeocin&trade; treated cells, stationary phase.</paragraph>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">EXAMPLE 1 </heading>
</section>
<section>
<heading lvl="1">(A) Inactivation of Bioluminescent <highlight><italic>E. coil </italic></highlight>Method </heading>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> 1. Bioluminescent genetically modified <highlight><italic>E. coil </italic></highlight>strain HB101 (<highlight><italic>E. coli </italic></highlight>HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of <highlight><italic>Vibrio fischeri </italic></highlight>constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) were grown from a frozen stock in 5 ml of low salt medium (LB (5 g/ml NaCl)&plus;glycerol&plus;MgSO<highlight><subscript>4</subscript></highlight>) for 24 hours. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> 2. 1 ml of the 5 ml culture was then used to inoculate 200 ml of low salt medium in a shaker flask and the resultant culture grown to an OD<highlight><subscript>630 </subscript></highlight>of 0.407 (exponential growth phase). </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> 3. 50 ml of this culture was removed to a fresh sterile shaker flask (control cells). </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> 4. Zeocin&trade; was added to the 150 ml of culture in the original shaker flash, to a final concentration of 1.5 mg/ml. At the same time, an equivalent volume of water was added to the 50 ml culture removed from the original flask (control cells). </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> 5. The time course of cell inactivation was monitored by removing samples from the culture at 5, 60, 120, 180, 240 and 300 minutes after the addition of Zeocin&trade; and taking measurements of both light output (measured using a Deltatox luminometer) and viable count (per ml, determined using the method given in Example 3 below) for each of the samples. Samples of the control cells were removed at 5 and 300 minutes after the addition of water and measurements of light output and viable count taken as for the Zeocin&trade; treated cells. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows the effect of Zeocin&trade; treatment on the light output and viable count (per ml) of recombinant bioluminescent <highlight><italic>E. coil. </italic></highlight>Zeocin&trade; was added to a final concentration of 1.5 mg/ml at time zero. The number of viable cells in the culture was observed to decrease with increasing contact cells with Zeocin&trade;, the culture being completely inactivated after 3 hours. The light output from the culture was observed to decrease gradually with increasing Zeocin&trade; contact time. </paragraph>
</section>
<section>
<heading lvl="1">(B) Production of Assay Reagent </heading>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Five hours after the addition of Zeocin&trade; or water the remaining bacterial cells in the Zeocin&trade; treated and control cultures were harvested by the centrifugation, washed (to remove traces of Zeocin&trade; from the Zeocin&trade; treated culture), re-centrifuged and resuspended in cryoprotectant to an OD<highlight><subscript>630 </subscript></highlight>of 0.25. 200 &mgr;l aliquots of the cells in cryoprotectant were dispensed into single shot vials, and freeze dried. Freeze dried samples of the Zeocin&trade; treated cells and control cells were reconstituted in 0.2M sucrose to form assay reagents and the light output of the assay reagents measured at various times after reconstitution. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> The light output from assay reagent prepared from cells exposed to 1.5 mg/ml Zeocin&trade; for 5 hours was not significantly different to the light output from assay reagent prepared from control (Zeocin&trade; untreated) cells, indicating that Zeocin&trade; treatment does not affect the light output of the reconstituted freeze dried assay reagent. Both Zeocin&trade; treated and Zeocin&trade; untreated assay reagents produced stable light output 15 minutes after reconstitution. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows the light output from five separate vials of reconstituted Zeocin&trade; treated assay reagent inactivated according to the method of Example 1(A) and processed into assay reagent as described in Example 1(B). Reconstitution solution was added at time zero and thereafter light output was observed to increase steadily before stabilising out at around 15 minutes after reconstitution. All five vials were observed to give similar light profiles after reconstitution. </paragraph>
</section>
<section>
<heading lvl="1">EXAMPLE 2 </heading>
</section>
<section>
<heading lvl="1">Sensitivity of Zeocin&trade; Treated Assay Reagent to Toxicant Method </heading>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> 1. Bioluminescent genetically modified <highlight><italic>E. coil </italic></highlight>strain HB101 (<highlight><italic>E. coli </italic></highlight>HB101 made bioluminescent by transformation with a plasmid carrying the lux operon of <highlight><italic>vibrio fischeri </italic></highlight>constructed by the method of Shaw and Kado, as described in Biotechnology 4: 560-564) was grown in fermenter as a batch culture in low salt medium (LB(5 g/ml NaCl)&plus;glycerol&plus;MgSO<highlight><subscript>4</subscript></highlight>). </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> 2. Two aliquots of the culture were removed from the fermenter into separate sterile shaker flasks at each of three different stages of growth i.e. at OD<highlight><subscript>630 </subscript></highlight>values of 0.038 (lag phase growth), 1.31 (mid-exponential phase growth) and 2.468 (stationary phase growth). </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> 3. One aliquot of culture for each of the three growth stages was inactivated by contact with Zeocin&trade; (1 mg Zeocin&trade; added per 2.5&times;10<highlight><superscript>6 </superscript></highlight>cells, i.e. the concentration of Zeocin&trade; per cell is kept constant) for 300 minutes and then processed into assay reagent by freeze drying and reconstitution, as described in part (B) of Example 1. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> 4. An equal volume of water was added to the second aliquot of culture for each of the three growth stages and the cultures processed into assay reagent as described above. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> 5. Samples of each of the three Zeocin&trade; treated and three control assay reagents were then evaluated for sensitivity to toxicant (ZnSO<highlight><subscript>4</subscript></highlight>) according to the following assay protocol: </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> ZnSO<highlight><subscript>4 </subscript></highlight>Sensitivity Assay </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> 1. ZnSO<highlight><subscript>4 </subscript></highlight>solutions were prepared in pure water at 30, 10, 3, 1, 0.3 and 0.1 ppm. Pure water was also used as a control. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> 2. Seven vials of each of the three Zeocin&trade; treated and each of the three control assay reagents (i.e. one for each of the six ZnSO<highlight><subscript>4 </subscript></highlight>solutions and one for the pure water control) were reconstituted using 0.5 ml of reconstitution solution (eg 0.2M sucrose) and then left to stand at room temperature for 15 minutes to allow the light output to stabilize. Base line (time zero) readings of light output were then measured for each of the reconstituted reagents. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> 3. 0.5 ml aliquots of each of the six ZnSO<highlight><subscript>4 </subscript></highlight>solutions and the pure water control were added to separate vials of reconstituted assay reagent. This was repeated for each of the different Zeocin&trade; treated and control assay reagents. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> 4. The vials were incubated at room temperature and light output readings were taken 5, 10, 15, 20, 25 and 30 minutes after addition of ZnSO<highlight><subscript>4 </subscript></highlight>solution. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> 5. The % toxic effect for each sample was calculated as follows:  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mstyle>
    <mtext>% toxic effect</mtext>
  </mstyle>
  <mo>=</mo>
  <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mrow>
      <mrow>
        <mo>(</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>c</mi>
            </msub>
            <mo>&times;</mo>
            <msub>
              <mi>C</mi>
              <mi>o</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>o</mi>
            </msub>
            <mo>&times;</mo>
            <msub>
              <mi>C</mi>
              <mi>t</mi>
            </msub>
          </mrow>
        </mfrac>
        <mo>)</mo>
      </mrow>
      <mo>&times;</mo>
      <mn>100</mn>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20010031492A1-20011018-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="18.00225" file="US20010031492A1-20011018-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0056" lvl="7"><number>&lsqb;0056&rsqb;</number> where: C<highlight><subscript>o</subscript></highlight>&equals;light in control at time zero </paragraph>
<paragraph id="P-0057" lvl="2"><number>&lsqb;0057&rsqb;</number> C<highlight><subscript>t</subscript></highlight>&equals;light in control at reading time </paragraph>
<paragraph id="P-0058" lvl="2"><number>&lsqb;0058&rsqb;</number> S<highlight><subscript>o</subscript></highlight>&equals;light in sample at time zero </paragraph>
<paragraph id="P-0059" lvl="2"><number>&lsqb;0059&rsqb;</number> S<highlight><subscript>t</subscript></highlight>&equals;light in sample at reading time </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> The results of toxicity assays for sensitivity to ZnSO<highlight><subscript>4 </subscript></highlight>for all the Zeocin&trade; treated and control assay reagents are shown in FIGS. <highlight><bold>3</bold></highlight> to <highlight><bold>8</bold></highlight>: </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference>: Control cells, lag phase. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>: Zeocin&trade; treated cells, lag phase. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference>: Control cells, mid-exponential growth. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference>: Zeocin&trade; treated cells, mid-exponential growth. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference>: Control cells, stationary phase. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference>: Zeocin&trade; treated cells, stationary phase. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In each case, separate graphs of % toxic effect against log<highlight><subscript>10 </subscript></highlight>concentration of ZnSO<highlight><subscript>4 </subscript></highlight>were plotted on the same axes for each value of time (minutes) after addition of Zeocin&trade; or water. The sensitivities of the various reagents, expressed as an EC<highlight><subscript>50 </subscript></highlight>value for 15 minutes exposed to ZnSO<highlight><subscript>4</subscript></highlight>, are summarised in Table 1 below.  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="OFFSET" colwidth="91PT" align="left"/>
<colspec colname="1" colwidth="119PT" align="center"/>
<colspec colname="2" colwidth="7PT" align="left"/>
<tbody valign="top">
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>SENSITIVITY-EC<highlight><subscript>50 </subscript></highlight>VALUES</entry>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91PT" align="left"/>
<colspec colname="2" colwidth="63PT" align="left"/>
<colspec colname="3" colwidth="63PT" align="left"/>
<tbody valign="top">
<row>
<entry>GROWTH STAGE OF</entry>
<entry>ZEOCIN&thinsp;&trade;</entry>
<entry>CONTROL</entry>
</row>
<row>
<entry>ASSAY REAGENT</entry>
<entry>TREATED</entry>
<entry>CELLS</entry>
</row>
<row><entry namest="1" nameend="3" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Lag Phase</entry>
<entry>1.445 ppm ZnSO<highlight><subscript>4</subscript></highlight></entry>
<entry>1.580 ppm ZnSO<highlight><subscript>4</subscript></highlight></entry>
</row>
<row>
<entry>Expotential phase</entry>
<entry>0.446 ppm ZnSO<highlight><subscript>4</subscript></highlight></entry>
<entry>0.446 ZnSO<highlight><subscript>4</subscript></highlight></entry>
</row>
<row>
<entry>Stationary phase</entry>
<entry>0.426 ppm ZnSO<highlight><subscript>4</subscript></highlight></entry>
<entry>0.457 ppm ZnSO<highlight><subscript>4</subscript></highlight></entry>
</row>
<row><entry namest="1" nameend="3" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0068" lvl="7"><number>&lsqb;0068&rsqb;</number> Table 1: Sensitivity of the different assay reagents to ZnSo<highlight><subscript>4 </subscript></highlight>expressed as EC<highlight><subscript>50 </subscript></highlight>values for 15 minutes exposure to ZNSO<highlight><subscript>4</subscript></highlight>. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The results of the toxicity assays indicate that Zeocin&trade; treatment does not significantly affect the sensitivity of a recombinant bioluminescent <highlight><italic>E. coli </italic></highlight>derived assay reagent to ZnSO<highlight><subscript>4</subscript></highlight>. Similar results could be expected with other toxic substances which have an effect on signal-generating metabolic activities. </paragraph>
</section>
<section>
<heading lvl="1">EXAMPLE 3 </heading>
</section>
<section>
<heading lvl="1">Method to Determine Viable Count </heading>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> 1. Samples of bacterial culture to be assayed for viable count were centrifuged at 10,000 rpm for 5 minutes to pellet the bacterial cells. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> 2. Bacterial cells were washed by resuspending in 1 ml of M9 medium, re-centrifuged at 10,000 rpm for 5 minutes and finally re-suspended in 1 ml of M9 medium. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> 3. Serial dilutions of the bacterial cell suspension from 10<highlight><superscript>&minus;1 </superscript></highlight>to 10<highlight><superscript>&minus;7 </superscript></highlight>were prepared in M9 medium. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> 4. Three separate 10 &mgr;l aliquots of each of the serial dilutions were plated out on standard agar plates and the plates incubated at 37&deg; C. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> 5. The number of bacterial colonies present for each of the three aliquots at each of the serial dilutions were counted and the values averaged. Viable count was calculated per ml of bacterial culture. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of making a non-viable preparation of prokaryotic or eukaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of said cells having signal-generating metabolic activity with an antibiotic selected from the bleomycin/phleomycin family of antibiotics. </claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00001"><claim-text>claim 1</claim-text></dependent-claim-reference> wherein following contact with antibiotic, said cells are subjected to a stabilization step. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00002"><claim-text>claim 2</claim-text></dependent-claim-reference> wherein said stabilization step comprises freeze drying. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00001"><claim-text>claim 1</claim-text></dependent-claim-reference> wherein said antibiotic is phleomycin D<highlight><bold>1</bold></highlight>. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00005"><claim-text>claim 5</claim-text></dependent-claim-reference> wherein said signal-generating metabolic activity is bioluminescence. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00005"><claim-text>claim 5</claim-text></dependent-claim-reference> wherein said cells are bacteria. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00006"><claim-text>claim 6</claim-text></dependent-claim-reference> wherein said bacteria are in an exponential growth phase when contacted with said antibiotic. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00006"><claim-text>claim 6</claim-text></dependent-claim-reference> wherein said bacteria are genetically modified. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00008"><claim-text>claim 8</claim-text></dependent-claim-reference> wherein said genetically modified bacteria contain nucleic acid encoding luciferase. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00009"><claim-text>claim 9</claim-text></dependent-claim-reference> wherein said bacteria are <highlight><italic>E. coli. </italic></highlight></claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00005"><claim-text>claim 5</claim-text></dependent-claim-reference> wherein said cells are eukaryotic cells. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00011"><claim-text>claim 11</claim-text></dependent-claim-reference> wherein said eukaryotic cells are genetically modified. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00012"><claim-text>claim 12</claim-text></dependent-claim-reference> wherein said genetically modified eukaryotic cells contain nucleic acid encoding luciferase. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method of making a non-viable preparation of prokaryotic cells, which preparation has a signal-generating metabolic activity, which method comprises contacting a viable culture of a genetically modified <highlight><italic>E. coli </italic></highlight>strain made bioluminescent by transformation with a plasmid carrying the lux operon of <highlight><italic>Vibrio fischeri </italic></highlight>with an antibiotic selected from the bleomycin/phleomycin family of antibiotics. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00014"><claim-text>claim 14</claim-text></dependent-claim-reference> wherein said cells are contacted with phleomycin D<highlight><bold>1</bold></highlight> at a concentration of at least about 1.5 mg/ml. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00015"><claim-text>claim 15</claim-text></dependent-claim-reference> wherein said contact is maintained for at least about 3 hours. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00016"><claim-text>claim 16</claim-text></dependent-claim-reference> wherein said antibiotic-treated cells are harvested, washed and freeze-dried.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>NONE</representative-figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20010031492A1-20011018-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20010031492A1-20011018-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20010031492A1-20011018-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20010031492A1-20011018-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20010031492A1-20011018-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20010031492A1-20011018-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20010031492A1-20011018-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20010031492A1-20011018-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>

================================================
File: tests/data/uspto/tables_ipa20180000016.xml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="133pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="91pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Fluorescent material</entry>
<entry/>
<entry/>
<entry/>
</row>
<row>
<entry/>
<entry>(parts by mass)</entry>
<entry>Photon flux</entry>
<entry>Ratio of</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="7">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="70pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="63pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<colspec colname="5" colwidth="42pt" align="center"/>
<colspec colname="6" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>First fluorescent</entry>
<entry>Second fluorescent</entry>
<entry>density</entry>
<entry>photon</entry>
<entry>Fresh weight</entry>
<entry>Nitrate nitrogen</entry>
</row>
<row>
<entry/>
<entry>material</entry>
<entry>material</entry>
<entry>(&#x3bc;mol &#xb7; m<sup>&#x2212;2 </sup>&#xb7; s<sup>&#x2212;1</sup>)</entry>
<entry>flux densities</entry>
<entry>(Edible part)</entry>
<entry>content</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="10">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="70pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="21pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="21pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="21pt" align="center"/>
<colspec colname="8" colwidth="42pt" align="center"/>
<colspec colname="9" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>(MGF/CASN = 95:5)</entry>
<entry>(YAG: Ce, Cr)</entry>
<entry>B</entry>
<entry>R</entry>
<entry>FR</entry>
<entry>R/B</entry>
<entry>R/FR</entry>
<entry>(g)</entry>
<entry>(mg/100 g)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="10">
<colspec colname="1" colwidth="42pt" align="left"/>
<colspec colname="2" colwidth="70pt" align="center"/>
<colspec colname="3" colwidth="63pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="char" char="."/>
<colspec colname="5" colwidth="21pt" align="char" char="."/>
<colspec colname="6" colwidth="21pt" align="char" char="."/>
<colspec colname="7" colwidth="21pt" align="char" char="."/>
<colspec colname="8" colwidth="21pt" align="center"/>
<colspec colname="9" colwidth="42pt" align="char" char="."/>
<colspec colname="10" colwidth="49pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry>Comparative</entry>
<entry>&#x2014;</entry>
<entry>&#x2014;</entry>
<entry>35.5</entry>
<entry>88.8</entry>
<entry>0.0</entry>
<entry>2.5</entry>
<entry>&#x2014;</entry>
<entry>26.2</entry>
<entry>361.2</entry>
</row>
<row>
<entry>Example 1</entry>
</row>
<row>
<entry>Example 1</entry>
<entry>60</entry>
<entry>&#x2014;</entry>
<entry>31.5</entry>
<entry>74.9</entry>
<entry>12.6</entry>
<entry>2.4</entry>
<entry>6.0</entry>
<entry>35.4</entry>
<entry>430.8</entry>
</row>
<row>
<entry>Example 2</entry>
<entry>50</entry>
<entry>10</entry>
<entry>28.5</entry>
<entry>67.1</entry>
<entry>21.7</entry>
<entry>2.4</entry>
<entry>3.1</entry>
<entry>34.0</entry>
<entry>450.0</entry>
</row>
<row>
<entry>Example 3</entry>
<entry>40</entry>
<entry>20</entry>
<entry>25.8</entry>
<entry>62.0</entry>
<entry>28.7</entry>
<entry>2.4</entry>
<entry>2.2</entry>
<entry>33.8</entry>
<entry>452.4</entry>
</row>
<row>
<entry>Example 4</entry>
<entry>30</entry>
<entry>30</entry>
<entry>26.8</entry>
<entry>54.7</entry>
<entry>33.5</entry>
<entry>2.0</entry>
<entry>1.6</entry>
<entry>33.8</entry>
<entry>345.0</entry>
</row>
<row>
<entry>Example 5</entry>
<entry>25</entry>
<entry>39</entry>
<entry>23.4</entry>
<entry>52.8</entry>
<entry>38.1</entry>
<entry>2.3</entry>
<entry>1.4</entry>
<entry>28.8</entry>
<entry>307.2</entry>
</row>
<row>
<entry namest="1" nameend="10" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>

================================================
File: tests/data_scanned/groundtruth/docling_v1/ocr_test.doctags.txt
================================================
<document>
<paragraph><location><page_1><loc_12><loc_82><loc_85><loc_91></location>Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package</paragraph>
</document>

================================================
File: tests/data_scanned/groundtruth/docling_v1/ocr_test.json
================================================
{"_name": "", "type": "pdf-document", "description": {"title": null, "abstract": null, "authors": null, "affiliations": null, "subjects": null, "keywords": null, "publication_date": null, "languages": null, "license": null, "publishers": null, "url_refs": null, "references": null, "publication": null, "reference_count": null, "citation_count": null, "citation_date": null, "advanced": null, "analytics": null, "logs": [], "collection": null, "acquisition": null}, "file-info": {"filename": "ocr_test.pdf", "filename-prov": null, "document-hash": "80f38f5b87a84870681556176a9622186fd200dd32c5557be9e0c0af05b8bc61", "#-pages": 1, "collection-name": null, "description": null, "page-hashes": [{"hash": "14d896dc8bcb7ee7c08c0347eb6be8dcb92a3782501992f1ea14d2e58077d4e3", "model": "default", "page": 1}]}, "main-text": [{"prov": [{"bbox": [69.0, 688.5883585611979, 506.6666666666667, 767.2550252278646], "page": 1, "span": [0, 94], "__ref_s3_data": null}], "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package", "type": "paragraph", "payload": null, "name": "Text", "font": null}], "figures": [], "tables": [], "bitmaps": null, "equations": [], "footnotes": [], "page-dimensions": [{"height": 841.9216918945312, "page": 1, "width": 595.201171875}], "page-footers": [], "page-headers": [], "_s3_data": null, "identifiers": null}

================================================
File: tests/data_scanned/groundtruth/docling_v1/ocr_test.md
================================================
Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package

================================================
File: tests/data_scanned/groundtruth/docling_v1/ocr_test.pages.json
================================================
[{"page_no": 0, "size": {"width": 595.201171875, "height": 841.9216918945312}, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "predictions": {"layout": {"clusters": [{"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}]}, "tablestructure": {"table_map": {}}, "figures_classification": null, "equations_prediction": null}, "assembled": {"elements": [{"label": "text", "id": 0, "page_no": 0, "cluster": {"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}, "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package"}], "body": [{"label": "text", "id": 0, "page_no": 0, "cluster": {"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}, "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package"}], "headers": []}}]

================================================
File: tests/data_scanned/groundtruth/docling_v2/ocr_test.doctags.txt
================================================
<doctag><text><loc_58><loc_44><loc_426><loc_91>Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package</text>
</doctag>

================================================
File: tests/data_scanned/groundtruth/docling_v2/ocr_test.json
================================================
{"schema_name": "DoclingDocument", "version": "1.1.0", "name": "ocr_test", "origin": {"mimetype": "application/pdf", "binary_hash": 14853448746796404529, "filename": "ocr_test.pdf", "uri": null}, "furniture": {"self_ref": "#/furniture", "parent": null, "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "parent": null, "children": [{"cref": "#/texts/0"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [], "texts": [{"self_ref": "#/texts/0", "parent": {"cref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 69.0, "t": 767.2550252278646, "r": 506.6666666666667, "b": 688.5883585611979, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 94]}], "orig": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package", "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package"}], "pictures": [], "tables": [], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 595.201171875, "height": 841.9216918945312}, "image": null, "page_no": 1}}}

================================================
File: tests/data_scanned/groundtruth/docling_v2/ocr_test.md
================================================
Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package

================================================
File: tests/data_scanned/groundtruth/docling_v2/ocr_test.pages.json
================================================
[{"page_no": 0, "size": {"width": 595.201171875, "height": 841.9216918945312}, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "predictions": {"layout": {"clusters": [{"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}]}, "tablestructure": {"table_map": {}}, "figures_classification": null, "equations_prediction": null}, "assembled": {"elements": [{"label": "text", "id": 0, "page_no": 0, "cluster": {"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}, "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package"}], "body": [{"label": "text", "id": 0, "page_no": 0, "cluster": {"id": 0, "label": "text", "bbox": {"l": 69.0, "t": 74.66666666666667, "r": 506.6666666666667, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}, "confidence": 0.9715733528137207, "cells": [{"id": 0, "text": "Docling bundles PDF document conversion to", "bbox": {"l": 71.33333333333333, "t": 74.66666666666667, "r": 506.6666666666667, "b": 99.33333333333333, "coord_origin": "TOPLEFT"}}, {"id": 1, "text": "JSON and Markdown in an easy self contained", "bbox": {"l": 69.0, "t": 100.66666666666667, "r": 506.6666666666667, "b": 126.66666666666667, "coord_origin": "TOPLEFT"}}, {"id": 2, "text": "package", "bbox": {"l": 70.66666666666667, "t": 128.66666666666666, "r": 154.0, "b": 153.33333333333334, "coord_origin": "TOPLEFT"}}], "children": []}, "text": "Docling bundles PDF document conversion to JSON and Markdown in an easy self contained package"}], "headers": []}}]

================================================
File: .github/PULL_REQUEST_TEMPLATE.md
================================================
<!-- Thank you for contributing to Docling! -->

<!-- STEPS TO FOLLOW:
  1. Add a description of the changes (frequently the same as the commit description)
  2. Enter the issue number next to "Resolves #" below (if there is no tracking issue resolved, **remove that section**)
  3. Make sure the PR title follows the **Commit Message Formatting**: https://www.conventionalcommits.org/en/v1.0.0/#summary.
  4. Follow the steps in the checklist below, starting with the **Commit Message Formatting**.
-->

<!-- Uncomment this section with the issue number if an issue is being resolved
**Issue resolved by this Pull Request:**
Resolves #
--->

**Checklist:**

- [ ] Documentation has been updated, if necessary.
- [ ] Examples have been added, if necessary.
- [ ] Tests have been added, if necessary.


================================================
File: .github/SECURITY.md
================================================
# Security and Disclosure Information Policy for the Docling Project

The Docling team and community take security bugs seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.

## Reporting a Vulnerability

If you think you've identified a security issue in an Docling project repository, please DO NOT report the issue publicly via the GitHub issue tracker, etc.

Instead, send an email with as many details as possible to [deepsearch-core@zurich.ibm.com](mailto:deepsearch-core@zurich.ibm.com). This is a private mailing list for the maintainers team.

Please do not create a public issue.

## Security Vulnerability Response

Each report is acknowledged and analyzed by the core maintainers within 3 working days.

Any vulnerability information shared with core maintainers stays within the Docling project and will not be disseminated to other projects unless it is necessary to get the issue fixed.

After the initial reply to your report, the security team will keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.

## Security Alerts

We will send announcements of security vulnerabilities and steps to remediate on the [Docling announcements](https://github.com/DS4SD/docling/discussions/categories/announcements).


================================================
File: .github/mergify.yml
================================================
merge_protections:
  - name: Enforce conventional commit
    description: Make sure that we follow https://www.conventionalcommits.org/en/v1.0.0/
    if:
      - base = main
    success_conditions:
      - "title ~=
        ^(fix|feat|docs|style|refactor|perf|test|build|ci|chore|revert)(?:\\(.+\
        \\))?(!)?:"
  - name: Require two reviewer for test updates
    description: When test data is updated, we require two reviewers
    if:
      - base = main
      - or:
          - files ~= ^tests/data
          - files ~= ^tests/data_scanned
    success_conditions:
      - "#approved-reviews-by >= 2"


================================================
File: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Report an issue to help improve Docling
title: ''
labels: bug
assignees: ''

---

### Bug
<!-- Describe the buggy behavior you have observed. -->
...

### Steps to reproduce
<!-- Describe the sequence of steps for reproducing the bug. -->
...

### Docling version
<!-- Copy the output of `docling --version`. -->
...

### Python version
<!-- Copy the output of `python --version`. -->
...

<!-- ⚠️ ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->


================================================
File: .github/ISSUE_TEMPLATE/config.yml
================================================
blank_issues_enabled: false


================================================
File: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for enhancing Docling
title: ''
labels: enhancement
assignees: ''

---

### Requested feature
<!-- Describe the feature you have in mind and the user need it addresses. -->
...

### Alternatives
<!-- Describe any alternatives you have considered. -->
...

<!-- ⚠️ ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->


================================================
File: .github/ISSUE_TEMPLATE/question.md
================================================
---
name: Question
about: Ask a question
title: ''
labels: question
assignees: ''

---

### Question
<!-- Describe what you would like to achieve and which part you need help with. -->
...

<!-- ⚠️ ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->


================================================
File: .github/actions/setup-poetry/action.yml
================================================
name: 'Set up Poetry and install'
description: 'Set up a specific version of Poetry and install dependencies using caching.'
inputs:
  python-version:
    description: "Version range or exact version of Python or PyPy to use, using SemVer's version range syntax."
    default: '3.11'
runs:
  using: 'composite'
  steps:
    - name: Install poetry
      run: pipx install poetry==1.8.5
      shell: bash
    - uses: actions/setup-python@v5
      with:
        python-version: ${{ inputs.python-version }}
        cache: 'poetry'
    - name: Install dependencies
      run: poetry install --all-extras
      shell: bash


================================================
File: .github/scripts/release.sh
================================================
#!/bin/bash

set -e  # trigger failure on error - do not remove!
set -x  # display command on output

if [ -z "${TARGET_VERSION}" ]; then
    >&2 echo "No TARGET_VERSION specified"
    exit 1
fi
CHGLOG_FILE="${CHGLOG_FILE:-CHANGELOG.md}"

# update package version
poetry version "${TARGET_VERSION}"

# collect release notes
REL_NOTES=$(mktemp)
poetry run semantic-release changelog --unreleased >> "${REL_NOTES}"

# update changelog
TMP_CHGLOG=$(mktemp)
TARGET_TAG_NAME="v${TARGET_VERSION}"
RELEASE_URL="$(gh repo view --json url -q ".url")/releases/tag/${TARGET_TAG_NAME}"
printf "## [${TARGET_TAG_NAME}](${RELEASE_URL}) - $(date -Idate)\n\n" >> "${TMP_CHGLOG}"
cat "${REL_NOTES}" >> "${TMP_CHGLOG}"
if [ -f "${CHGLOG_FILE}" ]; then
    printf "\n" | cat - "${CHGLOG_FILE}" >> "${TMP_CHGLOG}"
fi
mv "${TMP_CHGLOG}" "${CHGLOG_FILE}"

# push changes
git config --global user.name 'github-actions[bot]'
git config --global user.email 'github-actions[bot]@users.noreply.github.com'
git add pyproject.toml "${CHGLOG_FILE}"
COMMIT_MSG="chore: bump version to ${TARGET_VERSION} [skip ci]"
git commit -m "${COMMIT_MSG}"
git push origin main

# create GitHub release (incl. Git tag)
gh release create "${TARGET_TAG_NAME}" -F "${REL_NOTES}"


================================================
File: .github/workflows/cd-docs.yml
================================================
name: "Run Docs CD"

on:
  push:
    branches:
      - "main"

jobs:
  build-deploy-docs:
    uses: ./.github/workflows/docs.yml
    with:
      deploy: true
    permissions:
      contents: write


================================================
File: .github/workflows/cd.yml
================================================
name: "Run CD"

on:
  workflow_dispatch:

env:
  # disable keyring (https://github.com/actions/runner-images/issues/6185):
  PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring

jobs:
  code-checks:
    uses: ./.github/workflows/checks.yml
  pre-release-check:
    runs-on: ubuntu-latest
    outputs:
      TARGET_TAG_V: ${{ steps.version_check.outputs.TRGT_VERSION }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # for fetching tags, required for semantic-release
      - uses: ./.github/actions/setup-poetry
      - name: Check version of potential release
        id: version_check
        run: |
            TRGT_VERSION=$(poetry run semantic-release print-version)
            echo "TRGT_VERSION=${TRGT_VERSION}" >> $GITHUB_OUTPUT
            echo "${TRGT_VERSION}"
      - name: Check notes of potential release
        run: poetry run semantic-release changelog --unreleased
  release:
    needs: [code-checks, pre-release-check]
    if: needs.pre-release-check.outputs.TARGET_TAG_V != ''
    environment: auto-release
    runs-on: ubuntu-latest
    concurrency: release
    steps:
      - uses: actions/create-github-app-token@v1
        id: app-token
        with:
          app-id: ${{ vars.CI_APP_ID }}
          private-key: ${{ secrets.CI_PRIVATE_KEY }}
      - uses: actions/checkout@v4
        with:
          token: ${{ steps.app-token.outputs.token }}
          fetch-depth: 0  # for fetching tags, required for semantic-release
      - uses: ./.github/actions/setup-poetry
      - name: Run release script
        env:
          GH_TOKEN: ${{ steps.app-token.outputs.token }}
          TARGET_VERSION: ${{ needs.pre-release-check.outputs.TARGET_TAG_V }}
          CHGLOG_FILE: CHANGELOG.md
        run: ./.github/scripts/release.sh
        shell: bash


================================================
File: .github/workflows/checks.yml
================================================
on:
  workflow_call:

jobs:
  run-checks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']
    steps:
      - uses: actions/checkout@v4
      - name: Install tesseract
        run: sudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-script-latn libleptonica-dev libtesseract-dev pkg-config
      - name: Set TESSDATA_PREFIX
        run: |
          echo "TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)" >> "$GITHUB_ENV"
      - uses: ./.github/actions/setup-poetry
        with:
          python-version: ${{ matrix.python-version }}
      - name: Run styling check
        run: poetry run pre-commit run --all-files
      - name: Install with poetry
        run: poetry install --all-extras
      - name: Testing
        run: |
          poetry run pytest -v tests
      - name: Run examples
        run: |
          for file in docs/examples/*.py; do
            # Skip batch_convert.py
            if [[ "$(basename "$file")" =~ ^(batch_convert|minimal_vlm_pipeline|minimal|export_multimodal|custom_convert|develop_picture_enrichment|rapidocr_with_custom_models|offline_convert|pictures_description|pictures_description_api).py ]]; then
                echo "Skipping $file"
                continue
            fi

            echo "Running example $file"
            poetry run python "$file" || exit 1
          done
      - name: Build with poetry
        run: poetry build


================================================
File: .github/workflows/ci-docs.yml
================================================
name: "Run Docs CI"

on:
  pull_request:
    types: [opened, reopened, synchronize]
  push:
    branches:
      - "**"
      - "!gh-pages"

jobs:
  build-docs:
    if: ${{ github.event_name == 'push' || (github.event.pull_request.head.repo.full_name != 'DS4SD/docling' && github.event.pull_request.head.repo.full_name != 'ds4sd/docling') }}
    uses: ./.github/workflows/docs.yml
    with:
      deploy: false


================================================
File: .github/workflows/ci.yml
================================================
name: "Run CI"

on:
  pull_request:
    types: [opened, reopened, synchronize]
  push:
    branches:
      - "**"
      - "!main"
      - "!gh-pages"

env:
  # disable keyring (https://github.com/actions/runner-images/issues/6185):
  PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring

jobs:
  code-checks:
    if: ${{ github.event_name == 'push' || (github.event.pull_request.head.repo.full_name != 'DS4SD/docling' && github.event.pull_request.head.repo.full_name != 'ds4sd/docling') }}
    uses: ./.github/workflows/checks.yml


================================================
File: .github/workflows/docs.yml
================================================
on:
    workflow_call:
        inputs:
            deploy:
                type: boolean
                description: "If true, the docs will be deployed."
                default: false

jobs:
    run-docs:
        runs-on: ubuntu-latest
        steps:
        - uses: actions/checkout@v4
        - uses: ./.github/actions/setup-poetry
        - name: Build docs
          run: poetry run mkdocs build --verbose --clean
        - name: Build and push docs
          if: inputs.deploy
          run: poetry run mkdocs gh-deploy --force


================================================
File: .github/workflows/pypi.yml
================================================
name: "Build and publish package"

on:
  release:
    types: [published]

permissions:
  contents: read

env:
  # disable keyring (https://github.com/actions/runner-images/issues/6185):
  PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-poetry
      - name: Build and publish
        run: poetry publish --build --no-interaction --username=__token__ --password=${{ secrets.PYPI_TOKEN }}


